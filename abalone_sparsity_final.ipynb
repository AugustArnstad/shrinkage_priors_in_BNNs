{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/abalone\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/abalone\"\n",
    "model_names_tanh_nodewise = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\"]\n",
    "\n",
    "config_path_small = \"abalone_N334_p8\"\n",
    "config_path_medium = \"abalone_N668_p8\"\n",
    "config_path_full = \"abalone_N3341_p8\"\n",
    "\n",
    "tanh_fit_nodewise_small = get_model_fits(\n",
    "    config=config_path_small,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh_nodewise,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "tanh_fit_nodewise_medium = get_model_fits(\n",
    "    config=config_path_medium,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh_nodewise,\n",
    "    include_prior=False,\n",
    ")\n",
    "tanh_fit_nodewise_full = get_model_fits(\n",
    "    config=config_path_full,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh_nodewise,\n",
    "    include_prior=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from properscoring import crps_ensemble\n",
    "from utils.generate_data import load_abalone_regression_data\n",
    "_, X_test_small, _, y_test_small = load_abalone_regression_data(standardized=False, frac=0.1)\n",
    "_, X_test_small, _, y_test_medium = load_abalone_regression_data(standardized=False, frac=0.2)\n",
    "_, X_test_small, _, y_test_full = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "\n",
    "def random_subset(y, n_sub=None, seed=1):\n",
    "    n_sub = y.shape[0] if n_sub is None else n_sub\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(len(y), size=n_sub, replace=False)\n",
    "    return idx\n",
    "\n",
    "def compute_crps(models, all_fits, y_test):\n",
    "    results = {}\n",
    "    idx = random_subset(y_test)\n",
    "    for model in models:\n",
    "        result = {}\n",
    "        try:\n",
    "            fit = all_fits[model]['posterior']\n",
    "            preds_raw = fit.stan_variable(\"output_test_rng\")\n",
    "        except KeyError:\n",
    "            print(f\"[SKIP] Model or posterior not found:\")\n",
    "            continue\n",
    "        preds = np.asarray(preds_raw).squeeze(-1)\n",
    "        preds_sub = preds[:, idx]\n",
    "        #print(preds.shape)\n",
    "        crps = crps_ensemble(y_test[idx], preds_sub.T)\n",
    "        \n",
    "        result = {\n",
    "            #'model': model,\n",
    "            'crps': crps,\n",
    "            'y_test': y_test[idx],\n",
    "            'preds': preds_sub.T\n",
    "        }\n",
    "        results[model] = result\n",
    "    return results     \n",
    "   \n",
    "res_small = compute_crps(model_names_tanh_nodewise, tanh_fit_nodewise_small, y_test_small)\n",
    "res_medium = compute_crps(model_names_tanh_nodewise, tanh_fit_nodewise_medium, y_test_medium)\n",
    "res_full = compute_crps(model_names_tanh_nodewise, tanh_fit_nodewise_full, y_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian tanh\": \"Gauss\",\n",
    "    \"Regularized Horseshoe tanh\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"DHS\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"DST\"\n",
    "}\n",
    "\n",
    "def plot_grouped_box_by_model(model_names, res_small, res_medium, res_full,\n",
    "                              sizes=(\"0.1N\", \"0.2N\", \"N\"),\n",
    "                              figsize=(7, 2.8)):\n",
    "    labels = [abbr.get(m, m) for m in model_names]\n",
    "    res_list = [res_small, res_medium, res_full]\n",
    "\n",
    "    x = np.arange(len(model_names)) + 1\n",
    "    offsets = np.array([-0.22, 0.00, 0.22])\n",
    "    width = 0.16\n",
    "    size_colors = [\"C0\", \"C1\", \"C2\"]\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    for j, (res_j, col) in enumerate(zip(res_list, size_colors)):\n",
    "        data_j = [res_j[m][\"crps\"] for m in model_names]\n",
    "        pos_j = x + offsets[j]\n",
    "\n",
    "        bp = plt.boxplot(\n",
    "            data_j,\n",
    "            positions=pos_j,\n",
    "            widths=width,\n",
    "            patch_artist=True,\n",
    "            showfliers=False\n",
    "        )\n",
    "\n",
    "        for box in bp[\"boxes\"]:\n",
    "            box.set_facecolor(col)\n",
    "            box.set_alpha(0.55)\n",
    "            box.set_edgecolor(\"black\")\n",
    "        for k in [\"whiskers\", \"caps\", \"medians\"]:\n",
    "            for line in bp[k]:\n",
    "                line.set_color(\"black\")\n",
    "                line.set_linewidth(1.0)\n",
    "\n",
    "    plt.xticks(x, labels)\n",
    "    plt.ylabel(\"CRPS\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Proper legend\n",
    "    legend_handles = [\n",
    "        Patch(facecolor=\"C0\", edgecolor=\"black\", alpha=0.55, label=sizes[0]),\n",
    "        Patch(facecolor=\"C1\", edgecolor=\"black\", alpha=0.55, label=sizes[1]),\n",
    "        Patch(facecolor=\"C2\", edgecolor=\"black\", alpha=0.55, label=sizes[2]),\n",
    "    ]\n",
    "    plt.legend(handles=legend_handles, title=\"\", frameon=False, loc=\"upper center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures_for_use_in_paper/abalone_crps.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_grouped_box_by_model(\n",
    "    model_names_tanh_nodewise,\n",
    "    res_small, res_medium, res_full,\n",
    "    #sizes=(\"N=100\", \"N=200\", \"N=500\"),\n",
    "    figsize=(4, 3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE and NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logsumexp(a, axis=None):\n",
    "    amax = np.max(a, axis=axis, keepdims=True)\n",
    "    out = amax + np.log(np.sum(np.exp(a - amax), axis=axis, keepdims=True))\n",
    "    return np.squeeze(out, axis=axis)\n",
    "\n",
    "def gaussian_nll_pointwise(y, mu, sigma):\n",
    "    return 0.5*np.log(2*np.pi*(sigma**2)) + 0.5*((y-mu)**2)/(sigma**2)\n",
    "\n",
    "def compute_rmse(\n",
    "    models, all_fits, forward_pass,\n",
    "    compute_nll=True,\n",
    "    noise_var_name=\"sigma\",\n",
    "    frac = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate on a large generated test set instead of the stored tiny X_test/y_test.\n",
    "    Assumes model was trained on standardized y if standardize_y=True.\n",
    "    \"\"\"\n",
    "    posterior_means = []\n",
    "    X_train, X_test, y_train, y_test = load_abalone_regression_data(standardized=False, frac=frac)\n",
    "\n",
    "    for model in models:\n",
    "        try:\n",
    "            fit = all_fits[model]['posterior']\n",
    "            W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "            W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "            b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "            b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "\n",
    "            noise_samples = None\n",
    "            if compute_nll:\n",
    "                try:\n",
    "                    noise_samples = fit.stan_variable(noise_var_name).squeeze()\n",
    "                except Exception:\n",
    "                    noise_samples = None\n",
    "        except KeyError:\n",
    "            print(f\"[SKIP] Model or posterior not found: -> {model}\")\n",
    "            continue\n",
    "\n",
    "        S = W1_samples.shape[0]\n",
    "        y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "        for i in range(S):\n",
    "            W1 = W1_samples[i]\n",
    "            W2 = W2_samples[i]\n",
    "            y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i]).squeeze()\n",
    "            y_hats[i] = y_hat\n",
    "\n",
    "        # posterior mean RMSE (standardized scale)\n",
    "        posterior_mean = y_hats.mean(axis=0)\n",
    "        posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test)**2))\n",
    "\n",
    "        out_pm = {\n",
    "            'N': X_train.shape[0],\n",
    "            'model': model,\n",
    "            'n_eval': y_test.shape[0],\n",
    "            'posterior_mean_rmse': posterior_mean_rmse,\n",
    "        }\n",
    "\n",
    "        if compute_nll:\n",
    "            if noise_samples is None:\n",
    "                sig_s = np.ones(S)\n",
    "            else:\n",
    "                sig_s = np.asarray(noise_samples).reshape(-1)[:S]\n",
    "\n",
    "            # Expected NLL\n",
    "            nll_draws = np.array([\n",
    "                gaussian_nll_pointwise(y_test, y_hats[i], sig_s[i]).mean()\n",
    "                for i in range(S)\n",
    "            ])\n",
    "            expected_nll = nll_draws.mean()\n",
    "\n",
    "            # Predictive (mixture) NLL\n",
    "            loglik = -np.stack([\n",
    "                gaussian_nll_pointwise(y_test, y_hats[i], sig_s[i])\n",
    "                for i in range(S)\n",
    "            ], axis=0)  # (S, n_eval)\n",
    "            lppd = (_logsumexp(loglik, axis=0) - np.log(S)).mean()\n",
    "            predictive_nll = -lppd\n",
    "\n",
    "            out_pm[\"expected_nll\"] = expected_nll\n",
    "            out_pm[\"predictive_nll\"] = predictive_nll\n",
    "\n",
    "\n",
    "        posterior_means.append(out_pm)\n",
    "\n",
    "    return pd.DataFrame(posterior_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_tanh\n",
    "rmse_nll_df_small = compute_rmse(\n",
    "        models=model_names_tanh_nodewise,\n",
    "        all_fits=tanh_fit_nodewise_small,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        compute_nll=True,\n",
    "        noise_var_name=\"sigma\",\n",
    "        frac=0.1\n",
    "    )\n",
    "\n",
    "rmse_nll_df_medium = compute_rmse(\n",
    "        models=model_names_tanh_nodewise,\n",
    "        all_fits=tanh_fit_nodewise_medium,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        compute_nll=True,\n",
    "        noise_var_name=\"sigma\",\n",
    "        frac=0.2\n",
    "    )\n",
    "\n",
    "rmse_nll_df_full = compute_rmse(\n",
    "        models=model_names_tanh_nodewise,\n",
    "        all_fits=tanh_fit_nodewise_full,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        compute_nll=True,\n",
    "        noise_var_name=\"sigma\",\n",
    "        frac=1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_nll_df_full.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARSITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "def compute_sparse_rmse_results_abalone(models, all_fits, forward_pass, frac,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "    for model in models:\n",
    "        try:\n",
    "            fit = all_fits[model]['posterior']\n",
    "            W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "            W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "            b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "            b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "        except KeyError:\n",
    "            print(f\"[SKIP] Model or posterior not found:\")\n",
    "            continue\n",
    "\n",
    "        S = W1_samples.shape[0]\n",
    "        rmses = np.zeros(S)\n",
    "        #print(y_test.shape)\n",
    "        _, X_test, _, y_test = load_abalone_regression_data(standardized=False, frac=frac)\n",
    "        y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "        for i in range(S):\n",
    "            W1 = W1_samples[i]\n",
    "            W2 = W2_samples[i]\n",
    "\n",
    "            # Apply pruning mask if requested\n",
    "            if prune_fn is not None and sparsity > 0.0:\n",
    "                masks = prune_fn([W1, W2], sparsity)\n",
    "                W1 = W1 * masks[0]\n",
    "                #W2 = W2 * masks[1]\n",
    "\n",
    "            y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i])\n",
    "            y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "            rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "            \n",
    "        posterior_mean = np.mean(y_hats, axis=0)\n",
    "        posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "        posterior_means.append({\n",
    "            'model': model,\n",
    "            'sparsity': sparsity,\n",
    "            'posterior_mean_rmse': posterior_mean_rmse\n",
    "        })\n",
    "\n",
    "        for i in range(S):\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'rmse': rmses[i]\n",
    "            })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "df_rmse_tanh_nodewise, df_posterior_rmse_tanh_nodewise = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:    \n",
    "    df_rmse_tanh_nodewise[sparsity], df_posterior_rmse_tanh_nodewise[sparsity] = compute_sparse_rmse_results_abalone(\n",
    "        models = model_names_tanh_nodewise,\n",
    "        all_fits = tanh_fit_nodewise, \n",
    "        forward_pass = forward_pass_tanh,\n",
    "        frac=1.0,\n",
    "        sparsity=sparsity, \n",
    "        prune_fn=local_prune_weights\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_tanh_full_nodewise_old = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_posterior_rmse_tanh_nodewise.items()],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST NEW PRUNING SCHEME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_global_mask_from_posterior(\n",
    "    W_samples,\n",
    "    sparsity,\n",
    "    method=\"Eabs\",          # \"Eabs\" or \"Eabs_stability\"\n",
    "    stability_quantile=0.1, # used if method=\"Eabs_stability\"\n",
    "    prune_smallest=True\n",
    "):\n",
    "    \"\"\"\n",
    "    W_samples: array (S, ..., ...) posterior draws of a weight matrix.\n",
    "    sparsity: fraction to prune (q). Keeps (1-q).\n",
    "    Returns mask with same trailing shape as one draw, dtype float {0,1}.\n",
    "    \"\"\"\n",
    "    assert 0.0 <= sparsity < 1.0\n",
    "    S = W_samples.shape[0]\n",
    "    W_abs = np.abs(W_samples)  # (S, ...)\n",
    "\n",
    "    # Importance score a = E|w|\n",
    "    a = W_abs.mean(axis=0)     # (..., ...)\n",
    "\n",
    "    if method == \"Eabs\":\n",
    "        score = a\n",
    "    elif method == \"Eabs_stability\":\n",
    "        # Stability proxy pi = P(|w| > t), where t is a small global quantile of |w|\n",
    "        t = np.quantile(W_abs.reshape(S, -1), stability_quantile)\n",
    "        pi = (W_abs > t).mean(axis=0)\n",
    "        # Combine: emphasize both \"large on average\" and \"consistently non-tiny\"\n",
    "        score = a * pi\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'Eabs' or 'Eabs_stability'\")\n",
    "\n",
    "    # Decide how many to prune\n",
    "    num_params = score.size\n",
    "    k_prune = int(np.floor(sparsity * num_params))\n",
    "    if k_prune == 0:\n",
    "        return np.ones_like(score, dtype=float)\n",
    "\n",
    "    flat = score.reshape(-1)\n",
    "\n",
    "    if prune_smallest:\n",
    "        # prune lowest scores\n",
    "        thresh = np.partition(flat, k_prune - 1)[k_prune - 1]\n",
    "        mask = (score > thresh).astype(float)\n",
    "        # if ties create too many kept/pruned, fix deterministically\n",
    "        # (rare but possible with many equal scores)\n",
    "        if mask.sum() > num_params - k_prune:\n",
    "            # drop some tied-at-threshold entries\n",
    "            idx_tied = np.where(score.reshape(-1) == thresh)[0]\n",
    "            need_drop = int(mask.sum() - (num_params - k_prune))\n",
    "            if need_drop > 0:\n",
    "                mask_flat = mask.reshape(-1)\n",
    "                mask_flat[idx_tied[:need_drop]] = 0.0\n",
    "                mask = mask_flat.reshape(score.shape)\n",
    "        elif mask.sum() < num_params - k_prune:\n",
    "            # add some tied entries if we kept too few\n",
    "            idx_tied = np.where(score.reshape(-1) == thresh)[0]\n",
    "            need_add = int((num_params - k_prune) - mask.sum())\n",
    "            if need_add > 0:\n",
    "                mask_flat = mask.reshape(-1)\n",
    "                # add back from tied\n",
    "                add_candidates = idx_tied[mask_flat[idx_tied] == 0.0]\n",
    "                mask_flat[add_candidates[:need_add]] = 1.0\n",
    "                mask = mask_flat.reshape(score.shape)\n",
    "    else:\n",
    "        # prune largest (not typical)\n",
    "        thresh = np.partition(flat, num_params - k_prune)[num_params - k_prune]\n",
    "        mask = (score < thresh).astype(float)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def precompute_global_masks(\n",
    "    all_fits,\n",
    "    model,\n",
    "    sparsity_levels,\n",
    "    prune_W2=False,\n",
    "    method=\"Eabs_stability\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns dict: sparsity -> (mask_W1, mask_W2 or None)\n",
    "    \"\"\"\n",
    "    fit = all_fits[model][\"posterior\"]\n",
    "\n",
    "    W1_samples = fit.stan_variable(\"W_1\")  # (S, P, H)\n",
    "    W2_samples = fit.stan_variable(\"W_L\")  # (S, H, O) or (S, H) depending on O\n",
    "\n",
    "    masks = {}\n",
    "    for q in sparsity_levels:\n",
    "        mask_W1 = build_global_mask_from_posterior(W1_samples, q, method=method)\n",
    "        mask_W2 = None\n",
    "        if prune_W2:\n",
    "            mask_W2 = build_global_mask_from_posterior(W2_samples, q, method=method)\n",
    "        masks[q] = (mask_W1, mask_W2)\n",
    "    return masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "def _logsumexp(a, axis=None):\n",
    "    amax = np.max(a, axis=axis, keepdims=True)\n",
    "    out = amax + np.log(np.sum(np.exp(a - amax), axis=axis, keepdims=True))\n",
    "    return np.squeeze(out, axis=axis)\n",
    "\n",
    "def gaussian_nll_pointwise(y, mu, sigma):\n",
    "    return 0.5*np.log(2*np.pi*(sigma**2)) + 0.5*((y-mu)**2)/(sigma**2)\n",
    "\n",
    "def compute_sparse_metrics_results_globalmask_large_eval(\n",
    "    models, all_fits, forward_pass,\n",
    "    sparsity=0.0,\n",
    "    masks_cache=None,\n",
    "    prune_W2=False,\n",
    "    compute_nll=True,\n",
    "    noise_var_name=\"sigma\",\n",
    "    frac = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate on a large generated test set instead of the stored tiny X_test/y_test.\n",
    "    Assumes model was trained on standardized y if standardize_y=True.\n",
    "    \"\"\"\n",
    "    posterior_means = []\n",
    "    # Build large eval set consistent with training split standardization\n",
    "    X_train, X_test, y_train, y_test = load_abalone_regression_data(standardized=False, frac=frac)\n",
    "    #y_std = y_train.std()\n",
    "    #y_mean = y_train.mean()\n",
    "\n",
    "    for model in models:\n",
    "        try:\n",
    "            fit = all_fits[model]['posterior']\n",
    "            W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "            W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "            b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "            b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "\n",
    "            noise_samples = None\n",
    "            if compute_nll:\n",
    "                try:\n",
    "                    noise_samples = fit.stan_variable(noise_var_name).squeeze()\n",
    "                except Exception:\n",
    "                    noise_samples = None\n",
    "        except KeyError:\n",
    "            print(f\"[SKIP] Model or posterior not found: -> {model}\")\n",
    "            continue\n",
    "\n",
    "        S = W1_samples.shape[0]\n",
    "        y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "        mask_W1 = mask_W2 = None\n",
    "        if masks_cache is not None and sparsity > 0.0:\n",
    "            mask_W1, mask_W2 = masks_cache[(model)][sparsity]\n",
    "\n",
    "        for i in range(S):\n",
    "            W1 = W1_samples[i]\n",
    "            W2 = W2_samples[i]\n",
    "\n",
    "            if mask_W1 is not None:\n",
    "                W1 = W1 * mask_W1\n",
    "            if prune_W2 and (mask_W2 is not None):\n",
    "                W2 = W2 * mask_W2\n",
    "\n",
    "            y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i]).squeeze()\n",
    "            y_hats[i] = y_hat\n",
    "\n",
    "        # posterior mean RMSE (standardized scale)\n",
    "        posterior_mean = y_hats.mean(axis=0)\n",
    "        posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test)**2))\n",
    "\n",
    "        out_pm = {\n",
    "            'N': X_train.shape[0],\n",
    "            'model': model,\n",
    "            'sparsity': sparsity,\n",
    "            'n_eval': y_test.shape[0],\n",
    "            'posterior_mean_rmse': posterior_mean_rmse,\n",
    "        }\n",
    "\n",
    "        if compute_nll:\n",
    "            if noise_samples is None:\n",
    "                sig_s = np.ones(S)\n",
    "            else:\n",
    "                sig_s = np.asarray(noise_samples).reshape(-1)[:S]\n",
    "\n",
    "            # Expected NLL\n",
    "            nll_draws = np.array([\n",
    "                gaussian_nll_pointwise(y_test, y_hats[i], sig_s[i]).mean()\n",
    "                for i in range(S)\n",
    "            ])\n",
    "            expected_nll = nll_draws.mean()\n",
    "\n",
    "            # Predictive (mixture) NLL\n",
    "            loglik = -np.stack([\n",
    "                gaussian_nll_pointwise(y_test, y_hats[i], sig_s[i])\n",
    "                for i in range(S)\n",
    "            ], axis=0)  # (S, n_eval)\n",
    "            lppd = (_logsumexp(loglik, axis=0) - np.log(S)).mean()\n",
    "            predictive_nll = -lppd\n",
    "\n",
    "            out_pm[\"expected_nll\"] = expected_nll\n",
    "            out_pm[\"predictive_nll\"] = predictive_nll\n",
    "\n",
    "\n",
    "        posterior_means.append(out_pm)\n",
    "\n",
    "    return pd.DataFrame(posterior_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "def build_masks_cache_for_all(\n",
    "    all_fits,\n",
    "    models,\n",
    "    sparsity_levels,\n",
    "    prune_W2=False,\n",
    "    method=\"Eabs_stability\"\n",
    "):\n",
    "    masks_cache = {}\n",
    "    for model in models:\n",
    "        try:\n",
    "            masks_cache[(model)] = precompute_global_masks(\n",
    "                all_fits=all_fits,\n",
    "                model=model,\n",
    "                sparsity_levels=sparsity_levels,\n",
    "                prune_W2=prune_W2,\n",
    "                method=method\n",
    "            )\n",
    "        except KeyError:\n",
    "            print(f\"[SKIP MASKS] Missing fit for -> {model}\")\n",
    "    return masks_cache\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "masks_tanh_nodewise = build_masks_cache_for_all(tanh_fit_nodewise, model_names_tanh_nodewise, sparsity_levels, prune_W2=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_tanh_nodewise = {}\n",
    "\n",
    "for q in sparsity_levels:\n",
    "    df_post_tanh_nodewise[q] = compute_sparse_metrics_results_globalmask_large_eval(\n",
    "        models=model_names_tanh_nodewise,\n",
    "        all_fits=tanh_fit_nodewise,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        sparsity=q,\n",
    "        masks_cache=masks_tanh_nodewise,\n",
    "        prune_W2=False,\n",
    "        compute_nll=True,\n",
    "        noise_var_name=\"sigma\",\n",
    "        frac=1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_tanh_full_nodewise = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_post_tanh_nodewise.items()],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_tanh_full_nodewise[df_post_tanh_full_nodewise['sparsity']==0.0].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# USER SETTINGS\n",
    "# -----------------------------\n",
    "colors = {\n",
    "    \"Gaussian tanh\": \"C0\",\n",
    "    \"Regularized Horseshoe tanh\": \"C1\",\n",
    "    \"Dirichlet Horseshoe tanh\": \"C2\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"C2\",\n",
    "    \"Dirichlet Student T tanh\": \"C3\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"C3\",\n",
    "    \"Beta Horseshoe tanh\": \"C4\",\n",
    "    \"Beta Horseshoe tanh nodewise\": \"C4\",\n",
    "    \"Beta Student T tanh\": \"C5\",\n",
    "    \"Beta Student T tanh nodewise\": \"C5\",\n",
    "}\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "rmse_col = \"posterior_mean_rmse\"\n",
    "\n",
    "Ns = [100, 200, 500]\n",
    "\n",
    "# Provide your dataframes here\n",
    "dfs = [(df_post_tanh_full_nodewise_old, \"Prune per sample\"), (df_post_tanh_full_nodewise, \"Posterior prune\")]\n",
    "\n",
    "# models you want to skip (keep your list exactly as intended)\n",
    "skip_models = {\n",
    "    #\"Gaussian tanh\",\n",
    "    #\"Dirichlet Horseshoe tanh nodewise\",\n",
    "    #\"Dirichlet Horseshoe tanh\",\n",
    "    #\"Dirichlet Student T tanh nodewise\",\n",
    "    #\"Dirichlet Student T tanh\",\n",
    "    # \"Beta Horseshoe tanh\",\n",
    "    # \"Beta Horseshoe tanh nodewise\",\n",
    "    # \"Beta Student T tanh\",\n",
    "    # \"Beta Student T tanh nodewise\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "\n",
    "for ax, (df, title) in zip(axes, dfs):\n",
    "    # filter + enforce sparsity ordering\n",
    "    df_plot = df[df[\"sparsity\"].isin(sparsity_levels)].copy()\n",
    "    df_plot[\"sparsity\"] = pd.Categorical(\n",
    "        df_plot[\"sparsity\"], categories=sparsity_levels, ordered=True\n",
    "    )\n",
    "\n",
    "    for model, g in df_plot.groupby(\"model\", sort=False):\n",
    "        if model in skip_models:\n",
    "            continue\n",
    "\n",
    "        g = g.sort_values(\"sparsity\")\n",
    "\n",
    "        # = \"nodewise\" in model\n",
    "        ax.plot(\n",
    "            g[\"sparsity\"].astype(float),          # safe for categorical x\n",
    "            g[rmse_col],\n",
    "            marker=\"o\", #v\" if is_nodewise else \"o\",\n",
    "            #linestyle=\"dashed\" if is_nodewise else \"-\",\n",
    "            color=colors.get(model, \"C7\"),\n",
    "            label=model,\n",
    "        )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"sparsity\")\n",
    "    ax.set_xticks(sparsity_levels[:-1])\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    ax.legend(loc=\"upper left\", frameon=False)\n",
    "\n",
    "axes[0].set_ylabel(\"RMSE\")\n",
    "plt.savefig(\"figures_for_use_in_paper/abalone_pruning_schemes.pdf\", bbox_inches=\"tight\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from utils.generate_data import load_abalone_regression_data\n",
    "X_train, _, _, _ = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "\n",
    "P = 8\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "\n",
    "def build_single_draw_weights(fits, layer_structure, draw_idx):\n",
    "    \"\"\"Return {model: {'W_1': (P,H), 'W_L': (H,O)}} for ONE draw.\"\"\"\n",
    "    out = {}\n",
    "    for name, fd in fits.items():\n",
    "        fit = fd[\"posterior\"]\n",
    "        W1 = fit.stan_variable(layer_structure['input_to_hidden']['name'])[draw_idx]\n",
    "        WL = fit.stan_variable(layer_structure['hidden_to_output']['name'])[draw_idx]\n",
    "        WL = WL.reshape(layer_structure['hidden_to_output']['shape'])\n",
    "        out[name] = {\"W_1\": W1, \"W_L\": WL}\n",
    "    return out\n",
    "\n",
    "def scale_W1_for_plot(model_means, mode='global'):\n",
    "    \"\"\"\n",
    "    Skalerer alle W_1 til [-1, 1] for rettferdig sammenligning av edge-tykkelser.\n",
    "\n",
    "    mode:\n",
    "      - 'global' : én felles skala over alle modeller (mest sammenlignbar)\n",
    "      - 'per_model': egen skala per modell (uavhengig sammenligning)\n",
    "      - 'per_node' : skalerer hver kolonne (node) separat til [-1,1]\n",
    "\n",
    "    Returnerer: scaled_model_means (samme struktur som input), scale_info\n",
    "    \"\"\"\n",
    "    scaled = {}\n",
    "    if mode == 'global':\n",
    "        gmax = max(np.abs(m['W_1']).max() for m in model_means.values())\n",
    "        gmax = max(gmax, 1e-12)\n",
    "        for name, m in model_means.items():\n",
    "            W1s = m['W_1'] / gmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'global', 'scale': gmax}\n",
    "\n",
    "    elif mode == 'per_model':\n",
    "        for name, m in model_means.items():\n",
    "            s = max(np.abs(m['W_1']).max(), 1e-12)\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = m['W_1'] / s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_model'}\n",
    "\n",
    "    elif mode == 'per_node':\n",
    "        for name, m in model_means.items():\n",
    "            W1 = m['W_1'].copy()\n",
    "            P, H = W1.shape\n",
    "            for h in range(H):\n",
    "                colmax = max(np.abs(W1[:, h]).max(), 1e-12)\n",
    "                W1[:, h] = W1[:, h] / colmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_node'}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'global', 'per_model', or 'per_node'\")\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian tanh\": \"Gauss\",\n",
    "    \"Regularized Horseshoe tanh\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"DHS\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"DST\",\n",
    "    \"Beta Horseshoe tanh\": \"BHS\",\n",
    "    \"Beta Student T tanh\": \"BST\",\n",
    "\n",
    "}\n",
    "\n",
    "def plot_models_with_activations(model_means, layer_sizes,\n",
    "                                 activations=None, activation_color_max=None,\n",
    "                                 ncols=3, figsize_per_plot=(5,4), signed_colors=False, feature_names=None):\n",
    "    \"\"\"\n",
    "    model_means: dict {model_name: {'W_1':(P,H), 'W_L':(H,O), optional 'W_internal':[...]} }\n",
    "    layer_sizes: f.eks [P, H, O] eller [P, H, H, O] ved internlag\n",
    "    activations: dict {model_name: (H,)} – aktiveringsfrekvens kun for første skjulte lag\n",
    "    activation_color_max: global maks for skalering av farger (hvis None brukes 1.0)\n",
    "    \"\"\"\n",
    "    names = list(model_means.keys())\n",
    "    n_models = len(names)\n",
    "    nrows = int(np.ceil(n_models / ncols))\n",
    "    figsize = (figsize_per_plot[0] * ncols, figsize_per_plot[1] * nrows)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    if nrows * ncols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Skru av blanke akser\n",
    "    for ax in axes[n_models:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax, name in zip(axes, names):\n",
    "        weights = model_means[name]\n",
    "        G = nx.DiGraph()\n",
    "        pos, nodes_per_layer, node_colors = {}, [], []\n",
    "\n",
    "        # Noder med posisjon og farge\n",
    "        for li, size in enumerate(layer_sizes):\n",
    "            ids = []\n",
    "            ycoords = np.linspace(size - 1, 0, size) - (size - 1) / 2\n",
    "            for i in range(size):\n",
    "                nid = f\"L{li}_{i}\"\n",
    "                G.add_node(nid)\n",
    "                pos[nid] = (li, ycoords[i])\n",
    "                ids.append(nid)\n",
    "                if li == 0 and feature_names is not None:\n",
    "                    ax.text(pos[nid][0]-0.12, pos[nid][1], feature_names[i],\n",
    "                            ha='right', va='center', fontsize=8)\n",
    "\n",
    "                if activations is not None and li == 1:  # kun første skjulte lag\n",
    "                    #a = activations.get(name, np.zeros(size))\n",
    "                    a = activations.get(name, np.zeros(size))\n",
    "                    a = np.asarray(a).ravel()   # <-- flater til 1D array\n",
    "                    scale = activation_color_max if activation_color_max is not None else 1.0\n",
    "                    val = float(np.clip(a[i] / max(scale, 1e-12), 0.0, 1.0))\n",
    "                    color = plt.cm.winter(val)\n",
    "                else:\n",
    "                    color = 'lightblue'\n",
    "                node_colors.append(color)\n",
    "\n",
    "            nodes_per_layer.append(ids)\n",
    "\n",
    "        edge_colors, edge_widths = [], []\n",
    "\n",
    "        def add_edges(W, inn, ut):\n",
    "            for j, out_n in enumerate(ut):\n",
    "                for i, in_n in enumerate(inn):\n",
    "                    w = float(W[i, j])\n",
    "                    G.add_edge(in_n, out_n, weight=abs(w))\n",
    "                    edge_colors.append('red' if w >= 0 else 'blue')\n",
    "                    edge_widths.append(abs(w))\n",
    "\n",
    "        # input -> hidden(1)\n",
    "        add_edges(weights['W_1'], nodes_per_layer[0], nodes_per_layer[1])\n",
    "\n",
    "        # ev. internlag\n",
    "        if 'W_internal' in weights:\n",
    "            for l, Win in enumerate(weights['W_internal']):\n",
    "                add_edges(Win, nodes_per_layer[l+1], nodes_per_layer[l+2])\n",
    "\n",
    "        # siste hidden -> output\n",
    "        add_edges(weights['W_L'], nodes_per_layer[-2], nodes_per_layer[-1])\n",
    "\n",
    "        nx.draw(G, pos, ax=ax,\n",
    "                node_color=node_colors,\n",
    "                edge_color=(edge_colors if signed_colors else 'red'),\n",
    "                width=[G[u][v]['weight'] for u,v in G.edges()],\n",
    "                with_labels=False, node_size=400, arrows=False)\n",
    "\n",
    "        ax.set_title(abbr[name], fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def compute_hidden_activation(fit_dict, x_train, draw_idx):\n",
    "    fit = fit_dict['posterior']\n",
    "    W1 = fit.stan_variable('W_1')[draw_idx, :, :]          # (P,H)\n",
    "    try:\n",
    "        b1 = fit.stan_variable('hidden_bias')[draw_idx, :] # (H,)\n",
    "    except Exception:\n",
    "        b1 = np.zeros(W1.shape[1])\n",
    "    # tanh i [-1,1]\n",
    "    a_full = np.tanh(x_train @ W1 + b1)             # (H,)\n",
    "    a=np.mean(a_full, axis=0)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velg en observasjon å \"lyse opp\" nodefargene med\n",
    "obs_idx = 3\n",
    "draw_idx = 69 #pick_draw_idx(prior_fits, seed=42)      # one common draw across models\n",
    "prior_draws = build_single_draw_weights(tanh_fit_nodewise_full, layer_structure, draw_idx)\n",
    "\n",
    "# 1) Beregn aktivasjoner for ALLE modellene\n",
    "activations = {}\n",
    "for name, fd in tanh_fit_nodewise_full.items():\n",
    "    a = compute_hidden_activation(fd, X_train, draw_idx)\n",
    "    activations[name] = np.abs(a)      \n",
    "\n",
    "# 2) Skaler vekter for plotting (som før)\n",
    "scaled, _ = scale_W1_for_plot(prior_draws, mode='per_model')\n",
    "\n",
    "# 3) Kall plottet med aktivasjoner\n",
    "# Siden tanh ∈ [-1,1] og vi bruker |a|, så sett activation_color_max=1.0\n",
    "fig = plot_models_with_activations(\n",
    "    scaled,\n",
    "    layer_sizes=[P, H, out_nodes],\n",
    "    activations=None,\n",
    "    activation_color_max=1.0,\n",
    "    ncols=2,\n",
    "    feature_names = None\n",
    ")\n",
    "plt.savefig(\"figures_for_use_in_paper/abalone_network_tanh.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
