{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman_correlated\"\n",
    "results_dir_relu_correlated = \"results/regression/single_layer/relu/friedman_correlated/no_lambda\"\n",
    "results_dir_tanh_correlated = \"results/regression/single_layer/tanh/friedman_correlated/no_lambda\"\n",
    "\n",
    "model_names_relu = [\"Dirichlet tau\", \"Beta tau\", \"Dirichlet\", \"Beta\"]\n",
    "model_names_tanh = [\"Dirichlet tau tanh\", \"Beta tau tanh\", \"Dirichlet tanh\", \"Beta tanh\"]\n",
    "\n",
    "relu_fits_correlated = {}\n",
    "tanh_fits_correlated = {}\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit_correlated = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu_correlated,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit_correlated = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh_correlated,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    relu_fits_correlated[base_config_name] = relu_fit_correlated  # use clean key\n",
    "    tanh_fits_correlated[base_config_name] = tanh_fit_correlated  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from properscoring import crps_ensemble\n",
    "from scores.probability import crps_for_ensemble\n",
    "\n",
    "def compute_rmse_from_fits(all_fits, model_names=None, folder=\"friedman\"):\n",
    "    \"\"\"\n",
    "    Iterate over all dataset keys in `all_fits` (e.g., relu_fits or tanh_fits).\n",
    "    For each model in `model_names` (or all models found if None), compute:\n",
    "      - RMSE for each posterior draw\n",
    "      - RMSE of the posterior mean predictor\n",
    "\n",
    "    Returns:\n",
    "        df_rmse: long DF with one row per posterior draw.\n",
    "        df_posterior_rmse: one row per model/dataset with posterior-mean RMSE.\n",
    "    \"\"\"\n",
    "    rmse_rows = []\n",
    "    post_mean_rows = []\n",
    "\n",
    "    for dataset_key, model_dict in all_fits.items():\n",
    "        N, sigma, seed = extract_friedman_metadata(dataset_key)\n",
    "        if N is None:\n",
    "            # Skip non-Friedman entries if any\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            path = f\"datasets/{folder}/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # shape (N_test,)\n",
    "        except FileNotFoundError:\n",
    "            path = f\"datasets/{folder}/many/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # shape (N_test,)\n",
    "            #print(f\"[SKIP] y_test not found: {path}\")\n",
    "            #continue\n",
    "\n",
    "        # Choose which models to evaluate\n",
    "        models_to_eval = model_names or list(model_dict.keys())\n",
    "\n",
    "        for model in models_to_eval:\n",
    "            # Some entries may be missing\n",
    "            entry = model_dict.get(model, None)\n",
    "            if not entry or \"posterior\" not in entry:\n",
    "                print(f\"[SKIP] Missing posterior: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            fit = entry[\"posterior\"]\n",
    "\n",
    "            # Expecting (S, N_test, 1) or (S, N_test)\n",
    "            output_test = fit.stan_variable(\"output_test\")\n",
    "            if output_test.ndim == 3 and output_test.shape[-1] == 1:\n",
    "                preds = output_test[..., 0]  # (S, N_test)\n",
    "            elif output_test.ndim == 2:\n",
    "                preds = output_test  # (S, N_test)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected output_test shape {output_test.shape} for {dataset_key} -> {model}\")\n",
    "\n",
    "            # Per-sample RMSE\n",
    "            sq_err = (preds - y_test[None, :])**2  # (S, N_test)\n",
    "            rmse_per_sample = np.sqrt(np.mean(sq_err, axis=1))  # (S,)\n",
    "\n",
    "            for s_idx, rmse in enumerate(rmse_per_sample):\n",
    "                rmse_rows.append({\n",
    "                    \"dataset_key\": dataset_key,\n",
    "                    \"model\": model,\n",
    "                    \"N\": N,\n",
    "                    \"sigma\": sigma,\n",
    "                    \"seed\": seed,\n",
    "                    \"sample_idx\": s_idx,\n",
    "                    \"rmse\": float(rmse)\n",
    "                })\n",
    "\n",
    "            # Posterior-mean RMSE\n",
    "            posterior_mean = preds.mean(axis=0)  # (N_test,)\n",
    "            post_mean_rmse = float(np.sqrt(np.mean((posterior_mean - y_test)**2)))\n",
    "            post_mean_rows.append({\n",
    "                \"dataset_key\": dataset_key,\n",
    "                \"model\": model,\n",
    "                \"N\": N,\n",
    "                \"sigma\": sigma,\n",
    "                \"seed\": seed,\n",
    "                \"posterior_mean_rmse\": post_mean_rmse\n",
    "            })\n",
    "\n",
    "    df_rmse = pd.DataFrame(rmse_rows)\n",
    "    df_posterior_rmse = pd.DataFrame(post_mean_rows)\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "_FRIEDMAN_KEY = re.compile(r\"Friedman_N(\\d+)_p\\d+_sigma([\\d.]+)_seed(\\d+)\")\n",
    "\n",
    "def extract_friedman_metadata(key: str):\n",
    "    \"\"\"\n",
    "    Parse 'Friedman_N{N}_p10_sigma{sigma}_seed{seed}' -> (N:int, sigma:float, seed:int)\n",
    "    Returns (None, None, None) if it doesn't match.\n",
    "    \"\"\"\n",
    "    m = _FRIEDMAN_KEY.search(key)\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "    N = int(m.group(1))\n",
    "    sigma = float(m.group(2))\n",
    "    seed = int(m.group(3))\n",
    "    return N, sigma, seed\n",
    "\n",
    "\n",
    "df_rmse_relu_correlated, df_posterior_rmse_relu_correlated = compute_rmse_from_fits(\n",
    "    relu_fits_correlated, model_names_relu, folder = \"friedman_correlated\"\n",
    ")\n",
    "\n",
    "\n",
    "df_rmse_tanh_correlated, df_posterior_rmse_tanh_correlated = compute_rmse_from_fits(\n",
    "    tanh_fits_correlated, model_names_tanh, folder = \"friedman_correlated\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df3 = df_rmse_relu_correlated.assign(activation=\"ReLU\", setting=\"Correlated\")\n",
    "df4 = df_rmse_tanh_correlated.assign(activation=\"Tanh\", setting=\"Correlated\")\n",
    "\n",
    "df_all = pd.concat([df3, df4], ignore_index=True)\n",
    "\n",
    "df3_pm = df_posterior_rmse_relu_correlated.assign(activation=\"ReLU\", setting=\"Correlated\")\n",
    "df4_pm = df_posterior_rmse_tanh_correlated.assign(activation=\"Tanh\", setting=\"Correlated\")\n",
    "\n",
    "df_all_pm = pd.concat([df3_pm, df4_pm], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# --- prepare data ---\n",
    "df = df_all.copy()\n",
    "\n",
    "abbr = {\n",
    "    \"Dirichlet tau\": r\"Dir - $\\tau$\",\n",
    "    \"Beta tau\": r\"Beta - $\\tau$\",\n",
    "    \"Dirichlet\": r\"Dir\",\n",
    "    \"Beta\": r\"Beta\",\n",
    "}\n",
    "\n",
    "# unify model names across activations (strip \" tanh\")\n",
    "df[\"model_clean\"] = df[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# summary stats per (setting, N, model, activation)\n",
    "summary = (\n",
    "    df.groupby([\"setting\", \"N\", \"model_clean\", \"activation\"], as_index=False)[\"rmse\"]\n",
    "      .agg(mean=\"mean\", std=\"std\")\n",
    ")\n",
    "\n",
    "# plotting order\n",
    "settings = [\"Original\", \"Correlated\"]\n",
    "Ns = [50, 100, 200, 500]\n",
    "models = [\"Dirichlet tau\", \"Beta tau\", \"Dirichlet\", \"Beta\"]\n",
    "\n",
    "# visuals\n",
    "markers = {\"ReLU\": \"o\", \"Tanh\": \"^\"}            # shapes\n",
    "offsets = {\"ReLU\": -0.12, \"Tanh\": +0.12}        # side-by-side jitter on x\n",
    "model_offsets = {\n",
    "    \"Dirichlet tau\": -0.04,\n",
    "    \"Beta tau\": -0.01,\n",
    "    \"Dirichlet\": +0.01,\n",
    "    \"Beta\": +0.04,\n",
    "}\n",
    "palette_list = plt.get_cmap(\"tab10\").colors\n",
    "palette = {m: palette_list[i] for i, m in enumerate(models)}\n",
    "\n",
    "# map N to base x positions and add offsets for activation\n",
    "xbase = {N: i for i, N in enumerate(Ns)}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 7), sharey=True)\n",
    "\n",
    "for ax, setting in zip(axes, settings):\n",
    "    sub = summary[summary[\"setting\"] == setting]\n",
    "    # plot each model+activation with errorbars, without lines\n",
    "    for m in models:\n",
    "        for act in [\"ReLU\", \"Tanh\"]:\n",
    "            g = sub[(sub[\"model_clean\"] == m) & (sub[\"activation\"] == act)]\n",
    "            if g.empty:\n",
    "                continue\n",
    "            #xs = [xbase[n] + offsets[act] for n in g[\"N\"]]\n",
    "            xs = [xbase[n] + offsets[act] + model_offsets[m] for n in g[\"N\"]]\n",
    "\n",
    "            ax.errorbar(\n",
    "                xs, g[\"mean\"], yerr=g[\"std\"],\n",
    "                fmt=markers[act], markersize=10,\n",
    "                linestyle=\"none\", capsize=3,\n",
    "                color=palette[m], markeredgecolor=\"black\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(f\"{setting}\", fontsize=15)\n",
    "    ax.set_xticks(range(len(Ns)))\n",
    "    ax.set_xticklabels(Ns, fontsize=15)\n",
    "    ax.set_xlabel(\"N\", fontsize=15)\n",
    "    ax.set_ylabel(\"RMSE\", fontsize=15)\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    ax.grid()\n",
    "\n",
    "# --- legends ---\n",
    "model_handles = [\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"none\",\n",
    "        color=palette[m],\n",
    "        markeredgecolor=\"black\",\n",
    "        markersize=12,\n",
    "        label=abbr.get(m, m)   # <- use abbreviation\n",
    "    )\n",
    "    for m in models\n",
    "]\n",
    "\n",
    "# activation legend (shapes)\n",
    "activation_handles = [\n",
    "    Line2D([0], [0], marker=markers[\"ReLU\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=12, label=\"ReLU\"),\n",
    "    Line2D([0], [0], marker=markers[\"Tanh\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=12, label=\"Tanh\"),\n",
    "]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(\n",
    "        handles=model_handles + activation_handles,\n",
    "        title=None,\n",
    "        loc=\"upper right\",\n",
    "        frameon=False,\n",
    "        ncol=1,\n",
    "        fontsize = 14\n",
    "    )\n",
    "plt.tight_layout(rect=(0, 0, 1, 1))\n",
    "#plt.grid()\n",
    "plt.savefig(\"figures_for_use_in_paper/friedman_RMSE_with_beta.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "# --- prepare data ---\n",
    "df = df_all_pm.copy()\n",
    "\n",
    "# unify model names across activations (strip \" tanh\")\n",
    "df[\"model_clean\"] = df[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# summary stats per (setting, N, model, activation)\n",
    "summary = (\n",
    "    df.groupby([\"setting\", \"N\", \"model_clean\", \"activation\"], as_index=False)[\"posterior_mean_rmse\"]\n",
    "      .agg(mean=\"mean\", std=\"std\")\n",
    ")\n",
    "# plotting order\n",
    "settings = [\"Original\", \"Correlated\"]\n",
    "Ns = [50, 100, 200, 500]\n",
    "models = [\"Dirichlet tau\", \"Beta tau\", \"Dirichlet\", \"Beta\"]\n",
    "\n",
    "# visuals\n",
    "markers = {\"ReLU\": \"o\", \"Tanh\": \"^\"}            # shapes\n",
    "offsets = {\"ReLU\": -0.12, \"Tanh\": +0.12}        # side-by-side jitter on x\n",
    "model_offsets = {\n",
    "    \"Dirichlet tau\": -0.04,\n",
    "    \"Beta tau\": -0.01,\n",
    "    \"Dirichlet\": +0.01,\n",
    "    \"Beta\": +0.04,\n",
    "}\n",
    "palette_list = plt.get_cmap(\"tab10\").colors\n",
    "palette = {m: palette_list[i] for i, m in enumerate(models)}\n",
    "\n",
    "# map N to base x positions and add offsets for activation\n",
    "xbase = {N: i for i, N in enumerate(Ns)}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "for ax, setting in zip(axes, settings):\n",
    "    sub = summary[summary[\"setting\"] == setting]\n",
    "    # plot each model+activation with errorbars, without lines\n",
    "    for m in models:\n",
    "        for act in [\"ReLU\", \"Tanh\"]:\n",
    "            g = sub[(sub[\"model_clean\"] == m) & (sub[\"activation\"] == act)]\n",
    "            if g.empty:\n",
    "                continue\n",
    "            #xs = [xbase[n] + offsets[act] for n in g[\"N\"]]\n",
    "            xs = [xbase[n] + offsets[act] + model_offsets[m] for n in g[\"N\"]]\n",
    "            \n",
    "            ax.plot(\n",
    "                xs, g[\"mean\"],\n",
    "                marker=markers[act],\n",
    "                markersize=7,\n",
    "                linestyle=\"none\",\n",
    "                color=palette[m],\n",
    "                markeredgecolor=\"black\",\n",
    "            )\n",
    "\n",
    "\n",
    "    ax.set_title(f\"{setting}\")\n",
    "    ax.set_xticks(range(len(Ns)))\n",
    "    ax.set_xticklabels(Ns)\n",
    "    ax.set_xlabel(\"N\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.grid()\n",
    "\n",
    "# --- legends ---\n",
    "model_handles = [\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"none\",\n",
    "        color=palette[m],\n",
    "        markeredgecolor=\"black\",\n",
    "        markersize=7,\n",
    "        label=abbr.get(m, m)   # <- use abbreviation\n",
    "    )\n",
    "    for m in models\n",
    "]\n",
    "\n",
    "# activation legend (shapes)\n",
    "activation_handles = [\n",
    "    Line2D([0], [0], marker=markers[\"ReLU\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=7, label=\"ReLU\"),\n",
    "    Line2D([0], [0], marker=markers[\"Tanh\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=7, label=\"Tanh\"),\n",
    "]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(\n",
    "        handles=model_handles + activation_handles,\n",
    "        title=None,\n",
    "        loc=\"upper right\",\n",
    "        frameon=False,\n",
    "        ncol=1\n",
    "    )\n",
    "plt.tight_layout(rect=(0, 0, 1, 1))\n",
    "#plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "def compute_sparse_rmse_results_corr(seeds, models, all_fits, get_N_sigma, forward_pass,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "        path = f\"datasets/friedman_correlated/{dataset_key}.npz\"\n",
    "\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "            #print(y_test.shape)\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                W2 = W2_samples[i]\n",
    "\n",
    "                # Apply pruning mask if requested\n",
    "                if prune_fn is not None and sparsity > 0.0:\n",
    "                    masks = prune_fn([W1, W2], sparsity)\n",
    "                    W1 = W1 * masks[0]\n",
    "                    #W2 = W2 * masks[1]\n",
    "\n",
    "                y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i])\n",
    "                y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "                rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "                \n",
    "            posterior_mean = np.mean(y_hats, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'sparsity': sparsity,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "\n",
    "seeds = [1] #[1, 2, 11]\n",
    "seeds_correlated = [1]#, 6, 11]\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    if seed == 1:\n",
    "        N=100\n",
    "    elif seed == 2:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma\n",
    "\n",
    "def get_N_sigma_correlated(seed):\n",
    "    if seed == 1:\n",
    "        N=100\n",
    "    elif seed == 6:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "df_rmse_relu_correlated, df_posterior_rmse_relu_correlated = {}, {}\n",
    "df_rmse_tanh_correlated, df_posterior_rmse_tanh_correlated = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    df_rmse_relu_correlated[sparsity], df_posterior_rmse_relu_correlated[sparsity]= compute_sparse_rmse_results_corr(\n",
    "        seeds_correlated, model_names_relu, relu_fits_correlated, get_N_sigma_correlated, forward_pass_relu,\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "        \n",
    "    df_rmse_tanh_correlated[sparsity], df_posterior_rmse_tanh_correlated[sparsity]= compute_sparse_rmse_results_corr(\n",
    "        seeds_correlated, model_names_tanh, tanh_fits_correlated, get_N_sigma_correlated, forward_pass_tanh,\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_rmse_full_relu_correlated = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_relu_correlated.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "\n",
    "df_rmse_full_tanh_correlated = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_tanh_correlated.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "\n",
    "df_posterior_rmse_full_relu_correlated = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_posterior_rmse_relu_correlated.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_posterior_rmse_full_tanh_correlated = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_posterior_rmse_tanh_correlated.items()],\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posterior_rmse_full_tanh_correlated[df_posterior_rmse_full_tanh_correlated['sparsity']==0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posterior_rmse_full_relu_correlated[df_posterior_rmse_full_relu_correlated['sparsity']==0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 1 — FUNCTIONS ONLY\n",
    "# =========================\n",
    "import numpy as np\n",
    "\n",
    "def forward_pass_relu(X, W1, b1, W2, b2):\n",
    "    pre = X @ W1 + b1.reshape(1, -1)\n",
    "    hid = np.maximum(0, pre)\n",
    "    out = hid @ W2 + b2.reshape(1, -1)\n",
    "    return out\n",
    "\n",
    "def forward_pass_tanh(X, W1, b1, W2, b2):\n",
    "    pre = X @ W1 + b1.reshape(1, -1)\n",
    "    hid = np.tanh(pre)\n",
    "    out = hid @ W2 + b2.reshape(1, -1)\n",
    "    return out\n",
    "\n",
    "def mse_loss_and_grads(X, y, W1, b1, W2, b2, activation=\"relu\"):\n",
    "    \"\"\"\n",
    "    MSE regression loss + exact backprop grads for 1-hidden-layer NN.\n",
    "    Shapes assumed:\n",
    "      X:  (N, D)\n",
    "      y:  (N, K)\n",
    "      W1: (D, H), b1: (H,)\n",
    "      W2: (H, K), b2: (K,)\n",
    "    \"\"\"\n",
    "    pre = X @ W1 + b1.reshape(1, -1)\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        hid = np.maximum(0, pre)\n",
    "        dhid_dpre = (pre > 0).astype(pre.dtype)\n",
    "    else:  # tanh\n",
    "        hid = np.tanh(pre)\n",
    "        dhid_dpre = 1.0 - hid**2\n",
    "\n",
    "    pred = hid @ W2 + b2.reshape(1, -1)\n",
    "\n",
    "    r = pred - y\n",
    "    loss = np.mean(r**2)\n",
    "\n",
    "    N = X.shape[0]\n",
    "    d_pred = (2.0 / N) * r          # (N, K)\n",
    "\n",
    "    dW2 = hid.T @ d_pred            # (H, K)\n",
    "    db2 = np.sum(d_pred, axis=0)    # (K,)\n",
    "\n",
    "    d_hid = d_pred @ W2.T           # (N, H)\n",
    "    d_pre = d_hid * dhid_dpre       # (N, H)\n",
    "\n",
    "    dW1 = X.T @ d_pre               # (D, H)\n",
    "    db1 = np.sum(d_pre, axis=0)     # (H,)\n",
    "\n",
    "    return loss, {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "def pack_grads(grads):\n",
    "    return np.concatenate([\n",
    "        grads[\"dW1\"].ravel(),\n",
    "        grads[\"db1\"].ravel(),\n",
    "        grads[\"dW2\"].ravel(),\n",
    "        grads[\"db2\"].ravel(),\n",
    "    ])\n",
    "\n",
    "def pack_params(W1, b1, W2, b2):\n",
    "    return np.concatenate([W1.ravel(), b1.ravel(), W2.ravel(), b2.ravel()])\n",
    "\n",
    "def unpack_params(theta, D, H, K):\n",
    "    i = 0\n",
    "    W1 = theta[i:i + D*H].reshape(D, H); i += D*H\n",
    "    b1 = theta[i:i + H];                i += H\n",
    "    W2 = theta[i:i + H*K].reshape(H, K); i += H*K\n",
    "    b2 = theta[i:i + K]\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def gradient_cosine_similarities(G):\n",
    "    meanG = G.mean(axis=0)\n",
    "    mean_norm = np.linalg.norm(meanG) + 1e-12\n",
    "    G_norms = np.linalg.norm(G, axis=1) + 1e-12\n",
    "    return (G @ meanG) / (G_norms * mean_norm)\n",
    "\n",
    "def gradient_snr_over_batches(grad_list):\n",
    "    \"\"\"\n",
    "    grad_list: list of flattened gradients across batches at fixed parameters\n",
    "    \"\"\"\n",
    "    G = np.stack(grad_list, axis=0)\n",
    "    mu = G.mean(axis=0)\n",
    "    num = np.linalg.norm(mu)\n",
    "    denom = np.sqrt(np.mean(np.sum((G - mu)**2, axis=1))) + 1e-12\n",
    "    return num / denom\n",
    "\n",
    "def directional_grad_lipschitz_fd(X, y, W1, b1, W2, b2, activation=\"relu\", eps=1e-3, rng=None):\n",
    "    \"\"\"\n",
    "    Approximates ||grad(w+eps*v) - grad(w)|| / eps along random unit direction v.\n",
    "    Cheap local curvature proxy.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    D, H = W1.shape\n",
    "    K = W2.shape[1]\n",
    "    theta = pack_params(W1, b1, W2, b2)\n",
    "\n",
    "    v = rng.normal(size=theta.shape)\n",
    "    v /= (np.linalg.norm(v) + 1e-12)\n",
    "\n",
    "    _, g0 = mse_loss_and_grads(X, y, W1, b1, W2, b2, activation=activation)\n",
    "    g0 = pack_grads(g0)\n",
    "\n",
    "    theta2 = theta + eps * v\n",
    "    W1_2, b1_2, W2_2, b2_2 = unpack_params(theta2, D, H, K)\n",
    "    _, g1 = mse_loss_and_grads(X, y, W1_2, b1_2, W2_2, b2_2, activation=activation)\n",
    "    g1 = pack_grads(g1)\n",
    "\n",
    "    return np.linalg.norm(g1 - g0) / eps\n",
    "\n",
    "def make_minibatches(X, y, batch_size=64, n_batches=10, seed=0):\n",
    "    \"\"\"\n",
    "    Minimal minibatch sampler for SNR section.\n",
    "    Returns list of (Xb, yb) with yb already shaped (B, K).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N = X.shape[0]\n",
    "    batches = []\n",
    "    for _ in range(n_batches):\n",
    "        ids = rng.choice(N, size=batch_size, replace=False)\n",
    "        batches.append((X[ids], y[ids]))\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 2 — EXECUTION ONLY\n",
    "# =========================\n",
    "\n",
    "# --- A) DATA LOADING + BATCH SETUP ---\n",
    "dataset_key = f'Friedman_N{500}_p10_sigma{1:.2f}_seed{11}'\n",
    "path = f\"datasets/friedman/{dataset_key}.npz\"\n",
    "data = np.load(path)\n",
    "\n",
    "X_train, X_test = data[\"X_train\"], data[\"X_test\"]\n",
    "y_train, y_test = data[\"y_train\"], data[\"y_test\"]\n",
    "\n",
    "# Pick batch (test set)\n",
    "X_batch = X_test\n",
    "y_batch = y_test.reshape(-1, 1)  # enforce (N,1) ONCE\n",
    "\n",
    "# --- B) MODEL + POSTERIOR DRAWS ---\n",
    "model_name = \"Regularized Horseshoe tanh\"\n",
    "activation = \"tanh\"   # \"relu\" or \"tanh\"\n",
    "\n",
    "post = posterior_N100_fits[model_name][\"posterior\"]\n",
    "W1_draws = post.stan_variable(\"W_1\")\n",
    "b1_draws = post.stan_variable(\"hidden_bias\")\n",
    "W2_draws = post.stan_variable(\"W_L\")\n",
    "b2_draws = post.stan_variable(\"output_bias\")\n",
    "\n",
    "S = W1_draws.shape[0]\n",
    "D, H = W1_draws.shape[1], W1_draws.shape[2]\n",
    "K = W2_draws.shape[2]\n",
    "\n",
    "# --- C) SUBSAMPLE DRAWS FOR DIAGNOSTICS ---\n",
    "S_use = min(S, 300)\n",
    "idx = np.arange(S_use)\n",
    "\n",
    "# --- D) POSTERIOR GRADIENT STABILITY (across draws) ---\n",
    "G = []\n",
    "losses = []\n",
    "for s in idx:\n",
    "    loss_s, grads_s = mse_loss_and_grads(\n",
    "        X_batch, y_batch,\n",
    "        W1_draws[s], b1_draws[s],\n",
    "        W2_draws[s], b2_draws[s],\n",
    "        activation=activation\n",
    "    )\n",
    "    losses.append(loss_s)\n",
    "    G.append(pack_grads(grads_s))\n",
    "\n",
    "G = np.stack(G, axis=0)\n",
    "g_norms = np.linalg.norm(G, axis=1)\n",
    "g_cos = gradient_cosine_similarities(G)\n",
    "\n",
    "print(\"Posterior gradient norms (5/50/95%):\", np.quantile(g_norms, [0.05, 0.5, 0.95]))\n",
    "print(\"Posterior grad cosine vs mean (5/50/95%):\", np.quantile(g_cos,   [0.05, 0.5, 0.95]))\n",
    "print(\"Posterior losses (5/50/95%):\",             np.quantile(np.array(losses), [0.05, 0.5, 0.95]))\n",
    "\n",
    "# --- E) CURVATURE PROXY (directional FD gradient Lipschitz) ---\n",
    "rng = np.random.default_rng(1)\n",
    "S_curv = min(S_use, 100)\n",
    "Ldir = []\n",
    "for s in idx[:S_curv]:\n",
    "    Ldir.append(directional_grad_lipschitz_fd(\n",
    "        X_batch, y_batch,\n",
    "        W1_draws[s], b1_draws[s],\n",
    "        W2_draws[s], b2_draws[s],\n",
    "        activation=activation,\n",
    "        eps=1e-3,\n",
    "        rng=rng\n",
    "    ))\n",
    "Ldir = np.array(Ldir)\n",
    "print(\"Directional grad-Lipschitz proxy (5/50/95%):\", np.quantile(Ldir, [0.05, 0.5, 0.95]))\n",
    "\n",
    "# --- F) (OPTIONAL) GRADIENT SNR ACROSS MINIBATCHES FOR ONE DRAW ---\n",
    "DO_SNR = True  # set False to skip this section cleanly\n",
    "\n",
    "if DO_SNR:\n",
    "    # build batches from train set; ensure y is (B,1) in each batch\n",
    "    y_train_2d = y_train.reshape(-1, 1)\n",
    "    batches = make_minibatches(X_train, y_train_2d, batch_size=64, n_batches=10, seed=0)\n",
    "\n",
    "    s0 = 0\n",
    "    grad_list = []\n",
    "    for Xb, yb in batches:\n",
    "        _, grads_b = mse_loss_and_grads(\n",
    "            Xb, yb,\n",
    "            W1_draws[s0], b1_draws[s0],\n",
    "            W2_draws[s0], b2_draws[s0],\n",
    "            activation=activation\n",
    "        )\n",
    "        grad_list.append(pack_grads(grads_b))\n",
    "\n",
    "    snr = gradient_snr_over_batches(grad_list)\n",
    "    print(\"Gradient SNR across minibatches at draw s0:\", snr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 2 — EXECUTION ONLY\n",
    "# =========================\n",
    "\n",
    "# --- A) DATA LOADING + BATCH SETUP ---\n",
    "dataset_key = f'Friedman_N{500}_p10_sigma{1:.2f}_seed{11}'\n",
    "path = f\"datasets/friedman/{dataset_key}.npz\"\n",
    "data = np.load(path)\n",
    "\n",
    "X_train, X_test = data[\"X_train\"], data[\"X_test\"]\n",
    "y_train, y_test = data[\"y_train\"], data[\"y_test\"]\n",
    "\n",
    "# Pick batch (test set)\n",
    "X_batch = X_test\n",
    "y_batch = y_test.reshape(-1, 1)  # enforce (N,1) ONCE\n",
    "\n",
    "# --- B) MODEL + POSTERIOR DRAWS ---\n",
    "model_name = \"Dirichlet Horseshoe tanh\"\n",
    "activation = \"tanh\"   # \"relu\" or \"tanh\"\n",
    "\n",
    "post = posterior_N100_fits[model_name][\"posterior\"]\n",
    "W1_draws = post.stan_variable(\"W_1\")\n",
    "b1_draws = post.stan_variable(\"hidden_bias\")\n",
    "W2_draws = post.stan_variable(\"W_L\")\n",
    "b2_draws = post.stan_variable(\"output_bias\")\n",
    "\n",
    "S = W1_draws.shape[0]\n",
    "D, H = W1_draws.shape[1], W1_draws.shape[2]\n",
    "K = W2_draws.shape[2]\n",
    "\n",
    "# --- C) SUBSAMPLE DRAWS FOR DIAGNOSTICS ---\n",
    "S_use = min(S, 300)\n",
    "idx = np.arange(S_use)\n",
    "\n",
    "# --- D) POSTERIOR GRADIENT STABILITY (across draws) ---\n",
    "G = []\n",
    "losses = []\n",
    "for s in idx:\n",
    "    loss_s, grads_s = mse_loss_and_grads(\n",
    "        X_batch, y_batch,\n",
    "        W1_draws[s], b1_draws[s],\n",
    "        W2_draws[s], b2_draws[s],\n",
    "        activation=activation\n",
    "    )\n",
    "    losses.append(loss_s)\n",
    "    G.append(pack_grads(grads_s))\n",
    "\n",
    "G = np.stack(G, axis=0)\n",
    "g_norms = np.linalg.norm(G, axis=1)\n",
    "g_cos = gradient_cosine_similarities(G)\n",
    "\n",
    "print(\"Posterior gradient norms (5/50/95%):\", np.quantile(g_norms, [0.05, 0.5, 0.95]))\n",
    "print(\"Posterior grad cosine vs mean (5/50/95%):\", np.quantile(g_cos,   [0.05, 0.5, 0.95]))\n",
    "print(\"Posterior losses (5/50/95%):\",             np.quantile(np.array(losses), [0.05, 0.5, 0.95]))\n",
    "\n",
    "# --- E) CURVATURE PROXY (directional FD gradient Lipschitz) ---\n",
    "rng = np.random.default_rng(1)\n",
    "S_curv = min(S_use, 100)\n",
    "Ldir = []\n",
    "for s in idx[:S_curv]:\n",
    "    Ldir.append(directional_grad_lipschitz_fd(\n",
    "        X_batch, y_batch,\n",
    "        W1_draws[s], b1_draws[s],\n",
    "        W2_draws[s], b2_draws[s],\n",
    "        activation=activation,\n",
    "        eps=1e-3,\n",
    "        rng=rng\n",
    "    ))\n",
    "Ldir = np.array(Ldir)\n",
    "print(\"Directional grad-Lipschitz proxy (5/50/95%):\", np.quantile(Ldir, [0.05, 0.5, 0.95]))\n",
    "\n",
    "# --- F) (OPTIONAL) GRADIENT SNR ACROSS MINIBATCHES FOR ONE DRAW ---\n",
    "DO_SNR = True  # set False to skip this section cleanly\n",
    "\n",
    "if DO_SNR:\n",
    "    # build batches from train set; ensure y is (B,1) in each batch\n",
    "    y_train_2d = y_train.reshape(-1, 1)\n",
    "    batches = make_minibatches(X_train, y_train_2d, batch_size=64, n_batches=10, seed=0)\n",
    "\n",
    "    s0 = 0\n",
    "    grad_list = []\n",
    "    for Xb, yb in batches:\n",
    "        _, grads_b = mse_loss_and_grads(\n",
    "            Xb, yb,\n",
    "            W1_draws[s0], b1_draws[s0],\n",
    "            W2_draws[s0], b2_draws[s0],\n",
    "            activation=activation\n",
    "        )\n",
    "        grad_list.append(pack_grads(grads_b))\n",
    "\n",
    "    snr = gradient_snr_over_batches(grad_list)\n",
    "    print(\"Gradient SNR across minibatches at draw s0:\", snr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOCK 2 — EXECUTION ONLY\n",
    "# =========================\n",
    "\n",
    "# --- A) DATA LOADING + BATCH SETUP ---\n",
    "dataset_key = f'Friedman_N{500}_p10_sigma{1:.2f}_seed{11}'\n",
    "path = f\"datasets/friedman/{dataset_key}.npz\"\n",
    "data = np.load(path)\n",
    "\n",
    "X_train, X_test = data[\"X_train\"], data[\"X_test\"]\n",
    "y_train, y_test = data[\"y_train\"], data[\"y_test\"]\n",
    "\n",
    "# Pick batch (test set)\n",
    "X_batch = X_test\n",
    "y_batch = y_test.reshape(-1, 1)  # enforce (N,1) ONCE\n",
    "\n",
    "# --- B) MODEL + POSTERIOR DRAWS ---\n",
    "model_name = \"Beta Horseshoe tanh\"\n",
    "activation = \"tanh\"   # \"relu\" or \"tanh\"\n",
    "\n",
    "post = posterior_N100_fits[model_name][\"posterior\"]\n",
    "W1_draws = post.stan_variable(\"W_1\")\n",
    "b1_draws = post.stan_variable(\"hidden_bias\")\n",
    "W2_draws = post.stan_variable(\"W_L\")\n",
    "b2_draws = post.stan_variable(\"output_bias\")\n",
    "\n",
    "S = W1_draws.shape[0]\n",
    "D, H = W1_draws.shape[1], W1_draws.shape[2]\n",
    "K = W2_draws.shape[2]\n",
    "\n",
    "# --- C) SUBSAMPLE DRAWS FOR DIAGNOSTICS ---\n",
    "S_use = min(S, 300)\n",
    "idx = np.arange(S_use)\n",
    "\n",
    "# --- D) POSTERIOR GRADIENT STABILITY (across draws) ---\n",
    "G = []\n",
    "losses = []\n",
    "for s in idx:\n",
    "    loss_s, grads_s = mse_loss_and_grads(\n",
    "        X_batch, y_batch,\n",
    "        W1_draws[s], b1_draws[s],\n",
    "        W2_draws[s], b2_draws[s],\n",
    "        activation=activation\n",
    "    )\n",
    "    losses.append(loss_s)\n",
    "    G.append(pack_grads(grads_s))\n",
    "\n",
    "G = np.stack(G, axis=0)\n",
    "g_norms = np.linalg.norm(G, axis=1)\n",
    "g_cos = gradient_cosine_similarities(G)\n",
    "\n",
    "print(\"Posterior gradient norms (5/50/95%):\", np.quantile(g_norms, [0.05, 0.5, 0.95]))\n",
    "print(\"Posterior grad cosine vs mean (5/50/95%):\", np.quantile(g_cos,   [0.05, 0.5, 0.95]))\n",
    "print(\"Posterior losses (5/50/95%):\",             np.quantile(np.array(losses), [0.05, 0.5, 0.95]))\n",
    "\n",
    "# --- E) CURVATURE PROXY (directional FD gradient Lipschitz) ---\n",
    "rng = np.random.default_rng(1)\n",
    "S_curv = min(S_use, 100)\n",
    "Ldir = []\n",
    "for s in idx[:S_curv]:\n",
    "    Ldir.append(directional_grad_lipschitz_fd(\n",
    "        X_batch, y_batch,\n",
    "        W1_draws[s], b1_draws[s],\n",
    "        W2_draws[s], b2_draws[s],\n",
    "        activation=activation,\n",
    "        eps=1e-3,\n",
    "        rng=rng\n",
    "    ))\n",
    "Ldir = np.array(Ldir)\n",
    "print(\"Directional grad-Lipschitz proxy (5/50/95%):\", np.quantile(Ldir, [0.05, 0.5, 0.95]))\n",
    "\n",
    "# --- F) (OPTIONAL) GRADIENT SNR ACROSS MINIBATCHES FOR ONE DRAW ---\n",
    "DO_SNR = True  # set False to skip this section cleanly\n",
    "\n",
    "if DO_SNR:\n",
    "    # build batches from train set; ensure y is (B,1) in each batch\n",
    "    y_train_2d = y_train.reshape(-1, 1)\n",
    "    batches = make_minibatches(X_train, y_train_2d, batch_size=64, n_batches=10, seed=0)\n",
    "\n",
    "    s0 = 0\n",
    "    grad_list = []\n",
    "    for Xb, yb in batches:\n",
    "        _, grads_b = mse_loss_and_grads(\n",
    "            Xb, yb,\n",
    "            W1_draws[s0], b1_draws[s0],\n",
    "            W2_draws[s0], b2_draws[s0],\n",
    "            activation=activation\n",
    "        )\n",
    "        grad_list.append(pack_grads(grads_b))\n",
    "\n",
    "    snr = gradient_snr_over_batches(grad_list)\n",
    "    print(\"Gradient SNR across minibatches at draw s0:\", snr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RHS\n",
    "Posterior gradient norms (5/50/95%): [0.16470399 0.25879643 0.51681143]\n",
    "\n",
    "Posterior grad cosine vs mean (5/50/95%): [-0.00202803  0.53864411  0.72677177]\n",
    "\n",
    "Posterior losses (5/50/95%): [0.03460988 0.04023591 0.04680309]\n",
    "\n",
    "Directional grad-Lipschitz proxy (5/50/95%): [ 1.53809331  6.23787591 18.88394891]\n",
    "\n",
    "Gradient SNR across minibatches at draw s0: 0.2733637598423519\n",
    "\n",
    "## Dirichlet\n",
    "Posterior gradient norms (5/50/95%): [0.16137586 0.24824739 0.47044927]\n",
    "\n",
    "Posterior grad cosine vs mean (5/50/95%): [0.05011153 0.43383093 0.65442166]\n",
    "\n",
    "Posterior losses (5/50/95%): [0.03467333 0.03932857 0.04483365]\n",
    "\n",
    "Directional grad-Lipschitz proxy (5/50/95%): [ 1.07342317  6.41491771 16.73721671]\n",
    "\n",
    "Gradient SNR across minibatches at draw s0: 0.36852927989323614\n",
    "\n",
    "## BETA\n",
    "Posterior gradient norms (5/50/95%): [0.17097207 0.25707368 0.49875474]\n",
    "\n",
    "Posterior grad cosine vs mean (5/50/95%): [-0.07632456  0.56714935  0.7694549 ]\n",
    "\n",
    "Posterior losses (5/50/95%): [0.03445648 0.03909852 0.04472353]\n",
    "\n",
    "Directional grad-Lipschitz proxy (5/50/95%): [ 1.52483842  6.88174365 21.6660652 ]\n",
    "\n",
    "Gradient SNR across minibatches at draw s0: 0.40063152311073363\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
