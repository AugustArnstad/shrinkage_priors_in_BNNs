{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman\"\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "model_names_tanh_nodewise = [\"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\", \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]\n",
    "\n",
    "tanh_fits = {}\n",
    "tanh_fits_nodewise = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n",
    "    tanh_fit_nodewise = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_nodewise,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fits_nodewise[base_config_name] = tanh_fit_nodewise  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_correlated = f\"datasets/friedman_correlated\"\n",
    "results_dir_tanh_correlated = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "model_names_tanh_correlated = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "model_names_tanh_correlated_nodewise = [\"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\", \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]\n",
    "\n",
    "tanh_fits_correlated = {}\n",
    "tanh_fits_correlated_nodewise = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir_correlated) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    tanh_fit_correlated = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh_correlated,\n",
    "        models=model_names_tanh_correlated,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fits_correlated[base_config_name] = tanh_fit_correlated  # use clean key\n",
    "    \n",
    "    tanh_fit_correlated_nodewise = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh_correlated,\n",
    "        models=model_names_tanh_correlated_nodewise,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fits_correlated_nodewise[base_config_name] = tanh_fit_correlated_nodewise  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.generate_data import sample_gaussian_copula_uniform\n",
    "\n",
    "def generate_Friedman_data_v2(N=100, D=10, sigma=1.0, test_size=0.2, seed=42, standardize_y=True, return_scale=True):\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.uniform(0, 1, size=(N, D))\n",
    "    x0, x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5.0 * x4\n",
    "    )\n",
    "    y = y_clean + np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    if not standardize_y:\n",
    "        return (X_train, X_test, y_train, y_test) if not return_scale else (X_train, X_test, y_train, y_test, 0.0, 1.0)\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() if y_train.std() > 0 else 1.0\n",
    "\n",
    "    y_train_s = (y_train - y_mean) / y_std\n",
    "    y_test_s = (y_test - y_mean) / y_std\n",
    "\n",
    "    if return_scale:\n",
    "        return X_train, X_test, y_train_s, y_test_s, y_mean, y_std\n",
    "    return X_train, X_test, y_train_s, y_test_s\n",
    "\n",
    "def generate_correlated_Friedman_data_v2(N=100, D=10, sigma=1.0, test_size=0.2, seed=42, standardize_y=True, return_scale=True):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data for Bayesian neural network experiments.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples.\n",
    "        D (int): Number of features.\n",
    "        sigma (float): Noise level.\n",
    "        test_size (float): Proportion for test split.\n",
    "        seed (int): Random seed.\n",
    "        standardize_y (bool): Whether to standardize the response variable.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test, y_mean, y_std) if standardize_y,\n",
    "               else (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    d = 10\n",
    "    S_custom = np.eye(d)\n",
    "    # Block 1 (vars 0..4): high Spearman, 0.7\n",
    "    for i in range(0, 3):\n",
    "        for j in range(i+1, 3):\n",
    "            S_custom[i, j] = S_custom[j, i] = 0.8\n",
    "    # Block 2 (vars 5..9): moderate Spearman, 0.4\n",
    "    for i in range(5, 10):\n",
    "        for j in range(i+1, 10):\n",
    "            S_custom[i, j] = S_custom[j, i] = -0.5\n",
    "    # Cross-block weaker, 0.15\n",
    "    for i in range(0, 5):\n",
    "        for j in range(5, 10):\n",
    "            S_custom[i, j] = S_custom[j, i] = 0.15\n",
    "    # A couple of bespoke pairs:\n",
    "    S_custom[0, 9] = S_custom[9, 0] = 0.4\n",
    "    S_custom[2, 7] = S_custom[7, 2] = 0.9  # very strong (will be projected if infeasible)\n",
    "    S_custom[3, 4] = S_custom[4, 3] = -0.9  # very strong (will be projected if infeasible)\n",
    "    S_custom[1, 6] = S_custom[6, 1] = -0.9  # very strong (will be projected if infeasible)\n",
    "\n",
    "    U, _ = sample_gaussian_copula_uniform(n=10000, S=S_custom, random_state=123)\n",
    "    #X = np.random.uniform(0, 1, size=(N, D))\n",
    "    if N != U.shape[0]:\n",
    "        idx = np.random.choice(U.shape[0], size=N, replace=False)\n",
    "        X = U[idx, :]\n",
    "    else:\n",
    "        X = U\n",
    "\n",
    "    x0, x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5.0 * x4\n",
    "    )\n",
    "\n",
    "    y = y_clean + np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    if not standardize_y:\n",
    "        return (X_train, X_test, y_train, y_test) if not return_scale else (X_train, X_test, y_train, y_test, 0.0, 1.0)\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() if y_train.std() > 0 else 1.0\n",
    "\n",
    "    y_train_s = (y_train - y_mean) / y_std\n",
    "    y_test_s = (y_test - y_mean) / y_std\n",
    "\n",
    "    if return_scale:\n",
    "        return X_train, X_test, y_train_s, y_test_s, y_mean, y_std\n",
    "    return X_train, X_test, y_train_s, y_test_s\n",
    "\n",
    "def make_large_eval_set(\n",
    "    generator_fn,\n",
    "    N_train,\n",
    "    D,\n",
    "    sigma,\n",
    "    seed,\n",
    "    n_eval=5000,\n",
    "    standardize_y=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns X_eval, y_eval (standardized if standardize_y=True), plus y_mean,y_std\n",
    "    defined from the training split.\n",
    "    \"\"\"\n",
    "    N_total = N_train + n_eval\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te, y_mean, y_std = generator_fn(\n",
    "        N=N_total, D=D, sigma=sigma, test_size=n_eval / N_total, seed=seed,\n",
    "        standardize_y=standardize_y, return_scale=True\n",
    "    )\n",
    "    # Now X_te has approx n_eval points (exact given test_size construction).\n",
    "    return X_te, np.asarray(y_te).squeeze(), y_mean, y_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "def compute_sparse_rmse_results(seeds, models, all_fits, get_N_sigma, forward_pass, folder,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "    \n",
    "    def choose_gen(folder):\n",
    "        return generate_correlated_Friedman_data_v2 if \"friedman_correlated\" in folder else generate_Friedman_data_v2\n",
    "\n",
    "    for seed in seeds:\n",
    "\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "\n",
    "        # Build large eval set consistent with training split standardization\n",
    "        gen_fn = choose_gen(folder)\n",
    "        X_test, y_test, y_mean, y_std = make_large_eval_set(\n",
    "            generator_fn=gen_fn,\n",
    "            N_train=N,\n",
    "            D=10,\n",
    "            sigma=sigma,\n",
    "            seed=seed,\n",
    "            n_eval=500,\n",
    "            standardize_y=True\n",
    "        )\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "            #print(y_test.shape)\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                W2 = W2_samples[i]\n",
    "\n",
    "                # Apply pruning mask if requested\n",
    "                if prune_fn is not None and sparsity > 0.0:\n",
    "                    masks = prune_fn([W1, W2], sparsity)\n",
    "                    W1 = W1 * masks[0]\n",
    "                    #W2 = W2 * masks[1]\n",
    "\n",
    "                y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i])\n",
    "                y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "                rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2)) * y_std\n",
    "                \n",
    "            posterior_mean = np.mean(y_hats, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse * y_std\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'sparsity': sparsity,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "seeds = [1, 2, 11]\n",
    "seeds_correlated = [1, 6, 11]\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    if seed == 1:\n",
    "        N=100\n",
    "    elif seed == 2:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma\n",
    "\n",
    "def get_N_sigma_correlated(seed):\n",
    "    if seed == 1:\n",
    "        N=100\n",
    "    elif seed == 6:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse_tanh, df_posterior_rmse_tanh = {}, {}\n",
    "df_rmse_tanh_nodewise, df_posterior_rmse_tanh_nodewise = {}, {}\n",
    "df_rmse_tanh_correlated, df_posterior_rmse_tanh_correlated = {}, {}\n",
    "df_rmse_tanh_correlated_nodewise, df_posterior_rmse_tanh_correlated_nodewise = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    df_rmse_tanh[sparsity], df_posterior_rmse_tanh[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_tanh, tanh_fits, get_N_sigma, forward_pass_tanh, folder = \"friedman\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_tanh_nodewise[sparsity], df_posterior_rmse_tanh_nodewise[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_tanh_nodewise, tanh_fits_nodewise, get_N_sigma, forward_pass_tanh, folder = \"friedman\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_tanh_correlated[sparsity], df_posterior_rmse_tanh_correlated[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds_correlated, model_names_tanh_correlated, tanh_fits_correlated, get_N_sigma_correlated, forward_pass_tanh, folder = \"friedman_correlated\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_tanh_correlated_nodewise[sparsity], df_posterior_rmse_tanh_correlated_nodewise[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds_correlated, model_names_tanh_correlated_nodewise, tanh_fits_correlated_nodewise, get_N_sigma_correlated, forward_pass_tanh, folder = \"friedman_correlated\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df_rmse_full_tanh = pd.concat(\n",
    "#     [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_tanh.items()],\n",
    "#     ignore_index=True\n",
    "# )\n",
    "# df_rmse_full_tanh_nodewise = pd.concat(\n",
    "#     [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_tanh_nodewise.items()],\n",
    "#     ignore_index=True\n",
    "# )\n",
    "\n",
    "# df_rmse_full_tanh_correlated = pd.concat(\n",
    "#     [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_tanh_correlated.items()],\n",
    "#     ignore_index=True\n",
    "# )\n",
    "# df_rmse_full_tanh_correlated_nodewise = pd.concat(\n",
    "#     [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_tanh_correlated_nodewise.items()],\n",
    "#     ignore_index=True\n",
    "# )\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_post_tanh_full = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_posterior_rmse_tanh.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "df_post_tanh_full_nodewise = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_posterior_rmse_tanh_nodewise.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_post_tanh_full_corr = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_posterior_rmse_tanh_correlated.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "df_post_tanh_full_corr_nodewise = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_posterior_rmse_tanh_correlated_nodewise.items()],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_post_tanh_full, df_post_tanh_full_nodewise])\n",
    "\n",
    "df_all_corr = pd.concat([df_post_tanh_full_corr, df_post_tanh_full_corr_nodewise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = {\n",
    "    \"Gaussian tanh\": \"C0\",\n",
    "    \"Regularized Horseshoe tanh\": \"C1\",\n",
    "    \"Dirichlet Horseshoe tanh\": \"C2\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"C2\",\n",
    "    \"Dirichlet Student T tanh\": \"C3\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"C3\",\n",
    "    \"Beta Horseshoe tanh\": \"C4\",\n",
    "    \"Beta Horseshoe tanh nodewise\": \"C4\",\n",
    "    \"Beta Student T tanh\": \"C5\",\n",
    "    \"Beta Student T tanh nodewise\": \"C5\",                 \n",
    "}\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "rmse_col = \"posterior_mean_rmse\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "Ns = [100, 200, 500]\n",
    "dfs = [(df_all, \"Uncorrelated\"), (df_all_corr, \"Correlated\")]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 7), sharex=True, sharey=\"row\")\n",
    "\n",
    "for r, (df_src, row_title) in enumerate(dfs):\n",
    "    for c, N in enumerate(Ns):\n",
    "        ax = axes[r, c]\n",
    "        df = df_src[(df_src[\"N\"] == N) & (df_src[\"sparsity\"].isin(sparsity_levels))].copy()\n",
    "        df[\"sparsity\"] = pd.Categorical(df[\"sparsity\"], categories=sparsity_levels, ordered=True)\n",
    "\n",
    "        for model, g in df.groupby(\"model\", sort=False):\n",
    "            # put your include/exclude logic here\n",
    "            if model in [\"Gaussian tanh\", \"Dirichlet Student T tanh\", \"Beta Student T tanh\",\n",
    "                         \"Dirichlet Student T tanh nodewise\", \"Beta Student T tanh nodewise\"]:\n",
    "            # if model in [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Beta Student T tanh\", \"Beta Horseshoe tanh\",\n",
    "            #              \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]:\n",
    "                continue\n",
    "\n",
    "            g = g.sort_values(\"sparsity\")\n",
    "            lw, alpha, ls, mk = (1.3, 0.7, \"--\", \"v\") if \"nodewise\" in model else (2.2, 1.0, \"-\", \"o\")\n",
    "            ax.plot(g[\"sparsity\"], g[rmse_col], lw=lw, alpha=alpha, ls=ls, marker=mk,\n",
    "                    color=colors[model], label=model)\n",
    "\n",
    "        ax.set_title(f\"N={N}\" if r == 0 else \"\")\n",
    "        ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "        ax.set_xticks(sparsity_levels[: -1])\n",
    "\n",
    "        if c == 0:\n",
    "            ax.set_ylabel(f\"{row_title}\\nRMSE\")\n",
    "        if r == 1:\n",
    "            ax.set_xlabel(\"sparsity\")\n",
    "\n",
    "# one legend for the whole figure (deduped)\n",
    "handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "axes[1, 1].legend(by_label.values(), by_label.keys(),\n",
    "            loc=\"upper left\",\n",
    "            #bbox_to_anchor=(1.02, 0.5),\n",
    "            frameon=False)\n",
    "axes[0, 1].legend(by_label.values(), by_label.keys(),\n",
    "            loc=\"upper left\",\n",
    "            #bbox_to_anchor=(1.02, 0.5),\n",
    "            frameon=False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = {\n",
    "    \"Gaussian tanh\": \"C0\",\n",
    "    \"Regularized Horseshoe tanh\": \"C1\",\n",
    "    \"Dirichlet Horseshoe tanh\": \"C2\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"C2\",\n",
    "    \"Dirichlet Student T tanh\": \"C3\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"C3\",\n",
    "    \"Beta Horseshoe tanh\": \"C4\",\n",
    "    \"Beta Horseshoe tanh nodewise\": \"C4\",\n",
    "    \"Beta Student T tanh\": \"C5\",\n",
    "    \"Beta Student T tanh nodewise\": \"C5\",                 \n",
    "}\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "rmse_col = \"posterior_mean_rmse\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "Ns = [100, 200, 500]\n",
    "dfs = [(df_all, \"Uncorrelated\"), (df_all_corr, \"Correlated\")]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 7), sharex=True, sharey='row')\n",
    "\n",
    "for r, (df_src, row_title) in enumerate(dfs):\n",
    "    for c, N in enumerate(Ns):\n",
    "        ax = axes[r, c]\n",
    "        df = df_src[(df_src[\"N\"] == N) & (df_src[\"sparsity\"].isin(sparsity_levels))].copy()\n",
    "        df[\"sparsity\"] = pd.Categorical(df[\"sparsity\"], categories=sparsity_levels, ordered=True)\n",
    "\n",
    "        for model, g in df.groupby(\"model\", sort=False):\n",
    "            # put your include/exclude logic here\n",
    "            if model in [\"Gaussian tanh\", \"Dirichlet Horseshoe tanh\", \"Beta Horseshoe tanh\",\n",
    "                         \"Beta Horseshoe tanh nodewise\", \"Dirichlet Horseshoe tanh nodewise\"]:\n",
    "                continue\n",
    "\n",
    "            g = g.sort_values(\"sparsity\")\n",
    "            lw, alpha, ls, mk = (1.3, 0.7, \"--\", \"v\") if \"nodewise\" in model else (2.2, 1.0, \"-\", \"o\")\n",
    "            ax.plot(g[\"sparsity\"], g[rmse_col], lw=lw, alpha=alpha, ls=ls, marker=mk,\n",
    "                    color=colors[model], label=model)\n",
    "\n",
    "        ax.set_title(f\"N={N}\" if r == 0 else \"\")\n",
    "        ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "        ax.set_xticks(sparsity_levels[:-1])\n",
    "\n",
    "        if c == 0:\n",
    "            ax.set_ylabel(f\"{row_title}\\nRMSE\")\n",
    "        if r == 1:\n",
    "            ax.set_xlabel(\"sparsity\")\n",
    "\n",
    "# one legend for the whole figure (deduped)\n",
    "handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "axes[1, 1].legend(by_label.values(), by_label.keys(),\n",
    "            loc=\"upper left\",\n",
    "            #bbox_to_anchor=(1.02, 0.5),\n",
    "            frameon=False)\n",
    "axes[0, 1].legend(by_label.values(), by_label.keys(),\n",
    "            loc=\"upper left\",\n",
    "            #bbox_to_anchor=(1.02, 0.5),\n",
    "            frameon=False)\n",
    "\n",
    "#plt.ylim((0.1, 2))\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = {\n",
    "    \"Gaussian tanh\": \"C0\",\n",
    "    \"Regularized Horseshoe tanh\": \"C1\",\n",
    "    \"Dirichlet Horseshoe tanh\": \"C2\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"C2\",\n",
    "    \"Dirichlet Student T tanh\": \"C3\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"C3\",\n",
    "    \"Beta Horseshoe tanh\": \"C4\",\n",
    "    \"Beta Horseshoe tanh nodewise\": \"C4\",\n",
    "    \"Beta Student T tanh\": \"C5\",\n",
    "    \"Beta Student T tanh nodewise\": \"C5\",                 \n",
    "}\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "rmse_col = \"posterior_mean_rmse\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "Ns = [100, 200, 500]\n",
    "dfs = [(df_all, \"Uncorrelated\"), (df_all_corr, \"Correlated\")]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 7), sharex=True, sharey=True)\n",
    "\n",
    "for r, (df_src, row_title) in enumerate(dfs):\n",
    "    for c, N in enumerate(Ns):\n",
    "        ax = axes[r, c]\n",
    "        df = df_src[(df_src[\"N\"] == N) & (df_src[\"sparsity\"].isin(sparsity_levels))].copy()\n",
    "        df[\"sparsity\"] = pd.Categorical(df[\"sparsity\"], categories=sparsity_levels, ordered=True)\n",
    "\n",
    "        for model, g in df.groupby(\"model\", sort=False):\n",
    "            # put your include/exclude logic heres\n",
    "            if model in [\"Gaussian tanh\", \"Beta Student T tanh\", \"Beta Horseshoe tanh\",\n",
    "                         \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\", ]:\n",
    "                continue\n",
    "\n",
    "            g = g.sort_values(\"sparsity\")\n",
    "            lw, alpha, ls, mk = (1.3, 0.7, \"--\", \"v\") if \"nodewise\" in model else (2.2, 1.0, \"-\", \"o\")\n",
    "            ax.plot(g[\"sparsity\"], g[rmse_col], lw=lw, alpha=alpha, ls=ls, marker=mk,\n",
    "                    color=colors[model], label=model)\n",
    "\n",
    "        ax.set_title(f\"N={N}\" if r == 0 else \"\")\n",
    "        ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "        ax.set_xticks(sparsity_levels[:-1])\n",
    "\n",
    "        if c == 0:\n",
    "            ax.set_ylabel(f\"{row_title}\\nRMSE\")\n",
    "        if r == 1:\n",
    "            ax.set_xlabel(\"sparsity\")\n",
    "\n",
    "# one legend for the whole figure (deduped)\n",
    "handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "axes[1, 1].legend(by_label.values(), by_label.keys(),\n",
    "            loc=\"upper left\",\n",
    "            #bbox_to_anchor=(1.02, 0.5),\n",
    "            frameon=False)\n",
    "axes[0, 1].legend(by_label.values(), by_label.keys(),\n",
    "            loc=\"upper left\",\n",
    "            #bbox_to_anchor=(1.02, 0.5),\n",
    "            frameon=False)\n",
    "\n",
    "#plt.ylim((0.1, 2))\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
