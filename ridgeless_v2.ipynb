{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_latent_data_sec54(n, p, d=20, r_theta=1.0, sigma_xi=0.0, rng=None):\n",
    "    \"\"\"\n",
    "    Section 5.4 latent model (Hastie–Montanari–Rosset–Tibshirani):\n",
    "      X = Z W^T + U,   y = Z θ + ξ\n",
    "      z_i ~ N(0, I_d), u_ij ~ N(0, 1), ξ_i ~ N(0, σ_ξ^2)\n",
    "    Rows w_j of W satisfy ||w_j|| = 1.               [Fig. 5/6 setup]\n",
    "    Population mapping to linear model:\n",
    "      Σ = I_p + W W^T,   β = W (I + W^T W)^{-1} θ.   [eqs. (26)-(27)]\n",
    "    Returns: X (n×p), y (n,), W (p×d), theta (d,), beta_true (p,), Sigma (p×p)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    # Random W with unit-norm rows (||w_j||=1)\n",
    "    W = rng.normal(size=(p, d))\n",
    "    W /= np.linalg.norm(W, axis=1, keepdims=True) + 1e-12  # enforce ||w_j||=1\n",
    "\n",
    "    # Latent Z, feature noise U, label noise ξ\n",
    "    Z = rng.normal(size=(n, d))\n",
    "    U = rng.normal(size=(n, p))\n",
    "    xi = rng.normal(scale=sigma_xi, size=n)\n",
    "\n",
    "    # Signal vector θ with ||θ|| = r_theta\n",
    "    theta = rng.normal(size=d)\n",
    "    theta *= r_theta / (np.linalg.norm(theta) + 1e-12)\n",
    "\n",
    "    # Data\n",
    "    X = Z @ W.T + U\n",
    "    y = Z @ theta + xi\n",
    "\n",
    "    # Population quantities for risk\n",
    "    Sigma = np.eye(p) + W @ W.T\n",
    "    beta_true = W @ np.linalg.solve(np.eye(d) + W.T @ W, theta)  # β = W (I + W^T W)^(-1) θ\n",
    "\n",
    "    return X, y, W, theta, beta_true, Sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit_min_norm(X, y):\n",
    "    \"\"\"\n",
    "    Minimum-ℓ2-norm least squares: β̂ = X^+ y (ridgeless limit of ridge).\n",
    "    \"\"\"\n",
    "    return np.linalg.pinv(X) @ y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_hidden_features_tanh_with_W1(\n",
    "    X,\n",
    "    rng,\n",
    "    H,\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = X.shape\n",
    "    tau0 = p_0 / (P - p_0)\n",
    "    alpha = np.full(P, alpha_scale)\n",
    "\n",
    "    tau = np.abs(rng.standard_cauchy()) * tau0\n",
    "    c_sq = 1.0 / rng.gamma(shape=a, scale=1.0 / b, size=H)\n",
    "    lambda_data = np.abs(rng.standard_cauchy(size=(H, P)))\n",
    "    phi_data = rng.dirichlet(alpha, size=H)\n",
    "\n",
    "    lam_sq = lambda_data**2\n",
    "    denom = c_sq[:, None] + lam_sq * (tau**2)\n",
    "    lambda_tilde = (c_sq[:, None] * lam_sq) / denom\n",
    "    lambda_tilde = np.maximum(lambda_tilde, 1e-12)\n",
    "\n",
    "    W1_raw = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    stddev = tau * np.sqrt(lambda_tilde.T) * np.sqrt(phi_data.T)  # (P,H)\n",
    "    W1 = W1_raw * stddev\n",
    "\n",
    "    Z = np.tanh(X @ W1)\n",
    "    return Z, W1\n",
    "\n",
    "def plot_risk_curve_hidden_units(\n",
    "    n=400,\n",
    "    gammas=(0.7, 0.9, 1.2, 1.5, 2, 3, 5, 8, 12, 20),\n",
    "    d=20,\n",
    "    r_theta=1.0,\n",
    "    sigma_xi=0.0,\n",
    "    reps=50,\n",
    "    risk_mc_samples=1000,\n",
    "    seed=0,\n",
    "    # prior hyperparams\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.5,\n",
    "    # NEW: schedule H as a function of (p, n)\n",
    "    H_of_p=lambda p, n: p,   # <- default ties H to p (so H grows with γ)\n",
    "):\n",
    "    \"\"\"\n",
    "    Same as before, but H now depends on p (and n) via H_of_p.\n",
    "    We reuse the same W1 for train and population risk.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    G, M, S = [], [], []\n",
    "\n",
    "    for gamma in gammas:\n",
    "        p = max(1, int(round(gamma * n)))\n",
    "        H = max(1, int(H_of_p(p, n)))\n",
    "        risks = []\n",
    "\n",
    "        for _ in range(reps):\n",
    "            # Your data generator\n",
    "            X, y, W, theta, beta_true, Sigma = make_latent_data_sec54(\n",
    "                n=n, p=p, d=d, r_theta=r_theta, sigma_xi=sigma_xi, rng=rng\n",
    "            )\n",
    "\n",
    "            # One sampled hidden map W1 for this rep; reuse it for population risk\n",
    "            Z, W1 = sample_hidden_features_tanh_with_W1(\n",
    "                X, rng, H=H, p_0=p_0, a=a, b=b, alpha_scale=alpha_scale\n",
    "            )\n",
    "\n",
    "            # Min-norm on hidden units\n",
    "            w_hat = fit_min_norm(Z, y)\n",
    "            if w_hat.ndim > 1 and w_hat.shape[1] == 1:\n",
    "                w_hat = w_hat.ravel()\n",
    "\n",
    "            # Monte Carlo population risk with the SAME W1\n",
    "            X_pop = rng.multivariate_normal(mean=np.zeros(p), cov=Sigma, size=risk_mc_samples)\n",
    "            Z_pop = np.tanh(X_pop @ W1)\n",
    "            y_true_pop = X_pop @ beta_true\n",
    "            y_pred_pop = Z_pop @ w_hat\n",
    "            risks.append(float(np.mean((y_pred_pop - y_true_pop) ** 2)))\n",
    "\n",
    "        G.append(gamma)\n",
    "        M.append(np.mean(risks))\n",
    "        S.append(np.std(risks, ddof=1))\n",
    "\n",
    "    return np.array(G), np.array(M), np.array(S)\n",
    "\n",
    "G_h, M_h, S_h = plot_risk_curve_hidden_units(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    gammas=[0.3, 0.7, 0.9, 1.5, 2, 3, 5, 8, 10],\n",
    "    reps=50, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reps = 50\n",
    "ci_h = 1.96 * S_h / np.sqrt(reps)\n",
    "\n",
    "plt.figure(figsize=(6.4, 4.4))\n",
    "plt.plot(G_h, M_h, marker=\"o\", linewidth=2)\n",
    "plt.fill_between(G_h, M_h - ci_h, M_h + ci_h, alpha=0.2)\n",
    "plt.axvline(1.0, linestyle=\"--\", linewidth=1)\n",
    "plt.xlabel(r\"Aspect ratio  $\\gamma = p/n$\")\n",
    "plt.ylabel(r\"Population risk  $R_X=(\\hat\\beta-\\beta)^\\top \\Sigma (\\hat\\beta-\\beta)$\")\n",
    "plt.title(f\"Latent space (§5.4) — min-norm risk vs γ  (n={400}, d={20}, r={1}, $\\sigma_ξ$={0})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_risk_curve_sec54(\n",
    "    n=400,\n",
    "    gammas=(0.7, 0.9, 1.2, 1.5, 2, 3, 5, 8, 12, 20),\n",
    "    d=20,\n",
    "    r_theta=1.0,          # \"r = 1\" in the captions\n",
    "    sigma_xi=0.0,         # use 0 for Fig. 5 behavior; try 0, 0.25, 0.5 like Fig. 6\n",
    "    reps=50,\n",
    "    seed=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Replicates the latent-space risk curve of §5.4 (Figs. 5–6):\n",
    "      For each γ = p/n, simulate (X,y), fit min-norm β̂, and compute\n",
    "      population risk R_X = (β̂−β)^T Σ (β̂−β), then average across reps.\n",
    "    Expectation from §5.4: spike near γ≈1 and then *monotone decrease* for γ>1,\n",
    "    reaching a global minimum as γ→∞ when β aligns with top eigenspace of Σ. \n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    G, M, S = [], [], []\n",
    "\n",
    "    for gamma in gammas:\n",
    "        p = max(1, int(round(gamma * n)))\n",
    "        risks = []\n",
    "\n",
    "        for _ in range(reps):\n",
    "            X, y, W, theta, beta_true, Sigma = make_latent_data_sec54(\n",
    "                n=n, p=p, d=d, r_theta=r_theta, sigma_xi=sigma_xi, rng=rng\n",
    "            )\n",
    "            beta_hat = fit_min_norm(X, y)\n",
    "\n",
    "            diff = beta_hat - beta_true\n",
    "            risks.append(float(diff @ (Sigma @ diff)))\n",
    "\n",
    "        G.append(gamma)\n",
    "        M.append(np.mean(risks))\n",
    "        S.append(np.std(risks, ddof=1))\n",
    "\n",
    "    G, M, S = np.array(G), np.array(M), np.array(S)\n",
    "    ci = 1.96 * S / np.sqrt(reps)\n",
    "    \n",
    "    return G, M, S\n",
    "\n",
    "# Example:\n",
    "G, M, S = plot_risk_curve_sec54(n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "                    gammas=[0.3, 0.7, 0.9, 1.5, 2, 3, 5, 8, 10], reps=50)\n",
    "# For a Fig. 6-style panel with noise: try sigma_xi in {0.0, 0.25, 0.5}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 50\n",
    "ci = 1.96 * S / np.sqrt(reps)\n",
    "\n",
    "plt.figure(figsize=(6.4, 4.4))\n",
    "plt.plot(G, M, marker=\"o\", linewidth=2)\n",
    "plt.fill_between(G, M - ci, M + ci, alpha=0.2)\n",
    "plt.axvline(1.0, linestyle=\"--\", linewidth=1)\n",
    "plt.xlabel(r\"Aspect ratio  $\\gamma = p/n$\")\n",
    "plt.ylabel(r\"Population risk  $R_X=(\\hat\\beta-\\beta)^\\top \\Sigma (\\hat\\beta-\\beta)$\")\n",
    "plt.title(f\"Latent space (§5.4) — min-norm risk vs γ  (n={400}, d={20}, r={1}, $\\sigma_ξ$={0})\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, W, theta, beta_true, Sigma = make_latent_data_sec54(\n",
    "n=400, p=2, d=20, r_theta=1, sigma_xi=0, rng=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_hat = fit_min_norm(H_output, y_train)\n",
    "diff = beta_hat - beta_true\n",
    "float(diff @ (Sigma @ diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from cmdstanpy import set_cmdstan_path\n",
    "\n",
    "def run_regression_model_local(model_name, config_name, X_train, X_test, y_train, y_test, args):\n",
    "    from cmdstanpy import CmdStanModel\n",
    "    from utils.stan_data_generator import make_stan_data\n",
    "    from utils.io_helpers import save_metadata\n",
    "    import os, shutil\n",
    "    import numpy as np\n",
    "    \n",
    "    # Set seed for reproducibility if provided\n",
    "    seed = getattr(args, 'seed', None)\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    task = \"prior\"\n",
    "    args.num_classes = 1  # Still needed\n",
    "\n",
    "    stan_data = make_stan_data(model_name, task, X_train, y_train, X_test, args)\n",
    "\n",
    "    model_path = f\"bnn_prior_models_double_descent/{model_name}.stan\"\n",
    "    model = CmdStanModel(stan_file=model_path, force_compile=True)\n",
    "\n",
    "    fit = model.sample(\n",
    "        data=stan_data,\n",
    "        chains=4,\n",
    "        iter_sampling=args.samples,\n",
    "        iter_warmup=args.burnin_samples,\n",
    "        adapt_delta=0.8,\n",
    "        parallel_chains=4,\n",
    "        show_console=False,\n",
    "        #max_treedepth = 12,\n",
    "    )\n",
    "    \n",
    "    if args.data_config == \"uci\": \n",
    "        if args.standardize:\n",
    "            output_dir = os.path.join(\n",
    "            args.model_output_dir, \"standardized\"\n",
    "        )\n",
    "        else:\n",
    "            output_dir = os.path.join(\n",
    "                args.model_output_dir\n",
    "            )\n",
    "    else:\n",
    "        output_dir = os.path.join(\n",
    "            args.model_output_dir\n",
    "        )\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    save_metadata(output_dir, args, config_name)\n",
    "\n",
    "    for i, path in enumerate(fit.runset.csv_files, start=1):\n",
    "        shutil.copy(path, os.path.join(output_dir, f\"chain_{i}.csv\"))\n",
    "\n",
    "    print(f\"[✓] Saved results to: {output_dir}\")\n",
    "\n",
    "\n",
    "# Sett CmdStan-stien\n",
    "set_cmdstan_path(\"/Users/augustarnstad/.cmdstan/cmdstan-2.36.0\")\n",
    "run_regression_model_local(\n",
    "    model_name=\"gaussian\",\n",
    "    config_name=\"latent_model\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    args=argparse.Namespace(\n",
    "        N=X_train.shape[0],\n",
    "        p=X_train.shape[1],\n",
    "        sigma=None,\n",
    "        data=\"\",\n",
    "        standardize=False,\n",
    "        test_shift=None,\n",
    "        model=\"gaussian\",\n",
    "        H=800,\n",
    "        L=1,\n",
    "        config=\"Latent\",\n",
    "        seed=1,\n",
    "        data_config=\"realworld\",\n",
    "        model_output_dir=\"results/ridgeless/double_descent/priors/gaussian_tanh\",\n",
    "        burnin_samples=1,\n",
    "        samples=500,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_tanh = \"results/ridgeless/double_descent/priors\"\n",
    "model_names_tanh = [\"Gaussian tanh\"]#, \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=\"\",\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_gauss  = tanh_fit['Gaussian tanh']['posterior']\n",
    "# post_RHS = tanh_fit['Regularized Horseshoe tanh']['posterior']\n",
    "# post_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior']\n",
    "# post_DST = tanh_fit['Dirichlet Student T tanh']['posterior']\n",
    "\n",
    "w1 = post_gauss.stan_variable(\"W_1\")\n",
    "b1 = post_gauss.stan_variable(\"hidden_bias\")\n",
    "\n",
    "W1= w1[0]\n",
    "B1=b1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# hidden layer pre-activation\n",
    "H_train = X_train @ W1.T + B1        # shape (n_train, m)\n",
    "H_test  = X_test  @ W1.T + B1        # shape (n_test, m)\n",
    "\n",
    "# apply tanh nonlinearity\n",
    "H_train = np.tanh(H_train)\n",
    "H_test  = np.tanh(H_test)\n",
    "\n",
    "# optional stabilization: scale by sqrt(m) so width is comparable\n",
    "m = W1.shape[0]\n",
    "H_train = H_train / np.sqrt(m)\n",
    "H_test  = H_test / np.sqrt(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridgeless: a_hat = H_train^+ y_train (minimum-norm readout)\n",
    "a_hat = np.linalg.pinv(H_train) @ y_train     # shape (m,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = H_test @ a_hat\n",
    "test_mse = np.mean((y_pred_test - y_test)**2)\n",
    "\n",
    "print(\"Test MSE for THIS ONE SAMPLE:\", test_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation (match your BNN; choose 'relu' or 'tanh')\n",
    "def activation(U, kind=\"relu\"):\n",
    "    if kind == \"relu\":\n",
    "        return np.maximum(0.0, U)\n",
    "    elif kind == \"tanh\":\n",
    "        return np.tanh(U)\n",
    "    else:\n",
    "        raise ValueError(\"Activation must be 'relu' or 'tanh'.\")\n",
    "\n",
    "# Build hidden features for a given (W1, b1) sample and a *chosen* width m_use (<= full width m_full)\n",
    "def build_features(Z, W1, b1, m_use=None, act=\"relu\", standardize=True):\n",
    "    \"\"\"\n",
    "    Z: (n,d) latent inputs from §5.4\n",
    "    W1: (m_full, d)   b1: (m_full,)\n",
    "    m_use: number of hidden units to keep (sweeps γ = m_use / n)\n",
    "    Returns H: (n, m_use) design matrix for readout.\n",
    "    \"\"\"\n",
    "    m_full = W1.shape[0]\n",
    "    m = m_full if m_use is None else int(m_use)\n",
    "    assert 1 <= m <= m_full\n",
    "    # Use first m units (you can also random-subselect or permute for robustness)\n",
    "    W1m = W1[:m, :]\n",
    "    b1m = b1[:m]\n",
    "    U = Z @ W1m.T + b1m  # pre-activation\n",
    "    H = activation(U, act)\n",
    "    # Scale features to be width-invariant and comparable across priors\n",
    "    # 1) column standardize (unit variance), 2) 1/sqrt(m) scaling\n",
    "    if standardize:\n",
    "        std = H.std(axis=0, ddof=1)\n",
    "        std[std < 1e-8] = 1.0\n",
    "        H = (H - H.mean(axis=0)) / std\n",
    "    H = H / np.sqrt(m)\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def min_norm_fit(H, y):\n",
    "    \"\"\"Ridgeless least squares readout for hidden features: a_hat = H^+ y.\"\"\"\n",
    "    return np.linalg.pinv(H) @ y\n",
    "\n",
    "def test_mse_for_sample(W1, b1, Z_tr, y_tr, Z_te, y_te, m_use, act=\"relu\"):\n",
    "    \"\"\"\n",
    "    Given one prior sample (W1,b1), build H_tr/H_te with m_use units, fit min-norm readout,\n",
    "    and return test MSE on fresh latent inputs Z_te (same §5.4 labeling rule).\n",
    "    \"\"\"\n",
    "    H_tr = build_features(Z_tr, W1, b1, m_use=m_use, act=act, standardize=True)\n",
    "    a_hat = min_norm_fit(H_tr, y_tr)\n",
    "\n",
    "    H_te = build_features(Z_te, W1, b1, m_use=m_use, act=act, standardize=True)\n",
    "    y_pred = H_te @ a_hat\n",
    "    return float(np.mean((y_pred - y_te)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prior_predictive_risk_curve(\n",
    "    W1_list, b1_list,                     # your S samples, all with the same full width m_full and input dim d\n",
    "    n_train=400, n_test=5000, d=20,\n",
    "    widths=None,                          # list of m_use values; by default a sweep around interpolation\n",
    "    act=\"relu\", sigma_xi=0.0, r_theta=1.0,\n",
    "    reps_data=3,                          # average over a few train/test redraws for stability\n",
    "    seed=0\n",
    "):\n",
    "    \"\"\"\n",
    "    For each width m_use (γ = m_use / n_train), average test MSE over:\n",
    "      - reps_data redraws of (Z_tr, y_tr, Z_te, y_te) per §5.4\n",
    "      - all S prior samples (W1,b1)\n",
    "    Returns (gammas, mean_risk, std_risk)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    S = len(W1_list)\n",
    "    m_full = W1_list[0].shape[0]\n",
    "\n",
    "    if widths is None:\n",
    "        # Sweep below/near/above interpolation threshold γ≈1\n",
    "        widths = [max(1, int(w)) for w in (0.5*n_train, 0.8*n_train, 0.95*n_train, n_train,\n",
    "                                           1.1*n_train, 1.5*n_train, 2*n_train, 3*n_train, 5*n_train)]\n",
    "        widths = [min(int(m_full), w) for w in widths]\n",
    "        widths = sorted(set(widths))\n",
    "\n",
    "    gammas = [w / n_train for w in widths]\n",
    "    mean_risks, std_risks = [], []\n",
    "\n",
    "    for m_use in widths:\n",
    "        risks = []\n",
    "        for _ in range(reps_data):\n",
    "            # Fresh §5.4 data each rep (same for all S samples)\n",
    "            Z_tr, y_tr, theta = make_sec54_data(n_train, d=d, r_theta=r_theta, sigma_xi=sigma_xi, rng=rng)\n",
    "            Z_te, y_te, _     = make_sec54_data(n_test,  d=d, r_theta=r_theta, sigma_xi=sigma_xi, rng=rng)\n",
    "\n",
    "            # Average across prior samples\n",
    "            for W1, b1 in zip(W1_list, b1_list):\n",
    "                risks.append(test_mse_for_sample(W1, b1, Z_tr, y_tr, Z_te, y_te, m_use=m_use, act=act))\n",
    "\n",
    "        mean_risks.append(float(np.mean(risks)))\n",
    "        std_risks.append(float(np.std(risks, ddof=1)))\n",
    "\n",
    "    # Plot\n",
    "    ci = 1.96 * (np.array(std_risks) / np.sqrt(S * reps_data))\n",
    "    plt.figure(figsize=(6.6, 4.6))\n",
    "    plt.plot(gammas, mean_risks, marker=\"o\", linewidth=2)\n",
    "    plt.fill_between(gammas, np.array(mean_risks) - ci, np.array(mean_risks) + ci, alpha=0.2)\n",
    "    plt.axvline(1.0, linestyle=\"--\", linewidth=1)\n",
    "    plt.xlabel(r\"Aspect ratio  $\\gamma = m / n_{\\rm train}$  (width / sample size)\")\n",
    "    plt.ylabel(\"Test MSE on §5.4 latent task\")\n",
    "    plt.title(f\"Prior-predictive descent profile (S={S}, reps={reps_data}, act={act})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return np.array(gammas), np.array(mean_risks), np.array(std_risks)\n",
    "\n",
    "# --- Example usage (one prior family) ---\n",
    "# Suppose you already have S samples: W1_list = [W1_s], b1_list = [b1_s]\n",
    "# Each W1_s: shape (m_full, d), each b1_s: shape (m_full,)\n",
    "# gammas, mean_risk, std_risk = prior_predictive_risk_curve(W1_list, b1_list, d=W1_list[0].shape[1],\n",
    "#                                                           n_train=400, n_test=5000, act=\"relu\",\n",
    "#                                                           sigma_xi=0.0, r_theta=1.0, reps_data=3, seed=123)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
