{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\noindent\n",
    "We consider the latent-factor model of Hastie--Montanari--Rosset--Tibshirani (§5.4). Let $z_i \\in \\mathbb{R}^d$ be latent factors, $u_i \\in \\mathbb{R}^p$ feature noise, and $\\xi_i$ response noise. The observed feature vectors and responses are\n",
    "\\begin{equation}\n",
    "x_i = W z_i + u_i, \\qquad y_i = z_i^\\top \\theta + \\xi_i,\n",
    "\\end{equation}\n",
    "where $W \\in \\mathbb{R}^{p \\times d}$ has row-wise unit norm $\\|w_j\\|=1$, the latent signal direction $\\theta \\in \\mathbb{R}^d$ satisfies $\\|\\theta\\| = r_\\theta$, and\n",
    "\\begin{equation}\n",
    "z_i \\sim \\mathcal{N}(0,I_d), \\qquad u_i \\sim \\mathcal{N}(0,I_p), \\qquad \\xi_i \\sim \\mathcal{N}(0,\\sigma_\\xi^2).\n",
    "\\end{equation}\n",
    "Stacking rows gives\n",
    "\\begin{equation}\n",
    "X = Z W^\\top + U, \\qquad y = Z\\theta + \\xi.\n",
    "\\end{equation}\n",
    "Although the response is generated from latent variables, there exists an equivalent population linear model in the observed coordinates. Define the population covariance\n",
    "\\begin{equation}\n",
    "\\Sigma = \\mathbb{E}[x_i x_i^\\top] = I_p + W W^\\top,\n",
    "\\end{equation}\n",
    "and the corresponding population regression coefficient\n",
    "\\begin{equation}\n",
    "\\beta_{\\ast} = W \\bigl(I_d + W^\\top W\\bigr)^{-1}\\theta.\n",
    "\\end{equation}\n",
    "Then\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[y \\mid x] = x^\\top \\beta_{\\ast},\n",
    "\\end{equation}\n",
    "so $\\beta_{\\ast}$ is the unique population-optimal linear predictor. Importantly, $\\beta_{\\ast}$ lies entirely in the $d$-dimensional signal subspace $\\mathcal{S} = \\mathrm{span}(W) \\subset \\mathbb{R}^p$.\n",
    "\n",
    "In the fitted model, we estimate $\\beta$ by the minimum-norm least squares estimator\n",
    "\\begin{equation}\n",
    "\\widehat{\\beta} = X^{+} y,\n",
    "\\end{equation}\n",
    "where $X^{+}$ is the Moore--Penrose pseudoinverse. When $p<n$, this reduces to ordinary least squares\n",
    "\\begin{equation}\n",
    "\\widehat{\\beta} = (X^\\top X)^{-1}X^\\top y,\n",
    "\\end{equation}\n",
    "while when $p\\ge n$ and $\\mathrm{rank}(X)=n$, it interpolates the data:\n",
    "\\begin{equation}\n",
    "\\widehat{\\beta} = X^\\top (X X^\\top)^{-1} y.\n",
    "\\end{equation}\n",
    "\n",
    "The performance of the estimator is measured using the population risk\n",
    "\\begin{equation}\n",
    "R = (\\widehat{\\beta} - \\beta_{\\ast})^\\top \\Sigma\\,(\\widehat{\\beta} - \\beta_{\\ast}).\n",
    "\\end{equation}\n",
    "\n",
    "The signal $\\beta_{\\ast}$ lies in the low-dimensional subspace $\\mathcal{S}$, while $X$ includes noise directions orthogonal to this subspace. The minimum-norm estimator does not impose knowledge of $\\mathcal{S}$, and therefore its alignment with $\\mathcal{S}$ depends critically on the aspect ratio\n",
    "\\begin{equation}\n",
    "\\gamma = \\frac{p}{n}.\n",
    "\\end{equation}\n",
    "When $\\gamma < 1$, the estimator behaves like classical least squares and its variance grows as $\\gamma \\to 1$. At $\\gamma = 1$, the smallest singular value of $X$ tends to zero, causing the risk $R$ to diverge. For $\\gamma > 1$, the minimum-norm constraint forces $\\widehat{\\beta}$ to place progressively less mass in the noise subspace $\\mathcal{S}^\\perp$, improving alignment with $\\mathcal{S}$ and decreasing the population risk. Consequently, the risk exhibits the characteristic double-descent shape: an initial increase near $\\gamma = 1$, followed by monotone decrease as $\\gamma$ grows and the estimator aligns with the latent signal subspace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now replace the direct linear regression on $X$ with a single-hidden-layer neural network feature map. For each choice of $p$ and $n$, we draw a random first-layer weight matrix $W_1 \\in \\mathbb{R}^{p \\times H}$ from a specified prior, and define the hidden-unit representation\n",
    "\\begin{equation}\n",
    "\\Phi = \\tanh(Z W_1).\n",
    "\\end{equation}\n",
    "No likelihood or data is used in sampling $W_1$; the hidden layer is drawn purely from the prior. The second-layer coefficients are then estimated by minimum-norm least squares:\n",
    "\\begin{equation}\n",
    "\\widehat{w} = \\Phi^{+} y.\n",
    "\\end{equation}\n",
    "Thus, the fitted model is\n",
    "\\begin{equation}\n",
    "\\widehat{f}(x) = \\sum_{h=1}^H \\widehat{w}_h \\tanh(w_{1,h}^\\top \\phi),\n",
    "\\end{equation}\n",
    "where $w_{1,h}$ denotes the $h$-th column of $W_1$. We evaluate performance using the same population risk as before,\n",
    "\\begin{equation}\n",
    "R = \\mathbb{E}\\bigl[(\\widehat{f}(x) - x^\\top \\beta_{\\ast})^2\\bigr]\n",
    "    = (\\widehat{\\beta} - \\beta_{\\ast})^\\top \\Sigma (\\widehat{\\beta} - \\beta_{\\ast}),\n",
    "\\end{equation}\n",
    "except that the estimator now belongs to the nonlinear feature space spanned by $\\{\\tanh(w_{1,h}^\\top z)\\}_{h=1}^H$. The quantity $\\widehat{\\beta}$ refers to the effective linear predictor induced by $\\widehat{f}$ in the observed coordinates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_latent_data_sec54(n, p, d=20, r_theta=1.0, sigma_xi=0.0, rng=None):\n",
    "    \"\"\"\n",
    "    Section 5.4 latent model (Hastie–Montanari–Rosset–Tibshirani):\n",
    "      X = Z W^T + U,   y = Z θ + ξ\n",
    "      z_i ~ N(0, I_d), u_ij ~ N(0, 1), ξ_i ~ N(0, σ_ξ^2)\n",
    "    Rows w_j of W satisfy ||w_j|| = 1.               [Fig. 5/6 setup]\n",
    "    Population mapping to linear model:\n",
    "      Σ = I_p + W W^T,   β = W (I + W^T W)^{-1} θ.   [eqs. (26)-(27)]\n",
    "    Returns: X (n×p), y (n,), W (p×d), theta (d,), beta_true (p,), Sigma (p×p)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    # Random W with unit-norm rows (||w_j||=1)\n",
    "    W = rng.normal(size=(p, d))\n",
    "    W /= np.linalg.norm(W, axis=1, keepdims=True) + 1e-12  # enforce ||w_j||=1\n",
    "\n",
    "    # Latent Z, feature noise U, label noise ξ\n",
    "    Z = rng.normal(size=(n, d))\n",
    "    U = rng.normal(size=(n, p))\n",
    "    xi = rng.normal(scale=sigma_xi, size=n)\n",
    "\n",
    "    # Signal vector θ with ||θ|| = r_theta\n",
    "    theta = rng.normal(size=d)\n",
    "    theta *= r_theta / (np.linalg.norm(theta) + 1e-12)\n",
    "\n",
    "    # Data\n",
    "    X = Z @ W.T + U\n",
    "    y = Z @ theta + xi\n",
    "\n",
    "    # Population quantities for risk\n",
    "    Sigma = np.eye(p) + W @ W.T\n",
    "    beta_true = W @ np.linalg.solve(np.eye(d) + W.T @ W, theta)  # β = W (I + W^T W)^(-1) θ\n",
    "\n",
    "    return X, Z, y, W, theta, beta_true, Sigma\n",
    "\n",
    "def fit_min_norm(X, y):\n",
    "    \"\"\"\n",
    "    Minimum-ℓ2-norm least squares: β̂ = X^+ y (ridgeless limit of ridge).\n",
    "    \"\"\"\n",
    "    return np.linalg.pinv(X) @ y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_hidden_features_gauss(\n",
    "    X,\n",
    "    rng,\n",
    "    H,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = X.shape\n",
    "    W1 = rng.normal(0.0, 1.0, size=(P, H))\n",
    "\n",
    "    Z = np.tanh(X @ W1)\n",
    "    return Z, W1\n",
    "\n",
    "def sample_hidden_features_RHS(\n",
    "    X,\n",
    "    rng,\n",
    "    H,\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = X.shape\n",
    "    tau0 = p_0 / (P - p_0)\n",
    "    #alpha = np.full(P, alpha_scale)\n",
    "\n",
    "    tau = np.abs(rng.standard_cauchy()) * tau0\n",
    "    c_sq = 1.0 / rng.gamma(shape=a, scale=1.0 / b, size=H)\n",
    "    lambda_data = np.abs(rng.standard_cauchy(size=(H, P)))\n",
    "    #phi_data = rng.dirichlet(alpha, size=H)\n",
    "\n",
    "    lam_sq = lambda_data**2\n",
    "    denom = c_sq[:, None] + lam_sq * (tau**2)\n",
    "    lambda_tilde = (c_sq[:, None] * lam_sq) / denom\n",
    "    lambda_tilde = np.maximum(lambda_tilde, 1e-12)\n",
    "\n",
    "    W1_raw = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    stddev = tau * np.sqrt(lambda_tilde.T) #* np.sqrt(phi_data.T)  # (P,H)\n",
    "    W1 = W1_raw * stddev\n",
    "\n",
    "    Z = np.tanh(X @ W1)\n",
    "    return Z, W1\n",
    "\n",
    "def sample_hidden_features_DHS(\n",
    "    X,\n",
    "    rng,\n",
    "    H,\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = X.shape\n",
    "    tau0 = p_0 / (P - p_0)\n",
    "    alpha = np.full(P, alpha_scale)\n",
    "\n",
    "    tau = np.sqrt(10)*np.abs(rng.standard_cauchy()) * tau0\n",
    "    c_sq = 1.0 / rng.gamma(shape=a, scale=1.0 / b, size=H)\n",
    "    lambda_data = np.abs(rng.standard_cauchy(size=(H, P)))\n",
    "    phi_data = rng.dirichlet(alpha, size=H)\n",
    "\n",
    "    lam_sq = lambda_data**2\n",
    "    denom = c_sq[:, None] + lam_sq * (tau**2)\n",
    "    lambda_tilde = (c_sq[:, None] * lam_sq) / denom\n",
    "    lambda_tilde = np.maximum(lambda_tilde, 1e-12)\n",
    "\n",
    "    W1_raw = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    stddev = tau * np.sqrt(lambda_tilde.T) * np.sqrt(phi_data.T)  # (P,H)\n",
    "    W1 = W1_raw * stddev\n",
    "\n",
    "    Z = np.tanh(X @ W1)\n",
    "    return Z, W1\n",
    "\n",
    "def sample_hidden_features_DST(\n",
    "    X,\n",
    "    rng,\n",
    "    H,\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = X.shape\n",
    "    tau0 = p_0 / (P - p_0)\n",
    "    alpha = np.full(P, alpha_scale)\n",
    "\n",
    "    tau = np.sqrt(10)*np.abs(rng.standard_cauchy()) * tau0\n",
    "    c_sq = 1.0 / rng.gamma(shape=a, scale=1.0 / b, size=H)\n",
    "    lambda_data = np.abs(rng.standard_t(df=3, size=(H, P)))\n",
    "    phi_data = rng.dirichlet(alpha, size=H)\n",
    "\n",
    "    lam_sq = lambda_data**2\n",
    "    denom = c_sq[:, None] + lam_sq * (tau**2)\n",
    "    lambda_tilde = (c_sq[:, None] * lam_sq) / denom\n",
    "    lambda_tilde = np.maximum(lambda_tilde, 1e-12)\n",
    "\n",
    "    W1_raw = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    stddev = tau * np.sqrt(lambda_tilde.T) * np.sqrt(phi_data.T)  # (P,H)\n",
    "    W1 = W1_raw * stddev\n",
    "\n",
    "    Z = np.tanh(X @ W1)\n",
    "    return Z, W1\n",
    "\n",
    "def plot_risk_curve_hidden_units(\n",
    "    n=400,\n",
    "    gammas=(0.7, 0.9, 1.2, 1.5, 2, 3, 5, 8, 12, 20),\n",
    "    model = \"DHS\",\n",
    "    d=20,\n",
    "    r_theta=1.0,\n",
    "    sigma_xi=0.0,\n",
    "    reps=50,\n",
    "    risk_mc_samples=1000,\n",
    "    seed=0,\n",
    "    # prior hyperparams\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.5,\n",
    "    # NEW: schedule H as a function of (p, n)\n",
    "    H_of_p=lambda p, n: p,   # <- default ties H to p (so H grows with γ)\n",
    "):\n",
    "    \"\"\"\n",
    "    Same as before, but H now depends on p (and n) via H_of_p.\n",
    "    We reuse the same W1 for train and population risk.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    G, M, S = [], [], []\n",
    "\n",
    "    for gamma in gammas:\n",
    "        p = max(1, int(round(gamma * n)))\n",
    "        H = max(1, int(H_of_p(p, n)))\n",
    "        risks = []\n",
    "\n",
    "        for _ in range(reps):\n",
    "            # Your data generator\n",
    "            X, Z, y, W, theta, beta_true, Sigma = make_latent_data_sec54(\n",
    "                n=n, p=p, d=d, r_theta=r_theta, sigma_xi=sigma_xi, rng=rng\n",
    "            )\n",
    "\n",
    "            # One sampled hidden map W1 for this rep; reuse it for population risk\n",
    "            if model ==\"gauss\":\n",
    "                Z, W1 = sample_hidden_features_gauss(\n",
    "                    X, rng, H=H\n",
    "                )\n",
    "            \n",
    "            elif model ==\"RHS\":\n",
    "                Z, W1 = sample_hidden_features_RHS(\n",
    "                    X, rng, H=H, p_0=p_0, a=a, b=b\n",
    "                )\n",
    "            elif model ==\"DHS\":\n",
    "                Z, W1 = sample_hidden_features_DHS(\n",
    "                    X, rng, H=H, p_0=p_0, a=a, b=b, alpha_scale=alpha_scale\n",
    "                )\n",
    "            else:\n",
    "                Z, W1 = sample_hidden_features_DST(\n",
    "                    X, rng, H=H, p_0=p_0, a=a, b=b, alpha_scale=alpha_scale\n",
    "                )\n",
    "\n",
    "            # Min-norm on hidden units\n",
    "            w_hat = fit_min_norm(Z, y)\n",
    "            if w_hat.ndim > 1 and w_hat.shape[1] == 1:\n",
    "                w_hat = w_hat.ravel()\n",
    "\n",
    "            # Monte Carlo population risk with the SAME W1\n",
    "            X_pop = rng.multivariate_normal(mean=np.zeros(p), cov=Sigma, size=risk_mc_samples)\n",
    "            Z_pop = np.tanh(X_pop @ W1)\n",
    "            y_true_pop = X_pop @ beta_true\n",
    "            y_pred_pop = Z_pop @ w_hat\n",
    "            risks.append(float(np.mean((y_pred_pop - y_true_pop) ** 2)))\n",
    "\n",
    "        G.append(gamma)\n",
    "        M.append(np.mean(risks))\n",
    "        S.append(np.std(risks, ddof=1))\n",
    "\n",
    "    return np.array(G), np.array(M), np.array(S)\n",
    "\n",
    "G_h_gauss, M_h_gauss, S_h_gauss = plot_risk_curve_hidden_units(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"gauss\",\n",
    "    gammas=[0.16, 0.3, 0.7, 0.9, 1.5, 2, 3, 5, 8, 10, 20],\n",
    "    reps=50, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_h_RHS, M_h_RHS, S_h_RHS = plot_risk_curve_hidden_units(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"RHS\",\n",
    "    gammas=[0.16, 0.3, 0.7, 0.9, 1.5, 2, 3, 5, 8, 10, 20],\n",
    "    reps=50, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_h_DHS, M_h_DHS, S_h_DHS = plot_risk_curve_hidden_units(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"DHS\",\n",
    "    gammas=[0.16, 0.3, 0.7, 0.9, 1.5, 2, 3, 5, 8, 10, 20],\n",
    "    reps=50, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_h_DST, M_h_DST, S_h_DST = plot_risk_curve_hidden_units(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"DST\",\n",
    "    gammas=[0.16, 0.3, 0.7, 0.9, 1.5, 2, 3, 5, 8, 10, 20],\n",
    "    reps=50, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_risk_curve_sec54(\n",
    "    n=400,\n",
    "    gammas=(0.7, 0.9, 1.2, 1.5, 2, 3, 5, 8, 12, 20),\n",
    "    d=20,\n",
    "    r_theta=1.0,          # \"r = 1\" in the captions\n",
    "    sigma_xi=0.0,         # use 0 for Fig. 5 behavior; try 0, 0.25, 0.5 like Fig. 6\n",
    "    reps=50,\n",
    "    seed=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Replicates the latent-space risk curve of §5.4 (Figs. 5–6):\n",
    "      For each γ = p/n, simulate (X,y), fit min-norm β̂, and compute\n",
    "      population risk R_X = (β̂−β)^T Σ (β̂−β), then average across reps.\n",
    "    Expectation from §5.4: spike near γ≈1 and then *monotone decrease* for γ>1,\n",
    "    reaching a global minimum as γ→∞ when β aligns with top eigenspace of Σ. \n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    G, M, S = [], [], []\n",
    "\n",
    "    for gamma in gammas:\n",
    "        p = max(1, int(round(gamma * n)))\n",
    "        risks = []\n",
    "\n",
    "        for _ in range(reps):\n",
    "            X, Z, y, W, theta, beta_true, Sigma = make_latent_data_sec54(\n",
    "                n=n, p=p, d=d, r_theta=r_theta, sigma_xi=sigma_xi, rng=rng\n",
    "            )\n",
    "            beta_hat = fit_min_norm(X, y)\n",
    "\n",
    "            diff = beta_hat - beta_true\n",
    "            risks.append(float(diff @ (Sigma @ diff)))\n",
    "\n",
    "        G.append(gamma)\n",
    "        M.append(np.mean(risks))\n",
    "        S.append(np.std(risks, ddof=1))\n",
    "\n",
    "    G, M, S = np.array(G), np.array(M), np.array(S)\n",
    "    ci = 1.96 * S / np.sqrt(reps)\n",
    "    \n",
    "    return G, M, S\n",
    "\n",
    "# Example:\n",
    "G, M, S = plot_risk_curve_sec54(n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "                        gammas=[0.16, 0.3, 0.7, 0.9, 1.5, 2, 3, 5, 8, 10, 20], reps=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import isfinite\n",
    "from typing import Iterable, Tuple, Dict, Any\n",
    "\n",
    "def _c0_closed_form(gamma, psi):\n",
    "    \"\"\"\n",
    "    Closed-form solution of Eq. (35) for c0 in Corollary 4.\n",
    "    Parameters assume gamma > 1 and 0 < psi < 1 (as in the latent model).\n",
    "    \"\"\"\n",
    "    A = float(gamma)\n",
    "    L = 1.0 - 1.0/A                  # LHS constant\n",
    "    t = 1.0 + 1.0/psi                # 1 + psi^{-1}\n",
    "\n",
    "    # Quadratic in y = c0:\n",
    "    # (L t A^2) y^2 + [A (L(1+t) - ((1-psi) t + psi))] y + (L - 1) = 0\n",
    "    Acoef = L * t * A**2\n",
    "    Bcoef = A * (L*(1.0 + t) - ((1.0 - psi)*t + psi))\n",
    "    Ccoef = L - 1.0                  # = -1/A\n",
    "\n",
    "    disc = Bcoef*Bcoef - 4.0*Acoef*Ccoef\n",
    "    if disc < 0:\n",
    "        # tiny negative from FP roundoff\n",
    "        disc = 0.0\n",
    "    # unique nonnegative root\n",
    "    return (-Bcoef + np.sqrt(disc)) / (2.0*Acoef)\n",
    "\n",
    "def corollary4_continuous(\n",
    "    gammas,\n",
    "    n,\n",
    "    d,\n",
    "    r_theta=1.0,\n",
    "    sigma_xi=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns analytic test risk from Corollary 4 for the latent-space model (Sec. 5.4).\n",
    "    For γ<1, uses the underparametrized variance formula R = σ^2 * γ/(1-γ) (Prop. 2).\n",
    "    For γ>1, uses Corollary 4 with the closed-form c0.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    gammas : array-like of γ = p/n (positive)\n",
    "    n      : sample size\n",
    "    d      : latent dimension\n",
    "    r_theta: ||θ||\n",
    "    sigma_xi: σ_ξ\n",
    "\n",
    "    Outputs (np.ndarray)\n",
    "    --------------------\n",
    "    dict with keys: 'gamma', 'risk', 'bias', 'var'\n",
    "    \"\"\"\n",
    "    g = np.asarray(gammas, dtype=float)\n",
    "    risk = np.empty_like(g); bias = np.empty_like(g); var = np.empty_like(g)\n",
    "\n",
    "    for i, gamma in enumerate(g):\n",
    "        if gamma <= 0:\n",
    "            raise ValueError(\"All gamma must be > 0.\")\n",
    "\n",
    "        # finite-sample plug-in for ψ = d/p with p = γ n\n",
    "        psi = d / (gamma * n)\n",
    "        # σ^2 = σ_ξ^2 + ψ r_θ^2 / (1+ψ)  (Eq. (27) and Cor. 4 text)\n",
    "        sigma2 = sigma_xi**2 + psi * (r_theta**2) / (1.0 + psi)\n",
    "\n",
    "        if gamma < 1.0:\n",
    "            # Underparametrized: pure variance (Prop. 2)\n",
    "            bias[i] = 0.0\n",
    "            var[i]  = sigma2 * gamma / (1.0 - gamma)\n",
    "            risk[i] = var[i]\n",
    "        else:\n",
    "            # Overparametrized: Cor. 4 Eqs. (30)–(35)\n",
    "            c0 = _c0_closed_form(gamma, psi)\n",
    "            t  = 1.0 + 1.0/psi\n",
    "            d1 = (1.0 + c0 * gamma)**2\n",
    "            d2 = (1.0 + c0 * t     * gamma)**2\n",
    "\n",
    "            # Eqs. (33)–(34)\n",
    "            E1 = (1.0 - psi)/d1 + psi*(t**2)/d2\n",
    "            E2 = (1.0 - psi)/d1 + (1.0 + psi)/d2\n",
    "\n",
    "            # Eqs. (31)–(32)\n",
    "            bias_i = (1.0 + gamma * c0 * (E1/E2)) * (r_theta**2) / ((1.0 + psi) * d2)\n",
    "            var_i  = sigma2 * gamma * c0 * (E1/E2)\n",
    "\n",
    "            # store\n",
    "            bias[i] = max(bias_i, 0.0)  # clip tiny negatives from FP\n",
    "            var[i]  = max(var_i,  0.0)\n",
    "            risk[i] = bias[i] + var[i]\n",
    "\n",
    "    return {\"gamma\": g, \"risk\": risk, \"bias\": bias, \"var\": var}\n",
    "\n",
    "\n",
    "g_under = np.geomspace(0.2, 0.9, 200)\n",
    "g_over  = np.geomspace(1.2, 20.0, 400)\n",
    "g_all   = np.concatenate([g_under, g_over])\n",
    "\n",
    "out = corollary4_continuous(\n",
    "    gammas=g_all, n=100, d=20, r_theta=1.0, sigma_xi=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reps = 50\n",
    "ci = 1.96 * S / np.sqrt(reps)\n",
    "\n",
    "gamma = out[\"gamma\"]\n",
    "risk  = out[\"risk\"]\n",
    "\n",
    "# Split into two segments\n",
    "mask_left  = gamma <= 0.9\n",
    "mask_right = gamma >= 1.2\n",
    "\n",
    "gamma_left,  risk_left  = gamma[mask_left],  risk[mask_left]\n",
    "gamma_right, risk_right = gamma[mask_right], risk[mask_right]\n",
    "\n",
    "plt.figure(figsize=(6.4, 4.4))\n",
    "plt.loglog(G_h_gauss, M_h_gauss, marker=\"o\", linewidth=2, label=\"Gauss\")\n",
    "plt.loglog(G_h_RHS,   M_h_RHS,   marker=\"o\", linewidth=2, label=\"RHS\")\n",
    "plt.loglog(G_h_DHS,   M_h_DHS,   marker=\"o\", linewidth=2, label=\"DHS\")\n",
    "plt.loglog(G_h_DST,   M_h_DST,   marker=\"o\", linewidth=2, label=\"DST\")\n",
    "plt.loglog(G,         M,         marker=\"o\", linewidth=2, label=\"Frequentist\")\n",
    "\n",
    "# *** Correct: plot left and right analytic segments separately ***\n",
    "plt.loglog(gamma_left,  risk_left,  linewidth=2, label=\"Analytic\", color=\"magenta\")\n",
    "plt.loglog(gamma_right, risk_right, linewidth=2, color=\"magenta\")\n",
    "\n",
    "# Optional shade the divergence gap\n",
    "plt.axvspan(0.9, 1.2, color=\"grey\", alpha=0.15)\n",
    "\n",
    "plt.axvline(1.0, linestyle=\"--\", linewidth=1)\n",
    "plt.xlabel(r\"Aspect ratio  $\\gamma = p/n$\")\n",
    "plt.ylabel(r\"Population risk  $R_X=(\\hat\\beta-\\beta)^\\top \\Sigma (\\hat\\beta-\\beta)$\")\n",
    "plt.title(r\"Latent space (§5.4) — min-norm risk vs $\\gamma$  (n=100, d=20, r=1, $\\sigma_\\xi=0$)\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST ONCE MORE WITH USING LOW DIM INPUT INSTEAD OF OBSERVED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_latent_data_sec54(n, p, d=20, r_theta=1.0, sigma_xi=0.0, rng=None):\n",
    "    \"\"\"\n",
    "    Section 5.4 latent model (Hastie–Montanari–Rosset–Tibshirani):\n",
    "      X = Z W^T + U,   y = Z θ + ξ\n",
    "      z_i ~ N(0, I_d), u_ij ~ N(0, 1), ξ_i ~ N(0, σ_ξ^2)\n",
    "    Rows w_j of W satisfy ||w_j|| = 1.               [Fig. 5/6 setup]\n",
    "    Population mapping to linear model:\n",
    "      Σ = I_p + W W^T,   β = W (I + W^T W)^{-1} θ.   [eqs. (26)-(27)]\n",
    "    Returns: X (n×p), y (n,), W (p×d), theta (d,), beta_true (p,), Sigma (p×p)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "\n",
    "    # Random W with unit-norm rows (||w_j||=1)\n",
    "    W = rng.normal(size=(p, d))\n",
    "    W /= np.linalg.norm(W, axis=1, keepdims=True) + 1e-12  # enforce ||w_j||=1\n",
    "\n",
    "    # Latent Z, feature noise U, label noise ξ\n",
    "    Z = rng.normal(size=(n, d))\n",
    "    U = rng.normal(size=(n, p))\n",
    "    xi = rng.normal(scale=sigma_xi, size=n)\n",
    "\n",
    "    # Signal vector θ with ||θ|| = r_theta\n",
    "    theta = rng.normal(size=d)\n",
    "    theta *= r_theta / (np.linalg.norm(theta) + 1e-12)\n",
    "\n",
    "    # Data\n",
    "    X = Z @ W.T + U\n",
    "    y = Z @ theta + xi\n",
    "\n",
    "    # Population quantities for risk\n",
    "    Sigma = np.eye(p) + W @ W.T\n",
    "    beta_true = W @ np.linalg.solve(np.eye(d) + W.T @ W, theta)  # β = W (I + W^T W)^(-1) θ\n",
    "\n",
    "    return X, Z, y, W, theta, beta_true, Sigma\n",
    "\n",
    "def fit_min_norm(X, y):\n",
    "    \"\"\"\n",
    "    Minimum-ℓ2-norm least squares: β̂ = X^+ y (ridgeless limit of ridge).\n",
    "    \"\"\"\n",
    "    return np.linalg.pinv(X) @ y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_hidden_features_gauss(\n",
    "    Z,\n",
    "    rng,\n",
    "    H,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = Z.shape\n",
    "    W1 = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    \n",
    "    post_acts = np.tanh(Z @ W1)\n",
    "    return post_acts, W1\n",
    "\n",
    "def sample_hidden_features_RHS(\n",
    "    Z,\n",
    "    rng,\n",
    "    H,\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = Z.shape\n",
    "    tau0 = p_0 / (P - p_0)\n",
    "    #alpha = np.full(P, alpha_scale)\n",
    "\n",
    "    tau = np.abs(rng.standard_cauchy()) * tau0\n",
    "    c_sq = 1.0 / rng.gamma(shape=a, scale=1.0 / b, size=H)\n",
    "    lambda_data = np.abs(rng.standard_cauchy(size=(H, P)))\n",
    "    #phi_data = rng.dirichlet(alpha, size=H)\n",
    "\n",
    "    lam_sq = lambda_data**2\n",
    "    denom = c_sq[:, None] + lam_sq * (tau**2)\n",
    "    lambda_tilde = (c_sq[:, None] * lam_sq) / denom\n",
    "    lambda_tilde = np.maximum(lambda_tilde, 1e-12)\n",
    "\n",
    "    W1_raw = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    stddev = tau * np.sqrt(lambda_tilde.T) #* np.sqrt(phi_data.T)  # (P,H)\n",
    "    W1 = W1_raw * stddev\n",
    "\n",
    "    post_acts = np.tanh(Z @ W1)\n",
    "    return post_acts, W1\n",
    "\n",
    "def sample_hidden_features_DHS(\n",
    "    Z,\n",
    "    rng,\n",
    "    H,\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = Z.shape\n",
    "    tau0 = p_0 / (P - p_0)\n",
    "    alpha = np.full(P, alpha_scale)\n",
    "\n",
    "    tau = np.sqrt(10)*np.abs(rng.standard_cauchy()) * tau0\n",
    "    c_sq = 1.0 / rng.gamma(shape=a, scale=1.0 / b, size=H)\n",
    "    lambda_data = np.abs(rng.standard_cauchy(size=(H, P)))\n",
    "    phi_data = rng.dirichlet(alpha, size=H)\n",
    "\n",
    "    lam_sq = lambda_data**2\n",
    "    denom = c_sq[:, None] + lam_sq * (tau**2)\n",
    "    lambda_tilde = (c_sq[:, None] * lam_sq) / denom\n",
    "    lambda_tilde = np.maximum(lambda_tilde, 1e-12)\n",
    "\n",
    "    W1_raw = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    stddev = tau * np.sqrt(lambda_tilde.T) * np.sqrt(phi_data.T)  # (P,H)\n",
    "    W1 = W1_raw * stddev\n",
    "\n",
    "    post_acts = np.tanh(Z @ W1)\n",
    "    return post_acts, W1\n",
    "\n",
    "def sample_hidden_features_DST(\n",
    "    Z,\n",
    "    rng,\n",
    "    H,\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = Z.shape\n",
    "    tau0 = p_0 / (P - p_0)\n",
    "    alpha = np.full(P, alpha_scale)\n",
    "\n",
    "    tau = np.sqrt(10)*np.abs(rng.standard_cauchy()) * tau0\n",
    "    c_sq = 1.0 / rng.gamma(shape=a, scale=1.0 / b, size=H)\n",
    "    lambda_data = np.abs(rng.standard_t(df=3, size=(H, P)))\n",
    "    phi_data = rng.dirichlet(alpha, size=H)\n",
    "\n",
    "    lam_sq = lambda_data**2\n",
    "    denom = c_sq[:, None] + lam_sq * (tau**2)\n",
    "    lambda_tilde = (c_sq[:, None] * lam_sq) / denom\n",
    "    lambda_tilde = np.maximum(lambda_tilde, 1e-12)\n",
    "\n",
    "    W1_raw = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    stddev = tau * np.sqrt(lambda_tilde.T) * np.sqrt(phi_data.T)  # (P,H)\n",
    "    W1 = W1_raw * stddev\n",
    "\n",
    "    post_acts = np.tanh(Z @ W1)\n",
    "    return post_acts, W1\n",
    "\n",
    "def sample_hidden_features_BHS(\n",
    "    Z,\n",
    "    rng,\n",
    "    H,\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = Z.shape\n",
    "    tau0 = p_0 / (P - p_0)\n",
    "    #alpha = np.full(P, alpha_scale)\n",
    "\n",
    "    tau = np.sqrt(10)*np.abs(rng.standard_cauchy()) * tau0\n",
    "    c_sq = 1.0 / rng.gamma(shape=a, scale=1.0 / b, size=H)\n",
    "    lambda_data = np.abs(rng.standard_cauchy(size=(H, P)))\n",
    "    phi_data = rng.beta(a = alpha_scale, b = (P-1)*alpha_scale, size=H)\n",
    "\n",
    "    lam_sq = lambda_data**2\n",
    "    denom = c_sq[:, None] + lam_sq * (tau**2)\n",
    "    lambda_tilde = (c_sq[:, None] * lam_sq) / denom\n",
    "    lambda_tilde = np.maximum(lambda_tilde, 1e-12)\n",
    "\n",
    "    W1_raw = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    stddev = tau * np.sqrt(lambda_tilde.T) * np.sqrt(phi_data.T)  # (P,H)\n",
    "    W1 = W1_raw * stddev\n",
    "\n",
    "    post_acts = np.tanh(Z @ W1)\n",
    "    return post_acts, W1\n",
    "\n",
    "def sample_hidden_features_BST(\n",
    "    Z,\n",
    "    rng,\n",
    "    H,\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (Z, W1) with Z = tanh(X @ W1), and W1 sampled from your prior.\n",
    "    \"\"\"\n",
    "    n, P = Z.shape\n",
    "    tau0 = p_0 / (P - p_0)\n",
    "    #alpha = np.full(P, alpha_scale)\n",
    "\n",
    "    tau = np.sqrt(10)*np.abs(rng.standard_cauchy()) * tau0\n",
    "    c_sq = 1.0 / rng.gamma(shape=a, scale=1.0 / b, size=H)\n",
    "    lambda_data = np.abs(rng.standard_t(df=3, size=(H, P)))\n",
    "    phi_data = rng.beta(a = alpha_scale, b = (P-1)*alpha_scale, size=H)\n",
    "\n",
    "    lam_sq = lambda_data**2\n",
    "    denom = c_sq[:, None] + lam_sq * (tau**2)\n",
    "    lambda_tilde = (c_sq[:, None] * lam_sq) / denom\n",
    "    lambda_tilde = np.maximum(lambda_tilde, 1e-12)\n",
    "\n",
    "    W1_raw = rng.normal(0.0, 1.0, size=(P, H))\n",
    "    stddev = tau * np.sqrt(lambda_tilde.T) * np.sqrt(phi_data.T)  # (P,H)\n",
    "    W1 = W1_raw * stddev\n",
    "\n",
    "    post_acts = np.tanh(Z @ W1)\n",
    "    return post_acts, W1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_risk_curve_hidden_units_alternative(\n",
    "    n=400,\n",
    "    gammas=(0.7, 0.9, 1.2, 1.5, 2, 3, 5, 8, 12, 20),\n",
    "    model = \"DHS\",\n",
    "    d=20,\n",
    "    r_theta=1.0,\n",
    "    sigma_xi=0.0,\n",
    "    reps=50,\n",
    "    risk_mc_samples=1000,\n",
    "    seed=0,\n",
    "    # prior hyperparams\n",
    "    p_0=3,\n",
    "    a=2.0,\n",
    "    b=2.0,\n",
    "    alpha_scale=0.5,\n",
    "    # NEW: schedule H as a function of (p, n)\n",
    "    H_of_p=lambda p, n: p,   # <- default ties H to p (so H grows with γ)\n",
    "):\n",
    "    \"\"\"\n",
    "    Same as before, but H now depends on p (and n) via H_of_p.\n",
    "    We reuse the same W1 for train and population risk.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    G, M, S = [], [], []\n",
    "\n",
    "    for gamma in gammas:\n",
    "        p = max(1, int(round(gamma * n)))\n",
    "        H = max(1, int(H_of_p(p, n)))\n",
    "        risks = []\n",
    "\n",
    "        for _ in range(reps):\n",
    "            # Your data generator\n",
    "            X, Z, y, W, theta, beta_true, Sigma = make_latent_data_sec54(\n",
    "                n=n, p=p, d=d, r_theta=r_theta, sigma_xi=sigma_xi, rng=rng\n",
    "            )\n",
    "            \n",
    "            #print(X.shape, Z.shape)\n",
    "            # One sampled hidden map W1 for this rep; reuse it for population risk\n",
    "            if model ==\"gauss\":\n",
    "                post_acts, W1 = sample_hidden_features_gauss(\n",
    "                    Z, rng, H=H\n",
    "                )\n",
    "            \n",
    "            elif model ==\"RHS\":\n",
    "                post_acts, W1 = sample_hidden_features_RHS(\n",
    "                    Z, rng, H=H, p_0=p_0, a=a, b=b\n",
    "                )\n",
    "            elif model ==\"DHS\":\n",
    "                post_acts, W1 = sample_hidden_features_DHS(\n",
    "                    Z, rng, H=H, p_0=p_0, a=a, b=b, alpha_scale=alpha_scale\n",
    "                )\n",
    "            elif model ==\"DST\":\n",
    "                post_acts, W1 = sample_hidden_features_DST(\n",
    "                    Z, rng, H=H, p_0=p_0, a=a, b=b, alpha_scale=alpha_scale\n",
    "                )\n",
    "            elif model ==\"BHS\":\n",
    "                post_acts, W1 = sample_hidden_features_BHS(\n",
    "                    Z, rng, H=H, p_0=p_0, a=a, b=b, alpha_scale=alpha_scale\n",
    "                )\n",
    "            else:\n",
    "                post_acts, W1 = sample_hidden_features_BST(\n",
    "                    Z, rng, H=H, p_0=p_0, a=a, b=b, alpha_scale=alpha_scale\n",
    "                )\n",
    "\n",
    "            # Min-norm on hidden units\n",
    "            w_hat = fit_min_norm(post_acts, y)\n",
    "            if w_hat.ndim > 1 and w_hat.shape[1] == 1:\n",
    "                w_hat = w_hat.ravel()\n",
    "\n",
    "            # Monte Carlo population risk with the SAME W1\n",
    "            Z_pop = rng.multivariate_normal(mean=np.zeros(d), cov=np.eye(d), size=risk_mc_samples)\n",
    "            post_acts_pop = np.tanh(Z_pop @ W1)\n",
    "            y_true_pop = Z_pop @ theta\n",
    "            y_pred_pop = post_acts_pop @ w_hat\n",
    "            risks.append(float(np.mean((y_pred_pop - y_true_pop) ** 2)))\n",
    "\n",
    "        G.append(gamma)\n",
    "        M.append(np.mean(risks))\n",
    "        S.append(np.std(risks, ddof=1))\n",
    "\n",
    "    return np.array(G), np.array(M), np.array(S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G_h_gauss, M_h_gauss, S_h_gauss = plot_risk_curve_hidden_units_alternative(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"gauss\",\n",
    "    gammas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.2, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 75, 100],\n",
    "    reps=100, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_h_RHS, M_h_RHS, S_h_RHS = plot_risk_curve_hidden_units_alternative(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"RHS\",\n",
    "    gammas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.2, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 75, 100],\n",
    "    reps=100, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_h_DHS, M_h_DHS, S_h_DHS = plot_risk_curve_hidden_units_alternative(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"DHS\",\n",
    "    gammas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.2, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 75, 100],\n",
    "    reps=100, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_h_DST, M_h_DST, S_h_DST = plot_risk_curve_hidden_units_alternative(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"DST\",\n",
    "    gammas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.2, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 75, 100],\n",
    "    reps=100, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_h_BHS, M_h_BHS, S_h_BHS = plot_risk_curve_hidden_units_alternative(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"BHS\",\n",
    "    gammas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.2, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 75, 100],\n",
    "    reps=100, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_h_BST, M_h_BST, S_h_BST = plot_risk_curve_hidden_units_alternative(\n",
    "    n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "    model = \"BST\",\n",
    "    gammas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.2, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 75, 100],\n",
    "    reps=100, risk_mc_samples=1000, seed=123,\n",
    "    H_of_p=lambda p, n: p  # H follows p (thus follows γ)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_risk_curve_sec54(\n",
    "    n=400,\n",
    "    gammas=(0.7, 0.9, 1.2, 1.5, 2, 3, 5, 8, 12, 20),\n",
    "    d=20,\n",
    "    r_theta=1.0,          # \"r = 1\" in the captions\n",
    "    sigma_xi=0.0,         # use 0 for Fig. 5 behavior; try 0, 0.25, 0.5 like Fig. 6\n",
    "    reps=50,\n",
    "    seed=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Replicates the latent-space risk curve of §5.4 (Figs. 5–6):\n",
    "      For each γ = p/n, simulate (X,y), fit min-norm β̂, and compute\n",
    "      population risk R_X = (β̂−β)^T Σ (β̂−β), then average across reps.\n",
    "    Expectation from §5.4: spike near γ≈1 and then *monotone decrease* for γ>1,\n",
    "    reaching a global minimum as γ→∞ when β aligns with top eigenspace of Σ. \n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    G, M, S = [], [], []\n",
    "\n",
    "    for gamma in gammas:\n",
    "        p = max(1, int(round(gamma * n)))\n",
    "        risks = []\n",
    "\n",
    "        for _ in range(reps):\n",
    "            X, Z, y, W, theta, beta_true, Sigma = make_latent_data_sec54(\n",
    "                n=n, p=p, d=d, r_theta=r_theta, sigma_xi=sigma_xi, rng=rng\n",
    "            )\n",
    "            beta_hat = fit_min_norm(X, y)\n",
    "\n",
    "            diff = beta_hat - beta_true\n",
    "            risks.append(float(diff @ (Sigma @ diff)))\n",
    "\n",
    "        G.append(gamma)\n",
    "        M.append(np.mean(risks))\n",
    "        S.append(np.std(risks, ddof=1))\n",
    "\n",
    "    G, M, S = np.array(G), np.array(M), np.array(S)\n",
    "    ci = 1.96 * S / np.sqrt(reps)\n",
    "    \n",
    "    return G, M, S\n",
    "\n",
    "# Example:\n",
    "G, M, S = plot_risk_curve_sec54(n=100, d=20, r_theta=1.0, sigma_xi=0.0,\n",
    "                            gammas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.2, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 75, 100],\n",
    "                            reps=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import isfinite\n",
    "from typing import Iterable, Tuple, Dict, Any\n",
    "\n",
    "def _c0_closed_form(gamma, psi):\n",
    "    \"\"\"\n",
    "    Closed-form solution of Eq. (35) for c0 in Corollary 4.\n",
    "    Parameters assume gamma > 1 and 0 < psi < 1 (as in the latent model).\n",
    "    \"\"\"\n",
    "    A = float(gamma)\n",
    "    L = 1.0 - 1.0/A                  # LHS constant\n",
    "    t = 1.0 + 1.0/psi                # 1 + psi^{-1}\n",
    "\n",
    "    # Quadratic in y = c0:\n",
    "    # (L t A^2) y^2 + [A (L(1+t) - ((1-psi) t + psi))] y + (L - 1) = 0\n",
    "    Acoef = L * t * A**2\n",
    "    Bcoef = A * (L*(1.0 + t) - ((1.0 - psi)*t + psi))\n",
    "    Ccoef = L - 1.0                  # = -1/A\n",
    "\n",
    "    disc = Bcoef*Bcoef - 4.0*Acoef*Ccoef\n",
    "    if disc < 0:\n",
    "        # tiny negative from FP roundoff\n",
    "        disc = 0.0\n",
    "    # unique nonnegative root\n",
    "    return (-Bcoef + np.sqrt(disc)) / (2.0*Acoef)\n",
    "\n",
    "def corollary4_continuous(\n",
    "    gammas,\n",
    "    n,\n",
    "    d,\n",
    "    r_theta=1.0,\n",
    "    sigma_xi=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns analytic test risk from Corollary 4 for the latent-space model (Sec. 5.4).\n",
    "    For γ<1, uses the underparametrized variance formula R = σ^2 * γ/(1-γ) (Prop. 2).\n",
    "    For γ>1, uses Corollary 4 with the closed-form c0.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    gammas : array-like of γ = p/n (positive)\n",
    "    n      : sample size\n",
    "    d      : latent dimension\n",
    "    r_theta: ||θ||\n",
    "    sigma_xi: σ_ξ\n",
    "\n",
    "    Outputs (np.ndarray)\n",
    "    --------------------\n",
    "    dict with keys: 'gamma', 'risk', 'bias', 'var'\n",
    "    \"\"\"\n",
    "    g = np.asarray(gammas, dtype=float)\n",
    "    risk = np.empty_like(g); bias = np.empty_like(g); var = np.empty_like(g)\n",
    "\n",
    "    for i, gamma in enumerate(g):\n",
    "        if gamma <= 0:\n",
    "            raise ValueError(\"All gamma must be > 0.\")\n",
    "\n",
    "        # finite-sample plug-in for ψ = d/p with p = γ n\n",
    "        psi = d / (gamma * n)\n",
    "        # σ^2 = σ_ξ^2 + ψ r_θ^2 / (1+ψ)  (Eq. (27) and Cor. 4 text)\n",
    "        sigma2 = sigma_xi**2 + psi * (r_theta**2) / (1.0 + psi)\n",
    "\n",
    "        if gamma < 1.0:\n",
    "            # Underparametrized: pure variance (Prop. 2)\n",
    "            bias[i] = 0.0\n",
    "            var[i]  = sigma2 * gamma / (1.0 - gamma)\n",
    "            risk[i] = var[i]\n",
    "        else:\n",
    "            # Overparametrized: Cor. 4 Eqs. (30)–(35)\n",
    "            c0 = _c0_closed_form(gamma, psi)\n",
    "            t  = 1.0 + 1.0/psi\n",
    "            d1 = (1.0 + c0 * gamma)**2\n",
    "            d2 = (1.0 + c0 * t     * gamma)**2\n",
    "\n",
    "            # Eqs. (33)–(34)\n",
    "            E1 = (1.0 - psi)/d1 + psi*(t**2)/d2\n",
    "            E2 = (1.0 - psi)/d1 + (1.0 + psi)/d2\n",
    "\n",
    "            # Eqs. (31)–(32)\n",
    "            bias_i = (1.0 + gamma * c0 * (E1/E2)) * (r_theta**2) / ((1.0 + psi) * d2)\n",
    "            var_i  = sigma2 * gamma * c0 * (E1/E2)\n",
    "\n",
    "            # store\n",
    "            bias[i] = max(bias_i, 0.0)  # clip tiny negatives from FP\n",
    "            var[i]  = max(var_i,  0.0)\n",
    "            risk[i] = bias[i] + var[i]\n",
    "\n",
    "    return {\"gamma\": g, \"risk\": risk, \"bias\": bias, \"var\": var}\n",
    "\n",
    "\n",
    "g_under = np.geomspace(0.1, 0.9, 200)\n",
    "g_over  = np.geomspace(1.2, 100.0, 400)\n",
    "g_all   = np.concatenate([g_under, g_over])\n",
    "\n",
    "out = corollary4_continuous(\n",
    "    gammas=g_all, n=100, d=20, r_theta=1.0, sigma_xi=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reps = 50\n",
    "ci = 1.96 * S / np.sqrt(reps)\n",
    "\n",
    "gamma = out[\"gamma\"]\n",
    "risk  = out[\"risk\"]\n",
    "\n",
    "# Split into two segments\n",
    "mask_left  = gamma <= 0.9\n",
    "mask_right = gamma >= 1.2\n",
    "\n",
    "gamma_left,  risk_left  = gamma[mask_left],  risk[mask_left]\n",
    "gamma_right, risk_right = gamma[mask_right], risk[mask_right]\n",
    "\n",
    "plt.figure(figsize=(6.4, 4.4))\n",
    "plt.loglog(G_h_gauss, M_h_gauss, marker=\"o\", linewidth=2, label=\"Gauss\")\n",
    "plt.loglog(G_h_RHS,   M_h_RHS,   marker=\"o\", linewidth=2, label=\"RHS\")\n",
    "plt.loglog(G_h_DHS,   M_h_DHS,   marker=\"o\", linewidth=2, label=\"DHS\")\n",
    "plt.loglog(G_h_DST,   M_h_DST,   marker=\"o\", linewidth=2, label=\"DST\")\n",
    "plt.loglog(G_h_BHS,   M_h_BHS,   marker=\"o\", linewidth=2, label=\"BHS\")\n",
    "plt.loglog(G_h_BST,   M_h_BST,   marker=\"o\", linewidth=2, label=\"BST\")\n",
    "plt.loglog(G,         M,         marker=\"o\", linewidth=2, label=\"Frequentist\")\n",
    "\n",
    "# *** Correct: plot left and right analytic segments separately ***\n",
    "plt.loglog(gamma_left,  risk_left,  linewidth=2, label=\"Analytic\", color=\"magenta\")\n",
    "plt.loglog(gamma_right, risk_right, linewidth=2, color=\"magenta\")\n",
    "\n",
    "# Optional shade the divergence gap\n",
    "plt.axvspan(0.9, 1.2, color=\"grey\", alpha=0.15)\n",
    "\n",
    "plt.axvline(1.0, linestyle=\"--\", linewidth=1)\n",
    "plt.xlabel(r\"Aspect ratio  $\\gamma = p/n$\")\n",
    "plt.ylabel(r\"Population risk  $R_X=(\\hat\\beta-\\beta)^\\top \\Sigma (\\hat\\beta-\\beta)$\")\n",
    "plt.title(r\"Latent space (§5.4) — min-norm risk vs $\\gamma$  (n=100, d=20, r=1, $\\sigma_\\xi=0$)\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "X, Z, y, W, theta, beta_true, Sigma = make_latent_data_sec54(\n",
    "    n=100, p=200, d=5, r_theta=1, rng=rng\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_units = 1000\n",
    "\n",
    "post_acts_gauss, W1 = sample_hidden_features_gauss(\n",
    "    Z, rng, H=H_units\n",
    ")\n",
    "\n",
    "post_acts_RHS, W1 = sample_hidden_features_RHS(\n",
    "    Z, rng, H=H_units\n",
    ")\n",
    "\n",
    "post_acts_DHS, W1 = sample_hidden_features_DHS(\n",
    "    Z, rng, H=H_units\n",
    ")\n",
    "\n",
    "post_acts_DST, W1 = sample_hidden_features_DST(\n",
    "    Z, rng, H=H_units\n",
    ")\n",
    "\n",
    "post_acts_BHS, W1 = sample_hidden_features_BHS(\n",
    "    Z, rng, H=H_units\n",
    ")\n",
    "\n",
    "post_acts_BST, W1 = sample_hidden_features_BST(\n",
    "    Z, rng, H=H_units\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def effective_rank(A, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Compute the effective rank of a matrix A, as defined in\n",
    "    Roy & Vetterli (2007), \"The Effective Rank: A Measure of Effective Dimensionality\".\n",
    "\n",
    "    erank(A) = exp( H(p) ),  where\n",
    "        p_k = σ_k / sum(σ_i)\n",
    "        H(p) = -sum_k p_k * log(p_k)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : ndarray\n",
    "        Input matrix (M×N).\n",
    "    tol : float, optional\n",
    "        Threshold below which singular values are ignored (default 1e-12).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    erank : float\n",
    "        The effective rank of A.\n",
    "    \"\"\"\n",
    "    # Singular values\n",
    "    s = np.linalg.svd(A, compute_uv=False)\n",
    "    s = s[s > tol]\n",
    "    if len(s) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Singular value distribution\n",
    "    p = s / np.sum(s)\n",
    "\n",
    "    # Shannon entropy\n",
    "    H = -np.sum(p * np.log(p + np.finfo(float).eps))\n",
    "\n",
    "    # Effective rank\n",
    "    er = np.exp(H)\n",
    "    return er\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_S(Phi, center=True):\n",
    "    Phi = np.asarray(Phi)\n",
    "    if center: Phi = Phi - Phi.mean(axis=0, keepdims=True)\n",
    "    n = Phi.shape[0]\n",
    "    return (Phi.T @ Phi) / max(n, 1)\n",
    "\n",
    "def spectral_stats(Phi, k_list=(1,3,5,10), tol=1e-12):\n",
    "    S = compute_S(Phi)\n",
    "    evals = np.linalg.eigvalsh(S)\n",
    "    evals = np.sort(evals)[::-1]\n",
    "    tot = max(evals.sum(), tol)\n",
    "\n",
    "    stats = {}\n",
    "    # 1) Effective rank (your function)\n",
    "    stats[\"erank\"] = effective_rank(S)\n",
    "\n",
    "    # 2) CEV_k and 3) kappa_k\n",
    "    for k in k_list:\n",
    "        k = min(k, len(evals))\n",
    "        cev = float(evals[:k].sum() / tot)\n",
    "        kappa = float(evals[0] / max(evals[k-1], tol))\n",
    "        stats[f\"CEV@{k}\"] = cev\n",
    "        stats[f\"kappa@{k}\"] = kappa\n",
    "\n",
    "    # 4) Mutual coherence of columns of Phi\n",
    "    G = Phi.T @ Phi\n",
    "    d = np.sqrt(np.clip(np.diag(G), tol, None))\n",
    "    C = (G / d[:,None]) / d[None,:]\n",
    "    np.fill_diagonal(C, 0.0)\n",
    "    stats[\"mu\"] = float(np.max(np.abs(C)))\n",
    "\n",
    "    return stats, evals\n",
    "\n",
    "# --- Example aggregation across priors ---\n",
    "def summarize_priors(post_acts_dict, k_list=(1,3,5,10)):\n",
    "    rows = []\n",
    "    for name, Phi in post_acts_dict.items():\n",
    "        stats, evals = spectral_stats(Phi, k_list=k_list)\n",
    "        row = {\"prior\": name, **stats}\n",
    "        rows.append(row)\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_acts = {\n",
    "    \"Gauss\": post_acts_gauss,\n",
    "    \"RHS\":   post_acts_RHS,\n",
    "    \"DHS\":   post_acts_DHS,\n",
    "    \"DST\":   post_acts_DST,\n",
    "    \"BHS\":   post_acts_BHS,\n",
    "    \"BST\":   post_acts_BST,\n",
    "}\n",
    "rows = summarize_priors(post_acts, k_list=(1,3,5,10))\n",
    "for r in rows: print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_anisotropy(Phi: np.ndarray, center: bool = True, k_leading: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute anisotropy metrics for S = (1/n) Phi^T Phi.\n",
    "\n",
    "    Phi: (n, d) post-activation matrix.\n",
    "    center: whether to center features before computing S.\n",
    "    k_leading: k in kappa_k = lambda_1 / lambda_k, using the k leading eigenvalues.\n",
    "\n",
    "    Returns a DataFrame with rows = metrics and a single column 'value':\n",
    "      - kappa_k : lambda_1 / lambda_k within the leading k-dimensional subspace\n",
    "      - CV      : coefficient of variation of eigenvalues\n",
    "      - r_eff   : spectral effective rank\n",
    "      - mu_mean : mean eigenvalue\n",
    "    \"\"\"\n",
    "    S = compute_S(Phi, center=center)\n",
    "    S_sym = 0.5 * (S + S.T)  # symmetrize for numerical safety\n",
    "\n",
    "    d = S_sym.shape[0]\n",
    "    eps = 1e-12\n",
    "\n",
    "    # Eigenvalues\n",
    "    eigvals = np.linalg.eigvalsh(S_sym)\n",
    "    eigvals = np.clip(eigvals, eps, None)\n",
    "\n",
    "    # Sort descending for lambda_1, lambda_k\n",
    "    eigvals_sorted = np.sort(eigvals)[::-1]\n",
    "\n",
    "    mu_mean = float(eigvals_sorted.mean())\n",
    "    cv      = float(eigvals_sorted.std() / mu_mean)\n",
    "\n",
    "    # Effective rank\n",
    "    p = eigvals_sorted / eigvals_sorted.sum()\n",
    "    r_eff = float(np.exp(-np.sum(p * np.log(p))))\n",
    "\n",
    "    # kappa_k = lambda_1 / lambda_k (within leading k subspace)\n",
    "    k = min(k_leading, d)\n",
    "    lam1 = eigvals_sorted[0]\n",
    "    lamk = eigvals_sorted[k - 1]\n",
    "    kappa_k = float(lam1 / lamk)\n",
    "\n",
    "    rows = [\n",
    "        {\"metric\": \"kappa_k\", \"value\": kappa_k},\n",
    "        {\"metric\": \"CV\",      \"value\": cv},\n",
    "        {\"metric\": \"r_eff\",   \"value\": r_eff},\n",
    "        {\"metric\": \"mu_mean\", \"value\": mu_mean},\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(rows).set_index(\"metric\")\n",
    "\n",
    "\n",
    "summary_gauss = summarize_anisotropy(post_acts_gauss)\n",
    "summary_RHS = summarize_anisotropy(post_acts_RHS)\n",
    "summary_DHS = summarize_anisotropy(post_acts_DHS)\n",
    "summary_DST = summarize_anisotropy(post_acts_DST)\n",
    "summary_BHS = summarize_anisotropy(post_acts_BHS)\n",
    "summary_BST = summarize_anisotropy(post_acts_BST)\n",
    "print(summary_gauss)\n",
    "print(summary_RHS)\n",
    "print(summary_DHS)\n",
    "print(summary_DST)\n",
    "print(summary_BHS)\n",
    "print(summary_BST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_gauss = compute_S(post_acts_gauss)\n",
    "S_RHS = compute_S(post_acts_RHS)\n",
    "S_DHS = compute_S(post_acts_DHS)\n",
    "S_DST = compute_S(post_acts_DST)\n",
    "S_BHS = compute_S(post_acts_BHS)\n",
    "S_BST = compute_S(post_acts_BST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12), sharex=True, sharey=True)\n",
    "\n",
    "corr_gauss = np.corrcoef(S_gauss, rowvar=False)\n",
    "corr_RHS   = np.corrcoef(S_RHS,   rowvar=False)\n",
    "corr_DHS   = np.corrcoef(S_DHS,   rowvar=False)\n",
    "corr_DST   = np.corrcoef(S_DST,   rowvar=False)\n",
    "corr_BHS   = np.corrcoef(S_BHS,   rowvar=False)\n",
    "corr_BST   = np.corrcoef(S_BST,   rowvar=False)\n",
    "\n",
    "sns.heatmap(corr_gauss, ax=axes[0, 0], vmin=-1, vmax=1, cmap='coolwarm')\n",
    "sns.heatmap(corr_RHS,   ax=axes[0, 1], vmin=-1, vmax=1, cmap='coolwarm')\n",
    "sns.heatmap(corr_DHS,   ax=axes[1, 0], vmin=-1, vmax=1, cmap='coolwarm')\n",
    "sns.heatmap(corr_DST,   ax=axes[1, 1], vmin=-1, vmax=1, cmap='coolwarm')\n",
    "sns.heatmap(corr_BHS,   ax=axes[2, 0], vmin=-1, vmax=1, cmap='coolwarm')\n",
    "sns.heatmap(corr_BST,   ax=axes[2, 1], vmin=-1, vmax=1, cmap='coolwarm')\n",
    "\n",
    "axes[0, 0].set_title(\"Gauss\")\n",
    "axes[0, 1].set_title(\"RHS\")\n",
    "axes[1, 0].set_title(\"DHS\")\n",
    "axes[1, 1].set_title(\"DST\")\n",
    "axes[2, 0].set_title(\"BHS\")\n",
    "axes[2, 1].set_title(\"BST\")\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel(\"$\\\\Phi$\")\n",
    "    ax.set_ylabel(\"$\\\\Phi$\")\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_gauss = np.linalg.eigvalsh(corr_gauss)\n",
    "eig_RHS   = np.linalg.eigvalsh(corr_RHS)\n",
    "eig_DHS   = np.linalg.eigvalsh(corr_DHS)\n",
    "eig_DST   = np.linalg.eigvalsh(corr_DST)\n",
    "eig_BHS   = np.linalg.eigvalsh(corr_BHS)\n",
    "eig_BST   = np.linalg.eigvalsh(corr_BST)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(eig_gauss[::-1], label=\"Gauss\")\n",
    "plt.plot(eig_RHS[::-1],   label=\"RHS\")\n",
    "plt.plot(eig_DHS[::-1],   label=\"DHS\")\n",
    "plt.plot(eig_DST[::-1],   label=\"DST\")\n",
    "plt.plot(eig_BHS[::-1],   label=\"BHS\")\n",
    "plt.plot(eig_BST[::-1],   label=\"BST\")\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Eigenvalue spectra of feature covariances\")\n",
    "plt.xlabel(\"Index (sorted)\")\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frob(A, B):\n",
    "    return np.linalg.norm(A - B, \"fro\")\n",
    "\n",
    "print(\"Frobenius distances from Gaussian:\")\n",
    "print(\"RHS:\", frob(corr_gauss, corr_RHS))\n",
    "print(\"DHS:\", frob(corr_gauss, corr_DHS))\n",
    "print(\"DST:\", frob(corr_gauss, corr_DST))\n",
    "print(\"BHS:\", frob(corr_gauss, corr_BHS))\n",
    "print(\"BST:\", frob(corr_gauss, corr_BST))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offdiag_energy(C):\n",
    "    return np.linalg.norm(C - np.eye(C.shape[0]), \"fro\")\n",
    "\n",
    "print(\"Off-diagonal energy:\")\n",
    "print(\"Gauss:\", offdiag_energy(corr_gauss))\n",
    "print(\"RHS:\",   offdiag_energy(corr_RHS))\n",
    "print(\"DHS:\",   offdiag_energy(corr_DHS))\n",
    "print(\"DST:\",   offdiag_energy(corr_DST))\n",
    "print(\"BHS:\",   offdiag_energy(corr_BHS))\n",
    "print(\"BST:\",   offdiag_energy(corr_BST))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more theoretical stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gauss:\", effective_rank(post_acts_gauss), \"\\n\", \"RHS:\", effective_rank(post_acts_RHS), \"\\n\", \"DHS:\", effective_rank(post_acts_DHS), \"\\n\", \"DST:\", effective_rank(post_acts_DST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.hist(post_acts_gauss[:, 0])\n",
    "plt.hist(post_acts_RHS[:, 0])\n",
    "plt.hist(post_acts_DHS[:, 0])\n",
    "plt.hist(post_acts_DST[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_S(Phi, center=True, scale_by_n=True):\n",
    "    \"\"\"\n",
    "    Feature covariance in feature-space:\n",
    "        S = Phi^T Phi / n   (optionally with column-centering)\n",
    "    Phi: (n, H) matrix of post-activations\n",
    "    \"\"\"\n",
    "    Phi = np.asarray(Phi)\n",
    "    if center:\n",
    "        Phi = Phi - Phi.mean(axis=0, keepdims=True)\n",
    "    n = Phi.shape[0]\n",
    "    S = Phi.T @ Phi\n",
    "    if scale_by_n:\n",
    "        S = S / max(n, 1)\n",
    "    return S\n",
    "\n",
    "def alignment_scores(Phi, y, k_list=(1, 3, 5), ridge=1e-10, center=True):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - S:     (H,H) feature covariance\n",
    "      - evals: eigenvalues of S (descending)\n",
    "      - evecs: eigenvectors (columns) of S\n",
    "      - frac_span: fraction of ||y||^2 captured by the span of Phi (projection energy)\n",
    "      - frac_topk: dict k -> fraction of ||y||^2 captured by the top-k eigenspace of S\n",
    "\n",
    "    Method:\n",
    "      Let S = Phi^T Phi / n, eigendecomp S = U Λ U^T (Λ diag with descending evals).\n",
    "      Full projection: y_hat = Phi (Phi^+ y)  (min-norm fit in feature space).\n",
    "      Top-k projection uses only top-k eigenvectors:\n",
    "        y_hat_k = Phi U_k Λ_k^{-1} U_k^T Phi^T y\n",
    "      (ridge added to Λ_k for numerical stability)\n",
    "    \"\"\"\n",
    "    Phi = np.asarray(Phi)\n",
    "    y = np.asarray(y).ravel()\n",
    "    if center:\n",
    "        Phi = Phi - Phi.mean(axis=0, keepdims=True)\n",
    "        y = y - y.mean()\n",
    "\n",
    "    n, H = Phi.shape\n",
    "    # S and eigendecomposition\n",
    "    S = (Phi.T @ Phi) / max(n, 1)\n",
    "    evals, evecs = np.linalg.eigh(S)\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evals, evecs = evals[idx], evecs[:, idx]\n",
    "\n",
    "    # Full-span projection (min-norm)\n",
    "    y_hat = Phi @ np.linalg.pinv(Phi) @ y\n",
    "    frac_span = float(np.dot(y_hat, y_hat) / np.dot(y, y)) if np.dot(y, y) > 0 else 0.0\n",
    "\n",
    "    # Top-k projections via eigenspace of S\n",
    "    g = Phi.T @ y                                 # (H,)\n",
    "    frac_topk = {}\n",
    "    for k in k_list:\n",
    "        k_eff = min(k, H)\n",
    "        Uk = evecs[:, :k_eff]                     # (H, k)\n",
    "        Lk = evals[:k_eff]                        # (k,)\n",
    "        inv_Lk = 1.0 / (Lk + ridge)\n",
    "        # y_hat_k = Phi Uk Λ_k^{-1} Uk^T Phi^T y\n",
    "        y_hat_k = Phi @ (Uk * inv_Lk) @ (Uk.T @ g)\n",
    "        frac_topk[k] = float(np.dot(y_hat_k, y_hat_k) / np.dot(y, y)) if np.dot(y, y) > 0 else 0.0\n",
    "\n",
    "    return S, evals, evecs, frac_span, frac_topk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "X, Z, y, W, theta, beta_true, Sigma = make_latent_data_sec54(\n",
    "    n=100, p=200, d=5, r_theta=1, rng=rng\n",
    ")\n",
    "\n",
    "# Sample Gaussian hidden features (your function)\n",
    "Phi, W1 = sample_hidden_features_gauss(Z, rng, H=100)\n",
    "\n",
    "# Build S and test alignment of S to y\n",
    "S, evals, evecs, frac_span, frac_topk = alignment_scores(Phi, y, k_list=(1,3,5,10))\n",
    "\n",
    "print(\"||projection of y onto span(Phi)||^2 / ||y||^2 =\", frac_span)\n",
    "print(\"Top-k alignment (fraction of ||y||^2):\", frac_topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "corr = np.corrcoef(S[:10, :10], rowvar=False)\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(corr, vmin=-1, vmax=1, cmap='coolwarm')\n",
    "\n",
    "# Number of features\n",
    "n = corr.shape[0]\n",
    "\n",
    "# Set tick labels to 1..n\n",
    "ax.set_xticks(np.arange(n) + 0.5)\n",
    "ax.set_yticks(np.arange(n) + 0.5)\n",
    "ax.set_xticklabels(np.arange(1, n+1))\n",
    "ax.set_yticklabels(np.arange(1, n+1))\n",
    "\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Gaussian hidden features (your function)\n",
    "Phi, W1 = sample_hidden_features_RHS(Z, rng, H=100)\n",
    "\n",
    "# Build S and test alignment of S to y\n",
    "S, evals, evecs, frac_span, frac_topk = alignment_scores(Phi, y, k_list=(1,3,5,10))\n",
    "\n",
    "print(\"||projection of y onto span(Phi)||^2 / ||y||^2 =\", frac_span)\n",
    "print(\"Top-k alignment (fraction of ||y||^2):\", frac_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Gaussian hidden features (your function)\n",
    "Phi, W1 = sample_hidden_features_DHS(Z, rng, H=100)\n",
    "\n",
    "# Build S and test alignment of S to y\n",
    "S, evals, evecs, frac_span, frac_topk = alignment_scores(Phi, y, k_list=(1,3,5,10))\n",
    "\n",
    "print(\"||projection of y onto span(Phi)||^2 / ||y||^2 =\", frac_span)\n",
    "print(\"Top-k alignment (fraction of ||y||^2):\", frac_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "corr = np.corrcoef(S[:10, :10], rowvar=False)\n",
    "\n",
    "plt.figure()\n",
    "ax = sns.heatmap(corr, vmin=-1, vmax=1, cmap='coolwarm')\n",
    "\n",
    "# Number of features\n",
    "n = corr.shape[0]\n",
    "\n",
    "# Set tick labels to 1..n\n",
    "ax.set_xticks(np.arange(n) + 0.5)\n",
    "ax.set_yticks(np.arange(n) + 0.5)\n",
    "ax.set_xticklabels(np.arange(1, n+1))\n",
    "ax.set_yticklabels(np.arange(1, n+1))\n",
    "\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Gaussian hidden features (your function)\n",
    "Phi, W1 = sample_hidden_features_DST(Z, rng, H=100)\n",
    "\n",
    "# Build S and test alignment of S to y\n",
    "S, evals, evecs, frac_span, frac_topk = alignment_scores(Phi, y, k_list=(1,3,5,10))\n",
    "\n",
    "print(\"||projection of y onto span(Phi)||^2 / ||y||^2 =\", frac_span)\n",
    "print(\"Top-k alignment (fraction of ||y||^2):\", frac_topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
