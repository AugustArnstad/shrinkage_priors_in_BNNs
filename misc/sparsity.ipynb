{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = 1\n",
    "data_dir = f\"datasets/type_{data_config}\"\n",
    "results_dir = \"results_relu_exhaustive/slow\"\n",
    "model_names = [\"Dirichlet Horseshoe\"]\n",
    "\n",
    "all_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"type_{data_config}/{base_config_name}\"  # → \"config_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    print(full_config_path)\n",
    "    print(base_config_name)\n",
    "    fits = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir,\n",
    "        models=model_names,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    all_fits[base_config_name] = fits  # use clean key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward pass for a single layer BNN.\n",
    "    \"\"\"\n",
    "    pre_act_1 = X @ W1 + b1.reshape(1, -1)\n",
    "    #pre_hidden += b1.reshape(1, -1)\n",
    "    post_act_1 = np.maximum(0, pre_act_1)\n",
    "    ouput = post_act_1 @ W2 + b2.reshape(1, -1)\n",
    "    return ouput\n",
    "\n",
    "def compute_rmse_results(seeds, models, all_fits, get_N_sigma, forward_pass,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'GAM_N{N}_p8_sigma{sigma:.2f}_seed{seed}'\n",
    "        path = f\"datasets/type_{data_config}/{dataset_key}.npz\"\n",
    "\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "            #print(y_test.shape)\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                W2 = W2_samples[i]\n",
    "\n",
    "                # Apply pruning mask if requested\n",
    "                if prune_fn is not None and sparsity > 0.0:\n",
    "                    masks = prune_fn([W1, W2], sparsity)\n",
    "                    W1 = W1 * masks[0]\n",
    "                    W2 = W2 * masks[1]\n",
    "\n",
    "                y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i])\n",
    "                y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "                rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "                \n",
    "            posterior_mean = np.mean(y_hats, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'sparsity': sparsity,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "seeds = [1, 2, 3, 4, 7, 8, 9, 10]\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    N = 100 if seed in [1, 2, 3, 4] else 200\n",
    "    sigma = 1.0 if seed in [1, 2, 7, 8] else 3.0\n",
    "    return N, sigma\n",
    "\n",
    "\n",
    "def global_prune_weights(weight_matrices, sparsity_level):\n",
    "    \"\"\"\n",
    "    Prune globally across multiple weight matrices.\n",
    "    \n",
    "    Args:\n",
    "        weight_matrices: List of numpy arrays (weight matrices).\n",
    "        sparsity_level: Float in [0, 1], fraction of weights to prune.\n",
    "\n",
    "    Returns:\n",
    "        List of binary masks with same shapes as weight_matrices.\n",
    "    \"\"\"\n",
    "    # Flatten all weights and concatenate\n",
    "    flat_weights = np.concatenate([w.flatten() for w in weight_matrices])\n",
    "    abs_weights = np.abs(flat_weights)\n",
    "    \n",
    "    # Determine number of weights to prune\n",
    "    total_weights = abs_weights.size\n",
    "    num_to_prune = int(np.floor(sparsity_level * total_weights))\n",
    "\n",
    "    # Get indices of smallest weights to prune\n",
    "    prune_indices = np.argpartition(abs_weights, num_to_prune)[:num_to_prune]\n",
    "    \n",
    "    # Create global mask\n",
    "    global_mask_flat = np.ones(total_weights, dtype=bool)\n",
    "    global_mask_flat[prune_indices] = False\n",
    "\n",
    "    # Split the global mask back into original shapes\n",
    "    masks = []\n",
    "    idx = 0\n",
    "    for w in weight_matrices:\n",
    "        size = w.size\n",
    "        mask = global_mask_flat[idx:idx + size].reshape(w.shape)\n",
    "        masks.append(mask.astype(float))\n",
    "        idx += size\n",
    "\n",
    "    return masks\n",
    "\n",
    "def local_prune_weights(weights, sparsity_level, index_to_prune=0):\n",
    "    \"\"\"\n",
    "    Apply pruning to only one weight matrix in a list, specified by index.\n",
    "\n",
    "    Parameters:\n",
    "    - weights: list of np.ndarray (e.g., [W1, W2])\n",
    "    - sparsity_level: fraction of weights to prune (0.0 to 1.0)\n",
    "    - index_to_prune: which weight matrix to prune in the list\n",
    "\n",
    "    Returns:\n",
    "    - list of masks (one for each weight matrix)\n",
    "    \"\"\"\n",
    "    masks = [np.ones_like(W) for W in weights]\n",
    "\n",
    "    W = weights[index_to_prune]\n",
    "    flat = np.abs(W.flatten())\n",
    "    num_to_prune = int(np.floor(sparsity_level * flat.size))\n",
    "\n",
    "    if num_to_prune > 0:\n",
    "        idx = np.argpartition(flat, num_to_prune)[:num_to_prune]\n",
    "        mask_flat = np.ones_like(flat, dtype=bool)\n",
    "        mask_flat[idx] = False\n",
    "        masks[index_to_prune] = mask_flat.reshape(W.shape).astype(float)\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "\n",
    "df_rmse_global, df_posterior_rmse_global = {}, {}\n",
    "df_rmse_local, df_posterior_rmse_local = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    df_rmse_global[sparsity], df_posterior_rmse_global[sparsity] = compute_rmse_results(\n",
    "        seeds, model_names, all_fits, get_N_sigma, forward_pass,\n",
    "        sparsity=sparsity, prune_fn=global_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_local[sparsity], df_posterior_rmse_local[sparsity] = compute_rmse_results(\n",
    "        seeds, model_names, all_fits, get_N_sigma, forward_pass,\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_rmse_full_local = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_local.items()],\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "for i, N_val in enumerate([100, 200]):\n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sigma = 1.0\n",
    "    else:\n",
    "        sigma = 3.0\n",
    "        ax.set_ylabel(\"RMSE\")\n",
    "    sns.lineplot(\n",
    "        data=df_rmse_full_local[df_rmse_full_local['sigma']==sigma],\n",
    "        x='sparsity', y='rmse',\n",
    "        hue='model', style='N', marker='o', errorbar=None, ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"sigma = {sigma}\")\n",
    "    ax.set_xlabel(\"Sparsity Level\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.grid(True)\n",
    "\n",
    "axes[1].legend(title=\"Sigma\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f\"figures/GAM_{data_config}/prune_local_sigma.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_rmse_full_global = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_global.items()],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "for i, N_val in enumerate([100, 200]):\n",
    "    ax = axes[i]\n",
    "    if i == 0:\n",
    "        sigma = 1.0\n",
    "    else:\n",
    "        sigma = 3.0\n",
    "        ax.set_ylabel(\"RMSE\")\n",
    "    sns.lineplot(\n",
    "        data=df_rmse_full_global[df_rmse_full_global['sigma']==sigma],\n",
    "        x='sparsity', y='rmse',\n",
    "        hue='model', style='N', marker='o', errorbar=None, ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"sigma = {sigma}\")\n",
    "    ax.set_xlabel(\"Sparsity Level\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.grid(True)\n",
    "\n",
    "axes[1].legend(title=\"Sigma\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(f\"figures/GAM_{data_config}/prune_global_sigma.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_gauss = all_fits['GAM_N100_p8_sigma1.00_seed1']['Gaussian']['posterior']\n",
    "fit_rhs = all_fits['GAM_N100_p8_sigma1.00_seed1']['Regularized Horseshoe']['posterior']\n",
    "fit_dhs = all_fits['GAM_N100_p8_sigma1.00_seed1']['Dirichlet Horseshoe']['posterior']\n",
    "\n",
    "W1_samples_gauss = fit_gauss.stan_variable(\"W_1\")           # (S, P, H)\n",
    "W1_samples_rhs = fit_rhs.stan_variable(\"W_1\")           # (S, P, H)\n",
    "W1_samples_dhs = fit_dhs.stan_variable(\"W_1\")           # (S, P, H)\n",
    "W2_samples_gauss = fit_gauss.stan_variable(\"W_L\")           # (S, H, O)\n",
    "W2_samples_rhs = fit_rhs.stan_variable(\"W_L\")           # (S, H, O)\n",
    "W2_samples_dhs = fit_dhs.stan_variable(\"W_L\")           # (S, H, O)\n",
    "b1_samples_gauss = fit_gauss.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "b1_samples_rhs = fit_rhs.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "b1_samples_dhs = fit_dhs.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "b2_samples_gauss = fit_gauss.stan_variable(\"output_bias\")   # (S, O)\n",
    "b2_samples_rhs = fit_rhs.stan_variable(\"output_bias\")   # (S, O)\n",
    "b2_samples_dhs = fit_dhs.stan_variable(\"output_bias\")   # (S, O)\n",
    "\n",
    "dataset_key = f'GAM_N{100}_p8_sigma{1:.2f}_seed{1}'\n",
    "path = f\"datasets/type_{data_config}/{dataset_key}.npz\"\n",
    "\n",
    "data = np.load(path)\n",
    "X_test, y_test = data[\"X_test\"], data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_contribs_all_gauss = np.einsum('nd,sdh->sdh', X_test, W1_samples_gauss) / len(X_test)\n",
    "mean_contribs_all_rhs = np.einsum('nd,sdh->sdh', X_test, W1_samples_rhs) / len(X_test)\n",
    "mean_contribs_all_dhs = np.einsum('nd,sdh->sdh', X_test, W1_samples_dhs) / len(X_test)\n",
    "\n",
    "H = W1_samples_gauss.shape[2]  # number of hidden units\n",
    "\n",
    "mean_contribs_gauss = mean_contribs_all_gauss.mean(axis=0)  # average over posterior samples\n",
    "mean_contribs_rhs = mean_contribs_all_rhs.mean(axis=0)  # average over posterior samples\n",
    "mean_contribs_dhs = mean_contribs_all_dhs.mean(axis=0)  # average over posterior samples\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, (mean_contribs, model_name) in enumerate(zip(\n",
    "    [mean_contribs_gauss, mean_contribs_rhs, mean_contribs_dhs],\n",
    "    [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\"]\n",
    ")):\n",
    "    sns.heatmap(mean_contribs, annot=False, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "                xticklabels=[f\"H{i}\" for i in range(H)],\n",
    "                yticklabels=[f\"X{i}\" for i in range(X_test.shape[1])],\n",
    "                ax=ax[i], vmin=-0.5, vmax=0.5,)\n",
    "    ax[i].set_title(f\"Average Input-to-Hidden Weights ({model_name})\")\n",
    "    ax[i].set_xlabel(\"Hidden Unit\")\n",
    "    ax[i].set_ylabel(\"Input Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_contribs_W2_gauss = W2_samples_gauss.reshape(W2_samples_gauss.shape[0], -1).mean(axis=0)\n",
    "mean_contribs_W2_rhs = W2_samples_rhs.reshape(W2_samples_rhs.shape[0], -1).mean(axis=0)\n",
    "mean_contribs_W2_dhs = W2_samples_dhs.reshape(W2_samples_dhs.shape[0], -1).mean(axis=0)\n",
    "\n",
    "\n",
    "std_contribs_W2_gauss = W2_samples_gauss.reshape(W2_samples_gauss.shape[0], -1).std(axis=0)\n",
    "std_contribs_W2_rhs = W2_samples_rhs.reshape(W2_samples_rhs.shape[0], -1).std(axis=0)\n",
    "std_contribs_W2_dhs = W2_samples_dhs.reshape(W2_samples_dhs.shape[0], -1).std(axis=0)\n",
    "\n",
    "H = mean_contribs_W2_gauss.shape[0]\n",
    "x = np.arange(H)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 4), sharey=True)\n",
    "for i, (mean, std, title) in enumerate(zip(\n",
    "    [mean_contribs_W2_gauss, mean_contribs_W2_rhs, mean_contribs_W2_dhs],\n",
    "    [std_contribs_W2_gauss, std_contribs_W2_rhs, std_contribs_W2_dhs],\n",
    "    [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\"]\n",
    ")):\n",
    "    ax[i].bar(x, mean, yerr=std, capsize=3)\n",
    "    #ax[i].bar(x, mean, capsize=3)\n",
    "    ax[i].set_title(f\"Hidden-to-Output Weights ({title})\")\n",
    "    ax[i].set_xlabel(\"Hidden Unit\")\n",
    "    ax[i].set_xticks(x)\n",
    "    ax[i].set_xticklabels([f\"H{i}\" for i in x])\n",
    "ax[0].set_ylabel(\"Weight Strength\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "eps = 1e-4\n",
    "n_obs = X_test.shape[0]\n",
    "n_features = X_test.shape[1]\n",
    "n_samples = W1_samples.shape[0]\n",
    "\n",
    "# Allocate sensitivity array: shape (n_samples, n_obs, n_features)\n",
    "sensitivities_all = np.zeros((n_samples, n_obs, n_features))\n",
    "\n",
    "# Loop over posterior samples and test points\n",
    "for s in range(n_samples):\n",
    "    W1 = W1_samples[s]\n",
    "    W2 = W2_samples[s]\n",
    "    b1 = b1_samples[s]\n",
    "    b2 = b2_samples[s]\n",
    "\n",
    "    for n in range(n_obs):\n",
    "        baseline = X_test[n:n+1]  # shape (1, D)\n",
    "        y_base = forward_pass(baseline, W1, b1, W2, b2).item()\n",
    "\n",
    "        for j in range(n_features):\n",
    "            x_eps = baseline.copy()\n",
    "            x_eps[0, j] += eps\n",
    "            y_eps = forward_pass(x_eps, W1, b1, W2, b2).item()\n",
    "            sensitivities_all[s, n, j] = (y_eps - y_base) / eps\n",
    "\n",
    "# Now you can compute:\n",
    "# (1) Sensitivities per observation (averaged over posterior)\n",
    "sens_per_obs = sensitivities_all.mean(axis=0)  # shape (n_obs, n_features)\n",
    "\n",
    "# (2) Global feature importance (average over both posterior + inputs)\n",
    "global_mean_sens = sens_per_obs.mean(axis=0)\n",
    "global_std_sens = sensitivities_all.std(axis=(0, 1))\n",
    "\n",
    "# Plot global sensitivity summary\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(range(n_features), global_mean_sens, yerr=global_std_sens, capsize=4)\n",
    "plt.xlabel(\"Input Feature\")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.title(\"Global Mean Sensitivity to Input Features (± SD)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "eps = 1e-4\n",
    "n_features = X_test.shape[1]\n",
    "n_samples = W1_samples.shape[0]\n",
    "baseline = X_test[10:11]  # shape (1, D)\n",
    "\n",
    "# Allocate sensitivity array\n",
    "sensitivities_all = np.zeros((n_samples, n_features))\n",
    "\n",
    "# Loop over posterior samples\n",
    "for s in range(n_samples):\n",
    "    W1 = W1_samples[s]\n",
    "    W2 = W2_samples[s]\n",
    "    b1 = b1_samples[s]\n",
    "    b2 = b2_samples[s]\n",
    "\n",
    "    y_base = forward_pass(baseline, W1, b1, W2, b2).item()\n",
    "\n",
    "    for j in range(n_features):\n",
    "        x_eps = baseline.copy()\n",
    "        x_eps[0, j] += eps\n",
    "        y_eps = forward_pass(x_eps, W1, b1, W2, b2).item()\n",
    "        sensitivities_all[s, j] = (y_eps - y_base) / eps\n",
    "\n",
    "# Average + std across posterior\n",
    "mean_sens = sensitivities_all.mean(axis=0)\n",
    "std_sens = sensitivities_all.std(axis=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(range(n_features), mean_sens, yerr=std_sens, capsize=4)\n",
    "plt.xlabel(\"Input Feature\")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.title(\"Mean Output Sensitivity to Input Features (± SD)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass intermediates\n",
    "pre_acts = X_test @ W1_samples + b1_samples.reshape(1, -1)  # shape (N_test, H)\n",
    "post_acts = np.maximum(0, pre_acts)                         # shape (N_test, H)\n",
    "\n",
    "dead_mask = (post_acts == 0)\n",
    "dead_neurons = np.all(dead_mask, axis=0)\n",
    "num_dead = dead_neurons.sum()\n",
    "\n",
    "print(f\"Number of dead neurons: {num_dead} / {post_acts.shape[1]}\")\n",
    "print(\"Mean pre-activations per neuron:\", pre_acts.mean(axis=0))\n",
    "\n",
    "bias_values = b1_samples.flatten()  # shape (H,)\n",
    "num_units = pre_acts.shape[1]\n",
    "y_positions = np.arange(1, num_units + 1)  # boxplot y-ticks start at 1\n",
    "\n",
    "plt.boxplot(pre_acts, vert=False)\n",
    "#plt.scatter(bias_values, y_positions, color='orange', marker='x', label='Bias value')\n",
    "plt.title(\"Distribution of Pre-Activations per Hidden Unit\")\n",
    "plt.xlabel(\"Pre-Activation Value\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_rate = np.mean(post_acts > 0, axis=0)\n",
    "plt.scatter(bias_values, activation_rate)\n",
    "plt.xlabel(\"Bias\")\n",
    "plt.ylabel(\"Activation Frequency\")\n",
    "plt.title(\"Hidden Bias vs Activation Rate\")\n",
    "\n",
    "# Add hidden unit index labels\n",
    "for i, (b, a) in enumerate(zip(bias_values, activation_rate)):\n",
    "    plt.text(b, a, str(i), fontsize=8, ha='right', va='bottom')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below only visualizes weights. I think the biases encodes much information, so I try to improve the interpretations by exploring the biases above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network import extract_posterior_means\n",
    "\n",
    "P = 8\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "layer_sizes = [P] + [H]*L + [out_nodes]\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "def extract_all_posterior_means_test(fits, layer_structure):\n",
    "    \"\"\"\n",
    "    Extract posterior mean weights from all models in a dictionary of fits.\n",
    "\n",
    "    Parameters:\n",
    "        fits (dict): Dictionary of fits structured as\n",
    "                    {model_name: {\"posterior\": CmdStanMCMC}}.\n",
    "        layer_structure (dict): Layer structure as required by extract_posterior_means().\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of posterior means for each model,\n",
    "            structured as {model_name: {param_name: mean_weights}}.\n",
    "    \"\"\"\n",
    "\n",
    "    model_means = {}\n",
    "    for name, fit_dict in fits.items():\n",
    "        model_means[name] = extract_posterior_means(fit_dict['posterior'], layer_structure)\n",
    "    return model_means\n",
    "\n",
    "dict = extract_all_posterior_means_test(all_fits['GAM_N100_p8_sigma1.00_seed2'], layer_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def plot_all_networks_subplots(model_dicts, layer_sizes, max_width=5.0, ncols=3, figsize_per_plot=(5, 4), signed_colors=False):\n",
    "    \"\"\"\n",
    "    Plot multiple neural networks as subplots, with edge thickness representing weight magnitude.\n",
    "\n",
    "    Parameters:\n",
    "        model_dicts (dict): Dictionary mapping model names to weight dicts.\n",
    "                            Each weight dict must include:\n",
    "                            - 'data_to_hidden': (input_dim, hidden_dim)\n",
    "                            - 'hidden_to_output': (hidden_dim, output_dim)\n",
    "                            - optionally 'hidden_to_hidden': list of (hidden_dim, hidden_dim) matrices\n",
    "        layer_sizes (list[int]): List of node counts for each layer (e.g. [5, 9, 1]).\n",
    "        max_width (float): Maximum line width for strongest edge. Default is 5.0.\n",
    "        ncols (int): Number of subplot columns. Default is 3.\n",
    "        figsize_per_plot (tuple): Base figure size per subplot (width, height).\n",
    "        signed_colors (bool): If True, positive weights are red and negative weights are blue.\n",
    "\n",
    "    Notes:\n",
    "        - Automatically handles any number of models and fills unused subplot slots.\n",
    "        - Weight matrices are assumed to follow (input_dim, output_dim) format.\n",
    "    \"\"\"\n",
    "\n",
    "    n_models = len(model_dicts)\n",
    "    nrows = int(np.ceil(n_models / ncols))\n",
    "    figsize = (figsize_per_plot[0] * ncols, figsize_per_plot[1] * nrows)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax in axes[n_models:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for idx, (title, weight_dict) in enumerate(model_dicts.items()):\n",
    "        G = nx.DiGraph()\n",
    "        pos = {}\n",
    "        node_ids_per_layer = []\n",
    "\n",
    "        # Add nodes\n",
    "        for layer_idx, size in enumerate(layer_sizes):\n",
    "            nodes = []\n",
    "            y_coords = np.linspace(size - 1, 0, size) - (size - 1) / 2\n",
    "            for i in range(size):\n",
    "                nid = f\"L{layer_idx}_{i}\"\n",
    "                G.add_node(nid)\n",
    "                pos[nid] = (layer_idx, y_coords[i])\n",
    "                nodes.append(nid)\n",
    "            node_ids_per_layer.append(nodes)\n",
    "\n",
    "        edge_colors = []\n",
    "        edge_widths = []\n",
    "\n",
    "        # Function to add edges from W\n",
    "        def add_edges(W, in_nodes, out_nodes):\n",
    "            for j, out_node in enumerate(out_nodes):\n",
    "                for i, in_node in enumerate(in_nodes):\n",
    "                    w = W[i, j]\n",
    "                    # if i == 7: # and j == 0:\n",
    "                    #     print(f\"Edge from {in_node} to {out_node} is {w}\")\n",
    "                    #     print(f\"Edge width to append is {abs(w)}\")\n",
    "                    G.add_edge(in_node, out_node, weight=abs(w))\n",
    "                    edge_colors.append('red' if w >= 0 else 'blue')\n",
    "                    edge_widths.append(abs(w))\n",
    "\n",
    "        # Input-to-hidden\n",
    "        add_edges(weight_dict['W_1'], node_ids_per_layer[0], node_ids_per_layer[1])\n",
    "\n",
    "        # Hidden-to-hidden\n",
    "        if 'W_internal' in weight_dict:\n",
    "            for l in range(len(weight_dict['W_internal'])):\n",
    "                add_edges(weight_dict['W_internal'][l], node_ids_per_layer[l+1], node_ids_per_layer[l+2])\n",
    "\n",
    "        # Hidden-to-output\n",
    "        add_edges(weight_dict['W_L'], node_ids_per_layer[-2], node_ids_per_layer[-1])\n",
    "\n",
    "        # Normalize widths\n",
    "        #max_w = max(edge_widths) if edge_widths else 1.0\n",
    "        #edge_widths = [max_width * (w / max_w) for w in edge_widths]\n",
    "        labels = {nid: nid for nid in G.nodes}\n",
    "        nx.draw_networkx_labels(G, pos, labels=labels, ax=axes[idx], font_size=8)\n",
    "\n",
    "        edge_widths = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "\n",
    "        nx.draw(G, pos, ax=axes[idx], node_color='lightgray',\n",
    "                edge_color=edge_colors if signed_colors else 'red',\n",
    "                width=edge_widths, with_labels=False,\n",
    "                node_size=400, arrows=False)\n",
    "\n",
    "\n",
    "        axes[idx].set_title(title, fontsize=10)\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    #plt.close()\n",
    "    return fig, edge_widths\n",
    "\n",
    "p1, widths_1 = plot_all_networks_subplots(dict, layer_sizes, signed_colors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fits['GAM_N100_p8_sigma1.00_seed2']['Regularized Horseshoe']['posterior'].stan_variable(\"W_1\")[0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
