{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman\"\n",
    "results_dir_local_lambda = \"results/regression/single_layer/tanh/friedman\"\n",
    "results_dir_nodewise_lambda = \"results/regression/single_layer/tanh/friedman/nodewise_lambda\"\n",
    "\n",
    "model_names_local_lambda = [\"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "model_names_nodewise_lambda = [\"Regularized Horseshoe tanh nodewise\", \"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\"]\n",
    "\n",
    "\n",
    "local_lambda_fits = {}\n",
    "nodewise_lambda_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    local_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_local_lambda,\n",
    "        models=model_names_local_lambda,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    nodewise_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_nodewise_lambda,\n",
    "        models=model_names_nodewise_lambda,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    local_lambda_fits[base_config_name] = local_fit  # use clean key\n",
    "    nodewise_lambda_fits[base_config_name] = nodewise_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman_correlated\"\n",
    "results_dir_local_lambda_correlated = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "results_dir_nodewise_lambda_correlated = \"results/regression/single_layer/tanh/friedman_correlated/nodewise_lambda\"\n",
    "\n",
    "model_names_local_lambda_correlated = [\"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "model_names_nodewise_lambda_correlated = [\"Regularized Horseshoe tanh nodewise\", \"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\"]\n",
    "\n",
    "\n",
    "local_lambda_fits_correlated = {}\n",
    "nodewise_lambda_fits_correlated = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    local_fit_correlated = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_local_lambda_correlated,\n",
    "        models=model_names_local_lambda_correlated,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    nodewise_fit_correlated = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_nodewise_lambda_correlated,\n",
    "        models=model_names_nodewise_lambda_correlated,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    local_lambda_fits_correlated[base_config_name] = local_fit_correlated  # use clean key\n",
    "    nodewise_lambda_fits_correlated[base_config_name] = nodewise_fit_correlated  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from properscoring import crps_ensemble\n",
    "\n",
    "_FRIEDMAN_KEY = re.compile(r\"Friedman_N(\\d+)_p\\d+_sigma([\\d.]+)_seed(\\d+)\")\n",
    "\n",
    "def extract_friedman_metadata(key: str):\n",
    "    \"\"\"\n",
    "    Parse 'Friedman_N{N}_p10_sigma{sigma}_seed{seed}' -> (N:int, sigma:float, seed:int)\n",
    "    Returns (None, None, None) if it doesn't match.\n",
    "    \"\"\"\n",
    "    m = _FRIEDMAN_KEY.search(key)\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "    N = int(m.group(1))\n",
    "    sigma = float(m.group(2))\n",
    "    seed = int(m.group(3))\n",
    "    return N, sigma, seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_from_fits(all_fits, model_names=None, folder=\"friedman\"):\n",
    "    \"\"\"\n",
    "    Iterate over all dataset keys in `all_fits` (e.g., relu_fits or tanh_fits).\n",
    "    For each model in `model_names` (or all models found if None), compute:\n",
    "      - RMSE for each posterior draw\n",
    "      - RMSE of the posterior mean predictor\n",
    "\n",
    "    Returns:\n",
    "        df_rmse: long DF with one row per posterior draw.\n",
    "        df_posterior_rmse: one row per model/dataset with posterior-mean RMSE.\n",
    "    \"\"\"\n",
    "    rmse_rows = []\n",
    "    post_mean_rows = []\n",
    "\n",
    "    for dataset_key, model_dict in all_fits.items():\n",
    "        N, sigma, seed = extract_friedman_metadata(dataset_key)\n",
    "        if N is None:\n",
    "            # Skip non-Friedman entries if any\n",
    "            continue\n",
    "\n",
    "        \n",
    "        try:\n",
    "            path = f\"datasets/{folder}/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # shape (N_test,)\n",
    "        except FileNotFoundError:\n",
    "            path = f\"datasets/{folder}/many/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # shape (N_test,)\n",
    "            #print(f\"[SKIP] y_test not found: {path}\")\n",
    "            #continue\n",
    "\n",
    "        # Choose which models to evaluate\n",
    "        models_to_eval = model_names or list(model_dict.keys())\n",
    "\n",
    "        for model in models_to_eval:\n",
    "            # Some entries may be missing\n",
    "            entry = model_dict.get(model, None)\n",
    "            if not entry or \"posterior\" not in entry:\n",
    "                print(f\"[SKIP] Missing posterior: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            fit = entry[\"posterior\"]\n",
    "\n",
    "            # Expecting (S, N_test, 1) or (S, N_test)\n",
    "            output_test = fit.stan_variable(\"output_test\")\n",
    "            if output_test.ndim == 3 and output_test.shape[-1] == 1:\n",
    "                preds = output_test[..., 0]  # (S, N_test)\n",
    "            elif output_test.ndim == 2:\n",
    "                preds = output_test  # (S, N_test)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected output_test shape {output_test.shape} for {dataset_key} -> {model}\")\n",
    "\n",
    "            # Per-sample RMSE\n",
    "            sq_err = (preds - y_test[None, :])**2  # (S, N_test)\n",
    "            rmse_per_sample = np.sqrt(np.mean(sq_err, axis=1))  # (S,)\n",
    "\n",
    "            for s_idx, rmse in enumerate(rmse_per_sample):\n",
    "                rmse_rows.append({\n",
    "                    \"dataset_key\": dataset_key,\n",
    "                    \"model\": model,\n",
    "                    \"N\": N,\n",
    "                    \"sigma\": sigma,\n",
    "                    \"seed\": seed,\n",
    "                    \"sample_idx\": s_idx,\n",
    "                    \"rmse\": float(rmse)\n",
    "                })\n",
    "\n",
    "            # Posterior-mean RMSE\n",
    "            posterior_mean = preds.mean(axis=0)  # (N_test,)\n",
    "            post_mean_rmse = float(np.sqrt(np.mean((posterior_mean - y_test)**2)))\n",
    "            post_mean_rows.append({\n",
    "                \"dataset_key\": dataset_key,\n",
    "                \"model\": model,\n",
    "                \"N\": N,\n",
    "                \"sigma\": sigma,\n",
    "                \"seed\": seed,\n",
    "                \"posterior_mean_rmse\": post_mean_rmse\n",
    "            })\n",
    "\n",
    "    df_rmse = pd.DataFrame(rmse_rows)\n",
    "    df_posterior_rmse = pd.DataFrame(post_mean_rows)\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "\n",
    "def compute_crps_from_fits(all_fits, model_names=None):\n",
    "    \"\"\"\n",
    "    Compute CRPS per dataset/model using all posterior predictive samples.\n",
    "\n",
    "    Returns:\n",
    "        df_crps: one row per dataset/model with mean CRPS.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for dataset_key, model_dict in all_fits.items():\n",
    "        N, sigma, seed = extract_friedman_metadata(dataset_key)\n",
    "        if N is None:\n",
    "            continue\n",
    "\n",
    "        path = f\"datasets/friedman/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # (N_test,)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] y_test not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        models_to_eval = model_names or list(model_dict.keys())\n",
    "\n",
    "        for model in models_to_eval:\n",
    "            entry = model_dict.get(model, None)\n",
    "            if not entry or \"posterior\" not in entry:\n",
    "                print(f\"[SKIP] Missing posterior: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            fit = entry[\"posterior\"]\n",
    "            output_test = fit.stan_variable(\"output_test\")\n",
    "\n",
    "            # Expecting (S, N_test, 1) or (S, N_test)\n",
    "            if output_test.ndim == 3 and output_test.shape[-1] == 1:\n",
    "                preds = output_test[..., 0]  # (S, N_test)\n",
    "            elif output_test.ndim == 2:\n",
    "                preds = output_test  # (S, N_test)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected output_test shape {output_test.shape} for {dataset_key} -> {model}\")\n",
    "\n",
    "            # crps_ensemble expects shape (N_test, S)\n",
    "            crps_point = crps_ensemble(y_test, preds.T)  # (N_test,)\n",
    "            rows.append({\n",
    "                \"dataset_key\": dataset_key,\n",
    "                \"model\": model,\n",
    "                \"N\": N,\n",
    "                \"sigma\": sigma,\n",
    "                \"seed\": seed,\n",
    "                \"crps\": float(crps_point.mean())\n",
    "            })\n",
    "\n",
    "    df_crps = pd.DataFrame(rows)\n",
    "    return df_crps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ReLU models\n",
    "df_rmse_local_lambda, df_posterior_rmse_local_lambda = compute_rmse_from_fits(\n",
    "    local_lambda_fits, model_names_local_lambda  # or None to use all found\n",
    ")\n",
    "# df_crps_relu = compute_crps_from_fits(\n",
    "#     relu_fits, model_names_relu\n",
    "# )\n",
    "\n",
    "df_rmse_local_lambda_correlated, df_posterior_rmse_local_lambda_correlated = compute_rmse_from_fits(\n",
    "    local_lambda_fits_correlated, model_names_local_lambda_correlated, folder=\"Friedman_correlated\"  # or None to use all found\n",
    ")\n",
    "\n",
    "# # Evaluate tanh models\n",
    "df_rmse_nodewise_lambda, df_posterior_rmse_nodewise_lambda = compute_rmse_from_fits(\n",
    "    nodewise_lambda_fits, model_names_nodewise_lambda\n",
    ")\n",
    "# df_crps_tanh = compute_crps_from_fits(\n",
    "#     tanh_fits, model_names_tanh\n",
    "# )\n",
    "\n",
    "df_rmse_nodewise_lambda_correlated, df_posterior_rmse_nodewise_lambda_correlated = compute_rmse_from_fits(\n",
    "    nodewise_lambda_fits_correlated, model_names_nodewise_lambda_correlated, folder=\"Friedman_correlated\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_local = df_rmse_local_lambda.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"rmse\", \"mean\"),\n",
    "    acc_std=(\"rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()\n",
    "\n",
    "summary_local_correlated = df_rmse_local_lambda_correlated.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"rmse\", \"mean\"),\n",
    "    acc_std=(\"rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "summary_nodewise = df_rmse_nodewise_lambda.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"rmse\", \"mean\"),\n",
    "    acc_std=(\"rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()\n",
    "\n",
    "summary_nodewise_correlated = df_rmse_nodewise_lambda_correlated.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"rmse\", \"mean\"),\n",
    "    acc_std=(\"rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()\n",
    "# print(summary_relu_correlated.to_latex(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "\n",
    "print(summary_local.to_latex(index=False, float_format=\"%.3f\"))\n",
    "print(summary_nodewise.to_latex(index=False, float_format=\"%.3f\"))\n",
    "print(summary_local_correlated.to_latex(index=False, float_format=\"%.3f\"))\n",
    "print(summary_nodewise_correlated.to_latex(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "# print(summary_tanh_correlated.to_latex(index=False, float_format=\"%.3f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = df_rmse_local_lambda.assign(activation=\"local\", setting=\"Original\")\n",
    "df2 = df_rmse_nodewise_lambda.assign(activation=\"nodewise\", setting=\"Original\")\n",
    "df3 = df_rmse_local_lambda_correlated.assign(activation=\"local\", setting=\"Correlated\")\n",
    "df4 = df_rmse_nodewise_lambda_correlated.assign(activation=\"nodewise\", setting=\"Correlated\")\n",
    "\n",
    "df_all = pd.concat([df1, df2, df3, df4], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# --- prepare data ---\n",
    "df = df_all.copy()\n",
    "\n",
    "abbr = {\n",
    "    \"Regularized Horseshoe\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DST\",\n",
    "}\n",
    "\n",
    "# unify model names across activations (strip \" tanh\")\n",
    "df[\"model_clean\"] = df[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "df[\"model_clean\"] = df[\"model_clean\"].str.replace(\" nodewise\", \"\", regex=False)\n",
    "# summary stats per (setting, N, model, activation)\n",
    "summary = (\n",
    "    df.groupby([\"setting\", \"N\", \"model_clean\", \"activation\"], as_index=False)[\"rmse\"]\n",
    "      .agg(mean=\"mean\", std=\"std\")\n",
    ")\n",
    "\n",
    "# plotting order\n",
    "settings = [\"Original\", \"Correlated\"]\n",
    "Ns = [100, 200, 500]\n",
    "models = [\"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "\n",
    "# visuals\n",
    "markers = {\"local\": \"o\", \"nodewise\": \"^\"}            # shapes\n",
    "offsets = {\"local\": -0.05, \"nodewise\": +0.05}        # side-by-side jitter on x\n",
    "model_offsets = {\n",
    "    \"Regularized Horseshoe\": -0.04,\n",
    "    \"Dirichlet Horseshoe\": +0.00,\n",
    "    \"Dirichlet Student T\": +0.04,\n",
    "}\n",
    "palette_list = plt.get_cmap(\"tab10\").colors\n",
    "palette = {m: palette_list[i+1] for i, m in enumerate(models)}\n",
    "\n",
    "# map N to base x positions and add offsets for activation\n",
    "xbase = {N: i for i, N in enumerate(Ns)}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "for ax, setting in zip(axes, settings):\n",
    "    sub = summary[summary[\"setting\"] == setting]\n",
    "    # plot each model+activation with errorbars, without lines\n",
    "    for m in models:\n",
    "        for act in [\"local\", \"nodewise\"]:\n",
    "            g = sub[(sub[\"model_clean\"] == m) & (sub[\"activation\"] == act)]\n",
    "            if g.empty:\n",
    "                continue\n",
    "            #xs = [xbase[n] + offsets[act] for n in g[\"N\"]]\n",
    "            xs = [xbase[n] + offsets[act] + model_offsets[m] for n in g[\"N\"]]\n",
    "\n",
    "            ax.errorbar(\n",
    "                xs, g[\"mean\"], yerr=g[\"std\"],\n",
    "                fmt=markers[act], markersize=7,\n",
    "                linestyle=\"none\", capsize=3,\n",
    "                color=palette[m], markeredgecolor=\"black\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(f\"{setting}\")\n",
    "    ax.set_xticks(range(len(Ns)))\n",
    "    ax.set_xticklabels(Ns)\n",
    "    ax.set_xlabel(\"N\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.grid()\n",
    "\n",
    "# --- legends ---\n",
    "model_handles = [\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"none\",\n",
    "        color=palette[m],\n",
    "        markeredgecolor=\"black\",\n",
    "        markersize=7,\n",
    "        label=abbr.get(m, m)   # <- use abbreviation\n",
    "    )\n",
    "    for m in models\n",
    "]\n",
    "\n",
    "# activation legend (shapes)\n",
    "activation_handles = [\n",
    "    Line2D([0], [0], marker=markers[\"local\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=7, label=\"Local\"),\n",
    "    Line2D([0], [0], marker=markers[\"nodewise\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=7, label=\"Nodewise\"),\n",
    "]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(\n",
    "        handles=model_handles + activation_handles,\n",
    "        title=None,\n",
    "        loc=\"upper right\",\n",
    "        frameon=False,\n",
    "        ncol=1\n",
    "    )\n",
    "plt.tight_layout(rect=(0, 0, 1, 1))\n",
    "#plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARSITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "def compute_sparse_rmse_results(seeds, models, all_fits, get_N_sigma, forward_pass, folder,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "        path = f\"datasets/{folder}/{dataset_key}.npz\"\n",
    "\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "            #print(y_test.shape)\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                W2 = W2_samples[i]\n",
    "\n",
    "                # Apply pruning mask if requested\n",
    "                if prune_fn is not None and sparsity > 0.0:\n",
    "                    masks = prune_fn([W1, W2], sparsity)\n",
    "                    W1 = W1 * masks[0]\n",
    "                    #W2 = W2 * masks[1]\n",
    "\n",
    "                y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i])\n",
    "                y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "                rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "                \n",
    "            posterior_mean = np.mean(y_hats, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'sparsity': sparsity,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "seeds = [1, 2, 11]\n",
    "seeds_correlated = [1, 6, 11]\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    if seed == 1:\n",
    "        N=100\n",
    "    elif seed == 2:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma\n",
    "\n",
    "def get_N_sigma_correlated(seed):\n",
    "    if seed == 1:\n",
    "        N=100\n",
    "    elif seed == 6:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse_local_lambda_sparse, df_posterior_rmse_local_lambda_sparse = {}, {}\n",
    "df_rmse_local_lambda_sparse_correlated, df_posterior_rmse_local_lambda_sparse_correlated = {}, {}\n",
    "df_rmse_nodewise_lambda_sparse, df_posterior_rmse__nodewise_lambda_sparse = {}, {}\n",
    "df_rmse_nodewise_lambda_sparse_correlated, df_posterior_rmse_nodewise_lambda_sparse_correlated = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    df_rmse_local_lambda_sparse[sparsity], df_posterior_rmse_local_lambda_sparse[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_local_lambda, local_lambda_fits, get_N_sigma, forward_pass_tanh, folder = \"friedman\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_local_lambda_sparse_correlated[sparsity], df_posterior_rmse_local_lambda_sparse_correlated[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds_correlated, model_names_local_lambda_correlated, local_lambda_fits_correlated, get_N_sigma_correlated, forward_pass_tanh, folder = \"friedman_correlated\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_nodewise_lambda_sparse[sparsity], df_posterior_rmse__nodewise_lambda_sparse[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_nodewise_lambda, nodewise_lambda_fits, get_N_sigma, forward_pass_tanh, folder = \"friedman\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_nodewise_lambda_sparse_correlated[sparsity], df_posterior_rmse_nodewise_lambda_sparse_correlated[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds_correlated, model_names_nodewise_lambda_correlated, nodewise_lambda_fits_correlated, get_N_sigma_correlated, forward_pass_tanh, folder = \"friedman_correlated\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_rmse_full_local = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_local_lambda_sparse.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_rmse_full_local_correlated = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_local_lambda_sparse_correlated.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "\n",
    "df_rmse_full_nodewise = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_nodewise_lambda_sparse.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_rmse_full_nodewise_correlated = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_nodewise_lambda_sparse_correlated.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "\n",
    "df_local_o = df_rmse_full_local.copy()\n",
    "df_local_o[\"model\"] = df_local_o[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "df_local_c = df_rmse_full_local_correlated.copy()\n",
    "df_local_c[\"model\"] = df_local_c[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "df_nodewise_o = df_rmse_full_nodewise.copy()\n",
    "df_nodewise_o[\"model\"] = df_nodewise_o[\"model\"].str.replace(\" tanh nodewise\", \"\", regex=False)\n",
    "\n",
    "df_nodewise_c = df_rmse_full_nodewise_correlated.copy()\n",
    "df_nodewise_c[\"model\"] = df_nodewise_c[\"model\"].str.replace(\" tanh nodewise\", \"\", regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "# --- Your palettes and abbreviations ---\n",
    "palette = {\n",
    "    \"Regularized Horseshoe\": \"C1\",\n",
    "    \"Dirichlet Horseshoe\": \"C2\",\n",
    "    \"Dirichlet Student T\": \"C3\",\n",
    "}\n",
    "abbr = {\n",
    "    \"Regularized Horseshoe\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DST\",\n",
    "}\n",
    "\n",
    "def make_merged_df(\n",
    "    df_local_o, df_local_c, df_nodewise_o, df_nodewise_c,\n",
    "    drop_tanh_suffix=True\n",
    "):\n",
    "    \"\"\"Return one long df with columns: N, sparsity, rmse, model, activation, setting.\"\"\"\n",
    "    dfs = []\n",
    "    # Tanh (optionally strip ' tanh' from model names if present)\n",
    "    for df, setting in [(df_local_o, \"Original\"), (df_local_c, \"Correlated\")]:\n",
    "        d = df.copy()\n",
    "        if drop_tanh_suffix and \" tanh\" in \"\".join(d[\"model\"].unique()):\n",
    "            d[\"model\"] = d[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "        d[\"activation\"] = \"Local\"\n",
    "        d[\"setting\"] = setting\n",
    "        dfs.append(d)\n",
    "    # ReLU\n",
    "    for df, setting in [(df_nodewise_o, \"Original\"), (df_nodewise_c, \"Correlated\")]:\n",
    "        d = df.copy()\n",
    "        if drop_tanh_suffix and \" tanh nodewise\" in \"\".join(d[\"model\"].unique()):\n",
    "            d[\"model\"] = d[\"model\"].str.replace(\" tanh nodewise\", \"\", regex=False)\n",
    "        d[\"activation\"] = \"Nodewise\"\n",
    "        d[\"setting\"] = setting\n",
    "        dfs.append(d)\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Keep only models that exist in BOTH activations so legend doesn't show ghosts\n",
    "    models_local = set(out.loc[out.activation==\"Local\",\"model\"].unique())\n",
    "    models_nodewise = set(out.loc[out.activation==\"Nodewise\",\"model\"].unique())\n",
    "    common_models = sorted(list(models_local & models_nodewise))\n",
    "    if common_models:\n",
    "        out = out[out[\"model\"].isin(common_models)]\n",
    "    return out\n",
    "\n",
    "df_all = make_merged_df(df_local_o, df_local_c, df_nodewise_o, df_nodewise_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_one_figure(\n",
    "    df_all,\n",
    "    Ns=(100, 200, 500), figsize=(12, 7), title=\"Original vs Correlated\"\n",
    "):\n",
    "    \n",
    "\n",
    "    # Orderings\n",
    "    setting_order = [\"Original\", \"Correlated\"]\n",
    "    activation_order = [\"Local\", \"Nodewise\"]\n",
    "\n",
    "    # Seaborn aesthetics (keeps your 'talk' sizing / whitegrid)\n",
    "    #sns.set_context(\"talk\")\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams.update({\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        #\"axes.titleweight\": \"semibold\",\n",
    "        \"legend.frameon\": True\n",
    "    })\n",
    "\n",
    "    fig, axes = plt.subplots(2, len(Ns), figsize=figsize, sharex=True, sharey=\"col\")\n",
    "    if len(Ns) == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "\n",
    "    # We’ll plot using seaborn’s style mapping (style=activation, markers=True, dashes=True)\n",
    "    # so tanh vs ReLU are visually distinct and consistent across the grid.\n",
    "    for j, Nval in enumerate(Ns):\n",
    "        for i, setting in enumerate(setting_order):\n",
    "            ax = axes[i, j]\n",
    "            dfN = df_all[(df_all[\"N\"] == Nval) & (df_all[\"setting\"] == setting)].copy()\n",
    "            # Safety: if empty, skip\n",
    "            if dfN.empty:\n",
    "                ax.set_visible(False)\n",
    "                continue\n",
    "\n",
    "            # Use abbreviated labels on the legend (we’ll build custom legends later anyway)\n",
    "            dfN[\"model_abbr\"] = dfN[\"model\"].map(lambda m: abbr.get(m, m))\n",
    "\n",
    "            sns.lineplot(\n",
    "                data=dfN,\n",
    "                x=\"sparsity\",\n",
    "                y=\"rmse\",\n",
    "                hue=\"model_abbr\",      # color = prior (abbr)\n",
    "                style=\"activation\",    # style = activation\n",
    "                markers=True,\n",
    "                dashes=True,\n",
    "                palette={abbr[k]: v for k, v in palette.items() if k in dfN[\"model\"].unique()},\n",
    "                hue_order=[abbr[m] for m in sorted(dfN[\"model\"].unique(), key=lambda x: list(palette).index(x) if x in palette else 999)],\n",
    "                style_order=activation_order,\n",
    "                errorbar=None,\n",
    "                ax=ax,\n",
    "            )\n",
    "            #ax.set_title(f\"N={Nval}\")\n",
    "            ax.set_title(f\"N={Nval}\", fontweight=\"normal\")\n",
    "\n",
    "            ax.set_xlabel(\"Sparsity\")\n",
    "            ax.set_ylabel(\"RMSE\" if j == 0 else \"\")\n",
    "            ax.grid(True, which=\"major\", alpha=0.25)\n",
    "            if ax.legend_:  # remove local legends\n",
    "                ax.legend_.remove()\n",
    "\n",
    "    # ---------- Build two clean, global legends ----------\n",
    "    # 1) Prior legend (colors), using abbreviations in desired order present in data\n",
    "    models_present = []\n",
    "    for m in [\"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]:\n",
    "        if (df_all[\"model\"] == m).any():\n",
    "            models_present.append(m)\n",
    "    prior_handles = [\n",
    "        Line2D([0],[0], color=palette[m], marker='o', linestyle='-', linewidth=2, markersize=7)\n",
    "        for m in models_present\n",
    "    ]\n",
    "    prior_labels = [abbr[m] for m in models_present]\n",
    "\n",
    "    # 2) Activation legend (styles) – black lines with linestyle/markers\n",
    "    # Let seaborn pick the default mapping; we emulate a solid for tanh and dashed for ReLU.\n",
    "    act_style = {\n",
    "        \"Local\": dict(linestyle='-'),#, marker='o'),\n",
    "        \"Nodewise\": dict(linestyle='--'),#, marker='s'),\n",
    "    }\n",
    "    activation_handles = [\n",
    "        Line2D([0],[0], color='black', linewidth=2, markersize=7, **act_style[act])\n",
    "        for act in activation_order\n",
    "        if (df_all[\"activation\"] == act).any()\n",
    "    ]\n",
    "    activation_labels = [act for act in activation_order if (df_all[\"activation\"] == act).any()]\n",
    "\n",
    "    # Place legends: priors on top center, activations below it\n",
    "    # (Adjust bbox_to_anchor if you prefer side-by-side or bottom placement.)\n",
    "    if prior_handles:\n",
    "        leg1 = fig.legend(\n",
    "            prior_handles, prior_labels,\n",
    "            title=\"Prior\",\n",
    "            loc=\"upper right\",\n",
    "            ncol=len(prior_handles),\n",
    "            frameon=True,\n",
    "            bbox_to_anchor=(0.6, 1.02)\n",
    "        )\n",
    "        fig.add_artist(leg1)\n",
    "    if activation_handles:\n",
    "        fig.legend(\n",
    "            activation_handles, activation_labels,\n",
    "            title=\"Activation\",\n",
    "            loc=\"upper left\",\n",
    "            ncol=len(activation_handles),\n",
    "            frameon=True,\n",
    "            bbox_to_anchor=(0.6, 1.02)\n",
    "        )\n",
    "    #fig.suptitle(title, y=1.08, fontsize=18)\n",
    "    #plt.tight_layout(rect=[0.02, 0.02, 0.98, 0.94])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_rmse_one_figure(df_all,\n",
    "                     Ns=(100, 200, 500),\n",
    "                     title=\"Original vs Correlated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import rayleigh, expon, dirichlet\n",
    "\n",
    "# --- scale parameter ---\n",
    "s = 1.0\n",
    "\n",
    "# --- distributions ---\n",
    "#x = np.linspace(0, 10, 1000)\n",
    "\n",
    "# version A: Rayleigh with scale s\n",
    "#pdf_A = rayleigh(scale=s).pdf(x)\n",
    "#pdf_C = expon(scale=1/2).pdf(x)\n",
    "\n",
    "# version B: Rayleigh(1) scaled outside: lambda = s * R\n",
    "# => density of s * R is 1/s * Rayleigh(1)(x/s)\n",
    "#pdf_B = (1/s) * rayleigh(scale=1).pdf(x / s)\n",
    "\n",
    "samples_A = rayleigh.rvs(size=1000, scale=1)\n",
    "samples_B = expon.rvs(size=1000, scale=2)\n",
    "\n",
    "plt.hist(samples_A**2, bins=30, alpha=0.6, label=\"Rayleigh\")\n",
    "plt.hist(samples_B, bins=30, alpha=0.6, label=\"Exponential\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "# --- settings ---\n",
    "alphas = [0.1, 0.5, 1, 2, 10, 1000]\n",
    "n_samples = 5000\n",
    "K = 3  # dimension of the Dirichlet\n",
    "bins = np.linspace(0, 1, 31)  # common bins for all histograms\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(8, 6), sharex=True, sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = [\"C0\", \"C1\", \"C2\"]\n",
    "labels = [\"Component 1\", \"Component 2\", \"Component 3\"]\n",
    "\n",
    "for ax, a in zip(axes, alphas):\n",
    "    # symmetric Dirichlet(alpha, alpha, alpha)\n",
    "    alpha_vec = np.full(K, a)\n",
    "    samples = dirichlet.rvs(alpha_vec, size=n_samples)\n",
    "\n",
    "    # plot all three components in the same subplot\n",
    "    for k in range(K):\n",
    "        ax.hist(\n",
    "            samples[:, k],\n",
    "            bins=bins,\n",
    "            density=True,\n",
    "            alpha=0.5,\n",
    "            color=colors[k],\n",
    "            label=labels[k] if a == alphas[0] else None,  # avoid repeated legend entries\n",
    "        )\n",
    "\n",
    "    ax.set_title(rf\"$\\alpha = {a}$\")\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "# common labels\n",
    "fig.text(0.5, 0.02, \"x\", ha=\"center\")\n",
    "fig.text(0.02, 0.5, \"Density\", va=\"center\", rotation=\"vertical\")\n",
    "\n",
    "# one shared legend\n",
    "handles, leg_labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, leg_labels, loc=\"upper center\", ncol=3)\n",
    "\n",
    "plt.tight_layout(rect=[0.03, 0.05, 1, 0.88])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize_networks import compute_activation_frequency, extract_all_pruned_means, plot_all_networks_subplots_activations\n",
    "path = \"datasets/friedman/Friedman_N100_p10_sigma1.00_seed1.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]\n",
    "path_correlated = \"datasets/friedman_correlated/Friedman_N100_p10_sigma1.00_seed1.npz\"\n",
    "data_correlated = np.load(path_correlated)\n",
    "x_train_correlated = data[\"X_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 10\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "layer_sizes = [P] + [H]*L + [out_nodes]\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "sparsity_level = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_means = extract_all_pruned_means(local_lambda_fits['Friedman_N100_p10_sigma1.00_seed1'], layer_structure, sparsity_level)\n",
    "\n",
    "p1, widths_1 = plot_all_networks_subplots_activations(pruned_model_means, \n",
    "                                                      layer_sizes, \n",
    "                                                      node_activation_colors = None, \n",
    "                                                      activation_color_max=None, \n",
    "                                                      signed_colors=False)\n",
    "\n",
    "pruned_model_means_nodewise = extract_all_pruned_means(nodewise_lambda_fits['Friedman_N100_p10_sigma1.00_seed1'], layer_structure, sparsity_level)\n",
    "\n",
    "p2, widths_2 = plot_all_networks_subplots_activations(pruned_model_means_nodewise, \n",
    "                                                      layer_sizes, \n",
    "                                                      node_activation_colors = None, \n",
    "                                                      activation_color_max=None, \n",
    "                                                      signed_colors=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
