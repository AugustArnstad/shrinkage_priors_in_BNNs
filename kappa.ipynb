{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#from sklearn.metrics import mean_squared_errosr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_priors = \"results/priors/single_layer/tanh/friedman\"\n",
    "results_dir_posteriors = \"results/regression/single_layer/tanh/friedman\"\n",
    "#results_dir_posteriors_dst = \"results/regression/single_layer/tanh/friedman/full_regularization\"\n",
    "\n",
    "prior_names_and_configs = {\"Gaussian\": \"gauss\", \"Regularized Horseshoe\": \"reg_hs\", \"Dirichlet Gamma\": \"dir_gam\", \"Dirichlet Horseshoe\": \"dir_hs\", \"Dirichlet Student T\": \"dir_stud_t\"}\n",
    "posterior_names = [\"Dirichlet Horseshoe tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Gaussian tanh\"]\n",
    "#posterior_names_and_configs_dst = [\"Dirichlet Student T tanh\"]\n",
    "   \n",
    "prior_fits = {}\n",
    "posterior_fits = {}\n",
    "\n",
    "for key, value in prior_names_and_configs.items():\n",
    "    prior_fit = get_model_fits(\n",
    "        config=value,\n",
    "        results_dir=results_dir_priors,\n",
    "        models=key,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    prior_fits[key] = prior_fit\n",
    "\n",
    "prior_fits = {outer: inner[outer] for outer, inner in prior_fits.items()}\n",
    "    \n",
    "posterior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N200_fits = get_model_fits(\n",
    "    config=\"Friedman_N200_p10_sigma1.00_seed2\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N500_fits = get_model_fits(\n",
    "    config=\"Friedman_N500_p10_sigma1.00_seed11\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpmath import hyper\n",
    "from mpmath import gamma\n",
    "from scipy.special import poch\n",
    "\n",
    "\n",
    "p=10\n",
    "alpha=1.0\n",
    "beta=(p-1)*alpha\n",
    "\n",
    "\n",
    "def p_kappa_dirichlet_horseshoe(kappa, alpha, beta, a_j=1.0):\n",
    "    if kappa <= 0 or kappa >= 1:\n",
    "        return 0.0\n",
    "    prefactor = (1/np.pi) * (a_j / ((1-kappa) * np.sqrt(kappa) * np.sqrt(1-kappa)))# * (1/p)\n",
    "    c = (kappa / (1-kappa))*(a_j**2)\n",
    "    # {}_3F_2([1, 1.1/2, 2.1/1], [1, 3/2], z)\n",
    "    gamma_const = (gamma(alpha+1/2)/gamma(alpha))*(gamma(alpha+beta)/(gamma(alpha+beta+1/2)))\n",
    "    hyper_val = hyper([1, alpha+1/2], [alpha+beta+1/2], -c)\n",
    "    return float(prefactor * gamma_const * hyper_val)\n",
    "\n",
    "\n",
    "def p_kappa_dirichlet_student_t(kappa, alpha, beta, nu=3.0, a_j=1.0):\n",
    "    if kappa <= 0 or kappa >= 1:\n",
    "        return 0.0\n",
    "    C = gamma((nu+1)/2) * 1/(np.sqrt(np.pi * nu) * gamma(nu/2))\n",
    "    prefactor = ((a_j**nu) * nu**((nu+1)/2) * kappa**(nu/2 - 1)) / ((1-kappa)**(nu/2 + 1)) # * poch(alpha, nu) / poch(p*alpha, nu)\n",
    "    c = (-kappa / (1-kappa))*(a_j**2)\n",
    "    gamma_const = (gamma(alpha+nu/2)/gamma(alpha))*(gamma(alpha+beta)/(gamma(alpha+beta+nu/2)))\n",
    "    hyper_val = hyper([(nu+1)/2, alpha + nu/2], [alpha+beta+nu/2], c)\n",
    "    return float(C * prefactor * gamma_const * hyper_val)\n",
    "\n",
    "\n",
    "\n",
    "def p_kappa_horseshoe(kappa, a_j=1.0):\n",
    "    if kappa <= 0 or kappa >= 1:\n",
    "        return 0.0\n",
    "    prefactor = (1/np.pi) * (a_j / ((a_j**2-1)*kappa + 1)) * 1/(np.sqrt(kappa) * np.sqrt(1-kappa))\n",
    "    return float(prefactor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have updated the density plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    \\frac{1}{\\pi} \\frac{\\sqrt{q_{j}} \\tau}{(1-\\kappa_j)\\sqrt{\\kappa_j}\\sqrt{1-\\kappa_j}} \\frac{\\Gamma(\\alpha+\\frac{1}{2})}{\\Gamma(\\alpha)}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha+\\beta+\\frac{1}{2})}{}_2F_1\\!\\left(\\begin{matrix} 1, \\alpha+\\frac{1}{2} \\\\ \\alpha + \\beta + \\frac{1}{2} \\end{matrix}; -c \\right)\n",
    "\\end{aligned} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}  \\frac{1}{(1-\\kappa)^{\\frac{\\nu}{2}+1}}\\nu^{\\frac{\\nu + 1}{2}} \\kappa^{\\frac{\\nu}{2}-1}q_{j}^{\\frac{\\nu}{2}}\\tau^{\\nu} \\mathbb{E}_{\\xi_j}\\left[\\frac{\\xi_j^{\\nu/2}}{\\left(1+c\\xi_j\\right)^{\\frac{\\nu+1}{2}}}\\right]\n",
    "\\end{aligned} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag kappa-grid\n",
    "kappa_vals = np.linspace(0.001, 0.999, 500)\n",
    "p_vals_dirichlet_horseshoe = [p_kappa_dirichlet_horseshoe(k, alpha, beta) for k in kappa_vals]\n",
    "p_vals_dirichlet_horseshoe_medium_low = [p_kappa_dirichlet_horseshoe(k, alpha, beta, a_j=0.7) for k in kappa_vals]\n",
    "p_vals_dirichlet_horseshoe_medium_high = [p_kappa_dirichlet_horseshoe(k, alpha, beta, a_j=2) for k in kappa_vals]\n",
    "p_vals_dirichlet_horseshoe_low = [p_kappa_dirichlet_horseshoe(k, alpha, beta, a_j=0.1) for k in kappa_vals]\n",
    "p_vals_dirichlet_horseshoe_high = [p_kappa_dirichlet_horseshoe(k, alpha, beta, a_j=5) for k in kappa_vals]\n",
    "\n",
    "p_vals_dirichlet_student_t = [p_kappa_dirichlet_student_t(k, alpha, beta) for k in kappa_vals]\n",
    "p_vals_dirichlet_student_t_medium_low = [p_kappa_dirichlet_student_t(k, alpha, beta, a_j=0.7) for k in kappa_vals]\n",
    "p_vals_dirichlet_student_t_medium_high = [p_kappa_dirichlet_student_t(k, alpha, beta, a_j=2) for k in kappa_vals]\n",
    "p_vals_dirichlet_student_t_low = [p_kappa_dirichlet_student_t(k, alpha, beta, a_j=0.1) for k in kappa_vals]\n",
    "p_vals_dirichlet_student_t_high = [p_kappa_dirichlet_student_t(k, alpha, beta, a_j=5) for k in kappa_vals]\n",
    "\n",
    "p_vals_horseshoe = [p_kappa_horseshoe(k) for k in kappa_vals]\n",
    "p_vals_horseshoe_medium_low = [p_kappa_horseshoe(k, a_j=0.7) for k in kappa_vals]\n",
    "p_vals_horseshoe_medium_high = [p_kappa_horseshoe(k, a_j=2) for k in kappa_vals]\n",
    "p_vals_horseshoe_low = [p_kappa_horseshoe(k, a_j=0.1) for k in kappa_vals]\n",
    "p_vals_horseshoe_high = [p_kappa_horseshoe(k, a_j=5) for k in kappa_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6), sharex=True, sharey=False)\n",
    "\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe_low)#, label=r\"$a_j = 0.1$\")\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe_medium_low)#, label=r\"$a_j = 0.7$\")\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe)#, label=r\"$a_j = 1$\")\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe_medium_high)#, label=r\"$a_j = 2$\")\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe_high)#, label=r\"$a_j = 5$\")\n",
    "axes[0].set_ylabel(r\"$p(\\kappa \\mid \\sigma, \\tau)$\")\n",
    "axes[0].set_xlabel(r\"$\\kappa$\")\n",
    "axes[0].set_title(\"Dirichlet–Horseshoe\")\n",
    "axes[0].set_ylim((0, 7))\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t_low, label=r\"$a_j = 0.1$\")\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t_medium_low, label=r\"$a_j = 0.7$\")\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t, label=r\"$a_j = 1$\")\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t_medium_high, label=r\"$a_j = 2$\")\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t_high, label=r\"$a_j = 5$\")\n",
    "#axes[1].set_ylabel(r\"$p(\\kappa \\mid \\sigma, \\tau)$\")\n",
    "axes[1].set_xlabel(r\"$\\kappa$\")\n",
    "axes[1].set_title(\"Dirichlet–Student T\")\n",
    "axes[1].set_ylim((0, 7))\n",
    "axes[1].legend(loc='upper center')\n",
    "\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe_low)#, label=r\"$a_j = 0.1$\")\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe_medium_low)#, label=r\"$a_j = 0.7$\")\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe)#, label=r\"$a_j = 1$\")\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe_medium_high)#, label=r\"$a_j = 2$\")\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe_high)#, label=r\"$a_j = 5$\")\n",
    "#axes[2].set_ylabel(r\"$p(\\kappa \\mid \\sigma, \\tau)$\")\n",
    "axes[2].set_xlabel(r\"$\\kappa$\")\n",
    "axes[2].set_title(\"Horseshoe\")\n",
    "axes[2].set_ylim((0, 7))\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, mpmath as mp\n",
    "\n",
    "def poch(x, n):  # rising factorial (x)_n for real n\n",
    "    return mp.gamma(x + n) / mp.gamma(x)\n",
    "\n",
    "def theory_beta_sqrt(alpha, beta, s):\n",
    "    t1 = mp.hyp2f1(1, alpha, alpha + beta, s**2)\n",
    "    t2 = s * poch(alpha, 0.5) / poch(alpha + beta, 0.5) * mp.hyp2f1(1, alpha + 0.5, alpha + beta + 0.5, s**2)\n",
    "    return t1 - t2\n",
    "\n",
    "def empirical_estimate(alpha, beta, s, n=100_000, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    xi = rng.beta(alpha, beta, size=n)\n",
    "    vals = 1.0 / (1.0 + s * np.sqrt(xi))\n",
    "    m = float(vals.mean())\n",
    "    se = float(vals.std(ddof=1) / np.sqrt(n))\n",
    "    return m, se\n",
    "\n",
    "# --- set parameters ---\n",
    "p=10\n",
    "alpha, beta = 0.1, (p-1)*alpha\n",
    "s, n, seed  = 1.0, 100_000, 123  # require s > -1\n",
    "# ----------------------\n",
    "\n",
    "theory = float(theory_beta_sqrt(alpha, beta, s))\n",
    "emp, se = empirical_estimate(alpha, beta, s, n, seed)\n",
    "diff, z = emp - theory, (emp - theory) / se\n",
    "\n",
    "\n",
    "print(\"theory\", np.round(theory, 5), \"\\nempirical\", np.round(emp, 5), \"\\nabs_diff\", np.round(diff, 5), \"\\nSE_MC\", np.round(se, 5), \"\\nz_score\", np.round(z, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, mpmath as mp\n",
    "from scipy.stats import betaprime\n",
    "from mpmath import hyper\n",
    "\n",
    "n = 1_00\n",
    "p=10\n",
    "alpha, beta = 0.1, (p-1)*alpha\n",
    "nu=3\n",
    "s=0.1\n",
    "t = betaprime.rvs(1/2, nu/2, size=n)\n",
    "hyper_vec = np.zeros(n)\n",
    "for i in range(n):\n",
    "    hyper_vec[i] = hyper([1, alpha], [alpha + beta], -s*nu*t[i])\n",
    "\n",
    "emp_mean = np.mean(hyper_vec)\n",
    "\n",
    "theory = mp.hyper([1, alpha, 1/2], [alpha + beta, (nu+1)/2], -s*nu)\n",
    "print(emp_mean-theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, mpmath as mp\n",
    "from scipy.stats import betaprime\n",
    "\n",
    "# Parametre\n",
    "n = 100\n",
    "p = 10\n",
    "alpha = 0.1\n",
    "beta = (p-1)*alpha  # 0.9\n",
    "nu = 3.0\n",
    "s = 1.0\n",
    "\n",
    "t = betaprime.rvs(0.5, nu/2, size=n)\n",
    "emp_vals = [mp.hyp2f1(1, alpha, alpha+beta, -s*nu*ti) for ti in t]\n",
    "emp_mean = float(np.mean([float(v) for v in emp_vals]))\n",
    "\n",
    "# Teori via dobbeltintegralet\n",
    "B05 = mp.beta(0.5, nu/2)\n",
    "norm_ab = mp.gamma(alpha+beta)/(mp.gamma(alpha)*mp.gamma(beta))\n",
    "\n",
    "def I_of_x(x):\n",
    "    f = lambda tt: (tt**(-0.5)) * ((1+tt)**(-(nu+1)/2)) / (1 + s*nu*x*tt)\n",
    "    return (1/B05) * mp.quad(f, [0, mp.inf])\n",
    "\n",
    "g = lambda x: (x**(alpha-1)) * ((1-x)**(beta-1)) * I_of_x(x)\n",
    "theory_int = norm_ab * mp.quad(g, [0, 1])\n",
    "\n",
    "print(\"Empirisk (s=1):\", emp_mean)\n",
    "print(\"Teori (integral):\", theory_int)\n",
    "print(\"Diff: \", emp_mean- theory_int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/friedman/Friedman_N500_p10_sigma1.00_seed11.npz\"\n",
    "data = np.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------- 1) Simuler X ----------\n",
    "def simulate_X(n, P, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.uniform(0.0, 1.0, size=(n, P))\n",
    "\n",
    "# ---------- 2) Aktivering og derivert ----------\n",
    "def get_activation(activation=\"tanh\"):\n",
    "    if activation == \"tanh\":\n",
    "        phi = np.tanh\n",
    "        def dphi(a): return 1.0 - np.tanh(a)**2\n",
    "    elif activation == \"relu\":\n",
    "        def phi(a): return np.maximum(0.0, a)\n",
    "        def dphi(a): return (a > 0.0).astype(a.dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    return phi, dphi\n",
    "\n",
    "# ---------- 3) Hovedfunksjon: q for alle trekk ----------\n",
    "def compute_q_for_fit(cmdstan_mcmc, N=1000, activation=\"tanh\", seed=1, output_index=0, X=None):\n",
    "    \"\"\"\n",
    "    Beregn q_{ell, j} for første-lagsvektene for hver trekk (draw).\n",
    "    Returnerer:\n",
    "      q_draws:  (n_draws, H, P)\n",
    "      q_mean:   (H, P)  – gjennomsnitt over trekk\n",
    "      X:        (N, P)  – datasettet brukt i beregningen\n",
    "    \"\"\"\n",
    "    # Hent ut variabler fra Stan\n",
    "    W1_all = cmdstan_mcmc.stan_variable(\"W_1\")            # (draws, P, H)\n",
    "    WL_all = cmdstan_mcmc.stan_variable(\"W_L\")             # (draws, H, O)\n",
    "    hb_all = cmdstan_mcmc.stan_variable(\"hidden_bias\")     # (draws, L, H)\n",
    "    sigma_all = cmdstan_mcmc.stan_variable(\"sigma\")        # (draws,)\n",
    "    Wint_all = cmdstan_mcmc.stan_variable(\"W_internal\")    # (draws, max(L-1,1), H, H)\n",
    "\n",
    "    draws, P, H = W1_all.shape\n",
    "    O = WL_all.shape[2]\n",
    "    L = hb_all.shape[1]\n",
    "\n",
    "    if O == 0:\n",
    "        raise ValueError(\"W_L has zero output nodes. Expected at least 1.\")\n",
    "    if output_index < 0 or output_index >= O:\n",
    "        raise ValueError(f\"output_index {output_index} out of range 0..{O-1}\")\n",
    "\n",
    "    if X is None:\n",
    "        X = simulate_X(N, P, seed=seed)\n",
    "\n",
    "    X_sq = X**2\n",
    "    phi, dphi = get_activation(activation)\n",
    "\n",
    "    q_draws = np.empty((draws, H, P), dtype=float)\n",
    "\n",
    "    for s in range(draws):\n",
    "        W1 = W1_all[s]            # (P, H)\n",
    "        WL = WL_all[s]            # (H, O)\n",
    "        hb = hb_all[s]            # (L, H)\n",
    "        Wints = Wint_all[s]       # (max(L-1,1), H, H)\n",
    "        sigma = float(sigma_all[s])\n",
    "\n",
    "        # ----- Forward pass -----\n",
    "        a_list = []\n",
    "        h_list = []\n",
    "\n",
    "        a = X @ W1 + hb[0]        # (N, H)\n",
    "        h = phi(a)\n",
    "        a_list.append(a); h_list.append(h)\n",
    "\n",
    "        for l in range(1, L):\n",
    "            Wl = Wints[l-1]       # (H, H)\n",
    "            a = h @ Wl + hb[l]    # (N, H)\n",
    "            h = phi(a)\n",
    "            a_list.append(a); h_list.append(h)\n",
    "\n",
    "        # ----- Backward: delta_L = d f / d a^(L) -----\n",
    "        # lineær utgang: df/dh^(L) = WL[:, output_index]\n",
    "        v = WL[:, output_index]           # (H,)\n",
    "        delta = dphi(a_list[-1]) * v      # (N, H), broadcast over N\n",
    "\n",
    "        # Bakover gjennom skjulte lag\n",
    "        for l in range(L-2, -1, -1):\n",
    "            Wnext = Wints[l]              # (H, H) – brukes bare hvis L>1\n",
    "            delta = (delta @ Wnext.T) * dphi(a_list[l]) if L > 1 else delta\n",
    "\n",
    "        delta1 = delta  # (N, H) == ∂f/∂a^(1)\n",
    "\n",
    "        # ----- q: (1/sigma^2) * sum_i (delta1[i,ell]^2 * X[i,j]^2) -----\n",
    "        D_sq = delta1**2                  # (N, H)\n",
    "        Q = (X_sq.T @ D_sq) / (sigma**2)  # (P, H)\n",
    "        q_draws[s] = Q.T                  # (H, P)\n",
    "\n",
    "    q_mean = q_draws.mean(axis=0)         # (H, P)\n",
    "    return q_draws, q_mean, X\n",
    "\n",
    "# ---------- 4) Eksempelbruk ----------\n",
    "# Velg riktig fit-objekt (CmdStanMCMC) fra dict-en din:\n",
    "prior_q_dhs, prior_q_mean_dhs, X_train = compute_q_for_fit(\n",
    "    prior_fits['Dirichlet Horseshoe']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "posterior_q_dhs, posterior_q_mean_dhs, X_train = compute_q_for_fit(\n",
    "    posterior_N500_fits['Dirichlet Horseshoe tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "prior_q_dst, prior_q_mean_dst, X_train = compute_q_for_fit(\n",
    "    prior_fits['Dirichlet Student T']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "posterior_q_dst, posterior_q_mean_dst, X_train = compute_q_for_fit(\n",
    "    posterior_N500_fits['Dirichlet Student T tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "prior_q_rhs, prior_q_mean_rhs, X_train = compute_q_for_fit(\n",
    "    prior_fits['Regularized Horseshoe']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "posterior_q_rhs, posterior_q_mean_rhs, X_train = compute_q_for_fit(\n",
    "    posterior_N500_fits['Regularized Horseshoe tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "prior_q_gauss, prior_q_mean_gauss, X_train = compute_q_for_fit(\n",
    "    prior_fits['Gaussian']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "posterior_q_gauss, posterior_q_mean_gauss, X_train = compute_q_for_fit(\n",
    "    posterior_N500_fits['Gaussian tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "prior_q_gamma, prior_q_mean_gamma, X_train = compute_q_for_fit(\n",
    "    prior_fits['Dirichlet Gamma']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- choose which node/input to inspect ----\n",
    "node_idx = 1\n",
    "input_idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Horseshoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- PRIOR --------\n",
    "prior_fit = prior_fits['Dirichlet Horseshoe']['posterior']\n",
    "tau_prior = prior_fit.stan_variable(\"tau\")                                   # (draws_prior,)\n",
    "lam_prior = prior_fit.stan_variable(\"lambda_tilde_data\")[:, :, node_idx][:, input_idx]  # (draws_prior,)\n",
    "phi_prior = prior_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]           # (draws_prior,)\n",
    "q_prior   = prior_q_dhs[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "# -------- POSTERIOR --------\n",
    "post_fit = posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior']\n",
    "tau_post = post_fit.stan_variable(\"tau\")                                      # (draws_post,)\n",
    "lam_post = post_fit.stan_variable(\"lambda_tilde_data\")[:, :, node_idx][:, input_idx]    # (draws_post,)\n",
    "phi_post = post_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]             # (draws_post,)\n",
    "q_post   = posterior_q_dhs[:, node_idx, input_idx]                           # (draws_post,)\n",
    "\n",
    "kappa_DHS_prior = 1.0 / (1.0 + q_prior * (tau_prior**2) * (lam_prior**2) * (phi_prior**2))\n",
    "kappa_DHS_post = 1.0 / (1.0 + q_post * (tau_post**2) * (lam_post**2) * (phi_post**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Student T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- PRIOR --------\n",
    "prior_fit = prior_fits['Dirichlet Student T']['posterior']\n",
    "tau_prior = prior_fit.stan_variable(\"tau\")                                   # (draws_prior,)\n",
    "lam_prior = prior_fit.stan_variable(\"lambda_tilde_data\")[:, :, node_idx][:, input_idx]  # (draws_prior,)\n",
    "phi_prior = prior_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]           # (draws_prior,)\n",
    "q_prior   = prior_q_dhs[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "# -------- POSTERIOR --------\n",
    "post_fit = posterior_N100_fits['Dirichlet Student T tanh']['posterior']\n",
    "tau_post = post_fit.stan_variable(\"tau\")                                      # (draws_post,)\n",
    "lam_post = post_fit.stan_variable(\"lambda_tilde_data\")[:, :, node_idx][:, input_idx]    # (draws_post,)\n",
    "phi_post = post_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]             # (draws_post,)\n",
    "q_post   = posterior_q_dhs[:, node_idx, input_idx]                           # (draws_post,)\n",
    "\n",
    "\n",
    "kappa_DST_prior = 1.0 / (1.0 + q_prior * (tau_prior**2) * (lam_prior**2) * (phi_prior**2))\n",
    "kappa_DST_post = 1.0 / (1.0 + q_post * (tau_post**2) * (lam_post**2) * (phi_post**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- PRIOR --------\n",
    "prior_fit = prior_fits['Dirichlet Gamma']['posterior']\n",
    "tau_prior = prior_fit.stan_variable(\"tau\")                                   # (draws_prior,)\n",
    "lam_prior = prior_fit.stan_variable(\"lambda_data\")[:, :, node_idx][:, input_idx]  # (draws_prior,)\n",
    "phi_prior = prior_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]           # (draws_prior,)\n",
    "q_prior   = prior_q_gamma[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "\n",
    "kappa_DG_prior = 1.0 / (1.0 + q_prior * (tau_prior**2) * (lam_prior**2) * (phi_prior**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Horseshoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_fit = prior_fits['Regularized Horseshoe']['posterior']\n",
    "tau_prior = prior_fit.stan_variable(\"tau\")                                   # (draws_prior,)\n",
    "lam_prior = prior_fit.stan_variable(\"lambda_tilde\")[:, :, node_idx][:, input_idx]  # (draws_prior,)\n",
    "q_prior   = prior_q_rhs[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "post_rhs_fit = posterior_N100_fits['Regularized Horseshoe tanh']['posterior']\n",
    "tau_rhs_post = post_rhs_fit.stan_variable(\"tau\")                                      # (draws_post,)\n",
    "lam_rhs_post = post_rhs_fit.stan_variable(\"lambda_tilde\")[:, :, node_idx][:, input_idx]    # (draws_post,)\n",
    "q_rhs_post   = posterior_q_rhs[:, node_idx, input_idx]                           # (draws_post,)\n",
    "\n",
    "kappa_HS_prior = 1.0 / (1.0 + q_prior * (tau_prior**2) * (lam_prior**2))\n",
    "kappa_HS_post = 1.0 / (1.0 + q_rhs_post * (tau_rhs_post**2) * (lam_rhs_post**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_prior   = prior_q_gauss[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "q_gauss_post   = posterior_q_gauss[:, node_idx, input_idx]                           # (draws_post,)\n",
    "\n",
    "kappa_gauss_prior = 1.0 / (1.0 + q_prior)\n",
    "kappa_gauss_post = 1.0 / (1.0 + q_gauss_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Plot: prior vs posterior overlays --------\n",
    "bins = np.linspace(0.0, 1.0, 40)  # common bins so densities are comparable\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "# Horseshoe\n",
    "axes[0, 0].hist(kappa_gauss_prior, bins=bins, density=True, alpha=0.45, label=\"Prior\")\n",
    "axes[0, 0].hist(kappa_gauss_post,  bins=bins, density=True, alpha=0.45, label=\"Posterior\")\n",
    "axes[0, 0].set_title(\"Gaussian\")\n",
    "axes[0, 0].set_xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Horseshoe\n",
    "axes[0, 1].hist(kappa_HS_prior, bins=bins, density=True, alpha=0.45, label=\"Prior\")\n",
    "axes[0, 1].hist(kappa_HS_post,  bins=bins, density=True, alpha=0.45, label=\"Posterior\")\n",
    "axes[0, 1].set_title(\"RHS\")\n",
    "axes[0, 1].set_xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Dirichlet–Horseshoe\n",
    "axes[1, 0].hist(kappa_DHS_prior, bins=bins, density=True, alpha=0.45, label=\"Prior\")\n",
    "axes[1, 0].hist(kappa_DHS_post,  bins=bins, density=True, alpha=0.45, label=\"Posterior\")\n",
    "axes[1, 0].set_title(\"DHS\")\n",
    "axes[1, 0].set_xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "axes[1, 0].set_ylabel(\"Density\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Dirichlet–Horseshoe\n",
    "axes[1, 1].hist(kappa_DST_prior, bins=bins, density=True, alpha=0.45, label=\"Prior\")\n",
    "axes[1, 1].hist(kappa_DST_post,  bins=bins, density=True, alpha=0.45, label=\"Posterior\")\n",
    "axes[1, 1].set_title(\"DS-T\")\n",
    "axes[1, 1].set_xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "\n",
    "plt.suptitle(f\"Node {node_idx}, input {input_idx}\")#: Prior vs Posterior\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()\n",
    "plt.hist(kappa_DG_prior, bins=50, density=True, alpha=0.45, label=\"Dirichlet Gamma prior\")\n",
    "plt.legend()\n",
    "#plt.xlim(3, 50)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "colors = {\n",
    "    \"Gaussian\": \"C0\",\n",
    "    \"RHS\": \"C1\",\n",
    "    \"DHS\": \"C2\",\n",
    "    \"DS-T\": \"C3\",\n",
    "}\n",
    "\n",
    "# Plot linjene\n",
    "sns.kdeplot(kappa_gauss_prior, linestyle=\"--\", color=colors[\"Gaussian\"])\n",
    "sns.kdeplot(kappa_gauss_post, linestyle=\"-\",  color=colors[\"Gaussian\"])\n",
    "sns.kdeplot(kappa_HS_prior, linestyle=\"--\", color=colors[\"RHS\"])\n",
    "sns.kdeplot(kappa_HS_post, linestyle=\"-\",  color=colors[\"RHS\"])\n",
    "sns.kdeplot(kappa_DHS_prior, linestyle=\"--\", color=colors[\"DHS\"])\n",
    "sns.kdeplot(kappa_DHS_post, linestyle=\"-\",  color=colors[\"DHS\"])\n",
    "sns.kdeplot(kappa_DST_prior, linestyle=\"--\", color=colors[\"DS-T\"])\n",
    "sns.kdeplot(kappa_DST_post, linestyle=\"-\",  color=colors[\"DS-T\"])\n",
    "\n",
    "plt.xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Prior shrinkage factors – KDE\")\n",
    "\n",
    "# --- Lag legend ---\n",
    "# Farger (modeller)\n",
    "model_handles = [Line2D([0], [0], color=color, lw=2, label=model) \n",
    "                 for model, color in colors.items()]\n",
    "\n",
    "# Linjestil (prior/posterior)\n",
    "style_handles = [\n",
    "    Line2D([0], [0], color=\"black\", linestyle=\"--\", lw=2, label=\"Prior\"),\n",
    "    Line2D([0], [0], color=\"black\", linestyle=\"-\",  lw=2, label=\"Posterior\")\n",
    "]\n",
    "\n",
    "# Kombiner\n",
    "handles = model_handles + style_handles\n",
    "labels = [h.get_label() for h in handles]\n",
    "\n",
    "plt.legend(handles=handles, labels=labels, title=\"Model / Distribution\", ncol=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized versions\n",
    "\n",
    "For the Regularized Horseshoe, when $c<\\infty$. the shrinkage profile becomes approximately equivalent to that of the horseshoe shifted from the interval $(0, 1)$ to $(b_{\\ell, j}, 1)$ where $b_{\\ell, j}=\\frac{1}{q_{\\ell, j}\\tau{\\ell}^2c^2}$. The shrinkage factor then approximately satisfies $\\tilde{\\kappa}_{\\ell, j}=(1-b_j)\\kappa_{\\ell, j} + b_j$ where $\\tilde{\\kappa}_{\\ell, j}$ is the shrinkage factor of the original horseshoe. From this we get $1-\\tilde{\\kappa}_{\\ell, j}=(1-b_j)(1-\\kappa_{\\ell, j})$. Assuming roughly the same scale of inputs, then $b_{\\ell, j}=b_{\\ell}$ and the effective model complexity becomes $\\tilde{m}_{\\ell, eff}=(1-b_{\\ell}) m_{\\ell, eff}$ where $m_{\\ell, eff}=\\sum_{j=1}^p(1-\\kappa_{\\ell, j})$ denotes the effective number of nonzero weights into node $l$ (I THINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_kappa(fit, q, node_idx, input_idx, model_type='gaussian'):\n",
    "    \"\"\"\n",
    "    Returnerer arrays per trekning (S,): kappa_original, b_j, kappa_tilde\n",
    "    \"\"\"\n",
    "    q_hp = np.asarray(q)[:, node_idx, input_idx]  # (S,)\n",
    "\n",
    "    if model_type == 'gaussian':\n",
    "        tau = 1.0\n",
    "        lam = 1.0\n",
    "        c_sq = 1.0\n",
    "        phi = 1.0\n",
    "    else:\n",
    "        tau = fit.stan_variable(\"tau\")                          # (S,)\n",
    "        c_sq = fit.stan_variable(\"c_sq\")[:, node_idx]           # (S,)\n",
    "\n",
    "        if model_type == 'rhs':\n",
    "            lam = fit.stan_variable(\"lambda_tilde\")[:, node_idx, input_idx]       # (S,)\n",
    "            phi = 1.0\n",
    "        elif model_type == 'dhs' or 'dst':\n",
    "            lam = fit.stan_variable(\"lambda_tilde_data\")[:, node_idx, input_idx]  # (S,)\n",
    "            phi = fit.stan_variable(\"phi_data\")[:, node_idx, input_idx]           # (S,)\n",
    "        # elif model_type == 'dst':\n",
    "        #     lam = fit.stan_variable(\"lambda_tilde_data\")[:, node_idx, input_idx]       # (S,)\n",
    "        #     phi = fit.stan_variable(\"phi_data\")[:, node_idx, input_idx]           # (S,)\n",
    "        else:\n",
    "            raise ValueError(\"model_type må være 'gaussian', 'rhs', 'dhs' eller 'dst'.\")\n",
    "\n",
    "    kappa_original = 1.0 / (1.0 + q_hp * (tau**2) * (lam**2) * (phi))  # (S,)\n",
    "    b_j            = 1.0 / (1.0 + q_hp * (tau**2) * c_sq * (phi))      # (S,)\n",
    "    kappa_tilde    = (1.0 - b_j) * kappa_original + b_j                # (S,)\n",
    "\n",
    "    return kappa_original, b_j, kappa_tilde\n",
    "\n",
    "\n",
    "def compute_kappa_per_input(fit, q, node_idx, model_type='gaussian'):\n",
    "    \"\"\"\n",
    "    For en gitt node: beregn per input p\n",
    "      - E[1 - kappa_tilde]_p\n",
    "      - E[(1 - kappa)(1 - b_j)]_p\n",
    "    og returnér begge som (P,) + totalsum som skalarer.\n",
    "    \"\"\"\n",
    "    S, H, P = np.asarray(q).shape\n",
    "    mean_1_minus_kappa_tilde = np.zeros(P)\n",
    "    mean_prod_identity       = np.zeros(P)\n",
    "\n",
    "    for p in range(P):\n",
    "        kappa, b_j, kappa_tilde = compute_kappa(fit, q, node_idx=node_idx, input_idx=p, model_type=model_type)\n",
    "\n",
    "        # Riktig aggregering: gjennomsnitt av uttrykket per trekning\n",
    "        mean_1_minus_kappa_tilde[p] = np.mean(1.0 - kappa_tilde)\n",
    "        mean_prod_identity[p]       = np.mean((1.0 - kappa) * (1.0 - b_j))\n",
    "\n",
    "    # Summér over inputs (antall ikke-null-vekter inn i noden i forventning)\n",
    "    total_meff_tilde = np.sum(mean_1_minus_kappa_tilde)\n",
    "    total_meff_check = np.sum(mean_prod_identity)\n",
    "\n",
    "    return total_meff_tilde, total_meff_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 16\n",
    "gauss_fit = prior_fits['Gaussian']['posterior']\n",
    "rhs_fit = prior_fits['Regularized Horseshoe']['posterior']\n",
    "dhs_fit = prior_fits['Dirichlet Horseshoe']['posterior']\n",
    "dst_fit = prior_fits['Dirichlet Student T']['posterior']\n",
    "\n",
    "nonzero = np.zeros((4, nodes))\n",
    "\n",
    "for i in range(nodes):\n",
    "    meff_tilde_g, _ = compute_kappa_per_input(\n",
    "        gauss_fit, prior_q_gauss, node_idx=i, model_type='gaussian'\n",
    "    )\n",
    "    meff_tilde_rhs, _ = compute_kappa_per_input(\n",
    "        rhs_fit, prior_q_rhs, node_idx=i, model_type='rhs'\n",
    "    )\n",
    "    meff_tilde_dhs, _ = compute_kappa_per_input(\n",
    "        dhs_fit, prior_q_dhs, node_idx=i, model_type='dhs'\n",
    "    )\n",
    "    meff_tilde_dst, _ = compute_kappa_per_input(\n",
    "        dst_fit, prior_q_dst, node_idx=i, model_type='dst'\n",
    "    )\n",
    "    nonzero[0, i] = meff_tilde_g\n",
    "    nonzero[1, i] = meff_tilde_rhs\n",
    "    nonzero[2, i] = meff_tilde_dhs\n",
    "    nonzero[3, i] = meff_tilde_dst\n",
    "    \n",
    "print(\"Prior number of nonzero weights: \", np.sum(nonzero, axis=1))\n",
    "print(\"Prior percentage of nonzero weights: \", np.sum(nonzero, axis=1)/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 16\n",
    "gauss_fit = posterior_N500_fits['Gaussian tanh']['posterior']\n",
    "rhs_fit = posterior_N500_fits['Regularized Horseshoe tanh']['posterior']\n",
    "dhs_fit = posterior_N500_fits['Dirichlet Horseshoe tanh']['posterior']\n",
    "dst_fit = posterior_N500_fits['Dirichlet Student T tanh']['posterior']\n",
    "\n",
    "nonzero = np.zeros((4, nodes))\n",
    "\n",
    "for i in range(nodes):\n",
    "    meff_tilde_g, _ = compute_kappa_per_input(\n",
    "        gauss_fit, posterior_q_gauss, node_idx=i, model_type='gaussian'\n",
    "    )\n",
    "    meff_tilde_rhs, _ = compute_kappa_per_input(\n",
    "        rhs_fit, posterior_q_rhs, node_idx=i, model_type='rhs'\n",
    "    )\n",
    "    meff_tilde_dhs, _ = compute_kappa_per_input(\n",
    "        dhs_fit, posterior_q_dhs, node_idx=i, model_type='dhs'\n",
    "    )\n",
    "    meff_tilde_dst, _ = compute_kappa_per_input(\n",
    "        dst_fit, posterior_q_dst, node_idx=i, model_type='dst'\n",
    "    )\n",
    "    nonzero[0, i] = meff_tilde_g\n",
    "    nonzero[1, i] = meff_tilde_rhs\n",
    "    nonzero[2, i] = meff_tilde_dhs\n",
    "    nonzero[3, i] = meff_tilde_dst\n",
    "    \n",
    "print(\"Posterior number of nonzero weights: \", np.sum(nonzero, axis=1))\n",
    "print(\"Posterior percentage of nonzero weights: \", np.sum(nonzero, axis=1)/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "path = \"datasets/friedman/Friedman_N100_p10_sigma1.00_seed1.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]\n",
    "\n",
    "fits = prior_fits\n",
    "\n",
    "P = 10\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "\n",
    "def build_single_draw_weights(fits, layer_structure, draw_idx):\n",
    "    \"\"\"Return {model: {'W_1': (P,H), 'W_L': (H,O)}} for ONE draw.\"\"\"\n",
    "    out = {}\n",
    "    for name, fd in fits.items():\n",
    "        fit = fd[\"posterior\"]\n",
    "        W1 = fit.stan_variable(layer_structure['input_to_hidden']['name'])[draw_idx]\n",
    "        WL = fit.stan_variable(layer_structure['hidden_to_output']['name'])[draw_idx]\n",
    "        WL = WL.reshape(layer_structure['hidden_to_output']['shape'])\n",
    "        out[name] = {\"W_1\": W1, \"W_L\": WL}\n",
    "    return out\n",
    "\n",
    "def scale_W1_for_plot(model_means, mode='global'):\n",
    "    \"\"\"\n",
    "    Skalerer alle W_1 til [-1, 1] for rettferdig sammenligning av edge-tykkelser.\n",
    "\n",
    "    mode:\n",
    "      - 'global' : én felles skala over alle modeller (mest sammenlignbar)\n",
    "      - 'per_model': egen skala per modell (uavhengig sammenligning)\n",
    "      - 'per_node' : skalerer hver kolonne (node) separat til [-1,1]\n",
    "\n",
    "    Returnerer: scaled_model_means (samme struktur som input), scale_info\n",
    "    \"\"\"\n",
    "    scaled = {}\n",
    "    if mode == 'global':\n",
    "        gmax = max(np.abs(m['W_1']).max() for m in model_means.values())\n",
    "        gmax = max(gmax, 1e-12)\n",
    "        for name, m in model_means.items():\n",
    "            W1s = m['W_1'] / gmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'global', 'scale': gmax}\n",
    "\n",
    "    elif mode == 'per_model':\n",
    "        for name, m in model_means.items():\n",
    "            s = max(np.abs(m['W_1']).max(), 1e-12)\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = m['W_1'] / s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_model'}\n",
    "\n",
    "    elif mode == 'per_node':\n",
    "        for name, m in model_means.items():\n",
    "            W1 = m['W_1'].copy()\n",
    "            P, H = W1.shape\n",
    "            for h in range(H):\n",
    "                colmax = max(np.abs(W1[:, h]).max(), 1e-12)\n",
    "                W1[:, h] = W1[:, h] / colmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_node'}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'global', 'per_model', or 'per_node'\")\n",
    "\n",
    "def plot_models_with_activations(model_means, layer_sizes,\n",
    "                                 activations=None, activation_color_max=None,\n",
    "                                 ncols=3, figsize_per_plot=(5,4), signed_colors=False):\n",
    "    \"\"\"\n",
    "    model_means: dict {model_name: {'W_1':(P,H), 'W_L':(H,O), optional 'W_internal':[...]} }\n",
    "    layer_sizes: f.eks [P, H, O] eller [P, H, H, O] ved internlag\n",
    "    activations: dict {model_name: (H,)} – aktiveringsfrekvens kun for første skjulte lag\n",
    "    activation_color_max: global maks for skalering av farger (hvis None brukes 1.0)\n",
    "    \"\"\"\n",
    "    names = list(model_means.keys())\n",
    "    n_models = len(names)\n",
    "    nrows = int(np.ceil(n_models / ncols))\n",
    "    figsize = (figsize_per_plot[0] * ncols, figsize_per_plot[1] * nrows)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    if nrows * ncols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Skru av blanke akser\n",
    "    for ax in axes[n_models:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax, name in zip(axes, names):\n",
    "        weights = model_means[name]\n",
    "        G = nx.DiGraph()\n",
    "        pos, nodes_per_layer, node_colors = {}, [], []\n",
    "\n",
    "        # Noder med posisjon og farge\n",
    "        for li, size in enumerate(layer_sizes):\n",
    "            ids = []\n",
    "            ycoords = np.linspace(size - 1, 0, size) - (size - 1) / 2\n",
    "            for i in range(size):\n",
    "                nid = f\"L{li}_{i}\"\n",
    "                G.add_node(nid)\n",
    "                pos[nid] = (li, ycoords[i])\n",
    "                ids.append(nid)\n",
    "\n",
    "                if activations is not None and li == 1:  # kun første skjulte lag\n",
    "                    #a = activations.get(name, np.zeros(size))\n",
    "                    a = activations.get(name, np.zeros(size))\n",
    "                    a = np.asarray(a).ravel()   # <-- flater til 1D array\n",
    "                    scale = activation_color_max if activation_color_max is not None else 1.0\n",
    "                    val = float(np.clip(a[i] / max(scale, 1e-12), 0.0, 1.0))\n",
    "                    color = plt.cm.winter(val)\n",
    "                else:\n",
    "                    color = 'lightgray'\n",
    "                node_colors.append(color)\n",
    "\n",
    "            nodes_per_layer.append(ids)\n",
    "\n",
    "        edge_colors, edge_widths = [], []\n",
    "\n",
    "        def add_edges(W, inn, ut):\n",
    "            for j, out_n in enumerate(ut):\n",
    "                for i, in_n in enumerate(inn):\n",
    "                    w = float(W[i, j])\n",
    "                    G.add_edge(in_n, out_n, weight=abs(w))\n",
    "                    edge_colors.append('red' if w >= 0 else 'blue')\n",
    "                    edge_widths.append(abs(w))\n",
    "\n",
    "        # input -> hidden(1)\n",
    "        add_edges(weights['W_1'], nodes_per_layer[0], nodes_per_layer[1])\n",
    "\n",
    "        # ev. internlag\n",
    "        if 'W_internal' in weights:\n",
    "            for l, Win in enumerate(weights['W_internal']):\n",
    "                add_edges(Win, nodes_per_layer[l+1], nodes_per_layer[l+2])\n",
    "\n",
    "        # siste hidden -> output\n",
    "        add_edges(weights['W_L'], nodes_per_layer[-2], nodes_per_layer[-1])\n",
    "\n",
    "        nx.draw(G, pos, ax=ax,\n",
    "                node_color=node_colors,\n",
    "                edge_color=(edge_colors if signed_colors else 'red'),\n",
    "                width=[G[u][v]['weight'] for u,v in G.edges()],\n",
    "                with_labels=False, node_size=400, arrows=False)\n",
    "\n",
    "        ax.set_title(name, fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def compute_hidden_activation(fit_dict, x_train, draw_idx):\n",
    "    fit = fit_dict['posterior']\n",
    "    W1 = fit.stan_variable('W_1')[draw_idx, :, :]          # (P,H)\n",
    "    try:\n",
    "        b1 = fit.stan_variable('hidden_bias')[draw_idx, :] # (H,)\n",
    "    except Exception:\n",
    "        b1 = np.zeros(W1.shape[1])\n",
    "    # tanh i [-1,1]\n",
    "    a_full = np.tanh(x_train @ W1 + b1)             # (H,)\n",
    "    a=np.mean(a_full, axis=0)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velg en observasjon å \"lyse opp\" nodefargene med\n",
    "obs_idx = 3\n",
    "draw_idx = 69 #pick_draw_idx(prior_fits, seed=42)      # one common draw across models\n",
    "prior_draws = build_single_draw_weights(prior_fits, layer_structure, draw_idx)\n",
    "\n",
    "# 1) Beregn aktivasjoner for ALLE modellene\n",
    "activations = {}\n",
    "for name, fd in prior_fits.items():\n",
    "    a = compute_hidden_activation(fd, x_train, draw_idx)\n",
    "    activations[name] = np.abs(a)      \n",
    "\n",
    "# 2) Skaler vekter for plotting (som før)\n",
    "scaled, _ = scale_W1_for_plot(prior_draws, mode='per_model')\n",
    "\n",
    "# 3) Kall plottet med aktivasjoner\n",
    "# Siden tanh ∈ [-1,1] og vi bruker |a|, så sett activation_color_max=1.0\n",
    "fig = plot_models_with_activations(\n",
    "    scaled,\n",
    "    layer_sizes=[P, H, out_nodes],\n",
    "    activations=activations,\n",
    "    activation_color_max=1.0,\n",
    "    ncols=2,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# 2) Bygg prunede posterior-mean vekter for alle modeller (pruner K minste |W_1|)\n",
    "def build_pruned_means(fits, layer_structure, sparsity_level):\n",
    "    \"\"\"\n",
    "    fits: dict {model_name: {\"posterior\": CmdStanMCMC}}\n",
    "    layer_structure: {'input_to_hidden': {'name':'W_1','shape':(P,H)},\n",
    "                      'hidden_to_output':{'name':'W_L','shape':(H,O)}}\n",
    "                      # optional: 'hidden_to_hidden': {'name':..., 'shape': [(H,H), ...]}\n",
    "    sparsity_level: andel av W_1-elementer som settes til 0 (0.0–1.0)\n",
    "\n",
    "    Returnerer dict {model_name: {'W_1':(P,H), 'W_L':(H,O), 'W_internal':[...](optional)}}\n",
    "    \"\"\"\n",
    "    model_means = {}\n",
    "\n",
    "    for name, fd in fits.items():\n",
    "        fit = fd[\"posterior\"]\n",
    "\n",
    "        # Posterior means\n",
    "        W1_name = layer_structure['input_to_hidden']['name']\n",
    "        WL_name = layer_structure['hidden_to_output']['name']\n",
    "\n",
    "        W1 = fit.stan_variable(W1_name).mean(axis=0)  # (P,H)\n",
    "        WL = fit.stan_variable(WL_name).mean(axis=0)  # (H,O)\n",
    "        WL = WL.reshape(layer_structure['hidden_to_output']['shape'])\n",
    "\n",
    "        means = {'W_1': W1.copy(), 'W_L': WL}\n",
    "\n",
    "        # Optional internlag\n",
    "        if 'hidden_to_hidden' in layer_structure:\n",
    "            HH_name = layer_structure['hidden_to_hidden']['name']\n",
    "            HH_shapes = layer_structure['hidden_to_hidden']['shape']  # list of (H,H)\n",
    "            raw = fit.stan_variable(HH_name).mean(axis=0)\n",
    "            means['W_internal'] = [raw[i].reshape(HH_shapes[i]) for i in range(len(HH_shapes))]\n",
    "\n",
    "        # Pruning av W_1 (beholder de største |w|). Samme logikk som i koden din.\n",
    "        P, H = means['W_1'].shape\n",
    "        K = int(np.floor(sparsity_level * P * H))\n",
    "        if K > 0:\n",
    "            flat_abs = np.abs(means['W_1'].ravel())\n",
    "            idx_prune = np.argpartition(flat_abs, K)[:K]   # indeksene til de K minste\n",
    "            mask = np.ones_like(flat_abs, dtype=float)\n",
    "            mask[idx_prune] = 0.0\n",
    "            means['W_1'] = (means['W_1'].ravel() * mask).reshape(P, H)\n",
    "\n",
    "        model_means[name] = means\n",
    "\n",
    "    return model_means\n",
    "\n",
    "def compute_hidden_activation_from_means(model_means, x_train):\n",
    "    \"\"\"\n",
    "    model_means: dict {model_name: {'W_1':(P,H), 'W_L':(H,O)}}\n",
    "    x_train: (N,P)\n",
    "\n",
    "    Returnerer dict {model_name: (H,)} med gj.snitt aktivasjon pr. node.\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    for name, m in model_means.items():\n",
    "        W1 = m['W_1']                 # (P,H)\n",
    "        b1 = m.get('hidden_bias', np.zeros(W1.shape[1]))  # hvis bias finnes, ellers null\n",
    "        a_full = np.tanh(x_train @ W1 + b1)   # (N,H)\n",
    "        activations[name] = np.abs(a_full).mean(axis=0)  # (H,)\n",
    "    return activations\n",
    "\n",
    "# def input_contribution_shares_W1(fit, x_train, weight_by_activity=True):\n",
    "#     \"\"\"\n",
    "#     Returnerer (P,H) med s_{i,h} ≥ 0 og sum_i s_{i,h} ≈ 1.\n",
    "#     s_{i,h} lages ved å:\n",
    "#       1) for hvert datapunkt n, beregne |x_{n,i} w_{i,h}| og normalisere over i,\n",
    "#       2) gjennomsnittlig (valgfritt) vektet med |tanh(pre_{n,h})| over n.\n",
    "\n",
    "#     weight_by_activity=True anbefales for tanh.\n",
    "#     \"\"\"\n",
    "#     W1 = fit['posterior'].stan_variable('W_1').mean(axis=0)   # (P,H)\n",
    "#     try:\n",
    "#         b1 = fit['posterior'].stan_variable('hidden_bias').mean(axis=0)\n",
    "#     except Exception:\n",
    "#         b1 = np.zeros(W1.shape[1])\n",
    "\n",
    "#     pre = x_train @ W1 + b1                                   # (N,H)\n",
    "#     act = np.abs(np.tanh(pre))                                # (N,H)\n",
    "\n",
    "#     N, H = pre.shape\n",
    "#     P = W1.shape[0]\n",
    "#     shares = np.zeros((P, H))\n",
    "\n",
    "#     absW = np.abs(W1)                   # (P,H)\n",
    "#     absX = np.abs(x_train)              # (N,P)\n",
    "\n",
    "#     eps = 1e-12\n",
    "#     for h in range(H):\n",
    "#         contrib = absX * absW[:, h].reshape(1, P)             # (N,P): |x|*|w|\n",
    "#         denom = np.clip(contrib.sum(axis=1, keepdims=True), eps, None)  # (N,1)\n",
    "#         s_n = contrib / denom                                  # (N,P), radvis sum=1\n",
    "\n",
    "#         if weight_by_activity:\n",
    "#             w = act[:, h]                                      # (N,)\n",
    "#             wsum = float(w.sum())\n",
    "#             if wsum < eps:\n",
    "#                 shares[:, h] = 1.0 / P\n",
    "#             else:\n",
    "#                 shares[:, h] = (s_n.T @ w) / wsum              # (P,)\n",
    "#         else:\n",
    "#             shares[:, h] = s_n.mean(axis=0)                    # uvektet snitt\n",
    "\n",
    "#     return shares\n",
    "\n",
    "# def summarize_active_weights(shares, thresholds=(0.5, 0.8, 0.9)):\n",
    "#     \"\"\"\n",
    "#     shares: (P,H) med s_{i,h} ≥ 0 og kolonnevis sum ~ 1.\n",
    "#     thresholds: iterable, f.eks (0.5, 0.8, 0.9)\n",
    "\n",
    "#     Returnerer:\n",
    "#       - k_for_threshold: dict {thr: array(H,)}  # minste k per node\n",
    "#       - neff: array(H,)\n",
    "#       - gini: array(H,)\n",
    "#     \"\"\"\n",
    "#     P, H = shares.shape\n",
    "#     k_for_threshold = {thr: np.zeros(H, dtype=int) for thr in thresholds}\n",
    "#     neff = np.zeros(H)\n",
    "#     gini = np.zeros(H)\n",
    "\n",
    "#     for h in range(H):\n",
    "#         v = np.sort(shares[:, h])[::-1]               # synkende\n",
    "#         csum = np.cumsum(v)\n",
    "#         total = csum[-1] if csum.size else 0.0\n",
    "#         total = max(total, 1e-12)\n",
    "\n",
    "#         for thr in thresholds:\n",
    "#             k = int(np.searchsorted(csum, thr * total) + 1)  # +1 -> antall elementer\n",
    "#             k_for_threshold[thr][h] = min(k, P)\n",
    "\n",
    "#         # N_eff = 1 / sum s_i^2\n",
    "#         neff[h] = 1.0 / max((shares[:, h] ** 2).sum(), 1e-12)\n",
    "\n",
    "#         # Gini (diskret): 1 - 2 * (sum_{i=1}^P (P+1-i) s_{(i)}) / (P * sum s_i)\n",
    "#         w = v\n",
    "#         gini[h] = 1.0 - 2.0 * ( (np.arange(1, P+1) * w).sum() ) / (P * total)\n",
    "\n",
    "#     return k_for_threshold, neff, gini\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/friedman/Friedman_N500_p10_sigma1.00_seed11.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]\n",
    "\n",
    "fits = posterior_N500_fits       # posterior_N100_fits\n",
    "\n",
    "P = 10\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "# 1) bygg/prune posterior-mean som før (fra tidligere funksjoner dine)\n",
    "pruned_model_means = build_pruned_means(fits, layer_structure, sparsity_level=0.0)\n",
    "\n",
    "# 2) skaler W1 for sammenlignbar tykkelse\n",
    "scaled_means, scale_info = scale_W1_for_plot(pruned_model_means, mode='per_model')  # eller 'per_model' / 'per_node'\n",
    "activations = compute_hidden_activation_from_means(pruned_model_means, x_train)\n",
    "all_vals = np.concatenate(list(activations.values()))\n",
    "vmin, vmax = all_vals.min(), all_vals.max()\n",
    "\n",
    "# 4) plot med skalerte vekter (W_1 byttes ut)\n",
    "fig = plot_models_with_activations(\n",
    "    scaled_means,\n",
    "    layer_sizes=[P, H, 1],\n",
    "    activations=activations,\n",
    "    activation_color_max=vmax,\n",
    "    ncols=2,\n",
    "    signed_colors=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "def mean_abs(arr):  # arr: (S, ...)\n",
    "    return np.mean(np.abs(np.asarray(arr)), axis=0)\n",
    "\n",
    "def nid_single_hidden(posterior, W1_name=\"W_1\", WL_name_candidates=(\"W_L\",\"W_2\")):\n",
    "    \"\"\"\n",
    "    posterior: CmdStanMCMC-objekt\n",
    "    W_1: shape (S, P, H)  (input -> hidden), som du har\n",
    "    W_L: shape (S, H) eller (S, H, O)  (hidden -> output)\n",
    "    \"\"\"\n",
    "    W1_samps = posterior.stan_variable(W1_name)          # (S, P, H)\n",
    "    # Finn navn for siste lag\n",
    "    for nm in WL_name_candidates:\n",
    "        try:\n",
    "            WL_samps = posterior.stan_variable(nm)       # (S, H) eller (S, H, O)\n",
    "            break\n",
    "        except Exception:\n",
    "            WL_samps = None\n",
    "    if WL_samps is None:\n",
    "        raise ValueError(\"Fant ikke siste-lag-vekter (prøv å angi riktig navn i WL_name_candidates).\")\n",
    "\n",
    "    # Posterior plug-in: gjennomsnitt av absoluttverdier\n",
    "    W1_abs = mean_abs(W1_samps)                          # (P, H)\n",
    "    WL_abs = mean_abs(WL_samps)                          # (H,) eller (H, O)\n",
    "\n",
    "    # z^(1): aggregert node-innflytelse (sum over outputs hvis flere)\n",
    "    if WL_abs.ndim == 1:\n",
    "        z1 = WL_abs                                      # (H,)\n",
    "    else:\n",
    "        z1 = WL_abs.sum(axis=1)                          # (H,)\n",
    "\n",
    "    P, H = W1_abs.shape\n",
    "\n",
    "    # Main effects: ω({j}) = Σ_i z_i * |W1[j,i]|\n",
    "    omega_main = (W1_abs * z1[None, :]).sum(axis=1)      # (P,)\n",
    "\n",
    "    # Pairwise: ω({j,k}) = Σ_i z_i * min(|W1[j,i]|, |W1[k,i]|)\n",
    "    omega_pair = np.zeros((P, P))\n",
    "    for j, k in combinations(range(P), 2):\n",
    "        mins = np.minimum(W1_abs[j, :], W1_abs[k, :])    # (H,)\n",
    "        omega = np.dot(z1, mins)                         # skalar\n",
    "        omega_pair[j, k] = omega_pair[k, j] = omega\n",
    "\n",
    "    return z1, omega_main, omega_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = posterior_N100_fits['Dirichlet Student T tanh']['posterior'] \n",
    "z1, omega_main, omega_pair = nid_single_hidden(post) # W_1=(S,P,H), W_L/(W_2)=(S,H[,O]) # Eksempler: # - topp 10 viktigste noder etter z1: \n",
    "top_nodes = np.argsort(-z1) # - topp 10 viktigste features (main effects): \n",
    "top_feats = np.argsort(-omega_main) # - sterkeste parvise interaksjoner: \n",
    "P = omega_pair.shape[0] \n",
    "pairs = [(j, k, omega_pair[j, k]) for j in range(P) for k in range(j+1, P)] \n",
    "top_pairs = sorted(pairs, key=lambda t: -t[2])[:10]\n",
    "\n",
    "res = np.array(omega_main/(np.sum(omega_main)))\n",
    "print(np.round(res, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop 5 feature pairs (interactions):\")\n",
    "for (j,k,score) in top_pairs[:10]:\n",
    "    print(f\" Pair ({j},{k}): {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gini_coefficient(x):\n",
    "    \"\"\"\n",
    "    Gini-indeks for en vektor x (ikke-negativ).\n",
    "    Hvis alle elementer like → 0, hvis konsentrert på få elementer → nær 1.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).flatten()\n",
    "    if np.all(x == 0):\n",
    "        return 0.0\n",
    "    x = np.sort(np.abs(x))   # bruker absoluttverdier av vektene\n",
    "    n = len(x)\n",
    "    cumx = np.cumsum(x)\n",
    "    return (n + 1 - 2*np.sum(cumx)/cumx[-1]) / n\n",
    "\n",
    "def node_weight_stats(W1_samples, node_idx):\n",
    "    \"\"\"\n",
    "    W1_samples: (S, P, H)\n",
    "    node_idx: valgt skjult node\n",
    "    Returnerer:\n",
    "      ginis: (S,) gini-indeks per sample\n",
    "      sums:  (S,) sum av abs(vekter) per sample\n",
    "    \"\"\"\n",
    "    ginis = []\n",
    "    for s in range(W1_samples.shape[0]):\n",
    "        w = W1_samples[s, :, node_idx]   # (P,)\n",
    "        ginis.append(gini_coefficient(w))\n",
    "    return np.array(ginis)\n",
    "\n",
    "# Gaussian\n",
    "W1_gauss = posterior_N500_fits['Gaussian tanh']['posterior'].stan_variable(\"W_1\")  # (S,P,H)\n",
    "ginis_g = node_weight_stats(W1_gauss, node_idx=2)\n",
    "\n",
    "# RHS\n",
    "W1_gauss = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_1\")  # (S,P,H)\n",
    "ginis_rhs = node_weight_stats(W1_gauss, node_idx=2)\n",
    "\n",
    "# DHS\n",
    "W1_dhs = posterior_N500_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_1\")\n",
    "ginis_dhs = node_weight_stats(W1_dhs, node_idx=2)\n",
    "\n",
    "# DS-T\n",
    "W1_dhs = posterior_N500_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_1\")\n",
    "ginis_dst = node_weight_stats(W1_dhs, node_idx=2)\n",
    "\n",
    "print(\"Gaussian node 1:\")\n",
    "print(f\"  Gini mean={ginis_g.mean():.3f}, q05={np.quantile(ginis_g,0.05):.3f}, q95={np.quantile(ginis_g,0.95):.3f}\")\n",
    "\n",
    "print(\"RHS node 1:\")\n",
    "print(f\"  Gini mean={ginis_rhs.mean():.3f}, q05={np.quantile(ginis_rhs,0.05):.3f}, q95={np.quantile(ginis_rhs,0.95):.3f}\")\n",
    "\n",
    "print(\"DHS node 1:\")\n",
    "print(f\"  Gini mean={ginis_dhs.mean():.3f}, q05={np.quantile(ginis_dhs,0.05):.3f}, q95={np.quantile(ginis_dhs,0.95):.3f}\")\n",
    "\n",
    "print(\"DS-T node 1:\")\n",
    "print(f\"  Gini mean={ginis_dst.mean():.3f}, q05={np.quantile(ginis_dst,0.05):.3f}, q95={np.quantile(ginis_dst,0.95):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NODES, maybe not as important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize_networks import compute_activation_frequency, extract_all_pruned_means, plot_all_networks_subplots_activations\n",
    "path = \"datasets/friedman/Friedman_N100_p10_sigma1.00_seed1.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]\n",
    "\n",
    "node_activation_colors = {\n",
    "    model_name: compute_activation_frequency(fit, x_train)\n",
    "    for model_name, fit in posterior_N100_fits.items()\n",
    "}\n",
    "layer_sizes = [P] + [H]*L + [out_nodes]\n",
    "# Flatten and find the global maximum\n",
    "all_freqs = np.concatenate(list(node_activation_colors.values()))\n",
    "global_max = all_freqs.max()\n",
    "print(global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize_networks import extract_all_pruned_node_means, plot_all_networks_subplots_activations\n",
    "num_nodes_to_prune = 14  # for example\n",
    "pruned_model_means_nodes = extract_all_pruned_node_means(posterior_N100_fits, layer_structure, num_nodes_to_prune)\n",
    "\n",
    "p_nodes, widths_nodes = plot_all_networks_subplots_activations(\n",
    "    pruned_model_means_nodes, layer_sizes, node_activation_colors,\n",
    "    activation_color_max=global_max, signed_colors=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
