{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#from sklearn.metrics import mean_squared_errosr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_priors = \"results/priors/single_layer/tanh/friedman\"\n",
    "results_dir_posteriors = \"results/regression/single_layer/tanh/friedman\"\n",
    "#results_dir_posteriors_dst = \"results/regression/single_layer/tanh/friedman/full_regularization\"\n",
    "\n",
    "prior_names_and_configs = {\"Gaussian\": \"gauss\", \"Regularized Horseshoe\": \"reg_hs\", \"Dirichlet Gamma\": \"dir_gam\", \"Dirichlet Horseshoe\": \"dir_hs\", \"Dirichlet Student T\": \"dir_stud_t\"}\n",
    "posterior_names = [\"Dirichlet Horseshoe tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Gaussian tanh\"]\n",
    "#posterior_names_and_configs_dst = [\"Dirichlet Student T tanh\"]\n",
    "   \n",
    "prior_fits = {}\n",
    "posterior_fits = {}\n",
    "\n",
    "for key, value in prior_names_and_configs.items():\n",
    "    prior_fit = get_model_fits(\n",
    "        config=value,\n",
    "        results_dir=results_dir_priors,\n",
    "        models=key,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    prior_fits[key] = prior_fit\n",
    "\n",
    "prior_fits = {outer: inner[outer] for outer, inner in prior_fits.items()}\n",
    "    \n",
    "posterior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N200_fits = get_model_fits(\n",
    "    config=\"Friedman_N200_p10_sigma1.00_seed2\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N500_fits = get_model_fits(\n",
    "    config=\"Friedman_N500_p10_sigma1.00_seed11\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpmath import hyper\n",
    "from mpmath import gamma\n",
    "from scipy.special import poch\n",
    "\n",
    "\n",
    "p=10\n",
    "alpha=1.0\n",
    "beta=(p-1)*alpha\n",
    "\n",
    "\n",
    "def p_kappa_dirichlet_horseshoe(kappa, alpha, beta, a_j=1.0):\n",
    "    if kappa <= 0 or kappa >= 1:\n",
    "        return 0.0\n",
    "    prefactor = (1/np.pi) * (a_j / ((1-kappa) * np.sqrt(kappa) * np.sqrt(1-kappa)))# * (1/p)\n",
    "    c = (kappa / (1-kappa))*(a_j**2)\n",
    "    # {}_3F_2([1, 1.1/2, 2.1/1], [1, 3/2], z)\n",
    "    gamma_const = (gamma(alpha+1/2)/gamma(alpha))*(gamma(alpha+beta)/(gamma(alpha+beta+1/2)))\n",
    "    hyper_val = hyper([1, alpha+1/2], [alpha+beta+1/2], -c)\n",
    "    return float(prefactor * gamma_const * hyper_val)\n",
    "\n",
    "\n",
    "def p_kappa_dirichlet_student_t(kappa, alpha, beta, nu=3.0, a_j=1.0):\n",
    "    if kappa <= 0 or kappa >= 1:\n",
    "        return 0.0\n",
    "    C = gamma((nu+1)/2) * 1/(np.sqrt(np.pi * nu) * gamma(nu/2))\n",
    "    prefactor = ((a_j**nu) * nu**((nu+1)/2) * kappa**(nu/2 - 1)) / ((1-kappa)**(nu/2 + 1)) # * poch(alpha, nu) / poch(p*alpha, nu)\n",
    "    c = (-kappa / (1-kappa))*(a_j**2)\n",
    "    gamma_const = (gamma(alpha+nu/2)/gamma(alpha))*(gamma(alpha+beta)/(gamma(alpha+beta+nu/2)))\n",
    "    hyper_val = hyper([(nu+1)/2, alpha + nu/2], [alpha+beta+nu/2], c)\n",
    "    return float(C * prefactor * gamma_const * hyper_val)\n",
    "\n",
    "\n",
    "\n",
    "def p_kappa_horseshoe(kappa, a_j=1.0):\n",
    "    if kappa <= 0 or kappa >= 1:\n",
    "        return 0.0\n",
    "    prefactor = (1/np.pi) * (a_j / ((a_j**2-1)*kappa + 1)) * 1/(np.sqrt(kappa) * np.sqrt(1-kappa))\n",
    "    return float(prefactor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have updated the density plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    \\frac{1}{\\pi} \\frac{\\sqrt{q_{j}} \\tau}{(1-\\kappa_j)\\sqrt{\\kappa_j}\\sqrt{1-\\kappa_j}} \\frac{\\Gamma(\\alpha+\\frac{1}{2})}{\\Gamma(\\alpha)}\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha+\\beta+\\frac{1}{2})}{}_2F_1\\!\\left(\\begin{matrix} 1, \\alpha+\\frac{1}{2} \\\\ \\alpha + \\beta + \\frac{1}{2} \\end{matrix}; -c \\right)\n",
    "\\end{aligned} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}  \\frac{1}{(1-\\kappa)^{\\frac{\\nu}{2}+1}}\\nu^{\\frac{\\nu + 1}{2}} \\kappa^{\\frac{\\nu}{2}-1}q_{j}^{\\frac{\\nu}{2}}\\tau^{\\nu} \\mathbb{E}_{\\xi_j}\\left[\\frac{\\xi_j^{\\nu/2}}{\\left(1+c\\xi_j\\right)^{\\frac{\\nu+1}{2}}}\\right]\n",
    "\\end{aligned} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag kappa-grid\n",
    "kappa_vals = np.linspace(0.001, 0.999, 500)\n",
    "p_vals_dirichlet_horseshoe = [p_kappa_dirichlet_horseshoe(k, alpha, beta) for k in kappa_vals]\n",
    "p_vals_dirichlet_horseshoe_medium_low = [p_kappa_dirichlet_horseshoe(k, alpha, beta, a_j=0.7) for k in kappa_vals]\n",
    "p_vals_dirichlet_horseshoe_medium_high = [p_kappa_dirichlet_horseshoe(k, alpha, beta, a_j=2) for k in kappa_vals]\n",
    "p_vals_dirichlet_horseshoe_low = [p_kappa_dirichlet_horseshoe(k, alpha, beta, a_j=0.1) for k in kappa_vals]\n",
    "p_vals_dirichlet_horseshoe_high = [p_kappa_dirichlet_horseshoe(k, alpha, beta, a_j=5) for k in kappa_vals]\n",
    "\n",
    "p_vals_dirichlet_student_t = [p_kappa_dirichlet_student_t(k, alpha, beta) for k in kappa_vals]\n",
    "p_vals_dirichlet_student_t_medium_low = [p_kappa_dirichlet_student_t(k, alpha, beta, a_j=0.7) for k in kappa_vals]\n",
    "p_vals_dirichlet_student_t_medium_high = [p_kappa_dirichlet_student_t(k, alpha, beta, a_j=2) for k in kappa_vals]\n",
    "p_vals_dirichlet_student_t_low = [p_kappa_dirichlet_student_t(k, alpha, beta, a_j=0.1) for k in kappa_vals]\n",
    "p_vals_dirichlet_student_t_high = [p_kappa_dirichlet_student_t(k, alpha, beta, a_j=5) for k in kappa_vals]\n",
    "\n",
    "p_vals_horseshoe = [p_kappa_horseshoe(k) for k in kappa_vals]\n",
    "p_vals_horseshoe_medium_low = [p_kappa_horseshoe(k, a_j=0.7) for k in kappa_vals]\n",
    "p_vals_horseshoe_medium_high = [p_kappa_horseshoe(k, a_j=2) for k in kappa_vals]\n",
    "p_vals_horseshoe_low = [p_kappa_horseshoe(k, a_j=0.1) for k in kappa_vals]\n",
    "p_vals_horseshoe_high = [p_kappa_horseshoe(k, a_j=5) for k in kappa_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6), sharex=True, sharey=False)\n",
    "\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe_low)#, label=r\"$a_j = 0.1$\")\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe_medium_low)#, label=r\"$a_j = 0.7$\")\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe)#, label=r\"$a_j = 1$\")\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe_medium_high)#, label=r\"$a_j = 2$\")\n",
    "axes[0].plot(kappa_vals, p_vals_dirichlet_horseshoe_high)#, label=r\"$a_j = 5$\")\n",
    "axes[0].set_ylabel(r\"$p(\\kappa \\mid \\sigma, \\tau)$\")\n",
    "axes[0].set_xlabel(r\"$\\kappa$\")\n",
    "axes[0].set_title(\"Dirichlet–Horseshoe\")\n",
    "axes[0].set_ylim((0, 7))\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t_low, label=r\"$a_j = 0.1$\")\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t_medium_low, label=r\"$a_j = 0.7$\")\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t, label=r\"$a_j = 1$\")\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t_medium_high, label=r\"$a_j = 2$\")\n",
    "axes[1].plot(kappa_vals, p_vals_dirichlet_student_t_high, label=r\"$a_j = 5$\")\n",
    "#axes[1].set_ylabel(r\"$p(\\kappa \\mid \\sigma, \\tau)$\")\n",
    "axes[1].set_xlabel(r\"$\\kappa$\")\n",
    "axes[1].set_title(\"Dirichlet–Student T\")\n",
    "axes[1].set_ylim((0, 7))\n",
    "axes[1].legend(loc='upper center')\n",
    "\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe_low)#, label=r\"$a_j = 0.1$\")\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe_medium_low)#, label=r\"$a_j = 0.7$\")\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe)#, label=r\"$a_j = 1$\")\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe_medium_high)#, label=r\"$a_j = 2$\")\n",
    "axes[2].plot(kappa_vals, p_vals_horseshoe_high)#, label=r\"$a_j = 5$\")\n",
    "#axes[2].set_ylabel(r\"$p(\\kappa \\mid \\sigma, \\tau)$\")\n",
    "axes[2].set_xlabel(r\"$\\kappa$\")\n",
    "axes[2].set_title(\"Horseshoe\")\n",
    "axes[2].set_ylim((0, 7))\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, mpmath as mp\n",
    "\n",
    "def poch(x, n):  # rising factorial (x)_n for real n\n",
    "    return mp.gamma(x + n) / mp.gamma(x)\n",
    "\n",
    "def theory_beta_sqrt(alpha, beta, s):\n",
    "    t1 = mp.hyp2f1(1, alpha, alpha + beta, s**2)\n",
    "    t2 = s * poch(alpha, 0.5) / poch(alpha + beta, 0.5) * mp.hyp2f1(1, alpha + 0.5, alpha + beta + 0.5, s**2)\n",
    "    return t1 - t2\n",
    "\n",
    "def empirical_estimate(alpha, beta, s, n=100_000, seed=123):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    xi = rng.beta(alpha, beta, size=n)\n",
    "    vals = 1.0 / (1.0 + s * np.sqrt(xi))\n",
    "    m = float(vals.mean())\n",
    "    se = float(vals.std(ddof=1) / np.sqrt(n))\n",
    "    return m, se\n",
    "\n",
    "# --- set parameters ---\n",
    "p=10\n",
    "alpha, beta = 0.1, (p-1)*alpha\n",
    "s, n, seed  = 1.0, 100_000, 123  # require s > -1\n",
    "# ----------------------\n",
    "\n",
    "theory = float(theory_beta_sqrt(alpha, beta, s))\n",
    "emp, se = empirical_estimate(alpha, beta, s, n, seed)\n",
    "diff, z = emp - theory, (emp - theory) / se\n",
    "\n",
    "\n",
    "print(\"theory\", np.round(theory, 5), \"\\nempirical\", np.round(emp, 5), \"\\nabs_diff\", np.round(diff, 5), \"\\nSE_MC\", np.round(se, 5), \"\\nz_score\", np.round(z, 5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, mpmath as mp\n",
    "from scipy.stats import betaprime\n",
    "from mpmath import hyper\n",
    "\n",
    "n = 1_00\n",
    "p=10\n",
    "alpha, beta = 0.1, (p-1)*alpha\n",
    "nu=3\n",
    "s=0.1\n",
    "t = betaprime.rvs(1/2, nu/2, size=n)\n",
    "hyper_vec = np.zeros(n)\n",
    "for i in range(n):\n",
    "    hyper_vec[i] = hyper([1, alpha], [alpha + beta], -s*nu*t[i])\n",
    "\n",
    "emp_mean = np.mean(hyper_vec)\n",
    "\n",
    "theory = mp.hyper([1, alpha, 1/2], [alpha + beta, (nu+1)/2], -s*nu)\n",
    "print(emp_mean-theory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, mpmath as mp\n",
    "from scipy.stats import betaprime\n",
    "\n",
    "# Parametre\n",
    "n = 100\n",
    "p = 10\n",
    "alpha = 0.1\n",
    "beta = (p-1)*alpha  # 0.9\n",
    "nu = 3.0\n",
    "s = 1.0\n",
    "\n",
    "t = betaprime.rvs(0.5, nu/2, size=n)\n",
    "emp_vals = [mp.hyp2f1(1, alpha, alpha+beta, -s*nu*ti) for ti in t]\n",
    "emp_mean = float(np.mean([float(v) for v in emp_vals]))\n",
    "\n",
    "# Teori via dobbeltintegralet\n",
    "B05 = mp.beta(0.5, nu/2)\n",
    "norm_ab = mp.gamma(alpha+beta)/(mp.gamma(alpha)*mp.gamma(beta))\n",
    "\n",
    "def I_of_x(x):\n",
    "    f = lambda tt: (tt**(-0.5)) * ((1+tt)**(-(nu+1)/2)) / (1 + s*nu*x*tt)\n",
    "    return (1/B05) * mp.quad(f, [0, mp.inf])\n",
    "\n",
    "g = lambda x: (x**(alpha-1)) * ((1-x)**(beta-1)) * I_of_x(x)\n",
    "theory_int = norm_ab * mp.quad(g, [0, 1])\n",
    "\n",
    "print(\"Empirisk (s=1):\", emp_mean)\n",
    "print(\"Teori (integral):\", theory_int)\n",
    "print(\"Diff: \", emp_mean- theory_int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/friedman/Friedman_N100_p10_sigma1.00_seed1.npz\"\n",
    "data = np.load(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------- 1) Simuler X ----------\n",
    "def simulate_X(n, P, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.uniform(0.0, 1.0, size=(n, P))\n",
    "\n",
    "# ---------- 2) Aktivering og derivert ----------\n",
    "def get_activation(activation=\"tanh\"):\n",
    "    if activation == \"tanh\":\n",
    "        phi = np.tanh\n",
    "        def dphi(a): return 1.0 - np.tanh(a)**2\n",
    "    elif activation == \"relu\":\n",
    "        def phi(a): return np.maximum(0.0, a)\n",
    "        def dphi(a): return (a > 0.0).astype(a.dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    return phi, dphi\n",
    "\n",
    "# ---------- 3) Hovedfunksjon: q for alle trekk ----------\n",
    "def compute_q_for_fit(cmdstan_mcmc, N=1000, activation=\"tanh\", seed=1, output_index=0, X=None):\n",
    "    \"\"\"\n",
    "    Beregn q_{ell, j} for første-lagsvektene for hver trekk (draw).\n",
    "    Returnerer:\n",
    "      q_draws:  (n_draws, H, P)\n",
    "      q_mean:   (H, P)  – gjennomsnitt over trekk\n",
    "      X:        (N, P)  – datasettet brukt i beregningen\n",
    "    \"\"\"\n",
    "    # Hent ut variabler fra Stan\n",
    "    W1_all = cmdstan_mcmc.stan_variable(\"W_1\")            # (draws, P, H)\n",
    "    WL_all = cmdstan_mcmc.stan_variable(\"W_L\")             # (draws, H, O)\n",
    "    hb_all = cmdstan_mcmc.stan_variable(\"hidden_bias\")     # (draws, L, H)\n",
    "    sigma_all = cmdstan_mcmc.stan_variable(\"sigma\")        # (draws,)\n",
    "    Wint_all = cmdstan_mcmc.stan_variable(\"W_internal\")    # (draws, max(L-1,1), H, H)\n",
    "\n",
    "    draws, P, H = W1_all.shape\n",
    "    O = WL_all.shape[2]\n",
    "    L = hb_all.shape[1]\n",
    "\n",
    "    if O == 0:\n",
    "        raise ValueError(\"W_L has zero output nodes. Expected at least 1.\")\n",
    "    if output_index < 0 or output_index >= O:\n",
    "        raise ValueError(f\"output_index {output_index} out of range 0..{O-1}\")\n",
    "\n",
    "    if X is None:\n",
    "        X = simulate_X(N, P, seed=seed)\n",
    "\n",
    "    X_sq = X**2\n",
    "    phi, dphi = get_activation(activation)\n",
    "\n",
    "    q_draws = np.empty((draws, H, P), dtype=float)\n",
    "\n",
    "    for s in range(draws):\n",
    "        W1 = W1_all[s]            # (P, H)\n",
    "        WL = WL_all[s]            # (H, O)\n",
    "        hb = hb_all[s]            # (L, H)\n",
    "        Wints = Wint_all[s]       # (max(L-1,1), H, H)\n",
    "        sigma = float(sigma_all[s])\n",
    "\n",
    "        # ----- Forward pass -----\n",
    "        a_list = []\n",
    "        h_list = []\n",
    "\n",
    "        a = X @ W1 + hb[0]        # (N, H)\n",
    "        h = phi(a)\n",
    "        a_list.append(a); h_list.append(h)\n",
    "\n",
    "        for l in range(1, L):\n",
    "            Wl = Wints[l-1]       # (H, H)\n",
    "            a = h @ Wl + hb[l]    # (N, H)\n",
    "            h = phi(a)\n",
    "            a_list.append(a); h_list.append(h)\n",
    "\n",
    "        # ----- Backward: delta_L = d f / d a^(L) -----\n",
    "        # lineær utgang: df/dh^(L) = WL[:, output_index]\n",
    "        v = WL[:, output_index]           # (H,)\n",
    "        delta = dphi(a_list[-1]) * v      # (N, H), broadcast over N\n",
    "\n",
    "        # Bakover gjennom skjulte lag\n",
    "        for l in range(L-2, -1, -1):\n",
    "            Wnext = Wints[l]              # (H, H) – brukes bare hvis L>1\n",
    "            delta = (delta @ Wnext.T) * dphi(a_list[l]) if L > 1 else delta\n",
    "\n",
    "        delta1 = delta  # (N, H) == ∂f/∂a^(1)\n",
    "\n",
    "        # ----- q: (1/sigma^2) * sum_i (delta1[i,ell]^2 * X[i,j]^2) -----\n",
    "        D_sq = delta1**2                  # (N, H)\n",
    "        Q = (X_sq.T @ D_sq) / (sigma**2)  # (P, H)\n",
    "        q_draws[s] = Q.T                  # (H, P)\n",
    "\n",
    "    q_mean = q_draws.mean(axis=0)         # (H, P)\n",
    "    return q_draws, q_mean, X\n",
    "\n",
    "# ---------- 4) Eksempelbruk ----------\n",
    "# Velg riktig fit-objekt (CmdStanMCMC) fra dict-en din:\n",
    "prior_q_dhs, prior_q_mean_dhs, X_train = compute_q_for_fit(\n",
    "    prior_fits['Dirichlet Horseshoe']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "posterior_q_dhs, posterior_q_mean_dhs, X_train = compute_q_for_fit(\n",
    "    posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "prior_q_dst, prior_q_mean_dst, X_train = compute_q_for_fit(\n",
    "    prior_fits['Dirichlet Student T']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "posterior_q_dst, posterior_q_mean_dst, X_train = compute_q_for_fit(\n",
    "    posterior_N100_fits['Dirichlet Student T tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "prior_q_rhs, prior_q_mean_rhs, X_train = compute_q_for_fit(\n",
    "    prior_fits['Regularized Horseshoe']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "posterior_q_rhs, posterior_q_mean_rhs, X_train = compute_q_for_fit(\n",
    "    posterior_N100_fits['Regularized Horseshoe tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "prior_q_gauss, prior_q_mean_gauss, X_train = compute_q_for_fit(\n",
    "    prior_fits['Gaussian']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "posterior_q_gauss, posterior_q_mean_gauss, X_train = compute_q_for_fit(\n",
    "    posterior_N100_fits['Gaussian tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")\n",
    "\n",
    "prior_q_gamma, prior_q_mean_gamma, X_train = compute_q_for_fit(\n",
    "    prior_fits['Dirichlet Gamma']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = data[\"X_train\"]      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- choose which node/input to inspect ----\n",
    "node_idx = 1\n",
    "input_idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Horseshoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- PRIOR --------\n",
    "prior_fit = prior_fits['Dirichlet Horseshoe']['Dirichlet Horseshoe']['posterior']\n",
    "tau_prior = prior_fit.stan_variable(\"tau\")                                   # (draws_prior,)\n",
    "lam_prior = prior_fit.stan_variable(\"lambda_tilde_data\")[:, :, node_idx][:, input_idx]  # (draws_prior,)\n",
    "phi_prior = prior_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]           # (draws_prior,)\n",
    "q_prior   = prior_q_dhs[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "# -------- POSTERIOR --------\n",
    "post_fit = posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior']\n",
    "tau_post = post_fit.stan_variable(\"tau\")                                      # (draws_post,)\n",
    "lam_post = post_fit.stan_variable(\"lambda_tilde_data\")[:, :, node_idx][:, input_idx]    # (draws_post,)\n",
    "phi_post = post_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]             # (draws_post,)\n",
    "q_post   = posterior_q_dhs[:, node_idx, input_idx]                           # (draws_post,)\n",
    "\n",
    "kappa_DHS_prior = 1.0 / (1.0 + q_prior * (tau_prior**2) * (lam_prior**2) * (phi_prior**2))\n",
    "kappa_DHS_post = 1.0 / (1.0 + q_post * (tau_post**2) * (lam_post**2) * (phi_post**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Student T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- PRIOR --------\n",
    "prior_fit = prior_fits['Dirichlet Student T']['Dirichlet Student T']['posterior']\n",
    "tau_prior = prior_fit.stan_variable(\"tau\")                                   # (draws_prior,)\n",
    "lam_prior = prior_fit.stan_variable(\"lambda_tilde\")[:, :, node_idx][:, input_idx]  # (draws_prior,)\n",
    "phi_prior = prior_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]           # (draws_prior,)\n",
    "q_prior   = prior_q_dhs[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "# -------- POSTERIOR --------\n",
    "post_fit = posterior_N100_fits['Dirichlet Student T tanh']['posterior']\n",
    "tau_post = post_fit.stan_variable(\"tau\")                                      # (draws_post,)\n",
    "lam_post = post_fit.stan_variable(\"lambda_tilde_data\")[:, :, node_idx][:, input_idx]    # (draws_post,)\n",
    "phi_post = post_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]             # (draws_post,)\n",
    "q_post   = posterior_q_dhs[:, node_idx, input_idx]                           # (draws_post,)\n",
    "\n",
    "\n",
    "kappa_DST_prior = 1.0 / (1.0 + q_prior * (tau_prior**2) * (lam_prior**2) * (phi_prior**2))\n",
    "kappa_DST_post = 1.0 / (1.0 + q_post * (tau_post**2) * (lam_post**2) * (phi_post**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- PRIOR --------\n",
    "prior_fit = prior_fits['Dirichlet Gamma']['Dirichlet Gamma']['posterior']\n",
    "tau_prior = prior_fit.stan_variable(\"tau\")                                   # (draws_prior,)\n",
    "lam_prior = prior_fit.stan_variable(\"lambda_data\")[:, :, node_idx][:, input_idx]  # (draws_prior,)\n",
    "phi_prior = prior_fit.stan_variable(\"phi_data\")[:, :, node_idx][:, input_idx]           # (draws_prior,)\n",
    "q_prior   = prior_q_gamma[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "\n",
    "kappa_DG_prior = 1.0 / (1.0 + q_prior * (tau_prior**2) * (lam_prior**2) * (phi_prior**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Horseshoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_fit = prior_fits['Regularized Horseshoe']['Regularized Horseshoe']['posterior']\n",
    "tau_prior = prior_fit.stan_variable(\"tau\")                                   # (draws_prior,)\n",
    "lam_prior = prior_fit.stan_variable(\"lambda_tilde\")[:, :, node_idx][:, input_idx]  # (draws_prior,)\n",
    "q_prior   = prior_q_rhs[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "post_rhs_fit = posterior_N100_fits['Regularized Horseshoe tanh']['posterior']\n",
    "tau_rhs_post = post_rhs_fit.stan_variable(\"tau\")                                      # (draws_post,)\n",
    "lam_rhs_post = post_rhs_fit.stan_variable(\"lambda_tilde\")[:, :, node_idx][:, input_idx]    # (draws_post,)\n",
    "q_rhs_post   = posterior_q_rhs[:, node_idx, input_idx]                           # (draws_post,)\n",
    "\n",
    "kappa_HS_prior = 1.0 / (1.0 + q_prior * (tau_prior**2) * (lam_prior**2))\n",
    "kappa_HS_post = 1.0 / (1.0 + q_rhs_post * (tau_rhs_post**2) * (lam_rhs_post**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_prior   = prior_q_gauss[:, node_idx, input_idx]                               # (draws_prior,)\n",
    "\n",
    "q_gauss_post   = posterior_q_gauss[:, node_idx, input_idx]                           # (draws_post,)\n",
    "\n",
    "kappa_gauss_prior = 1.0 / (1.0 + q_prior)\n",
    "kappa_gauss_post = 1.0 / (1.0 + q_gauss_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Plot: prior vs posterior overlays --------\n",
    "bins = np.linspace(0.0, 1.0, 40)  # common bins so densities are comparable\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6), sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "# Horseshoe\n",
    "axes[0, 0].hist(kappa_gauss_prior, bins=bins, density=True, alpha=0.45, label=\"Prior\")\n",
    "axes[0, 0].hist(kappa_gauss_post,  bins=bins, density=True, alpha=0.45, label=\"Posterior\")\n",
    "axes[0, 0].set_title(\"Gaussian\")\n",
    "axes[0, 0].set_xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Horseshoe\n",
    "axes[0, 1].hist(kappa_HS_prior, bins=bins, density=True, alpha=0.45, label=\"Prior\")\n",
    "axes[0, 1].hist(kappa_HS_post,  bins=bins, density=True, alpha=0.45, label=\"Posterior\")\n",
    "axes[0, 1].set_title(\"RHS\")\n",
    "axes[0, 1].set_xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Dirichlet–Horseshoe\n",
    "axes[1, 0].hist(kappa_DHS_prior, bins=bins, density=True, alpha=0.45, label=\"Prior\")\n",
    "axes[1, 0].hist(kappa_DHS_post,  bins=bins, density=True, alpha=0.45, label=\"Posterior\")\n",
    "axes[1, 0].set_title(\"DHS\")\n",
    "axes[1, 0].set_xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "axes[1, 0].set_ylabel(\"Density\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Dirichlet–Horseshoe\n",
    "axes[1, 1].hist(kappa_DST_prior, bins=bins, density=True, alpha=0.45, label=\"Prior\")\n",
    "axes[1, 1].hist(kappa_DST_post,  bins=bins, density=True, alpha=0.45, label=\"Posterior\")\n",
    "axes[1, 1].set_title(\"DS-T\")\n",
    "axes[1, 1].set_xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "\n",
    "plt.suptitle(f\"Node {node_idx}, input {input_idx}\")#: Prior vs Posterior\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()\n",
    "plt.hist(kappa_DG_prior, bins=50, density=True, alpha=0.45, label=\"Dirichlet Gamma prior\")\n",
    "plt.legend()\n",
    "#plt.xlim(3, 50)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "colors = {\n",
    "    \"Gaussian\": \"C0\",\n",
    "    \"RHS\": \"C1\",\n",
    "    \"DHS\": \"C2\",\n",
    "    \"DS-T\": \"C3\",\n",
    "}\n",
    "\n",
    "# Plot linjene\n",
    "sns.kdeplot(kappa_gauss_prior, linestyle=\"--\", color=colors[\"Gaussian\"])\n",
    "sns.kdeplot(kappa_gauss_post, linestyle=\"-\",  color=colors[\"Gaussian\"])\n",
    "sns.kdeplot(kappa_HS_prior, linestyle=\"--\", color=colors[\"RHS\"])\n",
    "sns.kdeplot(kappa_HS_post, linestyle=\"-\",  color=colors[\"RHS\"])\n",
    "sns.kdeplot(kappa_DHS_prior, linestyle=\"--\", color=colors[\"DHS\"])\n",
    "sns.kdeplot(kappa_DHS_post, linestyle=\"-\",  color=colors[\"DHS\"])\n",
    "sns.kdeplot(kappa_DST_prior, linestyle=\"--\", color=colors[\"DS-T\"])\n",
    "sns.kdeplot(kappa_DST_post, linestyle=\"-\",  color=colors[\"DS-T\"])\n",
    "\n",
    "plt.xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Prior shrinkage factors – KDE\")\n",
    "\n",
    "# --- Lag legend ---\n",
    "# Farger (modeller)\n",
    "model_handles = [Line2D([0], [0], color=color, lw=2, label=model) \n",
    "                 for model, color in colors.items()]\n",
    "\n",
    "# Linjestil (prior/posterior)\n",
    "style_handles = [\n",
    "    Line2D([0], [0], color=\"black\", linestyle=\"--\", lw=2, label=\"Prior\"),\n",
    "    Line2D([0], [0], color=\"black\", linestyle=\"-\",  lw=2, label=\"Posterior\")\n",
    "]\n",
    "\n",
    "# Kombiner\n",
    "handles = model_handles + style_handles\n",
    "labels = [h.get_label() for h in handles]\n",
    "\n",
    "plt.legend(handles=handles, labels=labels, title=\"Model / Distribution\", ncol=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(x):\n",
    "    x = np.sort(x)\n",
    "    y = np.arange(1, len(x)+1) / len(x)\n",
    "    return x, y\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for data, name in [(kappa_gauss_prior,\"Gaussian\"),\n",
    "                   (kappa_HS_prior,\"RHS\"),\n",
    "                   (kappa_DHS_prior,\"DHS\"),\n",
    "                   (kappa_DST_prior,\"DS-T\")]:\n",
    "    x,y = ecdf(data)\n",
    "    plt.plot(x,y,label=name)\n",
    "plt.xlabel(r\"$\\kappa_{\\ell j}$\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.title(\"Empirical CDFs of $\\kappa$ under priors\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized versions\n",
    "\n",
    "For the Regularized Horseshoe, when $c<\\infty$. the shrinkage profile becomes approximately equivalent to that of the horseshoe shifted from the interval $(0, 1)$ to $(b_{\\ell, j}, 1)$ where $b_{\\ell, j}=\\frac{1}{q_{\\ell, j}\\tau{\\ell}^2c^2}$. The shrinkage factor then approximately satisfies $\\tilde{\\kappa}_{\\ell, j}=(1-b_j)\\kappa_{\\ell, j} + b_j$ where $\\tilde{\\kappa}_{\\ell, j}$ is the shrinkage factor of the original horseshoe. From this we get $1-\\tilde{\\kappa}_{\\ell, j}=(1-b_j)(1-\\kappa_{\\ell, j})$. Assuming roughly the same scale of inputs, then $b_{\\ell, j}=b_{\\ell}$ and the effective model complexity becomes $\\tilde{m}_{\\ell, eff}=(1-b_{\\ell}) m_{\\ell, eff}$ where $m_{\\ell, eff}=\\sum_{j=1}^p(1-\\kappa_{\\ell, j})$ denotes the effective number of nonzero weights into node $l$ (I THINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_priors = \"results/priors/single_layer/tanh/friedman\"\n",
    "prior_names_and_configs = {\"Gaussian\": \"gauss\", \"Regularized Horseshoe\": \"reg_hs\",\"Dirichlet Horseshoe\": \"dir_hs\", \"Dirichlet Student T\": \"dir_stud_t\"}\n",
    "\n",
    "  \n",
    "prior_fits = {}\n",
    "posterior_fits = {}\n",
    "\n",
    "for key, value in prior_names_and_configs.items():\n",
    "    prior_fit = get_model_fits(\n",
    "        config=value,\n",
    "        results_dir=results_dir_priors,\n",
    "        models=key,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    prior_fits[key] = prior_fit\n",
    "\n",
    "prior_fits = {outer: inner[outer] for outer, inner in prior_fits.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kappa(fit, q_fit, node_idx, input_idx, dst=False, rhs=False, gauss=False):\n",
    "    if gauss:\n",
    "        tau=1.0\n",
    "    else:\n",
    "        tau = fit.stan_variable(\"tau\")                                              # Shape (4000,)\n",
    "    if gauss:\n",
    "        lam=1.0\n",
    "    else:   \n",
    "        if dst or rhs:\n",
    "            lam = fit.stan_variable(\"lambda_tilde\")[:, node_idx, input_idx]         # Shape (4000, 16, 10)\n",
    "        else:\n",
    "            lam = fit.stan_variable(\"lambda_tilde_data\")[:, node_idx, input_idx]    # Shape (4000, 16, 10)\n",
    "    if gauss:\n",
    "        c_sq = 1                                                                    # Shape (4000, 16)\n",
    "    else:\n",
    "        c_sq = fit.stan_variable(\"c_sq\")[:, node_idx]                               # Shape (4000, 16)\n",
    "    if rhs or gauss:\n",
    "        phi = 1.0                                                                   # Shape (4000, 16, 10)\n",
    "    else:\n",
    "        phi = fit.stan_variable(\"phi_data\")[:, node_idx, input_idx]                 # Shape (4000, 16, 10)\n",
    "    q = q_fit[:, node_idx, input_idx]                                               # Shape (4000, 16, 10)\n",
    "    \n",
    "    kappa_orignal = 1.0 / (1.0 + q * (tau**2) * (lam**2) * (phi))\n",
    "    b_j = 1.0 / (1.0 + q * (tau**2) * c_sq * (phi))\n",
    "    kappa_tilde = (1-b_j)*kappa_original+b_j\n",
    "    \n",
    "    return(kappa_orignal, b_j, kappa_tilde)\n",
    "\n",
    "\n",
    "\n",
    "gauss_fit = prior_fits['Gaussian']['Gaussian']['posterior']\n",
    "\n",
    "inputs = 9\n",
    "\n",
    "kappa_orignal_vals = np.zeros(inputs)\n",
    "bj_s = np.zeros(inputs)\n",
    "kappa_tilde_vals = np.zeros(inputs)\n",
    "\n",
    "for i in range(inputs):\n",
    "    kappa_original, b_j, kappa_tilde = compute_kappa(gauss_fit, posterior_q_gauss, gauss=True, node_idx=1, input_idx=i)\n",
    "    kappa_orignal_vals[i] = np.mean(kappa_original)\n",
    "    bj_s[i] = np.mean(b_j)\n",
    "    kappa_tilde_vals[i] = np.mean(kappa_tilde)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(1-kappa_orignal_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_term = 1-kappa_orignal_vals\n",
    "second_term = 1-bj_s\n",
    "\n",
    "np.sum(first_term*second_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "path = \"datasets/friedman/Friedman_N500_p10_sigma1.00_seed11.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]\n",
    "\n",
    "fits = prior_fits\n",
    "\n",
    "P = 10\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "\n",
    "def build_single_draw_weights(fits, layer_structure, draw_idx):\n",
    "    \"\"\"Return {model: {'W_1': (P,H), 'W_L': (H,O)}} for ONE draw.\"\"\"\n",
    "    out = {}\n",
    "    for name, fd in fits.items():\n",
    "        fit = fd[\"posterior\"]\n",
    "        W1 = fit.stan_variable(layer_structure['input_to_hidden']['name'])[draw_idx]\n",
    "        WL = fit.stan_variable(layer_structure['hidden_to_output']['name'])[draw_idx]\n",
    "        WL = WL.reshape(layer_structure['hidden_to_output']['shape'])\n",
    "        out[name] = {\"W_1\": W1, \"W_L\": WL}\n",
    "    return out\n",
    "\n",
    "def scale_W1_for_plot(model_means, mode='global'):\n",
    "    \"\"\"\n",
    "    Skalerer alle W_1 til [-1, 1] for rettferdig sammenligning av edge-tykkelser.\n",
    "\n",
    "    mode:\n",
    "      - 'global' : én felles skala over alle modeller (mest sammenlignbar)\n",
    "      - 'per_model': egen skala per modell (uavhengig sammenligning)\n",
    "      - 'per_node' : skalerer hver kolonne (node) separat til [-1,1]\n",
    "\n",
    "    Returnerer: scaled_model_means (samme struktur som input), scale_info\n",
    "    \"\"\"\n",
    "    scaled = {}\n",
    "    if mode == 'global':\n",
    "        gmax = max(np.abs(m['W_1']).max() for m in model_means.values())\n",
    "        gmax = max(gmax, 1e-12)\n",
    "        for name, m in model_means.items():\n",
    "            W1s = m['W_1'] / gmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'global', 'scale': gmax}\n",
    "\n",
    "    elif mode == 'per_model':\n",
    "        for name, m in model_means.items():\n",
    "            s = max(np.abs(m['W_1']).max(), 1e-12)\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = m['W_1'] / s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_model'}\n",
    "\n",
    "    elif mode == 'per_node':\n",
    "        for name, m in model_means.items():\n",
    "            W1 = m['W_1'].copy()\n",
    "            P, H = W1.shape\n",
    "            for h in range(H):\n",
    "                colmax = max(np.abs(W1[:, h]).max(), 1e-12)\n",
    "                W1[:, h] = W1[:, h] / colmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_node'}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'global', 'per_model', or 'per_node'\")\n",
    "\n",
    "def plot_models_with_activations(model_means, layer_sizes,\n",
    "                                 activations=None, activation_color_max=None,\n",
    "                                 ncols=3, figsize_per_plot=(5,4), signed_colors=False):\n",
    "    \"\"\"\n",
    "    model_means: dict {model_name: {'W_1':(P,H), 'W_L':(H,O), optional 'W_internal':[...]} }\n",
    "    layer_sizes: f.eks [P, H, O] eller [P, H, H, O] ved internlag\n",
    "    activations: dict {model_name: (H,)} – aktiveringsfrekvens kun for første skjulte lag\n",
    "    activation_color_max: global maks for skalering av farger (hvis None brukes 1.0)\n",
    "    \"\"\"\n",
    "    names = list(model_means.keys())\n",
    "    n_models = len(names)\n",
    "    nrows = int(np.ceil(n_models / ncols))\n",
    "    figsize = (figsize_per_plot[0] * ncols, figsize_per_plot[1] * nrows)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    if nrows * ncols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Skru av blanke akser\n",
    "    for ax in axes[n_models:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax, name in zip(axes, names):\n",
    "        weights = model_means[name]\n",
    "        G = nx.DiGraph()\n",
    "        pos, nodes_per_layer, node_colors = {}, [], []\n",
    "\n",
    "        # Noder med posisjon og farge\n",
    "        for li, size in enumerate(layer_sizes):\n",
    "            ids = []\n",
    "            ycoords = np.linspace(size - 1, 0, size) - (size - 1) / 2\n",
    "            for i in range(size):\n",
    "                nid = f\"L{li}_{i}\"\n",
    "                G.add_node(nid)\n",
    "                pos[nid] = (li, ycoords[i])\n",
    "                ids.append(nid)\n",
    "\n",
    "                if activations is not None and li == 1:  # kun første skjulte lag\n",
    "                    #a = activations.get(name, np.zeros(size))\n",
    "                    a = activations.get(name, np.zeros(size))\n",
    "                    a = np.asarray(a).ravel()   # <-- flater til 1D array\n",
    "                    scale = activation_color_max if activation_color_max is not None else 1.0\n",
    "                    val = float(np.clip(a[i] / max(scale, 1e-12), 0.0, 1.0))\n",
    "                    color = plt.cm.winter(val)\n",
    "                else:\n",
    "                    color = 'lightgray'\n",
    "                node_colors.append(color)\n",
    "\n",
    "            nodes_per_layer.append(ids)\n",
    "\n",
    "        edge_colors, edge_widths = [], []\n",
    "\n",
    "        def add_edges(W, inn, ut):\n",
    "            for j, out_n in enumerate(ut):\n",
    "                for i, in_n in enumerate(inn):\n",
    "                    w = float(W[i, j])\n",
    "                    G.add_edge(in_n, out_n, weight=abs(w))\n",
    "                    edge_colors.append('red' if w >= 0 else 'blue')\n",
    "                    edge_widths.append(abs(w))\n",
    "\n",
    "        # input -> hidden(1)\n",
    "        add_edges(weights['W_1'], nodes_per_layer[0], nodes_per_layer[1])\n",
    "\n",
    "        # ev. internlag\n",
    "        if 'W_internal' in weights:\n",
    "            for l, Win in enumerate(weights['W_internal']):\n",
    "                add_edges(Win, nodes_per_layer[l+1], nodes_per_layer[l+2])\n",
    "\n",
    "        # siste hidden -> output\n",
    "        add_edges(weights['W_L'], nodes_per_layer[-2], nodes_per_layer[-1])\n",
    "\n",
    "        nx.draw(G, pos, ax=ax,\n",
    "                node_color=node_colors,\n",
    "                edge_color=(edge_colors if signed_colors else 'red'),\n",
    "                width=[G[u][v]['weight'] for u,v in G.edges()],\n",
    "                with_labels=False, node_size=400, arrows=False)\n",
    "\n",
    "        ax.set_title(name, fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def compute_hidden_activation(fit_dict, x_train, draw_idx):\n",
    "    fit = fit_dict['posterior']\n",
    "    W1 = fit.stan_variable('W_1')[draw_idx, :, :]          # (P,H)\n",
    "    try:\n",
    "        b1 = fit.stan_variable('hidden_bias')[draw_idx, :] # (H,)\n",
    "    except Exception:\n",
    "        b1 = np.zeros(W1.shape[1])\n",
    "    # tanh i [-1,1]\n",
    "    a_full = np.tanh(x_train @ W1 + b1)             # (H,)\n",
    "    a=np.mean(a_full, axis=0)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velg en observasjon å \"lyse opp\" nodefargene med\n",
    "obs_idx = 3\n",
    "draw_idx = 69 #pick_draw_idx(prior_fits, seed=42)      # one common draw across models\n",
    "prior_draws = build_single_draw_weights(prior_fits, layer_structure, draw_idx)\n",
    "\n",
    "# 1) Beregn aktivasjoner for ALLE modellene\n",
    "activations = {}\n",
    "for name, fd in prior_fits.items():\n",
    "    a = compute_hidden_activation(fd, x_train, draw_idx)\n",
    "    activations[name] = np.abs(a)      \n",
    "\n",
    "# 2) Skaler vekter for plotting (som før)\n",
    "scaled, _ = scale_W1_for_plot(prior_draws, mode='per_model')\n",
    "\n",
    "# 3) Kall plottet med aktivasjoner\n",
    "# Siden tanh ∈ [-1,1] og vi bruker |a|, så sett activation_color_max=1.0\n",
    "fig = plot_models_with_activations(\n",
    "    scaled,\n",
    "    layer_sizes=[P, H, out_nodes],\n",
    "    activations=activations,\n",
    "    activation_color_max=1.0,\n",
    "    ncols=2,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# 2) Bygg prunede posterior-mean vekter for alle modeller (pruner K minste |W_1|)\n",
    "def build_pruned_means(fits, layer_structure, sparsity_level):\n",
    "    \"\"\"\n",
    "    fits: dict {model_name: {\"posterior\": CmdStanMCMC}}\n",
    "    layer_structure: {'input_to_hidden': {'name':'W_1','shape':(P,H)},\n",
    "                      'hidden_to_output':{'name':'W_L','shape':(H,O)}}\n",
    "                      # optional: 'hidden_to_hidden': {'name':..., 'shape': [(H,H), ...]}\n",
    "    sparsity_level: andel av W_1-elementer som settes til 0 (0.0–1.0)\n",
    "\n",
    "    Returnerer dict {model_name: {'W_1':(P,H), 'W_L':(H,O), 'W_internal':[...](optional)}}\n",
    "    \"\"\"\n",
    "    model_means = {}\n",
    "\n",
    "    for name, fd in fits.items():\n",
    "        fit = fd[\"posterior\"]\n",
    "\n",
    "        # Posterior means\n",
    "        W1_name = layer_structure['input_to_hidden']['name']\n",
    "        WL_name = layer_structure['hidden_to_output']['name']\n",
    "\n",
    "        W1 = fit.stan_variable(W1_name).mean(axis=0)  # (P,H)\n",
    "        WL = fit.stan_variable(WL_name).mean(axis=0)  # (H,O)\n",
    "        WL = WL.reshape(layer_structure['hidden_to_output']['shape'])\n",
    "\n",
    "        means = {'W_1': W1.copy(), 'W_L': WL}\n",
    "\n",
    "        # Optional internlag\n",
    "        if 'hidden_to_hidden' in layer_structure:\n",
    "            HH_name = layer_structure['hidden_to_hidden']['name']\n",
    "            HH_shapes = layer_structure['hidden_to_hidden']['shape']  # list of (H,H)\n",
    "            raw = fit.stan_variable(HH_name).mean(axis=0)\n",
    "            means['W_internal'] = [raw[i].reshape(HH_shapes[i]) for i in range(len(HH_shapes))]\n",
    "\n",
    "        # Pruning av W_1 (beholder de største |w|). Samme logikk som i koden din.\n",
    "        P, H = means['W_1'].shape\n",
    "        K = int(np.floor(sparsity_level * P * H))\n",
    "        if K > 0:\n",
    "            flat_abs = np.abs(means['W_1'].ravel())\n",
    "            idx_prune = np.argpartition(flat_abs, K)[:K]   # indeksene til de K minste\n",
    "            mask = np.ones_like(flat_abs, dtype=float)\n",
    "            mask[idx_prune] = 0.0\n",
    "            means['W_1'] = (means['W_1'].ravel() * mask).reshape(P, H)\n",
    "\n",
    "        model_means[name] = means\n",
    "\n",
    "    return model_means\n",
    "\n",
    "def compute_hidden_activation_from_means(model_means, x_train):\n",
    "    \"\"\"\n",
    "    model_means: dict {model_name: {'W_1':(P,H), 'W_L':(H,O)}}\n",
    "    x_train: (N,P)\n",
    "\n",
    "    Returnerer dict {model_name: (H,)} med gj.snitt aktivasjon pr. node.\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    for name, m in model_means.items():\n",
    "        W1 = m['W_1']                 # (P,H)\n",
    "        b1 = m.get('hidden_bias', np.zeros(W1.shape[1]))  # hvis bias finnes, ellers null\n",
    "        a_full = np.tanh(x_train @ W1 + b1)   # (N,H)\n",
    "        activations[name] = np.abs(a_full).mean(axis=0)  # (H,)\n",
    "    return activations\n",
    "\n",
    "def input_contribution_shares_W1(fit, x_train, weight_by_activity=True):\n",
    "    \"\"\"\n",
    "    Returnerer (P,H) med s_{i,h} ≥ 0 og sum_i s_{i,h} ≈ 1.\n",
    "    s_{i,h} lages ved å:\n",
    "      1) for hvert datapunkt n, beregne |x_{n,i} w_{i,h}| og normalisere over i,\n",
    "      2) gjennomsnittlig (valgfritt) vektet med |tanh(pre_{n,h})| over n.\n",
    "\n",
    "    weight_by_activity=True anbefales for tanh.\n",
    "    \"\"\"\n",
    "    W1 = fit['posterior'].stan_variable('W_1').mean(axis=0)   # (P,H)\n",
    "    try:\n",
    "        b1 = fit['posterior'].stan_variable('hidden_bias').mean(axis=0)\n",
    "    except Exception:\n",
    "        b1 = np.zeros(W1.shape[1])\n",
    "\n",
    "    pre = x_train @ W1 + b1                                   # (N,H)\n",
    "    act = np.abs(np.tanh(pre))                                # (N,H)\n",
    "\n",
    "    N, H = pre.shape\n",
    "    P = W1.shape[0]\n",
    "    shares = np.zeros((P, H))\n",
    "\n",
    "    absW = np.abs(W1)                   # (P,H)\n",
    "    absX = np.abs(x_train)              # (N,P)\n",
    "\n",
    "    eps = 1e-12\n",
    "    for h in range(H):\n",
    "        contrib = absX * absW[:, h].reshape(1, P)             # (N,P): |x|*|w|\n",
    "        denom = np.clip(contrib.sum(axis=1, keepdims=True), eps, None)  # (N,1)\n",
    "        s_n = contrib / denom                                  # (N,P), radvis sum=1\n",
    "\n",
    "        if weight_by_activity:\n",
    "            w = act[:, h]                                      # (N,)\n",
    "            wsum = float(w.sum())\n",
    "            if wsum < eps:\n",
    "                shares[:, h] = 1.0 / P\n",
    "            else:\n",
    "                shares[:, h] = (s_n.T @ w) / wsum              # (P,)\n",
    "        else:\n",
    "            shares[:, h] = s_n.mean(axis=0)                    # uvektet snitt\n",
    "\n",
    "    return shares\n",
    "\n",
    "def summarize_active_weights(shares, thresholds=(0.5, 0.8, 0.9)):\n",
    "    \"\"\"\n",
    "    shares: (P,H) med s_{i,h} ≥ 0 og kolonnevis sum ~ 1.\n",
    "    thresholds: iterable, f.eks (0.5, 0.8, 0.9)\n",
    "\n",
    "    Returnerer:\n",
    "      - k_for_threshold: dict {thr: array(H,)}  # minste k per node\n",
    "      - neff: array(H,)\n",
    "      - gini: array(H,)\n",
    "    \"\"\"\n",
    "    P, H = shares.shape\n",
    "    k_for_threshold = {thr: np.zeros(H, dtype=int) for thr in thresholds}\n",
    "    neff = np.zeros(H)\n",
    "    gini = np.zeros(H)\n",
    "\n",
    "    for h in range(H):\n",
    "        v = np.sort(shares[:, h])[::-1]               # synkende\n",
    "        csum = np.cumsum(v)\n",
    "        total = csum[-1] if csum.size else 0.0\n",
    "        total = max(total, 1e-12)\n",
    "\n",
    "        for thr in thresholds:\n",
    "            k = int(np.searchsorted(csum, thr * total) + 1)  # +1 -> antall elementer\n",
    "            k_for_threshold[thr][h] = min(k, P)\n",
    "\n",
    "        # N_eff = 1 / sum s_i^2\n",
    "        neff[h] = 1.0 / max((shares[:, h] ** 2).sum(), 1e-12)\n",
    "\n",
    "        # Gini (diskret): 1 - 2 * (sum_{i=1}^P (P+1-i) s_{(i)}) / (P * sum s_i)\n",
    "        w = v\n",
    "        gini[h] = 1.0 - 2.0 * ( (np.arange(1, P+1) * w).sum() ) / (P * total)\n",
    "\n",
    "    return k_for_threshold, neff, gini\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/friedman/Friedman_N500_p10_sigma1.00_seed11.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]\n",
    "\n",
    "fits = posterior_N500_fits       # posterior_N100_fits\n",
    "\n",
    "P = 10\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "# 1) bygg/prune posterior-mean som før (fra tidligere funksjoner dine)\n",
    "pruned_model_means = build_pruned_means(fits, layer_structure, sparsity_level=0.0)\n",
    "\n",
    "# 2) skaler W1 for sammenlignbar tykkelse\n",
    "scaled_means, scale_info = scale_W1_for_plot(pruned_model_means, mode='per_model')  # eller 'per_model' / 'per_node'\n",
    "activations = compute_hidden_activation_from_means(pruned_model_means, x_train)\n",
    "all_vals = np.concatenate(list(activations.values()))\n",
    "vmin, vmax = all_vals.min(), all_vals.max()\n",
    "\n",
    "# 4) plot med skalerte vekter (W_1 byttes ut)\n",
    "fig = plot_models_with_activations(\n",
    "    scaled_means,\n",
    "    layer_sizes=[P, H, 1],\n",
    "    activations=activations,\n",
    "    activation_color_max=vmax,\n",
    "    ncols=2,\n",
    "    signed_colors=False\n",
    ")\n",
    "\n",
    "# 5) “Hvor mange vekter er faktisk aktive?” – per modell\n",
    "summaries = {}\n",
    "for name, fd in fits.items():\n",
    "    shares = input_contribution_shares_W1(fd, x_train)  # (P,H)\n",
    "    k_thr, neff, gini = summarize_active_weights(shares, thresholds=(0.5, 0.8, 0.9))\n",
    "    summaries[name] = {\n",
    "        'k_for_threshold': k_thr,  # f.eks. k_thr[0.8] er en (H,) array\n",
    "        'N_eff': neff,             # (H,)\n",
    "        'Gini': gini               # (H,)\n",
    "    }\n",
    "\n",
    "# Eksempel på enkel oppsummering pr. modell:\n",
    "for name, s in summaries.items():\n",
    "    k80 = s['k_for_threshold'][0.8]\n",
    "    print(f\"{name}: median k@80% = {int(np.median(k80))}, \"\n",
    "          f\"mean N_eff = {s['N_eff'].mean():.2f}, mean Gini = {s['Gini'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_1d(x):\n",
    "    x = np.asarray(x, float).ravel()\n",
    "    x = np.clip(x, 0.0, None)           # vi bruker |w| uansett\n",
    "    s = x.sum()\n",
    "    if s == 0 or x.size < 2:\n",
    "        return 0.0\n",
    "    xs = np.sort(x)\n",
    "    n = xs.size\n",
    "    weights = (n - np.arange(1, n+1) + 0.5) / n  # <-- riktig vekting\n",
    "    return 1.0 - 2.0 * np.sum(weights * (xs / s))\n",
    "\n",
    "def model_gini(W1):\n",
    "    \"\"\"Tar en W1-matrise (P,H) og returnerer (node_ginis, model_gini).\"\"\"\n",
    "    Wabs = np.abs(W1)\n",
    "    node_ginis = [gini_1d(Wabs[:, h]) for h in range(Wabs.shape[1])]\n",
    "    return np.array(node_ginis), np.mean(node_ginis)\n",
    "print(\"---------- Prior ----------\")\n",
    "# Eksempel: prior for en spesifikk modell\n",
    "for name, weights in prior_draws.items():\n",
    "    node_g, model_g = model_gini(weights['W_1'])\n",
    "    print(f\"{name:25s}  model_gini = {model_g:.3f}\")\n",
    "print(\"---------- Posterior ----------\")\n",
    "# Eksempel: posterior (pruned posterior means)\n",
    "for name, weights in pruned_model_means.items():\n",
    "    node_g, model_g = model_gini(weights['W_1'])\n",
    "    print(f\"{name:25s}  model_gini = {model_g:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def weight_concentration(W1, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Måler hvor mange vekter som trengs for å dekke 'threshold' andel av\n",
    "    den totale absoluttvektsummen per node (kolonne i W1).\n",
    "\n",
    "    Returnerer:\n",
    "      node_fracs: (H,) array med andel vekter brukt per node\n",
    "      model_frac: gjennomsnitt over noder\n",
    "    \"\"\"\n",
    "    Wabs = np.abs(W1)  # (P,H)\n",
    "    P, H = Wabs.shape\n",
    "    node_fracs = []\n",
    "    for h in range(H):\n",
    "        col = np.sort(Wabs[:, h])[::-1]        # sortér synkende\n",
    "        tot = col.sum()\n",
    "        if tot <= 0:\n",
    "            node_fracs.append(0.0)\n",
    "            continue\n",
    "        cum = np.cumsum(col) / tot\n",
    "        k = np.searchsorted(cum, threshold) + 1  # hvor mange trengs for å nå threshold\n",
    "        #node_fracs.append(k / P)                 # andel av alle input\n",
    "    k = np.array(k)\n",
    "    return k, float(k.mean())\n",
    "\n",
    "\n",
    "# --- PRIOR (ett draw) ---\n",
    "print(\"\\nPRIOR models:\")\n",
    "for name, weights in prior_draws.items():\n",
    "    node_fracs, model_frac = weight_concentration(weights['W_1'], threshold=0.8)\n",
    "    print(f\"{name:25s}  mean_frac = {model_frac:.3f}\")\n",
    "\n",
    "# --- POSTERIOR (posterior mean/pruned) ---\n",
    "print(\"\\nPOSTERIOR models:\")\n",
    "for name, weights in pruned_model_means.items():\n",
    "    node_fracs, model_frac = weight_concentration(weights['W_1'], threshold=0.8)\n",
    "    print(f\"{name:25s}  mean_frac = {model_frac:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def activation_concentration(W1, X, b=None, threshold=0.8, account_nonlinearity=True):\n",
    "    \"\"\"\n",
    "    Hvor mange input-vekter trengs for å forklare 'threshold' andel av aktiverings-bidraget\n",
    "    for hver node (kolonne i W1), basert på datasettet X.\n",
    "\n",
    "    W1: (P, H)\n",
    "    X:  (N, P)\n",
    "    b:  (H,) eller None -> tolkes som 0\n",
    "    account_nonlinearity: hvis True, vekter bidrag med sech^2(z) (lokal følsomhet for tanh)\n",
    "\n",
    "    Returnerer:\n",
    "      k_per_node: (H,) heltall med antall vekter som dekker threshold-andel per node\n",
    "      k_mean: float, gjennomsnitt over noder\n",
    "    \"\"\"\n",
    "    W1 = np.asarray(W1, float)\n",
    "    X = np.asarray(X, float)\n",
    "    P, H = W1.shape\n",
    "    N = X.shape[0]\n",
    "    if b is None:\n",
    "        b = np.zeros(H, dtype=float)\n",
    "    else:\n",
    "        b = np.asarray(b, float)\n",
    "\n",
    "    # Basis-bidrag per sample, input og node: |x_{n,p} * w_{p,h}|\n",
    "    # Vi trenger bare middel over n, så vi kan gjøre dette effektivt:\n",
    "    # E_n[|x_{n,p}|] * |w_{p,h}| er en enkel approks; mer presist bruker vi gj.snitt av |x*w| over n.\n",
    "    # Vi tar den presise: mean_n |x_{n,p} * w_{p,h}|\n",
    "    X_abs = np.abs(X)                          # (N,P)\n",
    "    W_abs = np.abs(W1)                         # (P,H)\n",
    "    # mean over n av |x_{n,p}| for hver p:\n",
    "    Ex_abs = X_abs.mean(axis=0, keepdims=True) # (1,P)\n",
    "    # Bidrag uten ikke-linearitet: (P,H) matrise\n",
    "    C = Ex_abs.T * W_abs                       # (P,H)\n",
    "\n",
    "    if account_nonlinearity:\n",
    "        # z_{n,h} = X @ W1 + b  -> (N,H)\n",
    "        Z = X @ W1 + b\n",
    "        g = 1.0 - np.tanh(Z)**2                # sech^2(z), (N,H)\n",
    "        # Skaler bidraget per node med E_n[g_{n,h}]\n",
    "        g_mean = g.mean(axis=0, keepdims=True) # (1,H)\n",
    "        C = C * g_mean                         # (P,H)\n",
    "\n",
    "    k_per_node = np.zeros(H, dtype=int)\n",
    "    for h in range(H):\n",
    "        col = np.sort(C[:, h])[::-1]         # største bidrag først\n",
    "        tot = col.sum()\n",
    "        if tot <= 0:\n",
    "            k_per_node[h] = 0\n",
    "            continue\n",
    "        cum = np.cumsum(col) / tot\n",
    "        k_per_node[h] = int(np.searchsorted(cum, threshold) + 1)\n",
    "\n",
    "    return k_per_node, float(k_per_node.mean()) if H > 0 else 0.0\n",
    "\n",
    "\n",
    "# --- PRIOR (ett draw) ---\n",
    "print(\"\\nPRIOR models (k = #vekter for å nå 0.8 av aktiveringsbidrag):\")\n",
    "for name, weights in prior_draws.items():\n",
    "    W1 = weights['W_1']\n",
    "    # hvis du har bias i draws: b = weights.get('hidden_bias', None)\n",
    "    k_nodes, k_mean = activation_concentration(W1, x_train, b=None, threshold=0.8, account_nonlinearity=True)\n",
    "    print(f\"{name:25s}  mean_k = {k_mean:.2f}  (P={W1.shape[0]})\")\n",
    "\n",
    "# --- POSTERIOR (posterior mean/pruned) ---\n",
    "print(\"\\nPOSTERIOR models (k = #vekter for å nå 0.8 av aktiveringsbidrag):\")\n",
    "for name, weights in pruned_model_means.items():\n",
    "    W1 = weights['W_1']\n",
    "    # hvis bias-mean finnes: b = weights.get('hidden_bias', None)\n",
    "    k_nodes, k_mean = activation_concentration(W1, x_train, b=None, threshold=0.8, account_nonlinearity=True)\n",
    "    print(f\"{name:25s}  mean_k = {k_mean:.2f}  (P={W1.shape[0]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NODES, maybe not as important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize_networks import compute_activation_frequency, extract_all_pruned_means, plot_all_networks_subplots_activations\n",
    "path = \"datasets/friedman/Friedman_N100_p10_sigma1.00_seed1.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]\n",
    "\n",
    "node_activation_colors = {\n",
    "    model_name: compute_activation_frequency(fit, x_train)\n",
    "    for model_name, fit in posterior_N100_fits.items()\n",
    "}\n",
    "layer_sizes = [P] + [H]*L + [out_nodes]\n",
    "# Flatten and find the global maximum\n",
    "all_freqs = np.concatenate(list(node_activation_colors.values()))\n",
    "global_max = all_freqs.max()\n",
    "print(global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize_networks import extract_all_pruned_node_means, plot_all_networks_subplots_activations\n",
    "num_nodes_to_prune = 14  # for example\n",
    "pruned_model_means_nodes = extract_all_pruned_node_means(posterior_N100_fits, layer_structure, num_nodes_to_prune)\n",
    "\n",
    "p_nodes, widths_nodes = plot_all_networks_subplots_activations(\n",
    "    pruned_model_means_nodes, layer_sizes, node_activation_colors,\n",
    "    activation_color_max=global_max, signed_colors=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
