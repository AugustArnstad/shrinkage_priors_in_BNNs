{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman/many\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "model_names_tanh = [\"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "model_names_tanh_L2 = [\"Dirichlet Horseshoe tanh L2\", \"Dirichlet Student T tanh L2\", \"Beta Horseshoe tanh L2\", \"Beta Student T tanh L2\"]\n",
    "\n",
    "tanh_fits = {}\n",
    "tanh_fits_L2 = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit_L2 = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_L2,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fits[base_config_name] = tanh_fit\n",
    "    tanh_fits_L2[base_config_name] = tanh_fit_L2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman_correlated/many\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "model_names_tanh_corr = [\"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "model_names_tanh_L2_corr = [\"Dirichlet Horseshoe tanh L2\", \"Dirichlet Student T tanh L2\", \"Beta Horseshoe tanh L2\", \"Beta Student T tanh L2\"]\n",
    "\n",
    "tanh_fits_corr = {}\n",
    "tanh_fits_L2_corr = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit_L2_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_L2_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fits_corr[base_config_name] = tanh_fit_corr\n",
    "    tanh_fits_L2_corr[base_config_name] = tanh_fit_L2_corr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from properscoring import crps_ensemble\n",
    "from scores.probability import crps_for_ensemble\n",
    "\n",
    "_FRIEDMAN_KEY = re.compile(r\"Friedman_N(\\d+)_p\\d+_sigma([\\d.]+)_seed(\\d+)\")\n",
    "\n",
    "def extract_friedman_metadata(key: str):\n",
    "    \"\"\"\n",
    "    Parse 'Friedman_N{N}_p10_sigma{sigma}_seed{seed}' -> (N:int, sigma:float, seed:int)\n",
    "    Returns (None, None, None) if it doesn't match.\n",
    "    \"\"\"\n",
    "    m = _FRIEDMAN_KEY.search(key)\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "    N = int(m.group(1))\n",
    "    sigma = float(m.group(2))\n",
    "    seed = int(m.group(3))\n",
    "    return N, sigma, seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_from_fits_v2(all_fits, model_names=None, folder=\"friedman\"):\n",
    "    \"\"\"\n",
    "    Iterate over all dataset keys in `all_fits` (e.g., relu_fits or tanh_fits).\n",
    "    For each model in `model_names` (or all models found if None), compute:\n",
    "      - RMSE for each posterior draw (standardized scale)\n",
    "      - RMSE for each posterior draw (original scale)\n",
    "      - RMSE of the posterior mean predictor (standardized scale)\n",
    "      - RMSE of the posterior mean predictor (original scale)\n",
    "\n",
    "    Returns:\n",
    "        df_rmse: long DF with one row per posterior draw.\n",
    "        df_posterior_rmse: one row per model/dataset with posterior-mean RMSE.\n",
    "    \"\"\"\n",
    "    rmse_rows = []\n",
    "    post_mean_rows = []\n",
    "\n",
    "    for dataset_key, model_dict in all_fits.items():\n",
    "        N, sigma, seed = extract_friedman_metadata(dataset_key)\n",
    "        if N is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            path = f\"datasets/{folder}/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "        except FileNotFoundError:\n",
    "            path = f\"datasets/{folder}/many/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "\n",
    "        y_test = data[\"y_test\"].squeeze()  # (N_test,)\n",
    "        #y_mean = data[\"y_mean\"]\n",
    "        y_std = data[\"y_std\"]\n",
    "\n",
    "        # Make these safe scalars if they are stored as arrays like shape (1,)\n",
    "        y_mean = float(np.asarray(y_mean).squeeze())\n",
    "        y_std = float(np.asarray(y_std).squeeze())\n",
    "\n",
    "        # Original-scale y_test\n",
    "        y_test_orig = y_test * y_std + y_mean\n",
    "\n",
    "        models_to_eval = model_names or list(model_dict.keys())\n",
    "\n",
    "        for model in models_to_eval:\n",
    "            entry = model_dict.get(model, None)\n",
    "            if not entry or \"posterior\" not in entry:\n",
    "                print(f\"[SKIP] Missing posterior: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            fit = entry[\"posterior\"]\n",
    "\n",
    "            output_test = fit.stan_variable(\"output_test\")\n",
    "            if output_test.ndim == 3 and output_test.shape[-1] == 1:\n",
    "                preds = output_test[..., 0]  # (S, N_test)\n",
    "            elif output_test.ndim == 2:\n",
    "                preds = output_test  # (S, N_test)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unexpected output_test shape {output_test.shape} for {dataset_key} -> {model}\"\n",
    "                )\n",
    "\n",
    "            # ---- Standardized-scale RMSE per draw ----\n",
    "            sq_err = (preds - y_test[None, :]) ** 2\n",
    "            rmse_per_sample = np.sqrt(np.mean(sq_err, axis=1))  # (S,)\n",
    "\n",
    "            # ---- Original-scale RMSE per draw ----\n",
    "            preds_orig = preds * y_std + y_mean\n",
    "            sq_err_orig = (preds_orig - y_test_orig[None, :]) ** 2\n",
    "            rmse_per_sample_orig = np.sqrt(np.mean(sq_err_orig, axis=1))  # (S,)\n",
    "\n",
    "            for s_idx, (rmse, rmse_orig) in enumerate(zip(rmse_per_sample, rmse_per_sample_orig)):\n",
    "                rmse_rows.append({\n",
    "                    \"dataset_key\": dataset_key,\n",
    "                    \"model\": model,\n",
    "                    \"N\": N,\n",
    "                    \"sigma\": sigma,\n",
    "                    \"seed\": seed,\n",
    "                    \"sample_idx\": s_idx,\n",
    "                    \"rmse\": float(rmse),                 # standardized\n",
    "                    \"rmse_orig\": float(rmse_orig),       # original scale\n",
    "                })\n",
    "\n",
    "            # Posterior-mean RMSE (standardized)\n",
    "            posterior_mean = preds.mean(axis=0)  # (N_test,)\n",
    "            post_mean_rmse = float(np.sqrt(np.mean((posterior_mean - y_test) ** 2)))\n",
    "\n",
    "            # Posterior-mean RMSE (original)\n",
    "            posterior_mean_orig = posterior_mean * y_std + y_mean\n",
    "            post_mean_rmse_orig = float(np.sqrt(np.mean((posterior_mean_orig - y_test_orig) ** 2)))\n",
    "\n",
    "            post_mean_rows.append({\n",
    "                \"dataset_key\": dataset_key,\n",
    "                \"model\": model,\n",
    "                \"N\": N,\n",
    "                \"sigma\": sigma,\n",
    "                \"seed\": seed,\n",
    "                \"posterior_mean_rmse\": post_mean_rmse,                 # standardized\n",
    "                \"posterior_mean_rmse_orig\": post_mean_rmse_orig,       # original scale\n",
    "            })\n",
    "\n",
    "    df_rmse = pd.DataFrame(rmse_rows)\n",
    "    df_posterior_rmse = pd.DataFrame(post_mean_rows)\n",
    "    return df_rmse, df_posterior_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ReLU models\n",
    "df_rmse_tanh, df_posterior_rmse_tanh = compute_rmse_from_fits_v2(\n",
    "    tanh_fits, model_names_tanh\n",
    ")\n",
    "\n",
    "df_rmse_tanh_correlated, df_posterior_rmse_tanh_correlated = compute_rmse_from_fits_v2(\n",
    "    tanh_fits_corr, model_names_tanh, folder = \"friedman_correlated\"\n",
    ")\n",
    "\n",
    "# Evaluate tanh models\n",
    "df_rmse_tanh_L2, df_posterior_rmse_tanh_L2 = compute_rmse_from_fits_v2(\n",
    "    tanh_fits_L2, model_names_tanh_L2\n",
    ")\n",
    "\n",
    "df_rmse_tanh_correlated_L2, df_posterior_rmse_tanh_correlated_L2 = compute_rmse_from_fits_v2(\n",
    "    tanh_fits_L2_corr, model_names_tanh_L2, folder = \"friedman_correlated\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UNCORRELATED: \\n\")\n",
    "print(\"BHS L1: \", df_posterior_rmse_tanh[df_posterior_rmse_tanh['model'] == \"Beta Horseshoe tanh\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"DHS L1: \", df_posterior_rmse_tanh[df_posterior_rmse_tanh['model'] == \"Dirichlet Horseshoe tanh\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"BST L1: \", df_posterior_rmse_tanh[df_posterior_rmse_tanh['model'] == \"Beta Student T tanh\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"DST L1: \", df_posterior_rmse_tanh[df_posterior_rmse_tanh['model'] == \"Dirichlet Student T tanh\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "\n",
    "print(\"BHS L2: \", df_posterior_rmse_tanh_L2[df_posterior_rmse_tanh_L2['model'] == \"Beta Horseshoe tanh L2\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"DHS L2: \", df_posterior_rmse_tanh_L2[df_posterior_rmse_tanh_L2['model'] == \"Dirichlet Horseshoe tanh L2\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"BST L2: \", df_posterior_rmse_tanh_L2[df_posterior_rmse_tanh_L2['model'] == \"Beta Student T tanh L2\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"DST L2: \", df_posterior_rmse_tanh_L2[df_posterior_rmse_tanh_L2['model'] == \"Dirichlet Student T tanh L2\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "\n",
    "print(\"CORRELATED: \\n\")\n",
    "print(\"BHS L1: \", df_posterior_rmse_tanh_correlated[df_posterior_rmse_tanh_correlated['model'] == \"Beta Horseshoe tanh\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"DHS L1: \", df_posterior_rmse_tanh_correlated[df_posterior_rmse_tanh_correlated['model'] == \"Dirichlet Horseshoe tanh\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"BST L1: \", df_posterior_rmse_tanh_correlated[df_posterior_rmse_tanh_correlated['model'] == \"Beta Student T tanh\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"DST L1: \", df_posterior_rmse_tanh_correlated[df_posterior_rmse_tanh_correlated['model'] == \"Dirichlet Student T tanh\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "\n",
    "print(\"BHS L2: \", df_posterior_rmse_tanh_correlated_L2[df_posterior_rmse_tanh_correlated_L2['model'] == \"Beta Horseshoe tanh L2\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"DHS L2: \", df_posterior_rmse_tanh_correlated_L2[df_posterior_rmse_tanh_correlated_L2['model'] == \"Dirichlet Horseshoe tanh L2\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"BST L2: \", df_posterior_rmse_tanh_correlated_L2[df_posterior_rmse_tanh_correlated_L2['model'] == \"Beta Student T tanh L2\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n",
    "print(\"DST L2: \", df_posterior_rmse_tanh_correlated_L2[df_posterior_rmse_tanh_correlated_L2['model'] == \"Dirichlet Student T tanh L2\"]['posterior_mean_rmse_orig'].mean(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = df_rmse_tanh.assign(activation=\"Tanh\", setting=\"Original\")\n",
    "df2 = df_rmse_tanh_L2.assign(activation=\"Tanh L2\", setting=\"Original\")\n",
    "df3 = df_rmse_tanh_correlated.assign(activation=\"Tanh\", setting=\"Correlated\")\n",
    "df4 = df_rmse_tanh_correlated_L2.assign(activation=\"Tanh L2\", setting=\"Correlated\")\n",
    "\n",
    "df_all = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "\n",
    "df1_pm = df_posterior_rmse_tanh.assign(activation=\"Tanh\", setting=\"Original\")\n",
    "df2_pm = df_posterior_rmse_tanh_L2.assign(activation=\"Tanh L2\", setting=\"Original\")\n",
    "df3_pm = df_posterior_rmse_tanh_correlated.assign(activation=\"Tanh\", setting=\"Correlated\")\n",
    "df4_pm = df_posterior_rmse_tanh_correlated_L2.assign(activation=\"Tanh L2\", setting=\"Correlated\")\n",
    "\n",
    "df_all_pm = pd.concat([df1_pm, df2_pm, df3_pm, df4_pm], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- prepare data ---\n",
    "df = df_all.copy()\n",
    "\n",
    "abbr = {\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DST\",    \n",
    "    \"Beta Horseshoe\": \"BHS\",\n",
    "    \"Beta Student T\": \"BST\",\n",
    "    \"Dirichlet Horseshoe L2\": \"DHS L2\",\n",
    "    \"Dirichlet Student T L2\": \"DST L2\",    \n",
    "    \"Beta Horseshoe L2\": \"BHS L2\",\n",
    "    \"Beta Student T L2\": \"BST L2\",\n",
    "}\n",
    "\n",
    "# unify model names across activations (strip \" tanh\")\n",
    "df[\"model_clean\"] = df[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "df[\"model_clean\"] = df[\"model_clean\"].str.replace(\" L2\", \"\", regex=False)\n",
    "\n",
    "summary_df = (\n",
    "    df.groupby([\"setting\", \"model_clean\", \"activation\"], as_index=False)[\"rmse\"]\n",
    "      .agg(mean=\"mean\", std=\"std\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# --- plotting order ---\n",
    "settings = [\"Original\", \"Correlated\"]\n",
    "models = [\"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "models_abbr = [\"DHS\", \"DST\", \"BHS\", \"BST\"]\n",
    "activations = [\"Tanh\", \"Tanh L2\"]\n",
    "\n",
    "# --- visuals ---\n",
    "markers = {\"Tanh\": \"o\", \"Tanh L2\": \"^\"}\n",
    "act_offsets = {\"Tanh\": -0.12, \"Tanh L2\": +0.12}\n",
    "\n",
    "palette_list = plt.get_cmap(\"tab10\").colors\n",
    "palette = {m: palette_list[i] for i, m in enumerate(models)}\n",
    "\n",
    "# x positions for models\n",
    "xbase = {m: i for i, m in enumerate(models)}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(settings), figsize=(14, 4), sharey=True)\n",
    "\n",
    "for ax, setting in zip(axes, settings):\n",
    "    for m in models:\n",
    "        for act in activations:\n",
    "            g = summary_df[\n",
    "                (summary_df[\"setting\"] == setting) &\n",
    "                (summary_df[\"model_clean\"] == m) &\n",
    "                (summary_df[\"activation\"] == act)\n",
    "            ]\n",
    "\n",
    "            if g.empty:\n",
    "                continue  # skip missing combos\n",
    "\n",
    "            x = xbase[m] + act_offsets[act]\n",
    "\n",
    "            ax.errorbar(\n",
    "                x, float(g[\"mean\"].iloc[0]), yerr=float(g[\"std\"].iloc[0]),\n",
    "                fmt=markers[act], markersize=10,\n",
    "                linestyle=\"none\", capsize=3,\n",
    "                color=palette[m], markeredgecolor=\"black\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(setting, fontsize=15)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models_abbr, rotation=20, ha=\"right\", fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"RMSE\", fontsize=15)\n",
    "\n",
    "# --- legends (color = model, marker = activation) ---\n",
    "model_handles = [\n",
    "    Line2D([0], [0], marker=\"o\", linestyle=\"none\",\n",
    "           color=palette[m], markeredgecolor=\"black\",\n",
    "           markersize=10, label=abbr[m])\n",
    "    for m in models\n",
    "]\n",
    "activation_handles = [\n",
    "    Line2D([0], [0], marker=markers[\"Tanh\"], linestyle=\"none\",\n",
    "           color=\"black\", markersize=10, label=\"Tanh\"),\n",
    "    Line2D([0], [0], marker=markers[\"Tanh L2\"], linestyle=\"none\",\n",
    "           color=\"black\", markersize=10, label=\"Tanh L2\"),\n",
    "]\n",
    "\n",
    "axes[-1].legend(\n",
    "    handles=model_handles + activation_handles,\n",
    "    loc=\"upper right\",\n",
    "    frameon=False,\n",
    "    fontsize=11\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- prepare data ---\n",
    "df = df_all_pm.copy()\n",
    "\n",
    "abbr = {\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DST\",    \n",
    "    \"Beta Horseshoe\": \"BHS\",\n",
    "    \"Beta Student T\": \"BST\",\n",
    "    \"Dirichlet Horseshoe L2\": \"DHS L2\",\n",
    "    \"Dirichlet Student T L2\": \"DST L2\",    \n",
    "    \"Beta Horseshoe L2\": \"BHS L2\",\n",
    "    \"Beta Student T L2\": \"BST L2\",\n",
    "}\n",
    "\n",
    "# unify model names across activations (strip \" tanh\")\n",
    "df[\"model_clean\"] = df[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "df[\"model_clean\"] = df[\"model_clean\"].str.replace(\" L2\", \"\", regex=False)\n",
    "\n",
    "summary_df = (\n",
    "    df.groupby([\"setting\", \"model_clean\", \"activation\"], as_index=False)[\"posterior_mean_rmse\"]\n",
    "      .agg(mean=\"mean\", std=\"std\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# --- plotting order ---\n",
    "settings = [\"Original\", \"Correlated\"]\n",
    "models = [\"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "models_abbr = [\"DHS\", \"DST\", \"BHS\", \"BST\"]\n",
    "activations = [\"Tanh\", \"Tanh L2\"]\n",
    "\n",
    "# --- visuals ---\n",
    "markers = {\"Tanh\": \"o\", \"Tanh L2\": \"^\"}\n",
    "act_offsets = {\"Tanh\": -0.12, \"Tanh L2\": +0.12}\n",
    "\n",
    "palette_list = plt.get_cmap(\"tab10\").colors\n",
    "palette = {m: palette_list[i] for i, m in enumerate(models)}\n",
    "\n",
    "# x positions for models\n",
    "xbase = {m: i for i, m in enumerate(models)}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(settings), figsize=(14, 4), sharey=True)\n",
    "\n",
    "for ax, setting in zip(axes, settings):\n",
    "    for m in models:\n",
    "        for act in activations:\n",
    "            g = summary_df[\n",
    "                (summary_df[\"setting\"] == setting) &\n",
    "                (summary_df[\"model_clean\"] == m) &\n",
    "                (summary_df[\"activation\"] == act)\n",
    "            ]\n",
    "\n",
    "            if g.empty:\n",
    "                continue  # skip missing combos\n",
    "\n",
    "            x = xbase[m] + act_offsets[act]\n",
    "\n",
    "            ax.errorbar(\n",
    "                x, float(g[\"mean\"].iloc[0]), yerr=float(g[\"std\"].iloc[0]),\n",
    "                fmt=markers[act], markersize=10,\n",
    "                linestyle=\"none\", capsize=3,\n",
    "                color=palette[m], markeredgecolor=\"black\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(setting, fontsize=15)\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models_abbr, rotation=20, ha=\"right\", fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Posterior mean RMSE\", fontsize=15)\n",
    "\n",
    "# --- legends (color = model, marker = activation) ---\n",
    "model_handles = [\n",
    "    Line2D([0], [0], marker=\"o\", linestyle=\"none\",\n",
    "           color=palette[m], markeredgecolor=\"black\",\n",
    "           markersize=10, label=abbr[m])\n",
    "    for m in models\n",
    "]\n",
    "activation_handles = [\n",
    "    Line2D([0], [0], marker=markers[\"Tanh\"], linestyle=\"none\",\n",
    "           color=\"black\", markersize=10, label=\"Tanh\"),\n",
    "    Line2D([0], [0], marker=markers[\"Tanh L2\"], linestyle=\"none\",\n",
    "           color=\"black\", markersize=10, label=\"Tanh L2\"),\n",
    "]\n",
    "\n",
    "axes[-1].legend(\n",
    "    handles=model_handles + activation_handles,\n",
    "    loc=\"upper right\",\n",
    "    frameon=False,\n",
    "    fontsize=11\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_tanh, local_prune_weights\n",
    "\n",
    "def local_prune_weights(weights, sparsity_level, index_to_prune=0):\n",
    "    \"\"\"\n",
    "    Apply pruning to only one weight matrix in a list, specified by index.\n",
    "\n",
    "    Parameters:\n",
    "    - weights: list of np.ndarray (e.g., [W1, W2])\n",
    "    - sparsity_level: fraction of weights to prune (0.0 to 1.0)\n",
    "    - index_to_prune: which weight matrix to prune in the list\n",
    "\n",
    "    Returns:\n",
    "    - list of masks (one for each weight matrix)\n",
    "    \"\"\"\n",
    "    masks = [np.ones_like(W) for W in weights]\n",
    "\n",
    "    W = weights[index_to_prune]\n",
    "    flat = np.abs(W.flatten())\n",
    "    num_to_prune = int(np.floor(sparsity_level * flat.size))\n",
    "\n",
    "    if num_to_prune > 0:\n",
    "        idx = np.argpartition(flat, num_to_prune)[:num_to_prune]\n",
    "        mask_flat = np.ones_like(flat, dtype=bool)\n",
    "        mask_flat[idx] = False\n",
    "        masks[index_to_prune] = mask_flat.reshape(W.shape).astype(float)\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "def forward_pass_tanh_L2(X, W1, b1, W2, b2, WL, bL):\n",
    "    \"\"\"\n",
    "    Forward pass for a single layer BNN.\n",
    "    \"\"\"\n",
    "    pre_act_1 = X @ W1 + b1.reshape(1, -1)\n",
    "    post_act_1 = np.tanh(pre_act_1)\n",
    "    pre_act_2 = post_act_1 @ W2 + b2.reshape(1, -1)\n",
    "    post_act_2 = np.tanh(pre_act_2)\n",
    "    ouput = post_act_2 @ WL + bL.reshape(1, -1)\n",
    "    return ouput\n",
    "\n",
    "\n",
    "def compute_sparse_rmse_results(seeds, models, all_fits, get_N_sigma, forward_pass, folder, layers=1,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma()\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.1f}_seed{seed}'\n",
    "        path = f\"datasets/{folder}/{dataset_key}.npz\"\n",
    "\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                if layers == 2:\n",
    "                    W2_samples = fit.stan_variable(\"W_2\")           # (S, P, H)\n",
    "                WL_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b_samples = fit.stan_variable(\"hidden_bias\")   # (S, L, H)\n",
    "                b1_samples = b_samples[:, 0, :]\n",
    "                if layers == 2:\n",
    "                    b2_samples = b_samples[:, 1, :]\n",
    "                bL_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "            #print(y_test.shape)\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                if layers == 2:\n",
    "                    W2 = W2_samples[i]\n",
    "                WL = WL_samples[i]\n",
    "\n",
    "                # Apply pruning mask if requested\n",
    "                if prune_fn is not None and sparsity > 0.0:\n",
    "                    mask_W1 = prune_fn([W1, WL], sparsity)\n",
    "                    if layers == 2:\n",
    "                        mask_W2 = prune_fn([W1, W2], sparsity, index_to_prune=1)\n",
    "                    W1 = W1 * mask_W1[0]\n",
    "                    if layers == 2:\n",
    "                        W2 = W2 * mask_W2[1]\n",
    "                    # if i == 0:\n",
    "                    #     print(W2)\n",
    "                if layers == 1:\n",
    "                    y_hat = forward_pass(X_test, W1, b1_samples[i], WL, bL_samples[i])\n",
    "                    y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "                    rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "                else:\n",
    "                    y_hat = forward_pass(X_test, W1, b1_samples[i], W2, b2_samples[i], WL, bL_samples[i])\n",
    "                    y_hats[i] = y_hat.squeeze()\n",
    "                    rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "                \n",
    "            posterior_mean = np.mean(y_hats, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'sparsity': sparsity,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.97, 0.99]\n",
    "#sparsity_levels = [0.0, 0.1, 0.95]\n",
    "\n",
    "\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "\n",
    "def get_N_sigma():\n",
    "    N=100\n",
    "    sigma=5.00\n",
    "    return N, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse_sparse_tanh, df_posterior_rmse_sparse_tanh = {}, {}\n",
    "df_rmse_sparse_tanh_L2, df_posterior_rmse_sparse_tanh_L2 = {}, {}\n",
    "df_rmse_sparse_tanh_correlated, df_posterior_rmse_sparse_tanh_correlated = {}, {}\n",
    "df_rmse_sparse_tanh_correlated_L2, df_posterior_rmse_sparse_tanh_correlated_L2 = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    df_rmse_sparse_tanh[sparsity], df_posterior_rmse_sparse_tanh[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_tanh, tanh_fits, get_N_sigma, forward_pass_tanh, folder = \"friedman_std_5\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_sparse_tanh_L2[sparsity], df_posterior_rmse_sparse_tanh_L2[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_tanh_L2, tanh_fits_L2, get_N_sigma, forward_pass_tanh_L2, folder = \"friedman_std_5\", layers=2,\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_sparse_tanh_correlated[sparsity], df_posterior_rmse_sparse_tanh_correlated[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_tanh, tanh_fits_correlated, get_N_sigma, forward_pass_tanh, folder = \"friedman_std_5_correlated\",\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_sparse_tanh_correlated_L2[sparsity], df_posterior_rmse_sparse_tanh_correlated_L2[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_tanh_L2, tanh_fits_correlated_L2, get_N_sigma, forward_pass_tanh_L2, folder = \"friedman_std_5_correlated\", layers=2,\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_rmse_full_tanh = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_sparse_tanh.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_rmse_full_tanh_L2 = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_sparse_tanh_L2.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_rmse_full_tanh_correlated = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_sparse_tanh_correlated.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_rmse_full_tanh_correlated_L2 = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_sparse_tanh_correlated_L2.items()],\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tanh_o = df_rmse_full_tanh.copy()\n",
    "df_tanh_o[\"model\"] = df_tanh_o[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "df_tanh_o_L2 = df_rmse_full_tanh_L2.copy()\n",
    "df_tanh_o_L2[\"model\"] = df_tanh_o_L2[\"model\"].str.replace(\" tanh L2\", \"\", regex=False)\n",
    "\n",
    "df_tanh_c = df_rmse_full_tanh_correlated.copy()\n",
    "df_tanh_c[\"model\"] = df_tanh_c[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "df_tanh_c_L2 = df_rmse_full_tanh_correlated_L2.copy()\n",
    "df_tanh_c_L2[\"model\"] = df_tanh_c_L2[\"model\"].str.replace(\" tanh L2\", \"\", regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_meta(df, setting, depth):\n",
    "    out = df.copy()\n",
    "    out[\"setting\"] = setting      # \"Correlated\" or \"Original\"\n",
    "    out[\"depth\"] = depth          # \"1HL\" or \"2HL\"\n",
    "    out[\"group\"] = f\"{setting} | {depth}\"\n",
    "    return out\n",
    "\n",
    "df_all = pd.concat([\n",
    "    add_meta(df_tanh_c,     \"Corr\", \"1\"),\n",
    "    add_meta(df_tanh_c_L2,  \"Corr\", \"2\"),\n",
    "    add_meta(df_tanh_o,     \"Uncorr\",   \"1\"),\n",
    "    add_meta(df_tanh_o_L2,  \"Uncorr\",   \"2\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "# (optional) ensure numeric + sorted sparsity\n",
    "df_all[\"sparsity\"] = pd.to_numeric(df_all[\"sparsity\"])\n",
    "df_all[\"rmse\"] = pd.to_numeric(df_all[\"rmse\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seed_curve(df_all, seed=1, sigma=None, N=None):\n",
    "    d = df_all[df_all[\"seed\"] == seed].copy()\n",
    "    if sigma is not None:\n",
    "        d = d[d[\"sigma\"] == sigma]\n",
    "    if N is not None:\n",
    "        d = d[d[\"N\"] == N]\n",
    "\n",
    "    # Average duplicates at same sparsity (common if you have multiple runs/replicates)\n",
    "    dsum = (d.groupby([\"group\", \"model\", \"sparsity\"], as_index=False)[\"rmse\"]\n",
    "              .mean()\n",
    "              .sort_values(\"sparsity\"))\n",
    "\n",
    "    groups = dsum[\"group\"].unique()\n",
    "    fig, axes = plt.subplots(1, len(groups), figsize=(14, 4), sharey=False)\n",
    "\n",
    "    if len(groups) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, grp in zip(axes, groups):\n",
    "        dg = dsum[dsum[\"group\"] == grp]\n",
    "        for model, g in dg.groupby(\"model\"):\n",
    "            ax.plot(g[\"sparsity\"], g[\"rmse\"], marker=\"o\", linewidth=2, label=model)\n",
    "\n",
    "        ax.set_title(f\"{grp} (seed={seed}\" +\n",
    "                     (f\", N={N}\" if N is not None else \"\") +\n",
    "                     (f\", sigma={sigma}\" if sigma is not None else \"\") +\n",
    "                     \")\", fontsize=12)\n",
    "        ax.set_xlabel(\"Sparsity\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"RMSE\")\n",
    "    axes[-1].legend(loc=\"best\", frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_seed_curve(df_all, seed=1, N=100, sigma=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_curve(df_all, sigma=None, N=None, band=\"std\"):\n",
    "    d = df_all.copy()\n",
    "    if sigma is not None:\n",
    "        d = d[d[\"sigma\"] == sigma]\n",
    "    if N is not None:\n",
    "        d = d[d[\"N\"] == N]\n",
    "\n",
    "    # mean/std across seeds at each sparsity for each model+group\n",
    "    dsum = (d.groupby([\"group\", \"model\", \"sparsity\"], as_index=False)[\"rmse\"]\n",
    "              .agg(mean=\"mean\", std=\"std\")\n",
    "              .sort_values(\"sparsity\"))\n",
    "\n",
    "    groups = dsum[\"group\"].unique()\n",
    "    fig, axes = plt.subplots(1, len(groups), figsize=(14, 4), sharey=False)\n",
    "\n",
    "    if len(groups) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, grp in zip(axes, groups):\n",
    "        dg = dsum[dsum[\"group\"] == grp]\n",
    "        for model, g in dg.groupby(\"model\"):\n",
    "            ax.plot(g[\"sparsity\"], g[\"mean\"], linewidth=2, label=model)\n",
    "            if band == \"std\":\n",
    "                ax.fill_between(\n",
    "                    g[\"sparsity\"],\n",
    "                    g[\"mean\"] - g[\"std\"],\n",
    "                    g[\"mean\"] + g[\"std\"],\n",
    "                    alpha=0.15\n",
    "                )\n",
    "            elif band == \"errorbar\":\n",
    "                ax.errorbar(g[\"sparsity\"], g[\"mean\"], yerr=g[\"std\"], fmt=\"none\", capsize=2)\n",
    "\n",
    "        ax.set_title(f\"{grp} (mean over seeds\" +\n",
    "                     (f\", N={N}\" if N is not None else \"\") +\n",
    "                     (f\", sigma={sigma}\" if sigma is not None else \"\") +\n",
    "                     \")\", fontsize=12)\n",
    "        ax.set_xlabel(\"Sparsity\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"RMSE (mean over seeds)\")\n",
    "    axes[-1].legend(loc=\"best\", frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_mean_curve(df_all, N=100, sigma=5.0, band=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORIGINAL DATASET - ENDRE RUN_REGRESSION.py\n",
    "python3 utils/run_all_regression_models.py --model beta_tau_tanh --output_dir results/regression/single_layer/relu/friedman/no_lambda &&\n",
    "python3 utils/run_all_regression_models.py --model dirichlet_tau_tanh --output_dir results/regression/single_layer/relu/friedman/no_lambda &&\n",
    "\n",
    "python3 utils/run_all_regression_models.py --model beta --output_dir results/regression/single_layer/relu/friedman/no_lambda &&\n",
    "python3 utils/run_all_regression_models.py --model dirichlet --output_dir results/regression/single_layer/relu/friedman/no_lambda &&\n",
    "\n",
    "python3 utils/run_all_regression_models.py --model beta_tanh --output_dir results/regression/single_layer/relu/friedman/no_lambda &&\n",
    "python3 utils/run_all_regression_models.py --model dirichlet_tanh --output_dir results/regression/single_layer/relu/friedman/no_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import dirichlet, beta\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "K = 10\n",
    "alpha = 0.1\n",
    "\n",
    "# Dirichlet draw\n",
    "dirichlet_draw = dirichlet.rvs(alpha=np.full(K, alpha), size=1)[0]\n",
    "\n",
    "# Independent Beta draws (same marginals)\n",
    "beta_draw = beta.rvs(alpha, (K - 1) * alpha, size=K)\n",
    "\n",
    "print(\"Dirichlet draw:\")\n",
    "print(dirichlet_draw)\n",
    "print(\"Sum:\", dirichlet_draw.sum())\n",
    "\n",
    "print(\"\\nIndependent Beta draws:\")\n",
    "print(beta_draw)\n",
    "print(\"Sum:\", beta_draw.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100_000\n",
    "\n",
    "dirichlet_sums = np.ones(N)  # always 1\n",
    "beta_sums = beta.rvs(alpha, (K - 1) * alpha, size=(N, K)).sum(axis=1)\n",
    "\n",
    "print(\"Beta sum: mean =\", beta_sums.mean())\n",
    "print(\"Beta sum: std  =\", beta_sums.std())\n",
    "print(\"Beta sum: min/max =\", beta_sums.min(), beta_sums.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirichlet_samples = dirichlet.rvs(\n",
    "    alpha=np.full(K, alpha), size=N\n",
    ")\n",
    "\n",
    "beta_samples = beta.rvs(\n",
    "    alpha, (K - 1) * alpha, size=(N, K)\n",
    ")\n",
    "\n",
    "print(\"Dirichlet off-diagonal correlation (approx):\",\n",
    "      np.corrcoef(dirichlet_samples.T)[0, 1])\n",
    "\n",
    "print(\"Beta off-diagonal correlation (approx):\",\n",
    "      np.corrcoef(beta_samples.T)[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import dirichlet, beta\n",
    "\n",
    "# --- Settings ---\n",
    "np.random.seed(42)\n",
    "K = 10\n",
    "alpha = 0.1\n",
    "N = 80_000  # Monte Carlo draws\n",
    "\n",
    "# --- 1) Draw variance vectors ---\n",
    "v_dir = dirichlet.rvs(alpha=np.full(K, alpha), size=N)            # sums to 1\n",
    "v_beta = beta.rvs(alpha, (K - 1) * alpha, size=(N, K))            # iid, same marginals\n",
    "\n",
    "# --- 2) Draw weights with those variances: w_i ~ N(0, v_i) ---\n",
    "w1 = np.random.normal(loc=0.0, scale=v_dir, size=(N, K))   # Dirichlet variances\n",
    "w2 = np.random.normal(loc=0.0, scale=v_beta, size=(N, K))  # Beta variances\n",
    "\n",
    "# --- 3) Diagnostics for plotting ---\n",
    "sum_dir = v_dir.sum(axis=1)      # == 1\n",
    "sum_beta = v_beta.sum(axis=1)\n",
    "\n",
    "max_dir = v_dir.max(axis=1)\n",
    "max_beta = v_beta.max(axis=1)\n",
    "\n",
    "l2_w1 = np.linalg.norm(w1, axis=1)\n",
    "l2_w2 = np.linalg.norm(w2, axis=1)\n",
    "\n",
    "maxabs_w1 = np.max(np.abs(w1), axis=1)\n",
    "maxabs_w2 = np.max(np.abs(w2), axis=1)\n",
    "\n",
    "# Correlation matrices (neutral scaling: always plot on [-1, 1])\n",
    "corr_dir = np.corrcoef(v_dir.T)\n",
    "corr_beta = np.corrcoef(v_beta.T)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PLOT A: Sum of variances (Dirichlet fixed at 1 vs Beta random)\n",
    "# ------------------------------------------------------------\n",
    "plt.figure()\n",
    "plt.hist(sum_beta, bins=80, density=True, alpha=0.8,\n",
    "         label='Sum of iid Beta variances')\n",
    "plt.axvline(1.0, linewidth=2, label='Dirichlet sum (always 1)')\n",
    "plt.xlim(0, K)\n",
    "plt.xlabel('Sum of 10 variance components')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Sum constraint: Dirichlet vs iid Betas (same marginals)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PLOT B: Joint behavior (v1 vs v2) scatter with fixed axes [0,1]\n",
    "# ------------------------------------------------------------\n",
    "m = 6000\n",
    "idx = np.random.choice(N, size=m, replace=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(v_dir[idx, 0], v_dir[idx, 1], s=10, alpha=0.3,\n",
    "            label='Dirichlet (v1 vs v2)')\n",
    "plt.scatter(v_beta[idx, 0], v_beta[idx, 1], s=10, alpha=0.3,\n",
    "            label='iid Betas (v1 vs v2)')\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('v1')\n",
    "plt.ylabel('v2')\n",
    "plt.title('Two components (fixed [0,1] axes)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PLOT C: Correlation heatmap (Dirichlet) with neutral fixed scale\n",
    "# ------------------------------------------------------------\n",
    "plt.figure()\n",
    "plt.imshow(corr_dir, vmin=-1, vmax=1, interpolation='nearest')\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title('Corr(variance components): Dirichlet (fixed scale [-1,1])')\n",
    "plt.xticks(range(K))\n",
    "plt.yticks(range(K))\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PLOT D: Correlation heatmap (iid Betas) with neutral fixed scale\n",
    "# ------------------------------------------------------------\n",
    "plt.figure()\n",
    "plt.imshow(corr_beta, vmin=-1, vmax=1, interpolation='nearest')\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title('Corr(variance components): iid Betas (fixed scale [-1,1])')\n",
    "plt.xticks(range(K))\n",
    "plt.yticks(range(K))\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# PLOT E: Induced weight scale comparison via ||w||_2\n",
    "# ------------------------------------------------------------\n",
    "xmax = float(np.quantile(np.concatenate([l2_w1, l2_w2]), 0.995))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(l2_w1, bins=120, density=True, alpha=0.6,\n",
    "         label=r'$\\|w_1\\|_2$,  $v\\sim$ Dirichlet,  $w|v\\sim N(0,\\mathrm{diag}(v))$')\n",
    "plt.hist(l2_w2, bins=120, density=True, alpha=0.6,\n",
    "         label=r'$\\|w_2\\|_2$,  $v_i\\sim$ iid Beta,  $w|v\\sim N(0,\\mathrm{diag}(v))$')\n",
    "plt.xlim(0, xmax)\n",
    "plt.xlabel(r'$\\|w\\|_2$')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Weights induced by variance vectors: Dirichlet vs iid Betas')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# (Optional) PLOT F: Induced max |w_i|\n",
    "xmax2 = float(np.quantile(np.concatenate([maxabs_w1, maxabs_w2]), 0.995))\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(maxabs_w1, bins=120, density=True, alpha=0.6, label=r'$\\max_i |w_{1,i}|$')\n",
    "plt.hist(maxabs_w2, bins=120, density=True, alpha=0.6, label=r'$\\max_i |w_{2,i}|$')\n",
    "plt.xlim(0, xmax2)\n",
    "plt.xlabel(r'$\\max_i |w_i|$')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Induced extreme weights: Dirichlet vs iid Betas')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
