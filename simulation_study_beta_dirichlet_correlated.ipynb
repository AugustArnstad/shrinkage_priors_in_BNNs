{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman/many\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "model_names_tanh_corr = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "model_names_tanh_nodewise_corr = [\"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\", \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]\n",
    "\n",
    "tanh_fits_corr = {}\n",
    "tanh_fits_nodewise_corr = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    tanh_fit_nodewise_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_nodewise_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    tanh_fits_corr[base_config_name] = tanh_fit_corr  # use clean key\n",
    "    tanh_fits_nodewise_corr[base_config_name] = tanh_fit_nodewise_corr  # use clean key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "model_names_tanh_corr = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "model_names_tanh_nodewise_corr = [\"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\", \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    tanh_fit_nodewise_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_nodewise_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    tanh_fits_corr[base_config_name] = tanh_fit_corr  # use clean key\n",
    "    tanh_fits_nodewise_corr[base_config_name] = tanh_fit_nodewise_corr  # use clean key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def _find_run_key(fits_by_run, *, N, D, sigma, seed, correlated=False):\n",
    "    \"\"\"\n",
    "    Tries to find the right key inside e.g. tanh_fits for the requested (N, D, sigma, seed).\n",
    "    Works even if your sigma formatting differs a bit (e.g. 1, 1.0, 1.00).\n",
    "    \"\"\"\n",
    "    # Common naming you showed: Friedman_N100_p10_sigma1.00_seed3\n",
    "    # If you also have \"CorrelatedFriedman\" etc, we try both patterns.\n",
    "    sigma_strs = [\n",
    "        f\"{sigma}\",\n",
    "        f\"{sigma:.1f}\",\n",
    "        f\"{sigma:.2f}\",\n",
    "        f\"{sigma:.3f}\",\n",
    "    ]\n",
    "    candidates = []\n",
    "    for s in sigma_strs:\n",
    "        if correlated:\n",
    "            candidates.append(f\"CorrelatedFriedman_N{N}_p{D}_sigma{s}_seed{seed}\")\n",
    "        candidates.append(f\"Friedman_N{N}_p{D}_sigma{s}_seed{seed}\")\n",
    "\n",
    "    for k in candidates:\n",
    "        if k in fits_by_run:\n",
    "            return k\n",
    "\n",
    "    # If exact match not found, do a regex search (robust to extra prefixes/suffixes)\n",
    "    # Example matches: \"...Friedman_N100_p10_sigma1.00_seed3...\"\n",
    "    sig_pat = \"|\".join(re.escape(s) for s in sigma_strs)\n",
    "    base = r\"Friedman\" if not correlated else r\"(?:CorrelatedFriedman|Friedman)\"\n",
    "    pat = re.compile(\n",
    "        rf\"{base}_N{N}_p{D}_sigma(?:{sig_pat})_seed{seed}\"\n",
    "    )\n",
    "    matches = [k for k in fits_by_run.keys() if pat.search(k)]\n",
    "    if len(matches) == 1:\n",
    "        return matches[0]\n",
    "    if len(matches) > 1:\n",
    "        # Prefer the shortest/most exact-looking key\n",
    "        matches = sorted(matches, key=len)\n",
    "        return matches[0]\n",
    "\n",
    "    raise KeyError(\n",
    "        f\"Could not find a run in fits_by_run for N={N}, D={D}, sigma={sigma}, seed={seed}, correlated={correlated}.\\n\"\n",
    "        f\"Example expected key: 'Friedman_N{N}_p{D}_sigma{sigma:.2f}_seed{seed}'.\"\n",
    "    )\n",
    "\n",
    "def build_global_mask_from_posterior(\n",
    "    W_samples,\n",
    "    sparsity,\n",
    "    method=\"Eabs\",          # \"Eabs\" or \"Eabs_stability\"\n",
    "    stability_quantile=0.1, # used if method=\"Eabs_stability\"\n",
    "    prune_smallest=True\n",
    "):\n",
    "    \"\"\"\n",
    "    W_samples: array (S, ..., ...) posterior draws of a weight matrix.\n",
    "    sparsity: fraction to prune (q). Keeps (1-q).\n",
    "    Returns mask with same trailing shape as one draw, dtype float {0,1}.\n",
    "    \"\"\"\n",
    "    assert 0.0 <= sparsity < 1.0\n",
    "    S = W_samples.shape[0]\n",
    "    W_abs = np.abs(W_samples)  # (S, ...)\n",
    "\n",
    "    # Importance score a = E|w|\n",
    "    a = W_abs.mean(axis=0)     # (..., ...)\n",
    "\n",
    "    if method == \"Eabs\":\n",
    "        score = a\n",
    "    elif method == \"Eabs_stability\":\n",
    "        # Stability proxy pi = P(|w| > t), where t is a small global quantile of |w|\n",
    "        t = np.quantile(W_abs.reshape(S, -1), stability_quantile)\n",
    "        pi = (W_abs > t).mean(axis=0)\n",
    "        # Combine: emphasize both \"large on average\" and \"consistently non-tiny\"\n",
    "        score = a * pi\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'Eabs' or 'Eabs_stability'\")\n",
    "\n",
    "    # Decide how many to prune\n",
    "    num_params = score.size\n",
    "    k_prune = int(np.floor(sparsity * num_params))\n",
    "    if k_prune == 0:\n",
    "        return np.ones_like(score, dtype=float)\n",
    "\n",
    "    flat = score.reshape(-1)\n",
    "\n",
    "    if prune_smallest:\n",
    "        # prune lowest scores\n",
    "        thresh = np.partition(flat, k_prune - 1)[k_prune - 1]\n",
    "        mask = (score > thresh).astype(float)\n",
    "        # if ties create too many kept/pruned, fix deterministically\n",
    "        # (rare but possible with many equal scores)\n",
    "        if mask.sum() > num_params - k_prune:\n",
    "            # drop some tied-at-threshold entries\n",
    "            idx_tied = np.where(score.reshape(-1) == thresh)[0]\n",
    "            need_drop = int(mask.sum() - (num_params - k_prune))\n",
    "            if need_drop > 0:\n",
    "                mask_flat = mask.reshape(-1)\n",
    "                mask_flat[idx_tied[:need_drop]] = 0.0\n",
    "                mask = mask_flat.reshape(score.shape)\n",
    "        elif mask.sum() < num_params - k_prune:\n",
    "            # add some tied entries if we kept too few\n",
    "            idx_tied = np.where(score.reshape(-1) == thresh)[0]\n",
    "            need_add = int((num_params - k_prune) - mask.sum())\n",
    "            if need_add > 0:\n",
    "                mask_flat = mask.reshape(-1)\n",
    "                # add back from tied\n",
    "                add_candidates = idx_tied[mask_flat[idx_tied] == 0.0]\n",
    "                mask_flat[add_candidates[:need_add]] = 1.0\n",
    "                mask = mask_flat.reshape(score.shape)\n",
    "    else:\n",
    "        # prune largest (not typical)\n",
    "        thresh = np.partition(flat, num_params - k_prune)[num_params - k_prune]\n",
    "        mask = (score < thresh).astype(float)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def precompute_global_masks(\n",
    "    all_fits,\n",
    "    dataset_key,\n",
    "    model,\n",
    "    sparsity_levels,\n",
    "    prune_W2=False,\n",
    "    method=\"Eabs_stability\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns dict: sparsity -> (mask_W1, mask_W2 or None)\n",
    "    \"\"\"\n",
    "    fit = all_fits[dataset_key][model][\"posterior\"]\n",
    "\n",
    "    W1_samples = fit.stan_variable(\"W_1\")  # (S, P, H)\n",
    "    W2_samples = fit.stan_variable(\"W_L\")  # (S, H, O) or (S, H) depending on O\n",
    "\n",
    "    masks = {}\n",
    "    for q in sparsity_levels:\n",
    "        mask_W1 = build_global_mask_from_posterior(W1_samples, q, method=method)\n",
    "        mask_W2 = None\n",
    "        if prune_W2:\n",
    "            mask_W2 = build_global_mask_from_posterior(W2_samples, q, method=method)\n",
    "        masks[q] = (mask_W1, mask_W2)\n",
    "    return masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.generate_data import sample_gaussian_copula_uniform\n",
    "\n",
    "def generate_Friedman_data_v2(N=100, D=10, sigma=1.0, test_size=0.2, seed=42, standardize_y=True, return_scale=True):\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.uniform(0, 1, size=(N, D))\n",
    "    x0, x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5.0 * x4\n",
    "    )\n",
    "    y = y_clean + np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    if not standardize_y:\n",
    "        return (X_train, X_test, y_train, y_test) if not return_scale else (X_train, X_test, y_train, y_test, 0.0, 1.0)\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() if y_train.std() > 0 else 1.0\n",
    "\n",
    "    y_train_s = (y_train - y_mean) / y_std\n",
    "    y_test_s = (y_test - y_mean) / y_std\n",
    "\n",
    "    if return_scale:\n",
    "        return X_train, X_test, y_train_s, y_test_s, y_mean, y_std\n",
    "    return X_train, X_test, y_train_s, y_test_s\n",
    "\n",
    "def generate_correlated_Friedman_data_v2(N=100, D=10, sigma=1.0, test_size=0.2, seed=42, standardize_y=True, return_scale=True):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data for Bayesian neural network experiments.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples.\n",
    "        D (int): Number of features.\n",
    "        sigma (float): Noise level.\n",
    "        test_size (float): Proportion for test split.\n",
    "        seed (int): Random seed.\n",
    "        standardize_y (bool): Whether to standardize the response variable.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test, y_mean, y_std) if standardize_y,\n",
    "               else (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    d = 10\n",
    "    S_custom = np.eye(d)\n",
    "    # Block 1 (vars 0..4): high Spearman, 0.7\n",
    "    for i in range(0, 3):\n",
    "        for j in range(i+1, 3):\n",
    "            S_custom[i, j] = S_custom[j, i] = 0.8\n",
    "    # Block 2 (vars 5..9): moderate Spearman, 0.4\n",
    "    for i in range(5, 10):\n",
    "        for j in range(i+1, 10):\n",
    "            S_custom[i, j] = S_custom[j, i] = -0.5\n",
    "    # Cross-block weaker, 0.15\n",
    "    for i in range(0, 5):\n",
    "        for j in range(5, 10):\n",
    "            S_custom[i, j] = S_custom[j, i] = 0.15\n",
    "    # A couple of bespoke pairs:\n",
    "    S_custom[0, 9] = S_custom[9, 0] = 0.4\n",
    "    S_custom[2, 7] = S_custom[7, 2] = 0.9  # very strong (will be projected if infeasible)\n",
    "    S_custom[3, 4] = S_custom[4, 3] = -0.9  # very strong (will be projected if infeasible)\n",
    "    S_custom[1, 6] = S_custom[6, 1] = -0.9  # very strong (will be projected if infeasible)\n",
    "\n",
    "    U, _ = sample_gaussian_copula_uniform(n=10000, S=S_custom, random_state=123)\n",
    "    #X = np.random.uniform(0, 1, size=(N, D))\n",
    "    if N != U.shape[0]:\n",
    "        idx = np.random.choice(U.shape[0], size=N, replace=False)\n",
    "        X = U[idx, :]\n",
    "    else:\n",
    "        X = U\n",
    "\n",
    "    x0, x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5.0 * x4\n",
    "    )\n",
    "\n",
    "    y = y_clean + np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    if not standardize_y:\n",
    "        return (X_train, X_test, y_train, y_test) if not return_scale else (X_train, X_test, y_train, y_test, 0.0, 1.0)\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() if y_train.std() > 0 else 1.0\n",
    "\n",
    "    y_train_s = (y_train - y_mean) / y_std\n",
    "    y_test_s = (y_test - y_mean) / y_std\n",
    "\n",
    "    if return_scale:\n",
    "        return X_train, X_test, y_train_s, y_test_s, y_mean, y_std\n",
    "    return X_train, X_test, y_train_s, y_test_s\n",
    "\n",
    "def make_large_eval_set(\n",
    "    generator_fn,\n",
    "    N_train,\n",
    "    D,\n",
    "    sigma,\n",
    "    seed,\n",
    "    n_eval=5000,\n",
    "    standardize_y=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns X_eval, y_eval (standardized if standardize_y=True), plus y_mean,y_std\n",
    "    defined from the training split.\n",
    "    \"\"\"\n",
    "    N_total = N_train + n_eval\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te, y_mean, y_std = generator_fn(\n",
    "        N=N_total, D=D, sigma=sigma, test_size=n_eval / N_total, seed=seed,\n",
    "        standardize_y=standardize_y, return_scale=True\n",
    "    )\n",
    "    # Now X_te has approx n_eval points (exact given test_size construction).\n",
    "    return X_te, np.asarray(y_te).squeeze(), y_mean, y_std\n",
    "\n",
    "\n",
    "def _logsumexp(a, axis=None):\n",
    "    amax = np.max(a, axis=axis, keepdims=True)\n",
    "    out = amax + np.log(np.sum(np.exp(a - amax), axis=axis, keepdims=True))\n",
    "    return np.squeeze(out, axis=axis)\n",
    "\n",
    "def gaussian_nll_pointwise(y, mu, sigma):\n",
    "    return 0.5*np.log(2*np.pi*(sigma**2)) + 0.5*((y-mu)**2)/(sigma**2)\n",
    "\n",
    "def compute_sparse_metrics_results_globalmask_large_eval(\n",
    "    seeds, all_seeds, models, all_fits, get_N_sigma, forward_pass,\n",
    "    folder,\n",
    "    sparsity=0.0,\n",
    "    masks_cache=None,\n",
    "    prune_W2=False,\n",
    "    compute_nll=True,\n",
    "    noise_var_name=\"sigma\",\n",
    "    n_eval=5000,\n",
    "    D=10,\n",
    "    standardize_y=True,\n",
    "    # pass the correct generator functions\n",
    "    gen_uncorr=None,\n",
    "    gen_corr=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate on a large generated test set instead of the stored tiny X_test/y_test.\n",
    "    Assumes model was trained on standardized y if standardize_y=True.\n",
    "    \"\"\"\n",
    "    assert gen_uncorr is not None and gen_corr is not None, \"Pass both generator functions.\"\n",
    "\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    # choose generator based on folder name\n",
    "    def choose_gen(folder):\n",
    "        return gen_corr if \"friedman_correlated\" in folder else gen_uncorr\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed, all_seeds)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "\n",
    "        # Build large eval set consistent with training split standardization\n",
    "        gen_fn = choose_gen(folder)\n",
    "        X_test, y_test, y_mean, y_std = make_large_eval_set(\n",
    "            generator_fn=gen_fn,\n",
    "            N_train=N,\n",
    "            D=D,\n",
    "            sigma=sigma,\n",
    "            seed=seed,\n",
    "            n_eval=n_eval,\n",
    "            standardize_y=standardize_y\n",
    "        )\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "\n",
    "                noise_samples = None\n",
    "                if compute_nll:\n",
    "                    try:\n",
    "                        noise_samples = fit.stan_variable(noise_var_name).squeeze()\n",
    "                    except Exception:\n",
    "                        noise_samples = None\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "            #rmses = np.zeros(S)\n",
    "\n",
    "            mask_W1 = mask_W2 = None\n",
    "            if masks_cache is not None and sparsity > 0.0:\n",
    "                mask_W1, mask_W2 = masks_cache[(dataset_key, model)][sparsity]\n",
    "\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                W2 = W2_samples[i]\n",
    "\n",
    "                if mask_W1 is not None:\n",
    "                    W1 = W1 * mask_W1\n",
    "                if prune_W2 and (mask_W2 is not None):\n",
    "                    W2 = W2 * mask_W2\n",
    "\n",
    "                y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i]).squeeze()\n",
    "                y_hats[i] = y_hat\n",
    "                #rmses[i] = np.sqrt(np.mean((y_hat - y_test)**2))\n",
    "\n",
    "            # posterior mean RMSE (standardized scale)\n",
    "            posterior_mean = y_hats.mean(axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test)**2))\n",
    "\n",
    "            out_pm = {\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'n_eval': y_test.shape[0],\n",
    "                'posterior_mean_rmse': posterior_mean_rmse,\n",
    "                'posterior_mean_rmse_orig': posterior_mean_rmse * y_std,  # back to original y scale\n",
    "            }\n",
    "\n",
    "            if compute_nll:\n",
    "                if noise_samples is None:\n",
    "                    sig_s = np.ones(S)\n",
    "                else:\n",
    "                    sig_s = np.asarray(noise_samples).reshape(-1)[:S]\n",
    "\n",
    "                # Expected NLL\n",
    "                nll_draws = np.array([\n",
    "                    gaussian_nll_pointwise(y_test, y_hats[i], sig_s[i]).mean()\n",
    "                    for i in range(S)\n",
    "                ])\n",
    "                expected_nll = nll_draws.mean()\n",
    "\n",
    "                # Predictive (mixture) NLL\n",
    "                loglik = -np.stack([\n",
    "                    gaussian_nll_pointwise(y_test, y_hats[i], sig_s[i])\n",
    "                    for i in range(S)\n",
    "                ], axis=0)  # (S, n_eval)\n",
    "                lppd = (_logsumexp(loglik, axis=0) - np.log(S)).mean()\n",
    "                predictive_nll = -lppd\n",
    "\n",
    "                out_pm[\"expected_nll\"] = expected_nll\n",
    "                out_pm[\"predictive_nll\"] = predictive_nll\n",
    "\n",
    "                # Optional: predictive_nll on original scale (only if you also rescale sigma)\n",
    "                # If your sigma posterior is on standardized scale, original sigma = sig_s * y_std.\n",
    "                out_pm[\"predictive_nll_orig\"] = predictive_nll + np.log(y_std)  # see note below\n",
    "\n",
    "            posterior_means.append(out_pm)\n",
    "\n",
    "            # for i in range(S):\n",
    "            #     row = {\n",
    "            #         'seed': seed,\n",
    "            #         'N': N,\n",
    "            #         'sigma': sigma,\n",
    "            #         'model': model,\n",
    "            #         'sparsity': sparsity,\n",
    "            #         'n_eval': y_test.shape[0],\n",
    "            #         'rmse': rmses[i],\n",
    "            #         'rmse_orig': rmses[i] * y_std\n",
    "            #     }\n",
    "            #     if compute_nll:\n",
    "            #        row[\"nll\"] = gaussian_nll_pointwise(y_test, y_hats[i], sig_s[i]).mean()\n",
    "            #    results.append(row)\n",
    "\n",
    "#    return pd.DataFrame(results), pd.DataFrame(posterior_means)\n",
    "    return pd.DataFrame(posterior_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "def build_masks_cache_for_all(\n",
    "    all_fits,\n",
    "    dataset_keys,\n",
    "    models,\n",
    "    sparsity_levels,\n",
    "    prune_W2=False,\n",
    "    method=\"Eabs_stability\"\n",
    "):\n",
    "    masks_cache = {}\n",
    "    for dataset_key in dataset_keys:\n",
    "        for model in models:\n",
    "            try:\n",
    "                masks_cache[(dataset_key, model)] = precompute_global_masks(\n",
    "                    all_fits=all_fits,\n",
    "                    dataset_key=dataset_key,\n",
    "                    model=model,\n",
    "                    sparsity_levels=sparsity_levels,\n",
    "                    prune_W2=prune_W2,\n",
    "                    method=method\n",
    "                )\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP MASKS] Missing fit for {dataset_key} -> {model}\")\n",
    "    return masks_cache\n",
    "\n",
    "\n",
    "N100_seeds_corr = [1, 2, 3, 4, 5]\n",
    "N200_seeds_corr = [6, 7, 8, 9, 10]\n",
    "N500_seeds_corr = [11, 12, 13, 14, 15]\n",
    "all_seeds_corr = [N100_seeds_corr, N200_seeds_corr, N500_seeds_corr]\n",
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "def get_N_sigma_correlated(seed, all_seeds):\n",
    "    if seed in all_seeds[0]:\n",
    "        N=100\n",
    "    elif seed in all_seeds[1]:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma\n",
    "dataset_keys_corr = []\n",
    "for seed in seeds:\n",
    "    N, sigma = get_N_sigma_correlated(seed, all_seeds_corr)\n",
    "    dataset_keys_corr.append(f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}')\n",
    "\n",
    "# Precompute masks once\n",
    "masks_tanh_corr = build_masks_cache_for_all(tanh_fits_corr, dataset_keys_corr, model_names_tanh_corr, sparsity_levels, prune_W2=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_tanh_corr = {}\n",
    "for q in sparsity_levels:\n",
    "    df_post_tanh_corr[q] = compute_sparse_metrics_results_globalmask_large_eval(\n",
    "        seeds=seeds,\n",
    "        all_seeds=all_seeds_corr,\n",
    "        models=model_names_tanh_corr,\n",
    "        all_fits=tanh_fits_corr,\n",
    "        get_N_sigma=get_N_sigma_correlated,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        folder=\"friedman_correlated\",\n",
    "        sparsity=q,\n",
    "        masks_cache=masks_tanh_corr,\n",
    "        prune_W2=False,\n",
    "        compute_nll=True,\n",
    "        noise_var_name=\"sigma\",\n",
    "        n_eval=500,\n",
    "        D=10,\n",
    "        standardize_y=True,\n",
    "        gen_uncorr=generate_Friedman_data_v2,\n",
    "        gen_corr=generate_correlated_Friedman_data_v2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_keys_nodewise_corr = []\n",
    "for seed in seeds:\n",
    "    N, sigma = get_N_sigma_correlated(seed, all_seeds_corr)\n",
    "    dataset_keys_nodewise_corr.append(f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}')\n",
    "\n",
    "# Precompute masks once\n",
    "masks_tanh_nodewise_corr = build_masks_cache_for_all(tanh_fits_nodewise_corr, dataset_keys_nodewise_corr, model_names_tanh_nodewise_corr, sparsity_levels, prune_W2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_tanh_nodewise_corr = {}\n",
    "for q in sparsity_levels:\n",
    "    df_post_tanh_nodewise_corr[q] = compute_sparse_metrics_results_globalmask_large_eval(\n",
    "        seeds=seeds,\n",
    "        all_seeds=all_seeds_corr,\n",
    "        models=model_names_tanh_nodewise_corr,\n",
    "        all_fits=tanh_fits_nodewise_corr,\n",
    "        get_N_sigma=get_N_sigma_correlated,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        folder=\"friedman_correlated\",\n",
    "        sparsity=q,\n",
    "        masks_cache=masks_tanh_nodewise_corr,\n",
    "        prune_W2=False,\n",
    "        compute_nll=True,\n",
    "        noise_var_name=\"sigma\",\n",
    "        n_eval=500,\n",
    "        D=10,\n",
    "        standardize_y=True,\n",
    "        gen_uncorr=generate_Friedman_data_v2,\n",
    "        gen_corr=generate_correlated_Friedman_data_v2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_tanh_full_corr = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_post_tanh_corr.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_post_tanh_full_nodewise_corr = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_post_tanh_nodewise_corr.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_all = pd.concat([df_post_tanh_full_corr, df_post_tanh_full_nodewise_corr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# USER SETTINGS\n",
    "# -----------------------------\n",
    "colors = {\n",
    "    \"Gaussian tanh\": \"C0\",\n",
    "    \"Regularized Horseshoe tanh\": \"C1\",\n",
    "    \"Dirichlet Horseshoe tanh\": \"C2\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"C2\",\n",
    "    \"Dirichlet Student T tanh\": \"C3\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"C3\",\n",
    "    \"Beta Horseshoe tanh\": \"C4\",\n",
    "    \"Beta Horseshoe tanh nodewise\": \"C4\",\n",
    "    \"Beta Student T tanh\": \"C5\",\n",
    "    \"Beta Student T tanh nodewise\": \"C5\",\n",
    "}\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]  # , 0.9\n",
    "rmse_col = \"posterior_mean_rmse_orig\"\n",
    "\n",
    "Ns = [100, 200, 500]\n",
    "\n",
    "# Provide your dataframes here\n",
    "dfs = [(df_all, \"Uncorrelated\")]#, (df_all_corr, \"Correlated\")]\n",
    "\n",
    "# Models to skip (same logic as your example; edit freely)\n",
    "SKIP_MODELS = {\n",
    "    \"Gaussian tanh\",\n",
    "    \"Dirichlet Student T tanh\",\n",
    "    \"Dirichlet Student T tanh nodewise\",\n",
    "    \"Beta Student T tanh\",\n",
    "    \"Beta Student T tanh nodewise\",\n",
    "}\n",
    "\n",
    "# Aggregation across seeds:\n",
    "AGG_FUN = \"mean\"   # \"mean\" or \"median\"\n",
    "# CI / band options (optional)\n",
    "SHOW_BAND = True   # <- set to False to turn off uncertainty bands\n",
    "BAND_KIND = \"t_ci\" # \"t_ci\" (mean CI via t) or \"quantile\" (e.g., 0.25-0.75)\n",
    "CI_LEVEL = 0.80    # used only for t_ci (e.g., 0.80 or 0.95)\n",
    "Q_LO, Q_HI = 0.25, 0.75  # used only for quantile band\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# HELPERS\n",
    "# -----------------------------\n",
    "def _t_crit_approx(df: int, ci_level: float) -> float:\n",
    "    \"\"\"\n",
    "    Approximate t critical value without SciPy.\n",
    "    Uses a small lookup table + normal approx fallback.\n",
    "    Good enough for plotting bands.\n",
    "    \"\"\"\n",
    "    # two-sided alpha\n",
    "    alpha = 1.0 - ci_level\n",
    "\n",
    "    # Common t critical values for two-sided intervals:\n",
    "    # values correspond to ci_level in {0.80, 0.90, 0.95, 0.99} approximately\n",
    "    table = {\n",
    "        0.80: {1: 3.078, 2: 1.886, 3: 1.638, 4: 1.533, 5: 1.476, 6: 1.440, 7: 1.415, 8: 1.397, 9: 1.383,\n",
    "               10: 1.372, 15: 1.341, 20: 1.325, 30: 1.310, 60: 1.296},\n",
    "        0.90: {1: 6.314, 2: 2.920, 3: 2.353, 4: 2.132, 5: 2.015, 6: 1.943, 7: 1.895, 8: 1.860, 9: 1.833,\n",
    "               10: 1.812, 15: 1.753, 20: 1.725, 30: 1.697, 60: 1.671},\n",
    "        0.95: {1: 12.706, 2: 4.303, 3: 3.182, 4: 2.776, 5: 2.571, 6: 2.447, 7: 2.365, 8: 2.306, 9: 2.262,\n",
    "               10: 2.228, 15: 2.131, 20: 2.086, 30: 2.042, 60: 2.000},\n",
    "        0.99: {1: 63.657, 2: 9.925, 3: 5.841, 4: 4.604, 5: 4.032, 6: 3.707, 7: 3.499, 8: 3.355, 9: 3.250,\n",
    "               10: 3.169, 15: 2.947, 20: 2.845, 30: 2.750, 60: 2.660},\n",
    "    }\n",
    "\n",
    "    # Snap ci_level to the nearest supported if close\n",
    "    supported = np.array(sorted(table.keys()))\n",
    "    nearest = supported[np.argmin(np.abs(supported - ci_level))]\n",
    "    if abs(nearest - ci_level) < 1e-9:\n",
    "        ci_level_use = float(nearest)\n",
    "        row = table[ci_level_use]\n",
    "        # pick nearest df key in that row\n",
    "        df_keys = np.array(sorted(row.keys()))\n",
    "        df_key = int(df_keys[np.argmin(np.abs(df_keys - df))])\n",
    "        return float(row[df_key])\n",
    "\n",
    "    # Fallback: normal approx for large df (very close to t anyway)\n",
    "    # For common ci_levels this is fine; for unusual ci_level, keep it simple.\n",
    "    # Two-sided z approx using an inverse-erf approximation would be overkill here;\n",
    "    # use a small mapping around typical levels:\n",
    "    z_map = {0.80: 1.282, 0.90: 1.645, 0.95: 1.960, 0.99: 2.576}\n",
    "    ci_level_use = float(supported[np.argmin(np.abs(supported - ci_level))])\n",
    "    return float(z_map[ci_level_use])\n",
    "\n",
    "\n",
    "def aggregate_over_seeds(df: pd.DataFrame, metric_col: str, agg_fun: str = \"mean\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate metric_col across seeds within each (N, sparsity, model, sigma).\n",
    "    Returns a dataframe with columns:\n",
    "      N, sparsity, model, sigma, n_seeds, center, lo, hi, sd, se\n",
    "    where lo/hi depend on SHOW_BAND + BAND_KIND.\n",
    "    \"\"\"\n",
    "    group_cols = [\"N\", \"sparsity\", \"model\"]\n",
    "    if \"sigma\" in df.columns:\n",
    "        group_cols = [\"sigma\"] + group_cols\n",
    "\n",
    "    g = df.groupby(group_cols)[metric_col]\n",
    "\n",
    "    out = g.agg(\n",
    "        n_seeds=\"count\",\n",
    "        mean=\"mean\",\n",
    "        median=\"median\",\n",
    "        sd=\"std\",\n",
    "    ).reset_index()\n",
    "\n",
    "    out[\"sd\"] = out[\"sd\"].fillna(0.0)\n",
    "    out[\"se\"] = np.where(out[\"n_seeds\"] > 0, out[\"sd\"] / np.sqrt(out[\"n_seeds\"]), np.nan)\n",
    "\n",
    "    if agg_fun == \"median\":\n",
    "        out[\"center\"] = out[\"median\"]\n",
    "    else:\n",
    "        out[\"center\"] = out[\"mean\"]\n",
    "\n",
    "    # Bands\n",
    "    out[\"lo\"] = np.nan\n",
    "    out[\"hi\"] = np.nan\n",
    "\n",
    "    if SHOW_BAND:\n",
    "        if BAND_KIND == \"quantile\":\n",
    "            q = df.groupby(group_cols)[metric_col].quantile([Q_LO, Q_HI]).unstack(-1).reset_index()\n",
    "            q = q.rename(columns={Q_LO: \"lo\", Q_HI: \"hi\"})\n",
    "            out = out.merge(q[group_cols + [\"lo\", \"hi\"]], on=group_cols, how=\"left\", suffixes=(\"\", \"_q\"))\n",
    "            out[\"lo\"] = out[\"lo_q\"]\n",
    "            out[\"hi\"] = out[\"hi_q\"]\n",
    "            out = out.drop(columns=[\"lo_q\", \"hi_q\"])\n",
    "        elif BAND_KIND == \"t_ci\":\n",
    "            # t-based CI for the mean: center=mean, lo/hi = mean ± t * se\n",
    "            # if using median as center, still show CI around mean is weird -> use mean for CI in that case\n",
    "            # so: always compute CI around mean\n",
    "            def row_ci(row):\n",
    "                n = int(row[\"n_seeds\"])\n",
    "                if n <= 1:\n",
    "                    return (np.nan, np.nan)\n",
    "                tcrit = _t_crit_approx(df=n - 1, ci_level=CI_LEVEL)\n",
    "                lo = row[\"mean\"] - tcrit * row[\"se\"]\n",
    "                hi = row[\"mean\"] + tcrit * row[\"se\"]\n",
    "                return (lo, hi)\n",
    "\n",
    "            lohi = out.apply(row_ci, axis=1, result_type=\"expand\")\n",
    "            out[\"lo\"] = lohi[0]\n",
    "            out[\"hi\"] = lohi[1]\n",
    "        else:\n",
    "            raise ValueError(\"BAND_KIND must be 't_ci' or 'quantile'\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# PLOT\n",
    "# -----------------------------\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 7), sharex=True, sharey=True)\n",
    "\n",
    "for r, (df_src, row_title) in enumerate(dfs):\n",
    "    # Filter upfront\n",
    "    df_src = df_src.copy()\n",
    "    df_src = df_src[df_src[\"sparsity\"].isin(sparsity_levels)]\n",
    "    df_src[\"sparsity\"] = pd.Categorical(df_src[\"sparsity\"], categories=sparsity_levels, ordered=True)\n",
    "\n",
    "    # Aggregate across seeds\n",
    "    df_agg = aggregate_over_seeds(df_src, metric_col=rmse_col, agg_fun=AGG_FUN)\n",
    "\n",
    "    for c, N in enumerate(Ns):\n",
    "        ax = axes[r, c]\n",
    "        dN = df_agg[df_agg[\"N\"] == N].copy()\n",
    "        dN[\"sparsity\"] = pd.Categorical(dN[\"sparsity\"], categories=sparsity_levels, ordered=True)\n",
    "\n",
    "        for model, g in dN.groupby(\"model\", sort=False):\n",
    "            if model in SKIP_MODELS:\n",
    "                continue\n",
    "            if model not in colors:\n",
    "                # fallback if a new model label appears\n",
    "                colors[model] = \"C7\"\n",
    "\n",
    "            g = g.sort_values(\"sparsity\")\n",
    "\n",
    "            lw, alpha, ls, mk = (1.3, 0.7, \"--\", \"v\") if \"nodewise\" in model else (2.2, 1.0, \"-\", \"o\")\n",
    "\n",
    "            # main line\n",
    "            ax.plot(\n",
    "                g[\"sparsity\"].astype(float),\n",
    "                g[\"center\"],\n",
    "                lw=lw,\n",
    "                alpha=alpha,\n",
    "                ls=ls,\n",
    "                marker=mk,\n",
    "                color=colors[model],\n",
    "                label=model,\n",
    "            )\n",
    "\n",
    "            # optional band (only where we have lo/hi)\n",
    "            if SHOW_BAND:\n",
    "                m = g[\"lo\"].notna() & g[\"hi\"].notna()\n",
    "                if m.any():\n",
    "                    ax.fill_between(\n",
    "                        g.loc[m, \"sparsity\"].astype(float),\n",
    "                        g.loc[m, \"lo\"],\n",
    "                        g.loc[m, \"hi\"],\n",
    "                        alpha=0.15 if \"nodewise\" not in model else 0.10,\n",
    "                        color=colors[model],\n",
    "                        linewidth=0,\n",
    "                    )\n",
    "\n",
    "        ax.set_title(f\"N={N}\" if r == 0 else \"\")\n",
    "        ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "        ax.set_xticks(sparsity_levels)\n",
    "\n",
    "        if c == 0:\n",
    "            ax.set_ylabel(f\"{row_title}\\nRMSE\")\n",
    "        if r == 1:\n",
    "            ax.set_xlabel(\"sparsity\")\n",
    "\n",
    "# --- one legend for the whole figure (deduped) ---\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "\n",
    "# Put legends where you like (your original had two legends)\n",
    "axes[1, 1].legend(\n",
    "    by_label.values(),\n",
    "    by_label.keys(),\n",
    "    loc=\"upper left\",\n",
    "    frameon=False,\n",
    ")\n",
    "axes[0, 1].legend(\n",
    "    by_label.values(),\n",
    "    by_label.keys(),\n",
    "    loc=\"upper left\",\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "# Optional: annotate what the band means\n",
    "if SHOW_BAND:\n",
    "    if BAND_KIND == \"t_ci\":\n",
    "        band_txt = f\"{int(CI_LEVEL*100)}% t-CI (over seeds)\"\n",
    "    else:\n",
    "        band_txt = f\"quantile band [{Q_LO:.2f}, {Q_HI:.2f}] (over seeds)\"\n",
    "    fig.suptitle(f\"Aggregation: {AGG_FUN} over seeds; Band: {band_txt}\", y=1.02, fontsize=11)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_agg[df_agg['N']==100]\n",
    "test[test['sparsity']==0.0].sort_values(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# USER SETTINGS\n",
    "# -----------------------------\n",
    "colors = {\n",
    "    \"Gaussian tanh\": \"C0\",\n",
    "    \"Regularized Horseshoe tanh\": \"C1\",\n",
    "    \"Dirichlet Horseshoe tanh\": \"C2\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"C2\",\n",
    "    \"Dirichlet Student T tanh\": \"C3\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"C3\",\n",
    "    \"Beta Horseshoe tanh\": \"C4\",\n",
    "    \"Beta Horseshoe tanh nodewise\": \"C4\",\n",
    "    \"Beta Student T tanh\": \"C5\",\n",
    "    \"Beta Student T tanh nodewise\": \"C5\",\n",
    "}\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]  # , 0.9\n",
    "rmse_col = \"posterior_mean_rmse_orig\"\n",
    "\n",
    "Ns = [100, 200, 500]\n",
    "\n",
    "# Provide your dataframes here\n",
    "dfs = [(df_all, \"Uncorrelated\")]#, (df_all_corr, \"Correlated\")]\n",
    "\n",
    "# Models to skip (same logic as your example; edit freely)\n",
    "# Models to skip (same logic as your example; edit freely)\n",
    "SKIP_MODELS = {\n",
    "    \"Gaussian tanh\",\n",
    "    \"Dirichlet Horseshoe tanh\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\",\n",
    "    \"Beta Horseshoe tanh\",\n",
    "    \"Beta Horseshoe tanh nodewise\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# PLOT\n",
    "# -----------------------------\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 7), sharex=True, sharey=True)\n",
    "\n",
    "for r, (df_src, row_title) in enumerate(dfs):\n",
    "    # Filter upfront\n",
    "    df_src = df_src.copy()\n",
    "    df_src = df_src[df_src[\"sparsity\"].isin(sparsity_levels)]\n",
    "    df_src[\"sparsity\"] = pd.Categorical(df_src[\"sparsity\"], categories=sparsity_levels, ordered=True)\n",
    "\n",
    "    # Aggregate across seeds\n",
    "    df_agg = aggregate_over_seeds(df_src, metric_col=rmse_col, agg_fun=AGG_FUN)\n",
    "\n",
    "    for c, N in enumerate(Ns):\n",
    "        ax = axes[r, c]\n",
    "        dN = df_agg[df_agg[\"N\"] == N].copy()\n",
    "        dN[\"sparsity\"] = pd.Categorical(dN[\"sparsity\"], categories=sparsity_levels, ordered=True)\n",
    "\n",
    "        for model, g in dN.groupby(\"model\", sort=False):\n",
    "            if model in SKIP_MODELS:\n",
    "                continue\n",
    "            if model not in colors:\n",
    "                # fallback if a new model label appears\n",
    "                colors[model] = \"C7\"\n",
    "\n",
    "            g = g.sort_values(\"sparsity\")\n",
    "\n",
    "            lw, alpha, ls, mk = (1.3, 0.7, \"--\", \"v\") if \"nodewise\" in model else (2.2, 1.0, \"-\", \"o\")\n",
    "\n",
    "            # main line\n",
    "            ax.plot(\n",
    "                g[\"sparsity\"].astype(float),\n",
    "                g[\"center\"],\n",
    "                lw=lw,\n",
    "                alpha=alpha,\n",
    "                ls=ls,\n",
    "                marker=mk,\n",
    "                color=colors[model],\n",
    "                label=model,\n",
    "            )\n",
    "\n",
    "            # optional band (only where we have lo/hi)\n",
    "            if SHOW_BAND:\n",
    "                m = g[\"lo\"].notna() & g[\"hi\"].notna()\n",
    "                if m.any():\n",
    "                    ax.fill_between(\n",
    "                        g.loc[m, \"sparsity\"].astype(float),\n",
    "                        g.loc[m, \"lo\"],\n",
    "                        g.loc[m, \"hi\"],\n",
    "                        alpha=0.15 if \"nodewise\" not in model else 0.10,\n",
    "                        color=colors[model],\n",
    "                        linewidth=0,\n",
    "                    )\n",
    "\n",
    "        ax.set_title(f\"N={N}\" if r == 0 else \"\")\n",
    "        ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "        ax.set_xticks(sparsity_levels)\n",
    "\n",
    "        if c == 0:\n",
    "            ax.set_ylabel(f\"{row_title}\\nRMSE\")\n",
    "        if r == 1:\n",
    "            ax.set_xlabel(\"sparsity\")\n",
    "\n",
    "# --- one legend for the whole figure (deduped) ---\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "\n",
    "# Put legends where you like (your original had two legends)\n",
    "axes[1, 1].legend(\n",
    "    by_label.values(),\n",
    "    by_label.keys(),\n",
    "    loc=\"upper left\",\n",
    "    frameon=False,\n",
    ")\n",
    "axes[0, 1].legend(\n",
    "    by_label.values(),\n",
    "    by_label.keys(),\n",
    "    loc=\"upper left\",\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "# Optional: annotate what the band means\n",
    "if SHOW_BAND:\n",
    "    if BAND_KIND == \"t_ci\":\n",
    "        band_txt = f\"{int(CI_LEVEL*100)}% t-CI (over seeds)\"\n",
    "    else:\n",
    "        band_txt = f\"quantile band [{Q_LO:.2f}, {Q_HI:.2f}] (over seeds)\"\n",
    "    fig.suptitle(f\"Aggregation: {AGG_FUN} over seeds; Band: {band_txt}\", y=1.02, fontsize=11)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
