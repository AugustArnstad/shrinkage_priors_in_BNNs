{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman\"\n",
    "#results_dir_relu = \"results/regression/single_layer/relu/friedman/convergence\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman/convergence_extensive\"\n",
    "#model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "#model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\"]\n",
    "model_names_tanh = [\"Dirichlet Horseshoe tanh nodewise\"]\n",
    "\n",
    "#relu_fits = {}\n",
    "tanh_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    # relu_fit = get_model_fits(\n",
    "    #     config=full_config_path,\n",
    "    #     results_dir=results_dir_relu,\n",
    "    #     models=model_names_relu,\n",
    "    #     include_prior=False,\n",
    "    # )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    #relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman/many\"\n",
    "model_names_tanh = [\"Dirichlet Horseshoe tanh nodewise\"]\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    # relu_fit = get_model_fits(\n",
    "    #     config=full_config_path,\n",
    "    #     results_dir=results_dir_relu,\n",
    "    #     models=model_names_relu,\n",
    "    #     include_prior=False,\n",
    "    # )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    #relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "\n",
    "seeds = [2] #[1, 2, 11, 16]\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    if seed == 1:\n",
    "        N=100\n",
    "    elif seed == 2:\n",
    "        N=200\n",
    "    elif seed == 16:\n",
    "        N=50\n",
    "    else:\n",
    "        N=500\n",
    "    sigma = 1\n",
    "    return N, sigma\n",
    "\n",
    "def get_all_convergence_diagnostics(all_fits):\n",
    "    diagnostics = []\n",
    "    rhats_global = {}\n",
    "    for config_name, model_fits in all_fits.items():\n",
    "        rhats = {}\n",
    "        for model_name, fit in model_fits.items():\n",
    "            try:\n",
    "                idata = az.from_cmdstanpy(fit['posterior'])\n",
    "                y_pred = fit['posterior'].stan_variable('output_test')\n",
    "                \n",
    "                path = f'datasets/friedman/{config_name}.npz'\n",
    "                try:\n",
    "                    data = np.load(path)\n",
    "                    y_test = data[\"y_test\"]\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"[SKIP] File not found: {path}\")\n",
    "                    continue\n",
    "                \n",
    "                divergent = idata.sample_stats[\"diverging\"].values  # shape: (n_chains, n_draws)\n",
    "                divergent_flat = divergent.flatten()  # shape: (8000,)\n",
    "                divergences = np.sum(divergent_flat)\n",
    "                y_pred_no_div = y_pred[~divergent_flat]\n",
    "                S = y_pred.shape[0]\n",
    "                rmses = np.zeros(S)\n",
    "                rmses_no_div = np.zeros(S - np.sum(divergent_flat))\n",
    "                \n",
    "                for i in range(S):\n",
    "                   rmses[i] = np.sqrt(np.mean((y_pred[i].squeeze() - y_test.squeeze()) ** 2))\n",
    "                \n",
    "                for i in range(S - np.sum(divergent_flat)):\n",
    "                    rmses_no_div[i] = np.sqrt(np.mean((y_pred_no_div[i].squeeze() - y_test.squeeze()) ** 2))\n",
    "\n",
    "                summary = az.summary(idata, var_names=[\"output\"], round_to=3)\n",
    "                \n",
    "                rhat = summary[\"r_hat\"]\n",
    "                \n",
    "                rhats[model_name] = rhat\n",
    "\n",
    "                ess_bulk = summary[\"ess_bulk\"]\n",
    "                ess_tail = summary[\"ess_tail\"]\n",
    "                ess = summary[\"ess_tail\"]\n",
    "                \n",
    "                try:\n",
    "                    seed = int(config_name.split(\"_seed\")[-1])\n",
    "                    N, sigma = get_N_sigma(seed)\n",
    "                except:\n",
    "                    N, sigma = np.nan, np.nan\n",
    "\n",
    "                diagnostics.append({\n",
    "                    \"model\": model_name,\n",
    "                    \"max_rhat\": rhat.max(),\n",
    "                    \"median_rhat\": rhat.median(),\n",
    "                    \"prop_divergent\": divergences/S,\n",
    "                    #\"rmse\": np.mean(rmses, axis=0),\n",
    "                    #\"rmse_no_div\": np.mean(rmses_no_div, axis=0),\n",
    "                    \"median_ess_tail\": ess_tail.median()/S,\n",
    "                    \"median_ess_bulk\": ess_bulk.median()/S,\n",
    "                    \"N\": N,\n",
    "                    #\"sigma\": sigma,\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                diagnostics.append({\n",
    "                    #\"config\": config_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"max_rhat\": np.nan,\n",
    "                    \"median_rhat\": np.nan,\n",
    "                    \"p95_rhat\": np.nan,\n",
    "                    \"min_ess_bulk\": np.nan,\n",
    "                    \"min_ess_tail\": np.nan,\n",
    "                    #\"n_divergent\": np.nan,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        rhats_global[config_name] = rhats\n",
    "\n",
    "    return pd.DataFrame(diagnostics), rhats_global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relu_diagostic, rhats_relu = get_all_convergence_diagnostics(relu_fits)\n",
    "tanh_diagostic, rhats_tanh = get_all_convergence_diagnostics(tanh_fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu_grouped = relu_diagostic.assign(row_index=lambda df: df.index) \\\n",
    "#     .sort_values([\"model\", \"row_index\"]) \\\n",
    "#     .drop(columns=\"row_index\") \\\n",
    "#     .reset_index(drop=True)\n",
    "tanh_grouped = tanh_diagostic.assign(row_index=lambda df: df.index) \\\n",
    "    .sort_values([\"model\", \"row_index\"]) \\\n",
    "    .drop(columns=\"row_index\") \\\n",
    "    .reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(relu_grouped.to_latex(index=False))\n",
    "print((tanh_grouped.round(3)).to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVERGENCE vs ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True, sharey=True)\n",
    "\n",
    "# --- First panel: N=100 ---\n",
    "dataset_path = \"datasets/friedman/Friedman_N100_p10_sigma1.00_seed1.npz\"\n",
    "data = np.load(dataset_path)\n",
    "y_test = data[\"y_test\"]\n",
    "models_dict = tanh_fits['Friedman_N100_p10_sigma1.00_seed1']\n",
    "\n",
    "for model_name, model_dict in models_dict.items():\n",
    "    model = model_dict['posterior']\n",
    "    idata = az.from_cmdstanpy(model)\n",
    "    rhat = az.summary(idata, var_names=[\"output_test\"], round_to=3)[\"r_hat\"].values\n",
    "    y_pred = np.mean(model.stan_variable(\"output_test\"), axis=0).squeeze(-1)\n",
    "    rmse = np.sqrt((y_test - y_pred) ** 2)\n",
    "    axes[0].scatter(rhat, rmse, label=model_name, alpha=0.7)\n",
    "\n",
    "axes[0].set_title(\"N = 100\")\n",
    "axes[0].set_xlabel(r\"$\\hat{R}$\")\n",
    "axes[0].set_ylabel(\"RMSE per test point\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# --- Second panel: N=200 ---\n",
    "dataset_path = \"datasets/friedman/Friedman_N200_p10_sigma1.00_seed2.npz\"\n",
    "data = np.load(dataset_path)\n",
    "y_test = data[\"y_test\"]\n",
    "models_dict = tanh_fits['Friedman_N200_p10_sigma1.00_seed2']\n",
    "\n",
    "for model_name, model_dict in models_dict.items():\n",
    "    model = model_dict['posterior']\n",
    "    idata = az.from_cmdstanpy(model)\n",
    "    rhat = az.summary(idata, var_names=[\"output_test\"], round_to=3)[\"r_hat\"].values\n",
    "    y_pred = np.mean(model.stan_variable(\"output_test\"), axis=0).squeeze(-1)\n",
    "    rmse = np.sqrt((y_test - y_pred) ** 2)\n",
    "    axes[1].scatter(rhat, rmse, label=model_name, alpha=0.7)\n",
    "\n",
    "axes[1].set_title(\"N = 200\")\n",
    "axes[1].set_xlabel(r\"$\\hat{R}$\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Shared legend\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper center\", ncol=len(labels))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.88)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRACEPLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "gauss_fit = tanh_fits['Friedman_N200_p10_sigma1.00_seed2']['Gaussian tanh']['posterior']\n",
    "idata = az.from_cmdstanpy(gauss_fit)\n",
    "divergent = idata.sample_stats[\"diverging\"].values  # shape (n_chains, n_draws)\n",
    "print(\"Divergent gaussian transitions:\", np.sum(divergent))\n",
    "print(divergent.shape)\n",
    "# Plot trace for output[0,0] through output[4,0]\n",
    "\n",
    "indices = [1, 3, 4, 12]\n",
    "az.plot_trace(\n",
    "    idata,\n",
    "    var_names=[\"output\"],\n",
    "    coords={\"output_dim_0\": indices, \"output_dim_1\": [0]}\n",
    ")\n",
    "plt.savefig(\"figures_for_use_in_paper/trace_plots/Gauss.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs_fit = tanh_fits['Friedman_N200_p10_sigma1.00_seed2']['Regularized Horseshoe tanh']['posterior']\n",
    "idata = az.from_cmdstanpy(rhs_fit)\n",
    "divergent = idata.sample_stats[\"diverging\"].values  # shape (n_chains, n_draws)\n",
    "print(\"Divergent RHS transitions:\", np.sum(divergent))\n",
    "print(divergent.shape)\n",
    "indices = [1, 3, 4, 12]\n",
    "az.plot_trace(\n",
    "    idata,\n",
    "    var_names=[\"output\"],\n",
    "    coords={\"output_dim_0\": indices, \"output_dim_1\": [0]}\n",
    ")\n",
    "plt.savefig(\"figures_for_use_in_paper/trace_plots/RHS.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_fit = tanh_fits['Friedman_N200_p10_sigma1.00_seed2']['Dirichlet Horseshoe tanh nodewise']['posterior']\n",
    "idata = az.from_cmdstanpy(dhs_fit)\n",
    "divergent = idata.sample_stats[\"diverging\"].values  # shape (n_chains, n_draws)\n",
    "print(\"Divergent DHS transitions:\", np.sum(divergent))\n",
    "print(divergent.shape)\n",
    "indices = [1, 3, 4, 12]\n",
    "az.plot_trace(\n",
    "    idata,\n",
    "    var_names=[\"output\"],\n",
    "    coords={\"output_dim_0\": indices, \"output_dim_1\": [0]}\n",
    ")\n",
    "plt.savefig(\"figures_for_use_in_paper/trace_plots/DHS.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_fit = tanh_fits['Friedman_N200_p10_sigma1.00_seed2']['Dirichlet Student T tanh nodewise']['posterior']\n",
    "idata = az.from_cmdstanpy(dhs_fit)\n",
    "divergent = idata.sample_stats[\"diverging\"].values  # shape (n_chains, n_draws)\n",
    "print(\"Divergent DST transitions:\", np.sum(divergent))\n",
    "print(divergent.shape)\n",
    "indices = [1, 3, 4, 12]\n",
    "az.plot_trace(\n",
    "    idata,\n",
    "    var_names=[\"output\"],\n",
    "    coords={\"output_dim_0\": indices, \"output_dim_1\": [0]}\n",
    ")\n",
    "plt.savefig(\"figures_for_use_in_paper/trace_plots/DST.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RHAT PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian tanh\": \"Gauss\",\n",
    "    \"Regularized Horseshoe tanh\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"DHS\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"DST\",\n",
    "}\n",
    "\n",
    "def plot_output_rhats_tanh_overlay_by_N(\n",
    "    rhats_by_N,          # dict: { \"N=100\": rhats_dict, \"N=200\": rhats_dict, ... }\n",
    "    bins=60,\n",
    "    log_y=True,\n",
    "    figsize_per_plot=(3.6, 2.8),\n",
    "    clip_q=0.999,\n",
    "):\n",
    "    \"\"\"\n",
    "    One row with one panel per model. In each panel, overlay histograms for each N (different colors).\n",
    "    rhats_by_N[label] should be a dict {model_name -> Series_of_rhat}.\n",
    "    \"\"\"\n",
    "    # Determine model order from the first entry\n",
    "    first_key = next(iter(rhats_by_N))\n",
    "    models = list(rhats_by_N[first_key].keys())\n",
    "    ncols = len(models)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        1, ncols,\n",
    "        figsize=(figsize_per_plot[0] * ncols, figsize_per_plot[1]),\n",
    "        sharey=True\n",
    "    )\n",
    "    if ncols == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Use a common x-range per model across N so overlays are comparable\n",
    "    for ax, model in zip(axes, models):\n",
    "        # collect all samples across N for that model to set a shared range\n",
    "        all_x = []\n",
    "        for _, rhats_dict in rhats_by_N.items():\n",
    "            s = rhats_dict[model]\n",
    "            x = np.asarray(s.values, dtype=float)\n",
    "            x = x[np.isfinite(x)]\n",
    "            if x.size:\n",
    "                all_x.append(x)\n",
    "        all_x = np.concatenate(all_x) if all_x else np.array([1.0])\n",
    "\n",
    "        xmax = np.quantile(all_x, clip_q) if all_x.size else 1.1\n",
    "        xrng = (1.0, xmax if xmax > 1.0 else 1.1)\n",
    "\n",
    "        # overlay one histogram per N\n",
    "        for label, rhats_dict in rhats_by_N.items():\n",
    "            s = rhats_dict[model]\n",
    "            x = np.asarray(s.values, dtype=float)\n",
    "            x = x[np.isfinite(x)]\n",
    "            ax.hist(\n",
    "                x,\n",
    "                bins=bins,\n",
    "                range=xrng,\n",
    "                histtype=\"step\",   # outline so overlays remain readable\n",
    "                linewidth=3,\n",
    "                label=label,\n",
    "                alpha=0.5\n",
    "            )\n",
    "\n",
    "        if log_y:\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "        ax.set_xlabel(r\"$\\hat{R}$\", fontsize=15)\n",
    "        ax.set_title(abbr.get(model, model), fontsize=15)\n",
    "        ax.tick_params(axis=\"both\", labelsize=12)\n",
    "        ax.grid(True, which=\"both\", axis=\"y\", alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Frequency\", fontsize=15)\n",
    "\n",
    "    # one shared legend for the whole figure\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper right\", frameon=True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"figures_for_use_in_paper/Friedman_rhat.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    #return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhats_by_N = {\n",
    "    \"N=100\": rhats_tanh[\"Friedman_N100_p10_sigma1.00_seed1\"],\n",
    "    \"N=200\": rhats_tanh[\"Friedman_N200_p10_sigma1.00_seed2\"],\n",
    "    \"N=500\": rhats_tanh[\"Friedman_N500_p10_sigma1.00_seed11\"],\n",
    "}\n",
    "\n",
    "plot_output_rhats_tanh_overlay_by_N(rhats_by_N, bins=100, log_y=True)\n",
    "# plt.savefig(\"figures_for_use_in_paper/abalone_sparsity_with_beta.pdf\", bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 utils/run_all_regression_models.py --model dirichlet_horseshoe_tanh_nodewise --output_dir results/regression/single_layer/tanh/friedman/convergence_extensive --warmup 5000 --sample 2000 --N 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST OF BETTER CONVERGENCE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from utils.generate_data import sample_gaussian_copula_uniform\n",
    "\n",
    "def generate_Friedman_data_v2(N=100, D=10, sigma=1.0, test_size=0.2, seed=42, standardize_y=True, return_scale=True):\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.uniform(0, 1, size=(N, D))\n",
    "    x0, x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5.0 * x4\n",
    "    )\n",
    "    y = y_clean + np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    if not standardize_y:\n",
    "        return (X_train, X_test, y_train, y_test) if not return_scale else (X_train, X_test, y_train, y_test, 0.0, 1.0)\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() if y_train.std() > 0 else 1.0\n",
    "\n",
    "    y_train_s = (y_train - y_mean) / y_std\n",
    "    y_test_s = (y_test - y_mean) / y_std\n",
    "\n",
    "    if return_scale:\n",
    "        return X_train, X_test, y_train_s, y_test_s, y_mean, y_std\n",
    "    return X_train, X_test, y_train_s, y_test_s\n",
    "\n",
    "def generate_correlated_Friedman_data_v2(N=100, D=10, sigma=1.0, test_size=0.2, seed=42, standardize_y=True, return_scale=True):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data for Bayesian neural network experiments.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples.\n",
    "        D (int): Number of features.\n",
    "        sigma (float): Noise level.\n",
    "        test_size (float): Proportion for test split.\n",
    "        seed (int): Random seed.\n",
    "        standardize_y (bool): Whether to standardize the response variable.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test, y_mean, y_std) if standardize_y,\n",
    "               else (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    d = 10\n",
    "    S_custom = np.eye(d)\n",
    "    # Block 1 (vars 0..4): high Spearman, 0.7\n",
    "    for i in range(0, 3):\n",
    "        for j in range(i+1, 3):\n",
    "            S_custom[i, j] = S_custom[j, i] = 0.8\n",
    "    # Block 2 (vars 5..9): moderate Spearman, 0.4\n",
    "    for i in range(5, 10):\n",
    "        for j in range(i+1, 10):\n",
    "            S_custom[i, j] = S_custom[j, i] = -0.5\n",
    "    # Cross-block weaker, 0.15\n",
    "    for i in range(0, 5):\n",
    "        for j in range(5, 10):\n",
    "            S_custom[i, j] = S_custom[j, i] = 0.15\n",
    "    # A couple of bespoke pairs:\n",
    "    S_custom[0, 9] = S_custom[9, 0] = 0.4\n",
    "    S_custom[2, 7] = S_custom[7, 2] = 0.9  # very strong (will be projected if infeasible)\n",
    "    S_custom[3, 4] = S_custom[4, 3] = -0.9  # very strong (will be projected if infeasible)\n",
    "    S_custom[1, 6] = S_custom[6, 1] = -0.9  # very strong (will be projected if infeasible)\n",
    "\n",
    "    U, _ = sample_gaussian_copula_uniform(n=10000, S=S_custom, random_state=123)\n",
    "    #X = np.random.uniform(0, 1, size=(N, D))\n",
    "    if N != U.shape[0]:\n",
    "        idx = np.random.choice(U.shape[0], size=N, replace=False)\n",
    "        X = U[idx, :]\n",
    "    else:\n",
    "        X = U\n",
    "\n",
    "    x0, x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5.0 * x4\n",
    "    )\n",
    "\n",
    "    y = y_clean + np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    if not standardize_y:\n",
    "        return (X_train, X_test, y_train, y_test) if not return_scale else (X_train, X_test, y_train, y_test, 0.0, 1.0)\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() if y_train.std() > 0 else 1.0\n",
    "\n",
    "    y_train_s = (y_train - y_mean) / y_std\n",
    "    y_test_s = (y_test - y_mean) / y_std\n",
    "\n",
    "    if return_scale:\n",
    "        return X_train, X_test, y_train_s, y_test_s, y_mean, y_std\n",
    "    return X_train, X_test, y_train_s, y_test_s\n",
    "\n",
    "def make_large_eval_set(\n",
    "    generator_fn,\n",
    "    N_train,\n",
    "    D,\n",
    "    sigma,\n",
    "    seed,\n",
    "    n_eval=5000,\n",
    "    standardize_y=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns X_eval, y_eval (standardized if standardize_y=True), plus y_mean,y_std\n",
    "    defined from the training split.\n",
    "    \"\"\"\n",
    "    N_total = N_train + n_eval\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te, y_mean, y_std = generator_fn(\n",
    "        N=N_total, D=D, sigma=sigma, test_size=n_eval / N_total, seed=seed,\n",
    "        standardize_y=standardize_y, return_scale=True\n",
    "    )\n",
    "    # Now X_te has approx n_eval points (exact given test_size construction).\n",
    "    return X_te, np.asarray(y_te).squeeze(), y_mean, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(\n",
    "    seeds, all_seeds, models, all_fits, get_N_sigma, forward_pass,\n",
    "    folder,\n",
    "    n_eval=5000,\n",
    "    D=10,\n",
    "    standardize_y=True,\n",
    "    # pass the correct generator functions\n",
    "    gen_uncorr=None,\n",
    "    gen_corr=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate on a large generated test set instead of the stored tiny X_test/y_test.\n",
    "    Assumes model was trained on standardized y if standardize_y=True.\n",
    "    \"\"\"\n",
    "    assert gen_corr is not None #and gen_uncorr is not None, \"Pass both generator functions.\"\n",
    "\n",
    "    posterior_means = []\n",
    "\n",
    "    # choose generator based on folder name\n",
    "    def choose_gen(folder):\n",
    "        return gen_corr if \"friedman_correlated\" in folder else gen_uncorr\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed, all_seeds)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "\n",
    "        # Build large eval set consistent with training split standardization\n",
    "        gen_fn = choose_gen(folder)\n",
    "        X_test, y_test, y_mean, y_std = make_large_eval_set(\n",
    "            generator_fn=gen_fn,\n",
    "            N_train=N,\n",
    "            D=D,\n",
    "            sigma=sigma,\n",
    "            seed=seed,\n",
    "            n_eval=n_eval,\n",
    "            standardize_y=standardize_y\n",
    "        )\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                W2 = W2_samples[i]\n",
    "                y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i]).squeeze()\n",
    "                y_hats[i] = y_hat\n",
    "                \n",
    "            posterior_mean = y_hats.mean(axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test)**2))\n",
    "            \n",
    "            out_pm = {\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'n_eval': y_test.shape[0],\n",
    "                'posterior_mean_rmse': posterior_mean_rmse,\n",
    "                'posterior_mean_rmse_orig': posterior_mean_rmse * y_std,  # back to original y scale\n",
    "            }\n",
    "            \n",
    "            posterior_means.append(out_pm)\n",
    "            \n",
    "    return pd.DataFrame(posterior_means)\n",
    "\n",
    "def get_N_sigma(seed, all_seeds):\n",
    "    if seed in all_seeds[0]:\n",
    "        N=100\n",
    "    elif seed in all_seeds[1]:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_tanh\n",
    "N100_seeds = [1, 3, 4, 5, 6]\n",
    "N200_seeds = [2, 7, 8, 9, 10]\n",
    "N500_seeds = [11, 12, 13, 14, 15]\n",
    "all_seeds = [N100_seeds, N200_seeds, N500_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = compute_rmse(\n",
    "        seeds=[2, 7, 8, 9, 10],\n",
    "        all_seeds=all_seeds,\n",
    "        models=model_names_tanh,\n",
    "        all_fits=tanh_fits,\n",
    "        get_N_sigma=get_N_sigma,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        folder=\"friedman\",\n",
    "        n_eval=1000,\n",
    "        D=10,\n",
    "        standardize_y=True,\n",
    "        gen_uncorr=generate_Friedman_data_v2,\n",
    "        gen_corr=generate_correlated_Friedman_data_v2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
