{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "data_dir = f\"datasets/friedman\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/friedman\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]#, \"Pred CP\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]#, \"Pred CP tanh\"]\n",
    "\n",
    "\n",
    "relu_fits = {}\n",
    "tanh_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_key = f'Friedman_N{100}_p10_sigma{1:.2f}_seed{1}'\n",
    "path = f\"datasets/friedman/{dataset_key}.npz\"\n",
    "\n",
    "data = np.load(path)\n",
    "X = data[\"X_train\"]\n",
    "Y = data[\"y_train\"]#.squeeze()  # shape (N_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train = X          # shape (n_train, d)\n",
    "y_train = Y          # shape (n_train,)\n",
    "\n",
    "X_test = data[\"X_test\"]\n",
    "y_test = data[\"y_test\"]#.squeeze()  # shape (N_test,)\n",
    "\n",
    "# Posterior draws of f(x_train)\n",
    "# Shape: (T, n_train), same as your func_samples example\n",
    "fit = tanh_fits['Friedman_N100_p10_sigma1.00_seed1']['Dirichlet Horseshoe tanh']['posterior']\n",
    "f_train_samples = fit.stan_variable(\"output\").squeeze(-1)\n",
    "f_test_samples = fit.stan_variable(\"output_test\").squeeze(-1)\n",
    "\n",
    "# OPTIONAL: posterior draws of noise std dev, shape (T,)\n",
    "# If you don't have this, we make a rough estimate later.\n",
    "sigma_samples = fit.stan_variable(\"sigma\")    # or np.array([...]) of shape (T,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aoi_weights(y_new, f_new_samples, sigma_samples):\n",
    "    \"\"\"\n",
    "    AOI importance weights for a single candidate y_new and one test point.\n",
    "    \n",
    "    f_new_samples: shape (T,)\n",
    "    sigma_samples: shape (T,)\n",
    "    \n",
    "    Returns: weights shape (T,) summing to 1 (in log-space for stability).\n",
    "    \"\"\"\n",
    "    # log p(y_new | f_new_samples, sigma)\n",
    "    loglik_new = norm.logpdf(y_new, loc=f_new_samples, scale=sigma_samples)  # shape (T,)\n",
    "    log_w = loglik_new - logsumexp(loglik_new)\n",
    "    w = np.exp(log_w)\n",
    "    return w  # shape (T,)\n",
    "\n",
    "def predictive_density_augmented(y_train, f_train_samples, y_new, f_new_samples, sigma_samples):\n",
    "    \"\"\"\n",
    "    Compute conformity scores (predictive density) for:\n",
    "      - each training point i = 0..n-1\n",
    "      - the new point n (the candidate y_new)\n",
    "    \n",
    "    using AOI weights for the augmented dataset.\n",
    "    \n",
    "    Returns:\n",
    "      scores_train: shape (n,)\n",
    "      score_new: scalar\n",
    "    \"\"\"\n",
    "    T, n = f_train_samples.shape\n",
    "    \n",
    "    # AOI weights for this candidate y_new\n",
    "    w = compute_aoi_weights(y_new, f_new_samples, sigma_samples)  # shape (T,)\n",
    "    \n",
    "    # likelihood for all training points under each draw: p(y_i | f^{(t)}(x_i), sigma_t)\n",
    "    # shape (T, n)\n",
    "    train_lik = norm.pdf(\n",
    "        y_train[None, :], \n",
    "        loc=f_train_samples, \n",
    "        scale=sigma_samples[:, None]\n",
    "    )\n",
    "    \n",
    "    # predictive density under augmented posterior: sum_t w_t * lik_t\n",
    "    scores_train = np.sum(w[:, None] * train_lik, axis=0)  # shape (n,)\n",
    "    \n",
    "    # same for the new point (candidate y_new)\n",
    "    new_lik = norm.pdf(\n",
    "        y_new,\n",
    "        loc=f_new_samples,\n",
    "        scale=sigma_samples\n",
    "    )  # shape (T,)\n",
    "    score_new = np.sum(w * new_lik)\n",
    "    \n",
    "    return scores_train, score_new\n",
    "\n",
    "def cb_value_for_y_candidate(\n",
    "    y_new, \n",
    "    y_train, \n",
    "    f_train_samples, \n",
    "    f_new_samples, \n",
    "    sigma_samples\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the CB value π(y_new) for a single test point and candidate y_new.\n",
    "    \"\"\"\n",
    "    scores_train, score_new = predictive_density_augmented(\n",
    "        y_train, f_train_samples, y_new, f_new_samples, sigma_samples\n",
    "    )\n",
    "    \n",
    "    # combine scores: n training + 1 new\n",
    "    all_scores = np.concatenate([scores_train, np.array([score_new])])\n",
    "    \n",
    "    # rank-based p-value (ties are negligible in continuous case)\n",
    "    cb_val = np.mean(all_scores <= score_new)\n",
    "    return cb_val\n",
    "\n",
    "def cb_interval_for_one_point(\n",
    "    j,\n",
    "    y_train,\n",
    "    f_train_samples,\n",
    "    f_test_samples,\n",
    "    sigma_samples,\n",
    "    y_grid,\n",
    "    alpha=0.1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute a CB interval for test point j at level 1-alpha using a candidate grid y_grid.\n",
    "    \n",
    "    Returns:\n",
    "      cb_vals: shape (len(y_grid),) with π(y_grid[k])\n",
    "      interval_mask: boolean mask where π(y) > alpha\n",
    "      (y_min, y_max): endpoints of the CB interval on the grid, or (None, None) if empty\n",
    "    \"\"\"\n",
    "    T, n_train = f_train_samples.shape\n",
    "    _, n_test = f_test_samples.shape\n",
    "    \n",
    "    assert 0 <= j < n_test\n",
    "    \n",
    "    # function samples for this test point\n",
    "    f_new_samples = f_test_samples[:, j]  # shape (T,)\n",
    "    \n",
    "    cb_vals = np.zeros_like(y_grid, dtype=float)\n",
    "    \n",
    "    for k, y_new in enumerate(y_grid):\n",
    "        cb_vals[k] = cb_value_for_y_candidate(\n",
    "            y_new=y_new,\n",
    "            y_train=y_train,\n",
    "            f_train_samples=f_train_samples,\n",
    "            f_new_samples=f_new_samples,\n",
    "            sigma_samples=sigma_samples\n",
    "        )\n",
    "    \n",
    "    interval_mask = cb_vals > alpha\n",
    "    if not np.any(interval_mask):\n",
    "        return cb_vals, interval_mask, (None, None)\n",
    "    \n",
    "    # Take min and max y where π(y) > alpha\n",
    "    y_in = y_grid[interval_mask]\n",
    "    return cb_vals, interval_mask, (float(y_in.min()), float(y_in.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get sigma_samples\n",
    "#sigma_samples = get_sigma_samples(f_train_samples, y_train, sigma_samples)\n",
    "\n",
    "# 2. Define a y-grid (e.g. based on posterior predictive range)\n",
    "#    For each test point you might center it differently, but here is one global example:\n",
    "f_all = np.concatenate([f_train_samples, f_test_samples], axis=1)  # shape (T, n_train + n_test)\n",
    "f_mean_all = f_all.mean(axis=0)\n",
    "f_sd_all   = f_all.std(axis=0)\n",
    "\n",
    "y_min = f_mean_all.min() - 3 * f_sd_all.max()\n",
    "y_max = f_mean_all.max() + 3 * f_sd_all.max()\n",
    "y_grid = np.linspace(y_min, y_max, 200)  # 200 grid points\n",
    "\n",
    "alpha = 0.2  # e.g. 80% CB interval\n",
    "\n",
    "n_test = f_test_samples.shape[1]\n",
    "cb_intervals = []\n",
    "\n",
    "for j in range(n_test):\n",
    "    cb_vals_j, mask_j, (lo_j, hi_j) = cb_interval_for_one_point(\n",
    "        j=j,\n",
    "        y_train=y_train,\n",
    "        f_train_samples=f_train_samples,\n",
    "        f_test_samples=f_test_samples,\n",
    "        sigma_samples=sigma_samples,\n",
    "        y_grid=y_grid,\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    cb_intervals.append((lo_j, hi_j))\n",
    "\n",
    "cb_intervals = np.array(cb_intervals)  # shape (n_test, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = {}\n",
    "\n",
    "def cp_runner(fits, dataset_key, y_train):\n",
    "    intervals = {}\n",
    "    for fit in fits:\n",
    "        f_train_samples = tanh_fits[dataset_key][fit]['posterior'].stan_variable(\"output\").squeeze(-1)\n",
    "        f_test_samples = tanh_fits[dataset_key][fit]['posterior'].stan_variable(\"output_test\").squeeze(-1)\n",
    "        sigma_samples = tanh_fits[dataset_key][fit]['posterior'].stan_variable(\"sigma\")\n",
    "        f_all = np.concatenate([f_train_samples, f_test_samples], axis=1)  # shape (T, n_train + n_test)\n",
    "        f_mean_all = f_all.mean(axis=0)\n",
    "        f_sd_all   = f_all.std(axis=0)\n",
    "\n",
    "        y_min = f_mean_all.min() - 3 * f_sd_all.max()\n",
    "        y_max = f_mean_all.max() + 3 * f_sd_all.max()\n",
    "        y_grid = np.linspace(y_min, y_max, 200)  # 200 grid points\n",
    "\n",
    "        alpha = 0.2  # e.g. 80% CB interval\n",
    "\n",
    "        n_test = f_test_samples.shape[1]\n",
    "        cb_intervals = []\n",
    "\n",
    "        for j in range(n_test):\n",
    "            cb_vals_j, mask_j, (lo_j, hi_j) = cb_interval_for_one_point(\n",
    "                j=j,\n",
    "                y_train=y_train,\n",
    "                f_train_samples=f_train_samples,\n",
    "                f_test_samples=f_test_samples,\n",
    "                sigma_samples=sigma_samples,\n",
    "                y_grid=y_grid,\n",
    "                alpha=alpha,\n",
    "            )\n",
    "            cb_intervals.append((lo_j, hi_j))\n",
    "\n",
    "        intervals[fit] = np.array(cb_intervals)\n",
    "    return intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_key = f'Friedman_N{500}_p10_sigma{1:.2f}_seed{11}'\n",
    "path = f\"datasets/friedman/{dataset_key}.npz\"\n",
    "\n",
    "data = np.load(path)\n",
    "X_train = data[\"X_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "\n",
    "X_test = data[\"X_test\"]\n",
    "y_test = data[\"y_test\"]\n",
    "\n",
    "fits = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]#, \"Pred CP tanh\"]\n",
    "\n",
    "ints = cp_runner(fits, dataset_key=dataset_key, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# intervals: your dict from before\n",
    "# intervals = {\n",
    "#     \"Gaussian tanh\": ...,\n",
    "#     \"Regularized Horseshoe tanh\": ...\n",
    "# }\n",
    "\n",
    "models = list(ints.keys())\n",
    "n_obs = ints[models[0]].shape[0]\n",
    "x = np.arange(n_obs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# small horizontal offsets so intervals don't sit exactly on top of each other\n",
    "offsets = np.linspace(-0.1, 0.1, len(models))\n",
    "\n",
    "for off, model in zip(offsets, models):\n",
    "    intv = ints[model]           # shape (n_obs, 2)\n",
    "    lows = intv[:, 0]\n",
    "    highs = intv[:, 1]\n",
    "    mids = 0.5 * (lows + highs)\n",
    "    yerr = np.vstack([mids - lows, highs - mids])\n",
    "\n",
    "    ax.errorbar(\n",
    "        x + off,\n",
    "        mids,\n",
    "        yerr=yerr,\n",
    "        fmt=\"o\",\n",
    "        capsize=3,\n",
    "        label=model,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Observation index\")\n",
    "ax.set_ylabel(\"Response\")\n",
    "ax.set_title(\"Credible intervals per observation for each model\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
