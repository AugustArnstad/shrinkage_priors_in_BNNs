{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/correlated\"\n",
    "results_dir_tanh = \"results/correlated\"\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "full_config_path = \"correlated_N400_p6\"\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import generate_correlated_data\n",
    "X_train, X_test, y_train, y_test = generate_correlated_data(n=500, p=6, random_state=42)\n",
    "\n",
    "rmse_gauss = np.zeros(4000)\n",
    "rmse_rhs = np.zeros(4000)\n",
    "rmse_dhs = np.zeros(4000)\n",
    "rmse_dst = np.zeros(4000)\n",
    "\n",
    "for i in range(4000):\n",
    "    rmse_gauss[i] = np.sqrt(np.mean((y_test - (tanh_fit['Gaussian tanh']['posterior'].stan_variable(\"output_test\")[i].squeeze(-1)))**2))\n",
    "    rmse_rhs[i] = np.sqrt(np.mean((y_test - (tanh_fit['Regularized Horseshoe tanh']['posterior'].stan_variable(\"output_test\")[i].squeeze(-1)))**2))\n",
    "    rmse_dhs[i] = np.sqrt(np.mean((y_test - (tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"output_test\")[i].squeeze(-1)))**2))\n",
    "    rmse_dst[i] = np.sqrt(np.mean((y_test - (tanh_fit['Dirichlet Student T tanh']['posterior'].stan_variable(\"output_test\")[i].squeeze(-1)))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rmse_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from properscoring import crps_ensemble\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORTANT: this y_test must correspond to the same test set used to make `output_test` in Stan,\n",
    "# otherwise scores won’t be comparable.\n",
    "from utils.generate_data import generate_correlated_data\n",
    "X_train, X_test, y_train, y_test = generate_correlated_data(n=500, p=6, random_state=42)\n",
    "\n",
    "rows = []\n",
    "for model_name, model_entry in tanh_fit.items():\n",
    "    post = model_entry[\"posterior\"]\n",
    "\n",
    "    # (S, n_test)\n",
    "    y_samps = post.stan_variable(\"output_test\").squeeze(-1)\n",
    "\n",
    "    # Posterior-mean predictions and RMSE\n",
    "    y_mean = y_samps.mean(axis=0)                                   # (n_test,)\n",
    "    rmse_post_mean = float(np.sqrt(mean_squared_error(y_test, y_mean)))\n",
    "\n",
    "    # Per-draw RMSEs and their mean\n",
    "    per_draw_rmse = np.sqrt(((y_samps - y_test[None, :])**2).mean(axis=1))  # (S,)\n",
    "    rmse_draw_mean = float(per_draw_rmse.mean())\n",
    "\n",
    "    # CRPS across the ensemble (expects shape (n_test, S))\n",
    "    crps = float(np.mean(crps_ensemble(y_test, y_samps.T)))\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE_posterior_mean\": rmse_post_mean,\n",
    "        \"RMSE_mean_over_draws\": rmse_draw_mean,\n",
    "        \"CRPS\": crps,\n",
    "        \"n_draws\": y_samps.shape[0]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values(\"RMSE_posterior_mean\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.robust_utils import build_pytorch_model_from_stan_sample\n",
    "from utils.generate_data import generate_correlated_data\n",
    "import torch \n",
    "\n",
    "P, H=6, 16\n",
    "\n",
    "X_train, X_test, y_train, y_test = generate_correlated_data(n=500, p=6, random_state=42)\n",
    "\n",
    "gauss_model = build_pytorch_model_from_stan_sample(\n",
    "        tanh_fit['Gaussian tanh']['posterior'], sample_idx=0, input_dim=P, hidden_dim=H,\n",
    "        output_dim=1, task=\"regression\", activation=torch.tanh\n",
    ")\n",
    "\n",
    "rhs_model = build_pytorch_model_from_stan_sample(\n",
    "        tanh_fit['Regularized Horseshoe tanh']['posterior'], sample_idx=0, input_dim=P, hidden_dim=H,\n",
    "        output_dim=1, task=\"regression\", activation=torch.tanh\n",
    ")\n",
    "\n",
    "dhs_model = build_pytorch_model_from_stan_sample(\n",
    "        tanh_fit['Dirichlet Horseshoe tanh']['posterior'], sample_idx=0, input_dim=P, hidden_dim=H,\n",
    "        output_dim=1, task=\"regression\", activation=torch.tanh\n",
    ")\n",
    "\n",
    "dst_model = build_pytorch_model_from_stan_sample(\n",
    "        tanh_fit['Dirichlet Student T tanh']['posterior'], sample_idx=0, input_dim=P, hidden_dim=H,\n",
    "        output_dim=1, task=\"regression\", activation=torch.tanh\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_hidden_representation(model, X_np, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Returns T = activation(linear1(X)) as a NumPy array of shape (n, H).\n",
    "    Assumes model has .linear1 and the activation is applied in forward.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    X_t = torch.tensor(X_np, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        z1 = model.linear1(X_t)                 # pre-activation\n",
    "        # Try to infer activation from model.forward by checking output\n",
    "        # but we don't want to call forward since it mixes linear2; so:\n",
    "        # If your model's activation is torch.tanh (as you built it), do:\n",
    "        T_t = torch.tanh(z1)\n",
    "    return T_t.cpu().numpy()\n",
    "\n",
    "def quantile_bin_1d(x, n_bins=10):\n",
    "    \"\"\"\n",
    "    Return integer bin indices 0..B-1 using quantile edges.\n",
    "    B may be < n_bins if many ties; we handle that downstream.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x).ravel()\n",
    "    edges = np.unique(np.quantile(x, np.linspace(0, 1, n_bins+1)))\n",
    "    # if edges all equal => only one unique value\n",
    "    if edges.size <= 2:\n",
    "        # everything in a single bin\n",
    "        b = np.zeros_like(x, dtype=np.int64)\n",
    "        return b, edges\n",
    "    b = np.digitize(x, edges[1:-1], right=False)\n",
    "    # clip to ensure indices in [0, B-1]\n",
    "    b = np.clip(b, 0, edges.size - 2)\n",
    "    return b, edges\n",
    "\n",
    "def mi_from_bincounts(bx, by):\n",
    "    \"\"\"\n",
    "    bx, by: integer bin indices (n,)\n",
    "    Returns MI in nats. Robust to degenerate cases.\n",
    "    \"\"\"\n",
    "    bx = np.asarray(bx, dtype=np.int64)\n",
    "    by = np.asarray(by, dtype=np.int64)\n",
    "    n = bx.size\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    M = int(bx.max()) + 1\n",
    "    N = int(by.max()) + 1\n",
    "    if M < 2 or N < 2:\n",
    "        return 0.0  # one side collapsed -> no info in this projection\n",
    "\n",
    "    joint = np.zeros((M, N), dtype=np.float64)\n",
    "    np.add.at(joint, (bx, by), 1.0)\n",
    "    joint /= n\n",
    "\n",
    "    px = joint.sum(axis=1)   # (M,)\n",
    "    py = joint.sum(axis=0)   # (N,)\n",
    "\n",
    "    # Guard: if any zero-marginals, MI=0 for this projection (degenerate)\n",
    "    if not np.all(px > 0) or not np.all(py > 0):\n",
    "        return 0.0\n",
    "\n",
    "    mask = joint > 0\n",
    "\n",
    "    # Build full broadcasted matrices BEFORE masking\n",
    "    # (boolean indexing does not broadcast!)\n",
    "    log_joint = np.zeros_like(joint)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        log_joint[mask] = np.log(joint[mask])\n",
    "\n",
    "    # Full (M,N) logs of marginals\n",
    "    log_px_full = np.log(px)[:, None] + np.zeros((1, N))\n",
    "    log_py_full = np.log(py)[None, :] + np.zeros((M, 1))\n",
    "\n",
    "    mi = np.sum(joint[mask] * (log_joint[mask] - log_px_full[mask] - log_py_full[mask]))\n",
    "    return float(mi)\n",
    "\n",
    "def sliced_mi_binned(X, T, n_bins=10, n_projections=64, seed=0):\n",
    "    \"\"\"\n",
    "    Estimate I(X;T) via random 1D projections and binned MI, averaging over\n",
    "    valid projections (those that produce >=2 bins on both sides).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = np.asarray(X, float)\n",
    "    T = np.asarray(T, float)\n",
    "    n, dx = X.shape\n",
    "    _, dt = T.shape\n",
    "\n",
    "    U = rng.normal(size=(n_projections, dx))\n",
    "    U /= (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    V = rng.normal(size=(n_projections, dt))\n",
    "    V /= (np.linalg.norm(V, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    mis = []\n",
    "    skipped = 0\n",
    "    for r in range(n_projections):\n",
    "        x_proj = X @ U[r]\n",
    "        t_proj = T @ V[r]\n",
    "        # pre-check: do we get at least 2 unique quantile bins on both sides?\n",
    "        bx, ex = quantile_bin_1d(x_proj, n_bins)\n",
    "        by, ey = quantile_bin_1d(t_proj, n_bins)\n",
    "        if (ex.size <= 2) or (ey.size <= 2) or (np.unique(bx).size < 2) or (np.unique(by).size < 2):\n",
    "            skipped += 1\n",
    "            continue\n",
    "        mis.append(mi_from_bincounts(bx, by))\n",
    "\n",
    "    if len(mis) == 0:\n",
    "        # fall back to zero if everything degenerated (shouldn’t happen with real data)\n",
    "        return 0.0, 0.0, np.array([])\n",
    "\n",
    "    mis = np.array(mis, dtype=float)\n",
    "    return float(mis.mean()), float(mis.std()), mis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def make_projections(X, T, n_random=512, nx_pca=3, nt_top=6, seed=0):\n",
    "    \"\"\"\n",
    "    Build a fixed set of 1D projection pairs (u, v):\n",
    "      - PCA directions of X (nx_pca)\n",
    "      - Top-variance neurons of T (nt_top)\n",
    "      - Plus n_random random unit directions for each side\n",
    "    Returns U: (R, dx), V: (R, dt), where R = deterministic_pairs + n_random\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = np.asarray(X, float); T = np.asarray(T, float)\n",
    "    dx = X.shape[1]; dt = T.shape[1]\n",
    "\n",
    "    U_list, V_list = [], []\n",
    "\n",
    "    # Deterministic projections to hit informative directions\n",
    "    if nx_pca > 0:\n",
    "        X_pca = PCA(n_components=min(nx_pca, dx)).fit(X)\n",
    "        U_list.extend([X_pca.components_[i] for i in range(X_pca.n_components_)])\n",
    "    if nt_top > 0:\n",
    "        var_T = T.var(axis=0)\n",
    "        top_idx = np.argsort(var_T)[::-1][:min(nt_top, dt)]\n",
    "        V_list.extend([np.eye(dt)[j] for j in top_idx])\n",
    "\n",
    "    # Pair up deterministic lists (cross-pair min length)\n",
    "    R_det = min(len(U_list), len(V_list))\n",
    "    U_det = np.array(U_list[:R_det]) if R_det > 0 else np.zeros((0, dx))\n",
    "    V_det = np.array(V_list[:R_det]) if R_det > 0 else np.zeros((0, dt))\n",
    "\n",
    "    # Random projections\n",
    "    U_rand = rng.normal(size=(n_random, dx))\n",
    "    U_rand /= (np.linalg.norm(U_rand, axis=1, keepdims=True) + 1e-12)\n",
    "    V_rand = rng.normal(size=(n_random, dt))\n",
    "    V_rand /= (np.linalg.norm(V_rand, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    # Concatenate\n",
    "    U = np.vstack([U_det, U_rand])\n",
    "    V = np.vstack([V_det, V_rand])\n",
    "    R = min(U.shape[0], V.shape[0])\n",
    "    U = U[:R]; V = V[:R]\n",
    "    return U, V\n",
    "\n",
    "def sliced_mi_binned_with_projections(X, T, U, V, n_bins=7, trim=0.10):\n",
    "    \"\"\"\n",
    "    Compute sliced binned MI with a FIXED set of projection pairs (U, V).\n",
    "    Uses a trimmed mean across projections to reduce outlier influence.\n",
    "    Returns (mean_trimmed, std_across_projections, all_projection_MIs)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float); T = np.asarray(T, float)\n",
    "    mis = []\n",
    "    for u, v in zip(U, V):\n",
    "        x_proj = X @ u\n",
    "        t_proj = T @ v\n",
    "        bx, ex = quantile_bin_1d(x_proj, n_bins)\n",
    "        by, ey = quantile_bin_1d(t_proj, n_bins)\n",
    "        if (ex.size <= 2) or (ey.size <= 2) or (np.unique(bx).size < 2) or (np.unique(by).size < 2):\n",
    "            continue\n",
    "        mis.append(mi_from_bincounts(bx, by))\n",
    "    if len(mis) == 0:\n",
    "        return 0.0, 0.0, np.array([])\n",
    "    mis = np.asarray(mis, float)\n",
    "    # trimmed mean\n",
    "    if 0.0 < trim < 0.5:\n",
    "        lo = int(np.floor(trim * len(mis)))\n",
    "        hi = int(np.ceil((1 - trim) * len(mis)))\n",
    "        mis_sorted = np.sort(mis)\n",
    "        mean_trim = mis_sorted[lo:hi].mean()\n",
    "    else:\n",
    "        mean_trim = mis.mean()\n",
    "    return float(mean_trim), float(mis.std()), mis\n",
    "\n",
    "def bootstrap_sliced_mi(X, T, n_bins=7, n_random=512, nx_pca=3, nt_top=6,\n",
    "                        trim=0.10, B=200, seed=0):\n",
    "    \"\"\"\n",
    "    Bootstrap MI over rows with a fixed projection set:\n",
    "      1) Build fixed U,V from full (X,T)\n",
    "      2) For b=1..B: resample rows with replacement, recompute sliced MI (trimmed)\n",
    "    Returns dict with mean, se, ci, and raw bootstrap values.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    U, V = make_projections(X, T, n_random=n_random, nx_pca=nx_pca, nt_top=nt_top, seed=seed)\n",
    "\n",
    "    n = X.shape[0]\n",
    "    boot_vals = []\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, size=n)\n",
    "        m, _, _ = sliced_mi_binned_with_projections(X[idx], T[idx], U, V, n_bins=n_bins, trim=trim)\n",
    "        boot_vals.append(m)\n",
    "    boot_vals = np.asarray(boot_vals, float)\n",
    "\n",
    "    mean_est = float(boot_vals.mean())\n",
    "    se_est = float(boot_vals.std(ddof=1))\n",
    "    lo, hi = np.quantile(boot_vals, [0.025, 0.975])\n",
    "    return {\n",
    "        \"mean\": mean_est,\n",
    "        \"se\": se_est,\n",
    "        \"ci95\": (float(lo), float(hi)),\n",
    "        \"boot\": boot_vals,\n",
    "        \"U\": U, \"V\": V\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden representation T\n",
    "T_train_gauss = get_hidden_representation(gauss_model, X_train)\n",
    "T_train_rhs = get_hidden_representation(rhs_model, X_train)\n",
    "T_train_dhs = get_hidden_representation(dhs_model, X_train)\n",
    "T_train_dst = get_hidden_representation(dst_model, X_train)\n",
    "\n",
    "# Standardize columns (helps binning stability)\n",
    "X_std = (X_train - X_train.mean(axis=0)) / (X_train.std(axis=0) + 1e-12)\n",
    "T_std_gauss = (T_train_gauss - T_train_gauss.mean(axis=0)) / (T_train_gauss.std(axis=0) + 1e-12)\n",
    "T_std_rhs = (T_train_rhs - T_train_rhs.mean(axis=0)) / (T_train_rhs.std(axis=0) + 1e-12)\n",
    "T_std_dhs = (T_train_dhs - T_train_dhs.mean(axis=0)) / (T_train_dhs.std(axis=0) + 1e-12)\n",
    "T_std_dst = (T_train_dst - T_train_dst.mean(axis=0)) / (T_train_dst.std(axis=0) + 1e-12)\n",
    "\n",
    "out_gauss = bootstrap_sliced_mi(\n",
    "    X_std, T_std_gauss,\n",
    "    n_bins=7,          # fewer bins -> lower variance\n",
    "    n_random=1024,     # many projections -> stable\n",
    "    nx_pca=3, nt_top=6,# deterministic informative projections\n",
    "    trim=0.10,         # trimmed mean across projections\n",
    "    B=50,             # bootstrap reps\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "out_rhs = bootstrap_sliced_mi(\n",
    "    X_std, T_std_rhs,\n",
    "    n_bins=7,          # fewer bins -> lower variance\n",
    "    n_random=1024,     # many projections -> stable\n",
    "    nx_pca=3, nt_top=6,# deterministic informative projections\n",
    "    trim=0.10,         # trimmed mean across projections\n",
    "    B=50,             # bootstrap reps\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "out_dhs = bootstrap_sliced_mi(\n",
    "    X_std, T_std_dhs,\n",
    "    n_bins=7,          # fewer bins -> lower variance\n",
    "    n_random=1024,     # many projections -> stable\n",
    "    nx_pca=3, nt_top=6,# deterministic informative projections\n",
    "    trim=0.10,         # trimmed mean across projections\n",
    "    B=50,             # bootstrap reps\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "out_dst = bootstrap_sliced_mi(\n",
    "    X_std, T_std_dst,\n",
    "    n_bins=7,          # fewer bins -> lower variance\n",
    "    n_random=1024,     # many projections -> stable\n",
    "    nx_pca=3, nt_top=6,# deterministic informative projections\n",
    "    trim=0.10,         # trimmed mean across projections\n",
    "    B=50,             # bootstrap reps\n",
    "    seed=123\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from scipy.stats import norm\n",
    "\n",
    "def plot_axis_aligned_ellipse(x_samples, y_samples, n_std=2.0, ax=None,\n",
    "                              edgecolor='crimson', facecolor='none', alpha=0.8, lw=2, label=None):\n",
    "    \"\"\"\n",
    "    Draw an axis-aligned ellipse centered at (mean(x), mean(y)),\n",
    "    with semi-axes = n_std * (std_x, std_y).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x_samples).ravel()\n",
    "    y = np.asarray(y_samples).ravel()\n",
    "\n",
    "    x_mu, y_mu = x.mean(), y.mean()\n",
    "    x_std, y_std = x.std(ddof=1), y.std(ddof=1)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "    width  = 2 * n_std * x_std\n",
    "    height = 2 * n_std * y_std\n",
    "\n",
    "    ell = Ellipse((x_mu, y_mu), width=width, height=height, angle=0,\n",
    "                  edgecolor=edgecolor, facecolor=facecolor, lw=lw, alpha=alpha, label=label)\n",
    "    ax.add_patch(ell)\n",
    "    ax.scatter([x_mu], [y_mu], color=edgecolor, s=40)  # mark center\n",
    "\n",
    "    return ax\n",
    "\n",
    "# ---- Plot all in one figure ----\n",
    "fig, ax = plt.subplots(figsize=(7,6))\n",
    "\n",
    "plot_axis_aligned_ellipse(out_gauss['boot'], rmse_gauss, n_std=2.0, ax=ax,\n",
    "                          edgecolor='red', label='Gaussian')\n",
    "plot_axis_aligned_ellipse(out_rhs['boot'], rmse_rhs, n_std=2.0, ax=ax,\n",
    "                          edgecolor='blue', label='RHS')\n",
    "plot_axis_aligned_ellipse(out_dhs['boot'], rmse_dhs, n_std=2.0, ax=ax,\n",
    "                          edgecolor='green', label='DHS')\n",
    "plot_axis_aligned_ellipse(out_dst['boot'], rmse_dst, n_std=2.0, ax=ax,\n",
    "                          edgecolor='purple', label='DST')\n",
    "\n",
    "ax.set_xlabel(\"I(X;T)\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_title(\"Axis-Aligned 2σ Ellipses (Uncertainty Regions)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_toy_data(n=5000, sigma=0.5, seed=0, standardize=True):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X1 = rng.normal(size=n)\n",
    "    X2 = rng.normal(size=n)\n",
    "    X3 = rng.normal(size=n)  # pure noise feature, irrelevant\n",
    "\n",
    "    # Nonlinear signal in X1,X2 only\n",
    "    f = np.tanh(X1 + 0.8*X2 + 0.5*X1*X2) + 0.3*np.sin(0.7*X2 - 0.2*X1)\n",
    "    y = f + rng.normal(scale=sigma, size=n)\n",
    "\n",
    "    X = np.stack([X1, X2, X3], axis=1)\n",
    "\n",
    "    if standardize:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-12)\n",
    "        y = (y - y.mean()) / (y.std() + 1e-12)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = gen_toy_data(n=500, sigma=0.5, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_correlated_data_noisy(\n",
    "    n, p=None, p_init=6, random_state=42, test_size=0.2,\n",
    "    rho_strong=0.92, rho_weak=0.5, noise_scale=0.5,\n",
    "    n_noise=0, noise_rho=0.0,\n",
    "    standardize_features=True, standardize_target=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Nonlinear dataset with interactions and optional extra non-influential features.\n",
    "\n",
    "    Base structure (always present):\n",
    "      - 6 features total in core set:\n",
    "          X1..X4: strongly correlated block, main drivers of y (with interactions)\n",
    "          X5..X6: moderately correlated weak block, small effect on y\n",
    "      - y depends ONLY on X1..X6 (noise features have zero effect by construction)\n",
    "      - Optional noise features (X7..X_{6+n_noise}) can be added; they are pure distractors.\n",
    "\n",
    "    Correlation:\n",
    "      - Strong block uses a latent factor with corr ~ rho_strong\n",
    "      - Weak block uses a latent factor with corr ~ rho_weak\n",
    "      - Noise block can be independent (noise_rho=0) or share a latent factor (noise_rho>0)\n",
    "\n",
    "    Standardization:\n",
    "      - Features are standardized columnwise if standardize_features=True\n",
    "      - Target standardized if standardize_target=True\n",
    "\n",
    "    Args:\n",
    "      n: number of samples\n",
    "      p: (optional) total number of features to enforce; must equal 6 + n_noise if given\n",
    "      n_noise: number of additional noise features (default 0)\n",
    "      noise_rho: correlation level among noise features (0 => independent)\n",
    "      noise_scale: std of observation noise added to y\n",
    "      test_size, random_state: as in scikit-learn\n",
    "\n",
    "    Returns:\n",
    "      X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    total_p = p_init + int(n_noise)\n",
    "    if p is not None and p != total_p:\n",
    "        raise ValueError(f\"p must equal 6 + n_noise (= {total_p}), got p={p}.\")\n",
    "\n",
    "    # Latent factors for correlation structure\n",
    "    h1 = rng.normal(size=n)  # drives strong block X1..X4\n",
    "    h2 = rng.normal(size=n)  # drives weak block   X5..X6\n",
    "\n",
    "    def make_block(h, m, rho):\n",
    "        eps = rng.normal(size=(n, m))\n",
    "        return rho * h[:, None] + np.sqrt(max(1e-12, 1 - rho**2)) * eps\n",
    "\n",
    "    # Core features\n",
    "    X_strong = make_block(h1, 4, rho_strong)    # X1..X4\n",
    "    X_weak   = make_block(h2, 2, rho_weak)      # X5..X6\n",
    "\n",
    "    # Optional noise features (pure distractors)\n",
    "    if n_noise > 0:\n",
    "        if abs(noise_rho) < 1e-12:\n",
    "            X_noise = rng.normal(size=(n, n_noise))  # independent noise features\n",
    "        else:\n",
    "            h3 = rng.normal(size=n)\n",
    "            X_noise = make_block(h3, n_noise, noise_rho)\n",
    "        X = np.concatenate([X_strong, X_weak, X_noise], axis=1)\n",
    "    else:\n",
    "        X = np.concatenate([X_strong, X_weak], axis=1)\n",
    "\n",
    "    # Standardize features (before building y won't change y because y only uses first 6)\n",
    "    if standardize_features:\n",
    "        X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-12)\n",
    "\n",
    "    # Alias the first six for readability\n",
    "    X1, X2, X3, X4, X5, X6 = [X[:, j] for j in range(6)]\n",
    "\n",
    "    # Nonlinear target with interactions (noise features do not enter)\n",
    "    strong_lin = 0.6*X1 + 0.4*X2 - 0.2*X3 + 0.1*X4\n",
    "    strong_int = 1.2*(X1*X2) + 0.8*(X3*X4) - 0.6*(X1*X4)\n",
    "    weak_part  = 0.15*(0.7*X5 + 0.3*X6) + 0.12*(X5*X6)\n",
    "\n",
    "    g = np.tanh(strong_lin + strong_int) + 0.3*np.sin(0.5*X2 - 0.25*X3)\n",
    "    y = g + weak_part + rng.normal(scale=noise_scale, size=n)\n",
    "\n",
    "    if standardize_target:\n",
    "        y = (y - y.mean()) / (y.std() + 1e-12)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train_12, X_test_12, y_train_12, y_test_12 = generate_correlated_data_noisy(n=10000, p=2, p_init=2, random_state=42, n_noise=0)\n",
    "\n",
    "X_train_123, X_test_123, y_train_123, y_test_123 = generate_correlated_data_noisy(n=10000, p=3, p_init=2, random_state=42, n_noise=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def mi_ksg_xy(X, y, k=5, metric='chebyshev', seed=0):\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float).reshape(-1, 1)\n",
    "    n, d = X.shape\n",
    "\n",
    "    Z = np.concatenate([X, y], axis=1)\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, metric=metric)\n",
    "    nbrs.fit(Z)\n",
    "    distances, _ = nbrs.kneighbors(Z, return_distance=True)\n",
    "    eps = distances[:, k] - 1e-12\n",
    "    eps = np.maximum(eps, 1e-15)\n",
    "\n",
    "    nbrs_x = NearestNeighbors(metric=metric).fit(X)\n",
    "    nx = np.array([len(idx)-1 for idx in nbrs_x.radius_neighbors(X, radius=eps, return_distance=False)], float)\n",
    "\n",
    "    nbrs_y = NearestNeighbors(metric=metric).fit(y)\n",
    "    ny = np.array([len(idx)-1 for idx in nbrs_y.radius_neighbors(y, radius=eps, return_distance=False)], float)\n",
    "\n",
    "    mi = digamma(k) + digamma(n) - np.mean(digamma(nx+1) + digamma(ny+1))\n",
    "    return float(mi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X12 = X[:, :2]   # only the true signal\n",
    "# X123 = X         # includes noise X3\n",
    "\n",
    "mi_y_x12_k5  = mi_ksg_xy(X_train_12, y_train_12, k=5)\n",
    "mi_y_x12_k10 = mi_ksg_xy(X_train_12, y_train_12, k=10)\n",
    "mi_y_x123_k5 = mi_ksg_xy(X_train_123, y_train_123, k=5)\n",
    "mi_y_x123_k10 = mi_ksg_xy(X_train_123, y_train_123, k=10)\n",
    "\n",
    "print(\"KSG MI estimates (in nats):\")\n",
    "print(f\"I(Y; X1,X2)      k=5  = {mi_y_x12_k5:.4f}\")\n",
    "print(f\"I(Y; X1,X2)      k=10 = {mi_y_x12_k10:.4f}\")\n",
    "print(f\"I(Y; X1,X2,X3)   k=5  = {mi_y_x123_k5:.4f}\")\n",
    "print(f\"I(Y; X1,X2,X3)   k=10 = {mi_y_x123_k10:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
