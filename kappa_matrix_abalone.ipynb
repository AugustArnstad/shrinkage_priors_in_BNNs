{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/abalone\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/abalone\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/abalone\"\n",
    "#model_names_relu = [\"Dirichlet Student T\"]\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "full_config_path = \"abalone_N3341_p8\"\n",
    "# relu_fit = get_model_fits(\n",
    "#     config=full_config_path,\n",
    "#     results_dir=results_dir_relu,\n",
    "#     models=model_names_relu,\n",
    "#     include_prior=False,\n",
    "# )\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X, X_test, y, y_test = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "# Coerce everything to plain float64 NumPy arrays\n",
    "X      = np.asarray(X, dtype=float)\n",
    "X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "# y often comes as a (n,1) DataFrame/array — flatten to (n,)\n",
    "y      = np.asarray(y, dtype=float).reshape(-1)\n",
    "y_test = np.asarray(y_test, dtype=float).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import cholesky, solve\n",
    "from utils.kappa_matrix import shrinkage_matrix_stable\n",
    "\n",
    "def build_operators_from_PS(P, S):\n",
    "    \"\"\"\n",
    "    P, S: arrays of shape (S, d, d), SPD per sample.\n",
    "    Returns:\n",
    "      G        : P^{-1/2} S P^{-1/2}\n",
    "      shrink_PS: (P+S)^{-1} S\n",
    "      shrink_G : (I+G)^{-1} G\n",
    "    \"\"\"\n",
    "    S_, d, _ = P.shape\n",
    "    G         = np.empty_like(P, dtype=np.float64)\n",
    "    shrink_PS = np.empty_like(P, dtype=np.float64)\n",
    "    shrink_G  = np.empty_like(P, dtype=np.float64)\n",
    "\n",
    "    I = np.eye(d)\n",
    "\n",
    "    for s in range(S_):\n",
    "        Ps = P[s]; Ss = S[s]\n",
    "\n",
    "        # --- G = P^{-1/2} S P^{-1/2} via Cholesky (Ps = C C^T) -> C^{-T} S C^{-1}\n",
    "        C = cholesky(Ps)            # upper-triangular by NumPy convention\n",
    "        # temp = C^{-1}^T S\n",
    "        temp = solve(C.T, Ss)#, assume_a='sym')    # solves C^T X = S  -> X = C^{-T} S\n",
    "        Gs   = solve(C, temp.T)#, assume_a='sym').T  # solves C Y^T = temp^T -> Y = C^{-1} temp\n",
    "        G[s] = Gs\n",
    "\n",
    "        # # --- (P+S)^{-1} S\n",
    "        Rs = shrinkage_matrix_stable(Ps, Ss)\n",
    "        # A = Ps + Ss\n",
    "        # L = cholesky(A)\n",
    "        # # Solve A X = S  (two triangular solves)\n",
    "        # Y = solve(L, Ss)#, lower=False)           # L X = S  (NumPy returns upper L; set lower=False)\n",
    "        # X = solve(L.T, Y)#, lower=True)          # L^T X = Y\n",
    "        # shrink_PS[s] = X\n",
    "        shrink_PS[s] = np.eye(Ps.shape[0]) - Rs\n",
    "        \n",
    "\n",
    "        # --- (I+G)^{-1} G\n",
    "        B = I + Gs\n",
    "        LB = cholesky(B)\n",
    "        YB = solve(LB, Gs)#, lower=False)\n",
    "        XB = solve(LB.T, YB)#, lower=True)\n",
    "        shrink_G[s] = XB\n",
    "\n",
    "    return G, shrink_PS, shrink_G\n",
    "\n",
    "\n",
    "# Example usage after reloading a saved NPZ:\n",
    "dat = np.load(\"Abalone_matrices/Gaussian_PS.npz\")\n",
    "P_gauss, S_gauss = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_gauss, shrink_PS_gauss, shrink_G_gauss = build_operators_from_PS(P_gauss, S_gauss)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Regularized_Horseshoe_PS.npz\")\n",
    "P_RHS, S_RHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_RHS, shrink_PS_RHS, shrink_G_RHS = build_operators_from_PS(P_RHS, S_RHS)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_Horseshoe_PS.npz\")\n",
    "P_DHS, S_DHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DHS, shrink_PS_DHS, shrink_G_DHS = build_operators_from_PS(P_DHS, S_DHS)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_StudentT_PS.npz\")\n",
    "P_DST, S_DST = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DST, shrink_PS_DST, shrink_G_DST = build_operators_from_PS(P_DST, S_DST)\n",
    "\n",
    "\n",
    "# Example usage after reloading a saved NPZ:\n",
    "dat = np.load(\"Abalone_matrices/Gaussian_sparsity_90_PS.npz\")\n",
    "P_gauss_sparse, S_gauss_sparse = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_gauss_sparse, shrink_PS_gauss_sparse, shrink_G_gauss_sparse = build_operators_from_PS(P_gauss_sparse, S_gauss_sparse)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Regularized_Horseshoe_sparsity_90_PS.npz\")\n",
    "P_RHS_sparse, S_RHS_sparse = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_RHS_sparse, shrink_PS_RHS_sparse, shrink_G_RHS_sparse = build_operators_from_PS(P_RHS_sparse, S_RHS_sparse)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_Horseshoe_sparsity_90_PS.npz\")\n",
    "P_DHS_sparse, S_DHS_sparse = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DHS_sparse, shrink_PS_DHS_sparse, shrink_G_DHS = build_operators_from_PS(P_DHS_sparse, S_DHS_sparse)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_StudentT_sparsity_90_PS.npz\")\n",
    "P_DST_sparse, S_DST_sparse = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DST_sparse, shrink_PS_DST_sparse, shrink_G_DST_sparse = build_operators_from_PS(P_DST_sparse, S_DST_sparse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def idempotence_likeness(A, eps=1e-12):\n",
    "    likeness = np.linalg.norm(A @ A - A, 'fro') / (np.linalg.norm(A, 'fro') + eps)\n",
    "    return likeness\n",
    "\n",
    "def svd_effective_rank(A, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      s_sorted : singular values sorted descending\n",
    "      rho      : normalized singular weights (s / ||s||_1)\n",
    "      erank    : exp(Shannon entropy of rho)\n",
    "    \"\"\"\n",
    "    # singular values only\n",
    "    s = np.linalg.svd(A, compute_uv=False)\n",
    "    s_sorted = np.sort(s)[::-1]\n",
    "    s_sum = s_sorted.sum()\n",
    "    if s_sum <= eps:\n",
    "        # all-zero block: define uniform weights\n",
    "        rho = np.ones_like(s_sorted) / len(s_sorted)\n",
    "    else:\n",
    "        rho = s_sorted / s_sum\n",
    "    H = -np.sum(rho * np.log(rho + eps))\n",
    "    erank = float(np.exp(H))\n",
    "    return s_sorted, rho, erank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 4000\n",
    "blocks = 16\n",
    "p=8\n",
    "\n",
    "erank_gauss = np.zeros((S, blocks))\n",
    "erank_RHS = np.zeros((S, blocks))\n",
    "erank_DHS = np.zeros((S, blocks))\n",
    "erank_DST = np.zeros((S, blocks))\n",
    "\n",
    "idempotent_gauss = np.zeros((S, blocks))\n",
    "idempotent_RHS = np.zeros((S, blocks))\n",
    "idempotent_DHS = np.zeros((S, blocks))\n",
    "idempotent_DST = np.zeros((S, blocks))\n",
    "\n",
    "for b in range(blocks):\n",
    "    for i in range(S):\n",
    "        _, _, erank_gauss[i, b] = svd_effective_rank(shrink_PS_gauss[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        _, _, erank_RHS[i, b]   = svd_effective_rank(shrink_PS_RHS[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        _, _, erank_DHS[i, b]   = svd_effective_rank(shrink_PS_DHS[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        _, _, erank_DST[i, b]   = svd_effective_rank(shrink_PS_DST[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        \n",
    "        idempotent_gauss[i, b] = idempotence_likeness(shrink_PS_gauss[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        idempotent_RHS[i, b]   = idempotence_likeness(shrink_PS_RHS[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        idempotent_DHS[i, b]   = idempotence_likeness(shrink_PS_DHS[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        idempotent_DST[i, b]   = idempotence_likeness(shrink_PS_DST[i, (b*p):(p+b*p), (b*p):(p+b*p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0, 16), erank_gauss.mean(axis=0), label=\"Gauss\", marker='o')\n",
    "plt.plot(np.arange(0, 16), erank_RHS.mean(axis=0), label=\"RHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), erank_DHS.mean(axis=0), label=\"DHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), erank_DST.mean(axis=0), label=\"DST\", marker='o')\n",
    "plt.title(\"Effective rank of blocks\")\n",
    "plt.xlabel(\"Block\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erank_gauss.mean(axis=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 16), idempotent_gauss.mean(axis=0), label=\"Gauss\", marker='o')\n",
    "plt.plot(np.arange(0, 16), idempotent_RHS.mean(axis=0), label=\"RHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), idempotent_DHS.mean(axis=0), label=\"DHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), idempotent_DST.mean(axis=0), label=\"DST\", marker='o')\n",
    "plt.title(\"Idempotence error of blocks\")\n",
    "plt.xlabel(\"Block\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) BLOCK THE MATRICES\n",
    "# =========================\n",
    "def make_block_lists(shrink_3d, block_size=8):\n",
    "    \"\"\"\n",
    "    shrink_3d: array (M, D, D), M samples, D divisible by block_size.\n",
    "    Returns: list of length H = D//block_size.\n",
    "             Each element is a list of length M with (block_size x block_size) arrays.\n",
    "             blocks[h][m] = block h from sample m.\n",
    "    \"\"\"\n",
    "    M, D, _ = shrink_3d.shape\n",
    "    assert D % block_size == 0, \"D must be divisible by block_size\"\n",
    "    H = D // block_size\n",
    "    blocks = []\n",
    "    for h in range(H):\n",
    "        r = slice(h*block_size, (h+1)*block_size)\n",
    "        c = slice(h*block_size, (h+1)*block_size)\n",
    "        blocks.append([shrink_3d[m, r, c] for m in range(M)])\n",
    "    return blocks\n",
    "\n",
    "blocks_gauss = make_block_lists(shrink_PS_gauss, block_size=8)\n",
    "blocks_RHS   = make_block_lists(shrink_PS_RHS,   block_size=8)\n",
    "blocks_DHS   = make_block_lists(shrink_PS_DHS,   block_size=8)\n",
    "blocks_DST   = make_block_lists(shrink_PS_DST,   block_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 2) MEAN-REF SUBSPACE + WITHIN-PRIOR COSINES\n",
    "# ===========================================\n",
    "def topk_left_singular_vectors_meanref(blocks_for_one_diag_block, k=None, energy=0.90, symmetrize=True):\n",
    "    \"\"\"\n",
    "    blocks_for_one_diag_block: list of A_m (p x p) across M samples for a single diagonal block.\n",
    "    Returns (U_ref_k, s_ref_k) where U_ref_k is (p x k).\n",
    "    \"\"\"\n",
    "    Abar = np.mean(np.stack(blocks_for_one_diag_block, axis=0), axis=0)\n",
    "    if symmetrize:\n",
    "        Abar = 0.5 * (Abar + Abar.T)\n",
    "    U_ref, s_ref, _ = np.linalg.svd(Abar, full_matrices=False)\n",
    "\n",
    "    if k is None:\n",
    "        ssum = s_ref.sum()\n",
    "        if ssum <= 1e-12:\n",
    "            k = 1\n",
    "        else:\n",
    "            cume = np.cumsum(s_ref) / ssum\n",
    "            k = int(np.searchsorted(cume, energy) + 1)\n",
    "\n",
    "    return U_ref[:, :k], s_ref[:k]\n",
    "\n",
    "\n",
    "def within_prior_overlap_meanref(blocks_for_one_diag_block, k=None, energy=0.90, symmetrize=True):\n",
    "    \"\"\"\n",
    "    Cosines of principal angles between each sample's top-k subspace and\n",
    "    the reference subspace built from the MEAN block.\n",
    "    Returns: C with shape (M, K), M samples, K directions.\n",
    "    \"\"\"\n",
    "    U_ref, _ = topk_left_singular_vectors_meanref(blocks_for_one_diag_block, k=k, energy=energy, symmetrize=symmetrize)\n",
    "    K = U_ref.shape[1]\n",
    "    cosines = []\n",
    "    for A in blocks_for_one_diag_block:\n",
    "        if symmetrize:\n",
    "            A = 0.5 * (A + A.T)\n",
    "        U, _, _ = np.linalg.svd(A, full_matrices=False)\n",
    "        U_k = U[:, :K]\n",
    "        sig = np.linalg.svd(U_ref.T @ U_k, compute_uv=False)  # cosines of principal angles\n",
    "        cosines.append(np.sort(sig)[::-1])  # descending order\n",
    "    return np.array(cosines)  # (M, K)\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 3) PLOT: COMPARE FOUR MODELS FOR ONE BLOCK\n",
    "# ===========================================\n",
    "def plot_within_strip_four_models_for_block(\n",
    "    blocks_gauss, blocks_RHS, blocks_DHS, blocks_DST,\n",
    "    block_index=0, block_size=8, energy=0.90, symmetrize=True, k=None,\n",
    "    labels=('Gaussian', 'RHS', 'Dirichlet-HS', 'Dirichlet-Student-t')\n",
    "):\n",
    "    \"\"\"\n",
    "    Overlays sample-wise principal-angle cosines for ONE diagonal block across FOUR models.\n",
    "    Each model is jittered left/right per principal direction for visual separation.\n",
    "    \"\"\"\n",
    "    # Block the matrices\n",
    "\n",
    "    # Compute within-prior cosines (mean-ref) for this block\n",
    "    C_G  = within_prior_overlap_meanref(blocks_gauss[block_index], k=k, energy=energy, symmetrize=symmetrize)\n",
    "    C_R  = within_prior_overlap_meanref(blocks_RHS[block_index], k=k,   energy=energy, symmetrize=symmetrize)\n",
    "    C_DH = within_prior_overlap_meanref(blocks_DHS[block_index], k=k,   energy=energy, symmetrize=symmetrize)\n",
    "    C_DT = within_prior_overlap_meanref(blocks_DST[block_index], k=k,   energy=energy, symmetrize=symmetrize)\n",
    "\n",
    "    # Use a common number of directions across the four models\n",
    "    K = min(C_G.shape[1], C_R.shape[1], C_DH.shape[1], C_DT.shape[1])\n",
    "    C_G, C_R, C_DH, C_DT = C_G[:, :K], C_R[:, :K], C_DH[:, :K], C_DT[:, :K]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7,4))\n",
    "    jitter = 0.05\n",
    "    # Model offsets per direction\n",
    "    offsets = (-1.5*jitter, -0.5*jitter, +0.5*jitter, +1.5*jitter)\n",
    "    arrays  = (C_G, C_R, C_DH, C_DT)\n",
    "\n",
    "    model_colors = {\n",
    "        labels[0]: 'tab:blue',\n",
    "        labels[1]: 'tab:orange',\n",
    "        labels[2]: 'tab:green',\n",
    "        labels[3]: 'tab:red',\n",
    "    }\n",
    "\n",
    "    # replace the plotting loop with this:\n",
    "    for arr, off, lab in zip(arrays, offsets, labels):\n",
    "        M, K = arr.shape\n",
    "        col = model_colors[lab]          # <-- same color for all directions of this model\n",
    "        for k in range(K):\n",
    "            x = np.full(M, k+1) + off + 0.03*np.random.randn(M)\n",
    "            plt.plot(x, arr[:, k], '.', alpha=0.35, markersize=6,\n",
    "                    color=col, label=lab if k == 0 else None)\n",
    "\n",
    "    plt.axhline(0.9, lw=1, alpha=0.2)\n",
    "    plt.xticks(range(1, K+1))\n",
    "    plt.ylim(0, 1.02)\n",
    "    plt.xlabel('Principal direction index (top-k of mean-ref subspace)')\n",
    "    plt.ylabel('Cosine overlap to mean-ref')\n",
    "    plt.title(f'Within-prior principal-angle cosines — block {block_index}')\n",
    "    plt.legend(ncol=2, frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Example usage (block 0)\n",
    "# =========================\n",
    "plot_within_strip_four_models_for_block(\n",
    "    blocks_gauss=blocks_gauss, blocks_RHS=blocks_RHS, blocks_DHS=blocks_DHS, blocks_DST=blocks_DST,\n",
    "    block_index=6, block_size=8, energy=0.90, symmetrize=True, k=None,\n",
    "    labels=('Gaussian','RHS','Dirichlet-HS','Dirichlet-Student-t')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def make_block_lists(shrink_3d, block_size=8):\n",
    "    M, D, _ = shrink_3d.shape\n",
    "    H = D // block_size\n",
    "    return [[shrink_3d[m, h*block_size:(h+1)*block_size, h*block_size:(h+1)*block_size]\n",
    "             for m in range(M)] for h in range(H)]\n",
    "\n",
    "def top_r_mean_ref(blocks_one, r, symmetrize=True):\n",
    "    Abar = np.mean(np.stack(blocks_one, axis=0), axis=0)\n",
    "    if symmetrize: Abar = 0.5*(Abar + Abar.T)\n",
    "    U, s, _ = np.linalg.svd(Abar, full_matrices=False)\n",
    "    return U[:, :r], s[:r]\n",
    "\n",
    "def within_cosines_meanref(blocks_one, r, K=None, energy=0.90, symmetrize=True):\n",
    "    Uref, _ = top_r_mean_ref(blocks_one, r, symmetrize=symmetrize)\n",
    "    cosines = []\n",
    "    for A in blocks_one:\n",
    "        if symmetrize: A = 0.5*(A + A.T)\n",
    "        U, s, _ = np.linalg.svd(A, full_matrices=False)\n",
    "        if K is None:\n",
    "            cs = np.cumsum(s)/max(s.sum(), 1e-12)\n",
    "            K_eff = max(r, int(np.searchsorted(cs, energy) + 1))\n",
    "        else:\n",
    "            K_eff = max(r, K)\n",
    "        sig = np.linalg.svd(Uref.T @ U[:, :K_eff], compute_uv=False)\n",
    "        cosines.append(sig[:r])   # top r cosines\n",
    "    return np.array(cosines)      # (M, r)\n",
    "\n",
    "# ---------- compute ALL blocks -> heatmaps ----------\n",
    "def model_heatmap(shrink_3d, r=4, block_size=8, energy=0.90, symmetrize=True, agg='median'):\n",
    "    blocks = make_block_lists(shrink_3d, block_size)\n",
    "    stats = []\n",
    "    for b in range(len(blocks)):                      # per block\n",
    "        C = within_cosines_meanref(blocks[b], r=r, K=None, energy=energy, symmetrize=symmetrize)\n",
    "        stat = np.median(C, axis=0) if agg=='median' else np.mean(C, axis=0)\n",
    "        stats.append(stat)                            # (r,)\n",
    "    return np.array(stats)                            # (num_blocks, r)\n",
    "\n",
    "def plot_four_heatmaps(gauss, rhs, dhs, dst, r=4, titles=('Gaussian','RHS','Dirichlet-HS','Dirichlet-Student-t')):\n",
    "    mats = [\n",
    "        model_heatmap(gauss, r=r),\n",
    "        model_heatmap(rhs,   r=r),\n",
    "        model_heatmap(dhs,   r=r),\n",
    "        model_heatmap(dst,   r=r),\n",
    "    ]\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10,6), sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "    for ax, M, title in zip(axes, mats, titles):\n",
    "        im = ax.imshow(M, vmin=0, vmax=1, aspect='auto', cmap='viridis')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Principal direction (1..r)')\n",
    "        ax.set_ylabel('Block index')\n",
    "    fig.colorbar(im, ax=axes.tolist(), fraction=0.02, pad=0.02)\n",
    "    fig.suptitle('Within-prior cosine (to mean-ref) — median over samples', y=0.99)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------- usage ----------\n",
    "# r = number of directions to summarize (e.g., 3–5 is plenty)\n",
    "plot_four_heatmaps(shrink_PS_gauss, shrink_PS_RHS, shrink_PS_DHS, shrink_PS_DST, r=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shrinkage_feature_correlation(X, A):\n",
    "    \"\"\"\n",
    "    X: (n, p) feature matrix\n",
    "    A: (p, p) shrinkage matrix (single block)\n",
    "    \n",
    "    Returns scalar correlation (r) between |corr(X)| and |A|.\n",
    "    \"\"\"\n",
    "    # Feature correlation\n",
    "    corr_X = np.corrcoef(X, rowvar=False)  # shape (p, p)\n",
    "\n",
    "    # Take upper triangles (excluding diagonal)\n",
    "    iu = np.triu_indices_from(corr_X, k=1)\n",
    "    x_vals = np.abs(corr_X[iu])\n",
    "    a_vals = np.abs(A[iu])\n",
    "\n",
    "    # Pearson correlation between flattened patterns\n",
    "    r = np.corrcoef(x_vals, a_vals)[0, 1]\n",
    "    return r, corr_X\n",
    "\n",
    "means_gauss = np.zeros(len(blocks_RHS))\n",
    "means_RHS = np.zeros(len(blocks_DHS))\n",
    "means_DHS = np.zeros(len(blocks_RHS))\n",
    "means_DST = np.zeros(len(blocks_DHS))\n",
    "\n",
    "for b in range(len(blocks_RHS)):\n",
    "    means_gauss[b] = np.mean([shrinkage_feature_correlation(X, A)[0] for A in blocks_gauss[b]])\n",
    "    means_RHS[b] = np.mean([shrinkage_feature_correlation(X, A)[0] for A in blocks_RHS[b]])\n",
    "    means_DHS[b] = np.mean([shrinkage_feature_correlation(X, A)[0] for A in blocks_DHS[b]])\n",
    "    means_DST[b] = np.mean([shrinkage_feature_correlation(X, A)[0] for A in blocks_DST[b]])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0, 16), means_gauss, label=\"Gauss\", marker='o')\n",
    "plt.plot(np.arange(0, 16), means_RHS, label=\"RHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), means_DHS, label=\"DHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), means_DST, label=\"DST\", marker='o')\n",
    "plt.title(\"Mean alignment of blockwise shrinkage to correlation of features\")\n",
    "plt.xlabel(\"Block\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobenius_correlation(A, B):\n",
    "    A_c = A - A.mean()\n",
    "    B_c = B - B.mean()\n",
    "    return np.sum(A_c * B_c) / (np.linalg.norm(A_c, 'fro') * np.linalg.norm(B_c, 'fro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = 16\n",
    "p=8\n",
    "S = 4000\n",
    "rho_F_gauss = np.zeros((S, blocks))\n",
    "rho_F_RHS = np.zeros((S, blocks))\n",
    "rho_F_DHS = np.zeros((S, blocks))\n",
    "rho_F_DST = np.zeros((S, blocks))\n",
    "corr_X = np.corrcoef(X, rowvar=False)\n",
    "\n",
    "for b in range(blocks):\n",
    "    for i in range (S):\n",
    "        rho_F_gauss[i][b] = frobenius_correlation(shrink_PS_gauss[i, (b*p):(p+b*p), (b*p):(p+b*p)], corr_X)\n",
    "        rho_F_RHS[i][b] = frobenius_correlation(shrink_PS_RHS[i, (b*p):(p+b*p), (b*p):(p+b*p)], corr_X)\n",
    "        rho_F_DHS[i][b] = frobenius_correlation(shrink_PS_DHS[i, (b*p):(p+b*p), (b*p):(p+b*p)], corr_X)\n",
    "        rho_F_DST[i][b] = frobenius_correlation(shrink_PS_DST[i, (b*p):(p+b*p), (b*p):(p+b*p)], corr_X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0, 16), rho_F_gauss.mean(axis=0), label=\"Gauss\", marker='o')\n",
    "plt.plot(np.arange(0, 16), rho_F_RHS.mean(axis=0), label=\"RHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), rho_F_DHS.mean(axis=0), label=\"DHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), rho_F_DST.mean(axis=0), label=\"DST\", marker='o')\n",
    "plt.title(\"Mean alignment of blockwise shrinkage to correlation of features\")\n",
    "plt.xlabel(\"Block\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_X = np.corrcoef(X, rowvar=False)\n",
    "rho_F_RHS = frobenius_correlation(shrink_PS_RHS[1, 0:8, 0:8], corr_X)\n",
    "rho_F_DHS = frobenius_correlation(shrink_PS_DHS[1, 0:8, 0:8], corr_X)\n",
    "print(f\"Frobenius correlation between RHS shrinkage and feature corr: {rho_F_RHS:.3f}\")\n",
    "print(f\"Frobenius correlation between DHS shrinkage and feature corr: {rho_F_DHS:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import visualize_models\n",
    "\n",
    "matrices_S = [\n",
    "    np.mean(S_gauss, axis=0),\n",
    "    np.mean(S_RHS, axis=0),\n",
    "    np.mean(S_DHS, axis=0),\n",
    "    np.mean(S_DST, axis=0),\n",
    "]\n",
    "names_S = [\"S (Gauss)\", \"S (RHS)\", \"S (DHS)\", \"S (DST)\"]\n",
    "\n",
    "matrices_G = [\n",
    "    np.mean((G_gauss), axis=0),\n",
    "    np.mean((G_RHS), axis=0),\n",
    "    np.mean((G_DHS), axis=0),\n",
    "    np.mean((G_DST), axis=0),\n",
    "]\n",
    "\n",
    "names_G = [\"G (Gauss)\", \"G (RHS)\", \"G (DHS)\", \"G (DST)\"]\n",
    "\n",
    "matrices_shrink = [\n",
    "    np.mean((shrink_G_gauss), axis=0),\n",
    "    np.mean((shrink_G_RHS), axis=0),\n",
    "    np.mean((shrink_G_DHS), axis=0),\n",
    "    np.mean((shrink_G_DST), axis=0),\n",
    "]\n",
    "\n",
    "names_shrink = [\"(I+G)^{-1}G (Gauss)\", \"(I+G)^{-1}G (RHS)\", \"(I+G)^{-1}G (DHS)\", \"(I+G)^{-1}G (DST)\"]\n",
    "\n",
    "matrices_operator = [\n",
    "    np.mean((shrink_PS_gauss), axis=0),\n",
    "    np.mean((shrink_PS_RHS), axis=0),\n",
    "    np.mean((shrink_PS_DHS), axis=0),\n",
    "    np.mean((shrink_PS_DST), axis=0),\n",
    "]\n",
    "\n",
    "names_operator = [\"(P+S)^{-1}S (Gauss)\", \"(P+S)^{-1}S (RHS)\", \"(P+S)^{-1}S (DHS)\", \"(P+S)^{-1}S (DST)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_S, names_S, H=16, p=8, use_abs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_G, names_G, H=16, p=8, use_abs=False)#, cmap=\"magma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_shrink, names_shrink, H=16, p=8, use_abs=False)#, cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_operator, names_operator, H=16, p=8, use_abs=False)#, cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Traces as distributions (df_eff = tr(R) vs total shrinkage = tr(I-R)) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Effective dof: trace of (I+G)^{-1}G per draw\n",
    "tr_R_gauss = np.trace(shrink_G_gauss, axis1=1, axis2=2)\n",
    "tr_R_RHS   = np.trace(shrink_G_RHS,   axis1=1, axis2=2)\n",
    "tr_R_DHS   = np.trace(shrink_G_DHS,   axis1=1, axis2=2)\n",
    "tr_R_DST   = np.trace(shrink_G_DST,   axis1=1, axis2=2)\n",
    "\n",
    "# If you also want “total shrinkage”, use your SP_inv_S_* stacks (I - R):\n",
    "tr_SPinvS_gauss = np.trace(shrink_PS_gauss, axis1=1, axis2=2)\n",
    "tr_SPinvS_RHS   = np.trace(shrink_PS_RHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DHS   = np.trace(shrink_PS_DHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DST   = np.trace(shrink_PS_DST,   axis1=1, axis2=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot df_eff distributions\n",
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_R_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_R_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_R_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_R_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(\"trace((I+G)^{-1}G)  [effective dof]\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_SPinvS_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_SPinvS_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_SPinvS_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_SPinvS_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(r\"$tr((P+S)^{-1}S)$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Median eigenvalue curve (with bands) for shrink stacks ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def median_eigcurve(stack, q_lo=0.1, q_hi=0.9):\n",
    "    \"\"\"\n",
    "    stack: (D, N, N) of symmetric PSD matrices with eigenvalues in [0,1].\n",
    "    Returns: dict with 'median', 'lo', 'hi' over the sorted eigenvalues (descending).\n",
    "    \"\"\"\n",
    "    D, N, _ = stack.shape\n",
    "    evals = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        w = np.linalg.eigvalsh(stack[d])\n",
    "        evals[d] = np.sort(w)[::-1]  # descending\n",
    "    med = np.median(evals, axis=0)\n",
    "    lo  = np.quantile(evals, q_lo, axis=0)\n",
    "    hi  = np.quantile(evals, q_hi, axis=0)\n",
    "    return {\"median\": med, \"lo\": lo, \"hi\": hi}\n",
    "\n",
    "curves = {\n",
    "    \"Gauss\": median_eigcurve(shrink_G_gauss),\n",
    "    \"RHS\":   median_eigcurve(shrink_G_RHS),\n",
    "    \"DHS\":   median_eigcurve(shrink_G_DHS),\n",
    "    \"DST\":   median_eigcurve(shrink_G_DST),\n",
    "}\n",
    "\n",
    "# Plot 2x2 small multiples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8,6), dpi=150, constrained_layout=True)\n",
    "axes = axes.ravel()\n",
    "for ax, (name, c) in zip(axes, curves.items()):\n",
    "    x = np.arange(1, len(c[\"median\"])+1)\n",
    "    ax.plot(x, c[\"median\"], lw=1.8, label=f\"{name} median\")\n",
    "    ax.fill_between(x, c[\"lo\"], c[\"hi\"], alpha=0.25, label=f\"{name} {10}-{90}%\", step=None)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"eigenvalue rank\")\n",
    "    ax.set_ylabel(\"eigenvalue of (I+G)^{-1}G\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"upper right\", fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_point1_aligned(A, B, nameA=\"A\", nameB=\"B\",\n",
    "                        H=16, p=8, use_abs=False, q_low=0.05, q_high=0.99):\n",
    "    \"\"\"\n",
    "    Point (1): Best-scale–aligned difference.\n",
    "      - Panel 1: A\n",
    "      - Panel 2: c*·B  (c* = <A,B>_F / ||B||_F^2)\n",
    "      - Panel 3: A - c*·B\n",
    "      - Panel 4: (blank filler)\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, float); B = np.asarray(B, float)\n",
    "    num = np.sum(A * B)\n",
    "    den = np.sum(B * B) if np.sum(B * B) != 0 else 1.0\n",
    "    c_star = num / den\n",
    "    cosF = num / (np.linalg.norm(A, \"fro\") * (np.linalg.norm(B, \"fro\") + 1e-12))\n",
    "\n",
    "    mats  = [A, c_star * B, A - c_star * B, np.zeros_like(A)]\n",
    "    names = [\n",
    "        f\"{nameA}\",\n",
    "        f\"{nameB} scaled (c*={c_star:.3g})\",\n",
    "        f\"Aligned diff: {nameA} − c*·{nameB}\\ncos_F={cosF:.3f}\",\n",
    "        \"(unused)\"\n",
    "    ]\n",
    "    return mats, names #visualize_models(mats, names, H=H, p=p, use_abs=use_abs, q_low=q_low, q_high=q_high)\n",
    "\n",
    "def plot_point2_unit_energy(A, B, nameA=\"A\", nameB=\"B\",\n",
    "                            H=16, p=8, use_abs=False, q_low=0.05, q_high=0.99):\n",
    "    \"\"\"\n",
    "    Point (2): Unit-energy (Frobenius-normalized) side-by-side + difference.\n",
    "      - Panel 1: A / ||A||_F\n",
    "      - Panel 2: B / ||B||_F\n",
    "      - Panel 3: (A/||A||_F) − (B/||B||_F)\n",
    "      - Panel 4: (unused filler)\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, float); B = np.asarray(B, float)\n",
    "    Af = A / (np.linalg.norm(A, \"fro\") + 1e-12)\n",
    "    Bf = B / (np.linalg.norm(B, \"fro\") + 1e-12)\n",
    "\n",
    "    mats  = [Af, Bf, Af - Bf, np.zeros_like(A)]\n",
    "    names = [\n",
    "        f\"{nameA} / ||{nameA}||_F\",\n",
    "        f\"{nameB} / ||{nameB}||_F\",\n",
    "        \"Difference (unit-energy)\",\n",
    "        \"(unused)\"\n",
    "    ]\n",
    "    visualize_models(mats, names, H=H, p=p, use_abs=use_abs, q_low=q_low, q_high=q_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_DHS_v_RHS, names_DHS_v_RHS = plot_point1_aligned(np.mean((shrink_PS_DHS), axis=0), np.mean((shrink_PS_RHS), axis=0), \"DHS\", \"RHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_DHS_v_gauss, names_DHS_v_gauss = plot_point1_aligned(np.mean((shrink_PS_DHS), axis=0), np.mean((shrink_PS_gauss), axis=0), \"DHS\", \"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_DST_v_RHS, names_DST_v_RHS = plot_point1_aligned(np.mean((shrink_PS_DST), axis=0), np.mean((shrink_PS_RHS), axis=0), \"DST\", \"RHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_DST_v_gauss, names_DST_v_gauss = plot_point1_aligned(np.mean((shrink_PS_DST), axis=0), np.mean((shrink_PS_gauss), axis=0), \"DST\", \"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_combined = [\n",
    "    mats_DHS_v_RHS[2],\n",
    "    mats_DHS_v_gauss[2],\n",
    "    mats_DST_v_RHS[2],\n",
    "    mats_DST_v_gauss[2],\n",
    "]\n",
    "\n",
    "names_combined = [\n",
    "    names_DHS_v_RHS[2],\n",
    "    names_DHS_v_gauss[2],\n",
    "    names_DST_v_RHS[2],\n",
    "    names_DST_v_gauss[2],\n",
    "]\n",
    "\n",
    "visualize_models(mats_combined, names_combined, H=16, p=8, use_abs=False, q_low=0.05, q_high=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats = [np.mean(shrink_PS_gauss, axis=0), np.mean(shrink_PS_RHS, axis=0), np.mean(shrink_PS_DHS, axis=0), np.mean(shrink_PS_DST, axis=0)]\n",
    "names = [\"Gaussian\", \"RHS\", \"Dirichlet–HS\", \"Dirichlet–ST\"]\n",
    "visualize_models(mats, names, H=16, p=8, use_abs=False, q_low=0.05, q_high=0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Assume: shrink_PS_gauss etc. each are (4000, 128, 128)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "models = {\n",
    "    \"Gaussian\": shrink_PS_gauss,\n",
    "    \"RHS\": shrink_PS_RHS,\n",
    "    \"Dirichlet–HS\": shrink_PS_DHS,\n",
    "    \"Dirichlet–ST\": shrink_PS_DST,\n",
    "}\n",
    "\n",
    "# --- 1️⃣ Compute posterior means of each operator ---\n",
    "mean_ops = {name: np.mean(arr, axis=0) for name, arr in models.items()}\n",
    "\n",
    "# --- 2️⃣ Pairwise cosine distances between posterior means ---\n",
    "K_flat = np.stack([v.ravel() for v in mean_ops.values()])\n",
    "dist_matrix = squareform(pdist(K_flat, metric=\"cosine\"))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    dist_matrix,\n",
    "    annot=True, fmt=\".3f\",\n",
    "    xticklabels=list(mean_ops.keys()),\n",
    "    yticklabels=list(mean_ops.keys()),\n",
    "    cmap=\"mako\", square=True, cbar_kws={\"label\": \"Cosine distance\"}\n",
    ")\n",
    "plt.title(\"Pairwise cosine distances between mean shrinkage operators\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3️⃣ Eigenvalue spectra across posterior draws ---\n",
    "plt.figure(figsize=(7, 5))\n",
    "for name, arr in models.items():\n",
    "    eigs_all = np.linalg.eigvalsh(arr)   # shape (4000, 128)\n",
    "    eigs_mean = np.mean(eigs_all, axis=0)\n",
    "    eigs_std  = np.std(eigs_all, axis=0)\n",
    "    idx = np.argsort(eigs_mean)[::-1]\n",
    "    plt.plot(eigs_mean[idx], label=name)\n",
    "    plt.fill_between(np.arange(len(eigs_mean)), \n",
    "                     eigs_mean[idx] - eigs_std[idx],\n",
    "                     eigs_mean[idx] + eigs_std[idx],\n",
    "                     alpha=0.2)\n",
    "plt.xlabel(\"Eigenvalue index (sorted)\")\n",
    "plt.ylabel(\"Eigenvalue magnitude\")\n",
    "plt.legend()\n",
    "plt.title(\"Eigenvalue spectra of shrinkage operators\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- konstanter og blokkindekser (tilpass hvis din vec-rekkefølge er annerledes)\n",
    "H, p = 16, 8\n",
    "BLOCKS = [slice(h*p, (h+1)*p) for h in range(H)]\n",
    "\n",
    "def block_energy(U, blocks=BLOCKS):\n",
    "    BE = np.empty((U.shape[1], len(blocks)))  # (modes, H)\n",
    "    for b, sl in enumerate(blocks):\n",
    "        BE[:, b] = (U[sl, :]**2).sum(axis=0)\n",
    "    BE /= BE.sum(axis=1, keepdims=True)\n",
    "    return BE  # (modes, H)\n",
    "\n",
    "def evd_metrics(G):\n",
    "    w, U = np.linalg.eigh(G)                 # G sym/PSD\n",
    "    # sorter synkende på w\n",
    "    order = np.argsort(w)[::-1]\n",
    "    w, U = w[order], U[:, order]\n",
    "    rho = w / (1.0 + w)\n",
    "    m_eff = rho.sum()\n",
    "    ipr = (U**4).sum(axis=0)                  # inverse participation ratio\n",
    "    eff_support = 1.0 / ipr                   # effektiv støtte\n",
    "    return dict(w=w, U=U, rho=rho, m_eff=m_eff, ipr=ipr, eff_supp=eff_support)\n",
    "\n",
    "def m_eff_blocks_from_G(G):\n",
    "    M  = evd_metrics(G)\n",
    "    BE = block_energy(M['U'], BLOCKS)        # (modes, H)\n",
    "    m_eff_b = (M['rho'][:, None] * BE).sum(axis=0)  # (H,)\n",
    "    return m_eff_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Forutsetter at du har disse =====\n",
    "W2_gauss_samps = tanh_fit['Gaussian tanh']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "W2_RHS_samps = tanh_fit['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "W2_DHS_samps = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "W2_DST_samps = tanh_fit['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "\n",
    "S = 4000\n",
    "\n",
    "# --- beregn m_eff per blokk for ALLE samples\n",
    "m_eff_blocks_GAUSS = np.zeros((S, H))\n",
    "m_eff_blocks_RHS   = np.zeros((S, H))\n",
    "m_eff_blocks_DHS   = np.zeros((S, H))\n",
    "m_eff_blocks_DST = np.zeros((S, H))\n",
    "for s in range(S):\n",
    "    m_eff_blocks_GAUSS[s] = m_eff_blocks_from_G(G_gauss[s])\n",
    "    m_eff_blocks_RHS[s] = m_eff_blocks_from_G(G_RHS[s])\n",
    "    m_eff_blocks_DHS[s]   = m_eff_blocks_from_G(G_DHS[s])\n",
    "    m_eff_blocks_DST[s]   = m_eff_blocks_from_G(G_DST[s])\n",
    "    \n",
    "\n",
    "# --- klargjør |W2| i samme form\n",
    "W2_GAUSS_flat = np.abs(np.atleast_2d(W2_gauss_samps).reshape(S, H))\n",
    "W2_RHS_flat   = np.abs(np.atleast_2d(W2_RHS_samps).reshape(S, H))\n",
    "W2_DHS_flat   = np.abs(np.atleast_2d(W2_DHS_samps).reshape(S, H))\n",
    "W2_DST_flat   = np.abs(np.atleast_2d(W2_DST_samps).reshape(S, H))\n",
    "\n",
    "# --- flate til 1D for scatter\n",
    "x_gau = m_eff_blocks_GAUSS.ravel()\n",
    "y_gau = W2_GAUSS_flat.ravel()\n",
    "x_rhs = m_eff_blocks_RHS.ravel()\n",
    "y_rhs = W2_RHS_flat.ravel()\n",
    "x_dhs = m_eff_blocks_DHS.ravel()\n",
    "y_dhs = W2_DHS_flat.ravel()\n",
    "x_dst = m_eff_blocks_DST.ravel()\n",
    "y_dst = W2_DST_flat.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_gau, y_gau, label=\"Gaussian\", s=8, alpha=0.35)\n",
    "plt.scatter(x_rhs, y_rhs, label=\"RHS\", s=8, alpha=0.35)\n",
    "plt.scatter(x_dhs, y_dhs, label=\"DHS\", s=8, alpha=0.35)\n",
    "plt.scatter(x_dst, y_dst, label=\"DST\", s=8, alpha=0.35)\n",
    "plt.xlabel(r\"$m_{\\mathrm{eff}}^{(b)}$\")\n",
    "plt.ylabel(r\"$|W_2|$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X, X_test, y, y_test = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "# Coerce everything to plain float64 NumPy arrays\n",
    "X      = np.asarray(X, dtype=float)\n",
    "X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "# y often comes as a (n,1) DataFrame/array — flatten to (n,)\n",
    "y      = np.asarray(y, dtype=float).reshape(-1)\n",
    "y_test = np.asarray(y_test, dtype=float).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import extract_model_draws, compute_shrinkage_for_W_block, shrinkage_eigs_and_df\n",
    "from utils.sparsity import local_prune_weights\n",
    "\n",
    "def compute_shrinkage_with_pruning(\n",
    "    X,\n",
    "    W_all, b_all, v_all,          # (D,H,p), (D,H), (D,H)\n",
    "    sigma_all, tau_w_all, tau_v_all,  # (D,), (D,), (D,)\n",
    "    lambda_all,                   # (D,H,p)\n",
    "    activation=\"tanh\",\n",
    "    return_mats=True,             # set False if you only want summaries\n",
    "    include_b1_in_Sigma: bool = True,\n",
    "    include_b2_in_Sigma: bool = True,\n",
    "    sparsity = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop over draws and compute R=(P+S)^{-1}P per draw using your single-draw function.\n",
    "    Returns:\n",
    "      R_stack : (D, N, N) with N=H*p  (if return_mats=True, else None)\n",
    "      r_eigs  : (D, N)  sorted eigenvalues in [0,1]\n",
    "      df_eff  : (D,)    effective dof = tr(I-R) = N - tr(R)\n",
    "    \"\"\"\n",
    "    D, H, p = W_all.shape\n",
    "    N = H * p\n",
    "\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    S_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    P_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    G_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    shrink_stack= np.empty((D, N, N)) if return_mats else None\n",
    "    r_eigs  = np.empty((D, N))\n",
    "    df_eff  = np.empty(D)\n",
    "\n",
    "    for d in range(D):\n",
    "        mask = local_prune_weights(W_all[d], sparsity_level=sparsity)\n",
    "        W_pruned = mask[0]*W_all[d]\n",
    "        R, P, S, Sigma_y, _, _ = compute_shrinkage_for_W_block(\n",
    "            X=X,\n",
    "            W0=W_pruned,\n",
    "            b0=b_all[d],\n",
    "            v0=v_all[d],\n",
    "            noise=float(sigma_all[d]),\n",
    "            tau_w=float(tau_w_all[d]),\n",
    "            tau_v=float(tau_v_all[d]),\n",
    "            lambda_tilde=lambda_all[d],\n",
    "            activation=activation,\n",
    "            include_b1_in_Sigma=include_b1_in_Sigma,\n",
    "            include_b2_in_Sigma=include_b2_in_Sigma,\n",
    "        )\n",
    "        p = np.diag(P)                       \n",
    "        P_inv_sqrt = np.diag(1.0/np.sqrt(p))         \n",
    "        G = P_inv_sqrt @ S @ P_inv_sqrt \n",
    "        I = np.identity(N)\n",
    "        shrink_mat = np.linalg.inv(I + G)@G\n",
    "\n",
    "        if return_mats:\n",
    "            R_stack[d] = R\n",
    "            S_stack[d] = S\n",
    "            P_stack[d] = P\n",
    "            G_stack[d] = G\n",
    "            shrink_stack[d] = shrink_mat\n",
    "        \n",
    "        r, df = shrinkage_eigs_and_df(P, S)\n",
    "        r_eigs[d] = np.sort(r)\n",
    "        df_eff[d] = df\n",
    "\n",
    "    return R_stack, S_stack, P_stack, G_stack, shrink_stack, r_eigs, df_eff\n",
    "\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Gaussian tanh'\n",
    ")\n",
    "R_gauss, S_gauss, P_gauss, G_gauss, shrink_gauss, eigs_gauss, df_gauss = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with Gauss\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Regularized Horseshoe tanh'\n",
    ")\n",
    "\n",
    "R_RHS, S_RHS, P_RHS, G_RHS, shrink_RHS, eigs_RHS, df_eff_RHS = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with RHS\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Dirichlet Horseshoe tanh'\n",
    ")\n",
    "R_DHS, S_DHS, P_DHS, G_DHS, shrink_DHS, eigs_DHS, df_eff_DHS = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with DHS\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Dirichlet Student T tanh'\n",
    ")\n",
    "R_DST, S_DST, P_DST, G_DST, shrink_DST, eigs_DST, df_eff_DST = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with DST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs(\"Abalone_matrices\", exist_ok=True)\n",
    "\n",
    "def save_PS(model_name, P, S):\n",
    "    fn = os.path.join(\"Abalone_matrices\", f\"{model_name.replace(' ', '_')}_PS.npz\")\n",
    "    np.savez_compressed(fn, P=np.asarray(P, dtype=np.float32), S=np.asarray(S, dtype=np.float32))\n",
    "    print(f\"Saved {fn}  with P,S shapes={P.shape},{S.shape}  dtype=float32\")\n",
    "\n",
    "# Call once per model (arrays are shape (4000, 160, 160))\n",
    "save_PS(\"Gaussian_sparsity_90\",            P_gauss, S_gauss)\n",
    "save_PS(\"Regularized_Horseshoe_sparsity_90\",    P_RHS,   S_RHS)\n",
    "save_PS(\"Dirichlet_Horseshoe_sparsity_90\", P_DHS,   S_DHS)\n",
    "save_PS(\"Dirichlet_StudentT_sparsity_90\",  P_DST,   S_DST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Traces as distributions (df_eff = tr(R) vs total shrinkage = tr(I-R)) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Effective dof: trace of (I+G)^{-1}G per draw\n",
    "# tr_R_gauss = np.trace(shrink_G_gauss, axis1=1, axis2=2)\n",
    "# tr_R_RHS   = np.trace(shrink_G_RHS,   axis1=1, axis2=2)\n",
    "# tr_R_DHS   = np.trace(shrink_G_DHS,   axis1=1, axis2=2)\n",
    "# tr_R_DST   = np.trace(shrink_G_DST,   axis1=1, axis2=2)\n",
    "\n",
    "# If you also want “total shrinkage”, use your SP_inv_S_* stacks (I - R):\n",
    "tr_SPinvS_gauss = np.trace(shrink_gauss, axis1=1, axis2=2)\n",
    "tr_SPinvS_RHS   = np.trace(shrink_RHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DHS   = np.trace(shrink_DHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DST   = np.trace(shrink_DST,   axis1=1, axis2=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_SPinvS_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_SPinvS_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_SPinvS_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_SPinvS_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(r\"$tr((P+S)^{-1}S)$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
