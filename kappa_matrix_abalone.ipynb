{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/abalone\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/abalone\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/abalone\"\n",
    "#model_names_relu = [\"Dirichlet Student T\"]\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "full_config_path = \"abalone_N3341_p8\"\n",
    "# relu_fit = get_model_fits(\n",
    "#     config=full_config_path,\n",
    "#     results_dir=results_dir_relu,\n",
    "#     models=model_names_relu,\n",
    "#     include_prior=False,\n",
    "# )\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X, X_test, y, y_test = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "# Coerce everything to plain float64 NumPy arrays\n",
    "X      = np.asarray(X, dtype=float)\n",
    "X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "# y often comes as a (n,1) DataFrame/array — flatten to (n,)\n",
    "y      = np.asarray(y, dtype=float).reshape(-1)\n",
    "y_test = np.asarray(y_test, dtype=float).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import cholesky, solve\n",
    "from utils.kappa_matrix import shrinkage_matrix_stable\n",
    "\n",
    "def build_operators_from_PS(P, S):\n",
    "    \"\"\"\n",
    "    P, S: arrays of shape (S, d, d), SPD per sample.\n",
    "    Returns:\n",
    "      G        : P^{-1/2} S P^{-1/2}\n",
    "      shrink_PS: (P+S)^{-1} S\n",
    "      shrink_G : (I+G)^{-1} G\n",
    "    \"\"\"\n",
    "    S_, d, _ = P.shape\n",
    "    G         = np.empty_like(P, dtype=np.float64)\n",
    "    shrink_PS = np.empty_like(P, dtype=np.float64)\n",
    "    shrink_G  = np.empty_like(P, dtype=np.float64)\n",
    "\n",
    "    I = np.eye(d)\n",
    "\n",
    "    for s in range(S_):\n",
    "        Ps = P[s]; Ss = S[s]\n",
    "\n",
    "        # --- G = P^{-1/2} S P^{-1/2} via Cholesky (Ps = C C^T) -> C^{-T} S C^{-1}\n",
    "        C = cholesky(Ps)            # upper-triangular by NumPy convention\n",
    "        # temp = C^{-1}^T S\n",
    "        temp = solve(C.T, Ss)#, assume_a='sym')    # solves C^T X = S  -> X = C^{-T} S\n",
    "        Gs   = solve(C, temp.T)#, assume_a='sym').T  # solves C Y^T = temp^T -> Y = C^{-1} temp\n",
    "        G[s] = Gs\n",
    "\n",
    "        # # --- (P+S)^{-1} S\n",
    "        Rs = shrinkage_matrix_stable(Ps, Ss)\n",
    "        # A = Ps + Ss\n",
    "        # L = cholesky(A)\n",
    "        # # Solve A X = S  (two triangular solves)\n",
    "        # Y = solve(L, Ss)#, lower=False)           # L X = S  (NumPy returns upper L; set lower=False)\n",
    "        # X = solve(L.T, Y)#, lower=True)          # L^T X = Y\n",
    "        # shrink_PS[s] = X\n",
    "        shrink_PS[s] = np.eye(Ps.shape[0]) - Rs\n",
    "        \n",
    "\n",
    "        # --- (I+G)^{-1} G\n",
    "        B = I + Gs\n",
    "        LB = cholesky(B)\n",
    "        YB = solve(LB, Gs)#, lower=False)\n",
    "        XB = solve(LB.T, YB)#, lower=True)\n",
    "        shrink_G[s] = XB\n",
    "\n",
    "    return G, shrink_PS, shrink_G\n",
    "\n",
    "\n",
    "# Example usage after reloading a saved NPZ:\n",
    "dat = np.load(\"Abalone_matrices/Gaussian_PS.npz\")\n",
    "P_gauss, S_gauss = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_gauss, shrink_PS_gauss, shrink_G_gauss = build_operators_from_PS(P_gauss, S_gauss)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Regularized_Horseshoe_PS.npz\")\n",
    "P_RHS, S_RHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_RHS, shrink_PS_RHS, shrink_G_RHS = build_operators_from_PS(P_RHS, S_RHS)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_Horseshoe_PS.npz\")\n",
    "P_DHS, S_DHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DHS, shrink_PS_DHS, shrink_G_DHS = build_operators_from_PS(P_DHS, S_DHS)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_StudentT_PS.npz\")\n",
    "P_DST, S_DST = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DST, shrink_PS_DST, shrink_G_DST = build_operators_from_PS(P_DST, S_DST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import visualize_models\n",
    "\n",
    "matrices_S = [\n",
    "    np.mean(S_gauss, axis=0),\n",
    "    np.mean(S_RHS, axis=0),\n",
    "    np.mean(S_DHS, axis=0),\n",
    "    np.mean(S_DST, axis=0),\n",
    "]\n",
    "names_S = [\"S (Gauss)\", \"S (RHS)\", \"S (DHS)\", \"S (DST)\"]\n",
    "\n",
    "matrices_G = [\n",
    "    np.mean((G_gauss), axis=0),\n",
    "    np.mean((G_RHS), axis=0),\n",
    "    np.mean((G_DHS), axis=0),\n",
    "    np.mean((G_DST), axis=0),\n",
    "]\n",
    "\n",
    "names_G = [\"G (Gauss)\", \"G (RHS)\", \"G (DHS)\", \"G (DST)\"]\n",
    "\n",
    "matrices_shrink = [\n",
    "    np.mean((shrink_G_gauss), axis=0),\n",
    "    np.mean((shrink_G_RHS), axis=0),\n",
    "    np.mean((shrink_G_DHS), axis=0),\n",
    "    np.mean((shrink_G_DST), axis=0),\n",
    "]\n",
    "\n",
    "names_shrink = [\"(I+G)^{-1}G (Gauss)\", \"(I+G)^{-1}G (RHS)\", \"(I+G)^{-1}G (DHS)\", \"(I+G)^{-1}G (DST)\"]\n",
    "\n",
    "matrices_operator = [\n",
    "    np.mean((shrink_PS_gauss), axis=0),\n",
    "    np.mean((shrink_PS_RHS), axis=0),\n",
    "    np.mean((shrink_PS_DHS), axis=0),\n",
    "    np.mean((shrink_PS_DST), axis=0),\n",
    "]\n",
    "\n",
    "names_operator = [\"(P+S)^{-1}S (Gauss)\", \"(P+S)^{-1}S (RHS)\", \"(P+S)^{-1}S (DHS)\", \"(P+S)^{-1}S (DST)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.abs(matrices_S[0])>1e-4)/(128*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_S, names_S, H=16, p=8, use_abs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_G, names_G, H=16, p=8, use_abs=False)#, cmap=\"magma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_shrink, names_shrink, H=16, p=8, use_abs=False)#, cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_operator, names_operator, H=16, p=8, use_abs=False)#, cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Traces as distributions (df_eff = tr(R) vs total shrinkage = tr(I-R)) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Effective dof: trace of (I+G)^{-1}G per draw\n",
    "tr_R_gauss = np.trace(shrink_G_gauss, axis1=1, axis2=2)\n",
    "tr_R_RHS   = np.trace(shrink_G_RHS,   axis1=1, axis2=2)\n",
    "tr_R_DHS   = np.trace(shrink_G_DHS,   axis1=1, axis2=2)\n",
    "tr_R_DST   = np.trace(shrink_G_DST,   axis1=1, axis2=2)\n",
    "\n",
    "# If you also want “total shrinkage”, use your SP_inv_S_* stacks (I - R):\n",
    "tr_SPinvS_gauss = np.trace(shrink_PS_gauss, axis1=1, axis2=2)\n",
    "tr_SPinvS_RHS   = np.trace(shrink_PS_RHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DHS   = np.trace(shrink_PS_DHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DST   = np.trace(shrink_PS_DST,   axis1=1, axis2=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot df_eff distributions\n",
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_R_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_R_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_R_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_R_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(\"trace((I+G)^{-1}G)  [effective dof]\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_SPinvS_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_SPinvS_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_SPinvS_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_SPinvS_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(r\"$tr((P+S)^{-1}S)$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_point1_aligned(A, B, nameA=\"A\", nameB=\"B\",\n",
    "                        H=16, p=8, use_abs=False, q_low=0.05, q_high=0.99):\n",
    "    \"\"\"\n",
    "    Point (1): Best-scale–aligned difference.\n",
    "      - Panel 1: A\n",
    "      - Panel 2: c*·B  (c* = <A,B>_F / ||B||_F^2)\n",
    "      - Panel 3: A - c*·B\n",
    "      - Panel 4: (blank filler)\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, float); B = np.asarray(B, float)\n",
    "    num = np.sum(A * B)\n",
    "    den = np.sum(B * B) if np.sum(B * B) != 0 else 1.0\n",
    "    c_star = num / den\n",
    "    cosF = num / (np.linalg.norm(A, \"fro\") * (np.linalg.norm(B, \"fro\") + 1e-12))\n",
    "\n",
    "    mats  = [A, c_star * B, A - c_star * B, np.zeros_like(A)]\n",
    "    names = [\n",
    "        f\"{nameA}\",\n",
    "        f\"{nameB} scaled (c*={c_star:.3g})\",\n",
    "        f\"Aligned diff: {nameA} − c*·{nameB}\\ncos_F={cosF:.3f}\",\n",
    "        \"(unused)\"\n",
    "    ]\n",
    "    visualize_models(mats, names, H=H, p=p, use_abs=use_abs, q_low=q_low, q_high=q_high)\n",
    "\n",
    "def plot_point2_unit_energy(A, B, nameA=\"A\", nameB=\"B\",\n",
    "                            H=16, p=8, use_abs=False, q_low=0.05, q_high=0.99):\n",
    "    \"\"\"\n",
    "    Point (2): Unit-energy (Frobenius-normalized) side-by-side + difference.\n",
    "      - Panel 1: A / ||A||_F\n",
    "      - Panel 2: B / ||B||_F\n",
    "      - Panel 3: (A/||A||_F) − (B/||B||_F)\n",
    "      - Panel 4: (unused filler)\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, float); B = np.asarray(B, float)\n",
    "    Af = A / (np.linalg.norm(A, \"fro\") + 1e-12)\n",
    "    Bf = B / (np.linalg.norm(B, \"fro\") + 1e-12)\n",
    "\n",
    "    mats  = [Af, Bf, Af - Bf, np.zeros_like(A)]\n",
    "    names = [\n",
    "        f\"{nameA} / ||{nameA}||_F\",\n",
    "        f\"{nameB} / ||{nameB}||_F\",\n",
    "        \"Difference (unit-energy)\",\n",
    "        \"(unused)\"\n",
    "    ]\n",
    "    visualize_models(mats, names, H=H, p=p, use_abs=use_abs, q_low=q_low, q_high=q_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point1_aligned(np.mean((shrink_PS_DHS), axis=0), np.mean((shrink_PS_RHS), axis=0), \"Dirichlet–HS\", \"RHS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point1_aligned(np.mean((shrink_PS_DST), axis=0), np.mean((shrink_PS_RHS), axis=0), \"Dirichlet–ST\", \"RHS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point1_aligned(np.mean((shrink_PS_DHS), axis=0), np.mean((shrink_PS_gauss), axis=0), \"Dirichlet–HS\", \"Gaussian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point1_aligned(np.mean((shrink_PS_DST), axis=0), np.mean((shrink_PS_gauss), axis=0), \"Dirichlet–ST\", \"Gaussian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point2_unit_energy(np.mean((shrink_PS_DHS), axis=0), np.mean((shrink_PS_RHS), axis=0), \"Dirichlet–HS\", \"RHS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point2_unit_energy(np.mean((shrink_PS_DST), axis=0), np.mean((shrink_PS_RHS), axis=0), \"Dirichlet–ST\", \"RHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point2_unit_energy(np.mean((shrink_PS_DHS), axis=0), np.mean((shrink_PS_gauss), axis=0), \"Dirichlet–HS\", \"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point2_unit_energy(np.mean((shrink_PS_DST), axis=0), np.mean((shrink_PS_gauss), axis=0), \"Dirichlet–ST\", \"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point1_aligned(np.mean((shrink_PS_gauss), axis=0), np.mean((shrink_PS_RHS), axis=0), \"Gauss\", \"RHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_point2_unit_energy(np.mean((shrink_PS_gauss), axis=0), np.mean((shrink_PS_RHS), axis=0), \"Gauss\", \"RHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats = [np.mean(shrink_PS_gauss, axis=0), np.mean(shrink_PS_RHS, axis=0), np.mean(shrink_PS_DHS, axis=0), np.mean(shrink_PS_DST, axis=0)]\n",
    "names = [\"Gaussian\", \"RHS\", \"Dirichlet–HS\", \"Dirichlet–ST\"]\n",
    "visualize_models(mats, names, H=16, p=8, use_abs=False, q_low=0.05, q_high=0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Assume: shrink_PS_gauss etc. each are (4000, 128, 128)\n",
    "# ------------------------------------------------------\n",
    "\n",
    "models = {\n",
    "    \"Gaussian\": shrink_PS_gauss,\n",
    "    \"RHS\": shrink_PS_RHS,\n",
    "    \"Dirichlet–HS\": shrink_PS_DHS,\n",
    "    \"Dirichlet–ST\": shrink_PS_DST,\n",
    "}\n",
    "\n",
    "# --- 1️⃣ Compute posterior means of each operator ---\n",
    "mean_ops = {name: np.mean(arr, axis=0) for name, arr in models.items()}\n",
    "\n",
    "# --- 2️⃣ Pairwise cosine distances between posterior means ---\n",
    "K_flat = np.stack([v.ravel() for v in mean_ops.values()])\n",
    "dist_matrix = squareform(pdist(K_flat, metric=\"cosine\"))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    dist_matrix,\n",
    "    annot=True, fmt=\".3f\",\n",
    "    xticklabels=list(mean_ops.keys()),\n",
    "    yticklabels=list(mean_ops.keys()),\n",
    "    cmap=\"mako\", square=True, cbar_kws={\"label\": \"Cosine distance\"}\n",
    ")\n",
    "plt.title(\"Pairwise cosine distances between mean shrinkage operators\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3️⃣ Eigenvalue spectra across posterior draws ---\n",
    "plt.figure(figsize=(7, 5))\n",
    "for name, arr in models.items():\n",
    "    eigs_all = np.linalg.eigvalsh(arr)   # shape (4000, 128)\n",
    "    eigs_mean = np.mean(eigs_all, axis=0)\n",
    "    eigs_std  = np.std(eigs_all, axis=0)\n",
    "    idx = np.argsort(eigs_mean)[::-1]\n",
    "    plt.plot(eigs_mean[idx], label=name)\n",
    "    plt.fill_between(np.arange(len(eigs_mean)), \n",
    "                     eigs_mean[idx] - eigs_std[idx],\n",
    "                     eigs_mean[idx] + eigs_std[idx],\n",
    "                     alpha=0.2)\n",
    "plt.xlabel(\"Eigenvalue index (sorted)\")\n",
    "plt.ylabel(\"Eigenvalue magnitude\")\n",
    "plt.legend()\n",
    "plt.title(\"Eigenvalue spectra of shrinkage operators\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- konstanter og blokkindekser (tilpass hvis din vec-rekkefølge er annerledes)\n",
    "H, p = 16, 8\n",
    "BLOCKS = [slice(h*p, (h+1)*p) for h in range(H)]\n",
    "\n",
    "def block_energy(U, blocks=BLOCKS):\n",
    "    BE = np.empty((U.shape[1], len(blocks)))  # (modes, H)\n",
    "    for b, sl in enumerate(blocks):\n",
    "        BE[:, b] = (U[sl, :]**2).sum(axis=0)\n",
    "    BE /= BE.sum(axis=1, keepdims=True)\n",
    "    return BE  # (modes, H)\n",
    "\n",
    "def evd_metrics(G):\n",
    "    w, U = np.linalg.eigh(G)                 # G sym/PSD\n",
    "    # sorter synkende på w\n",
    "    order = np.argsort(w)[::-1]\n",
    "    w, U = w[order], U[:, order]\n",
    "    rho = w / (1.0 + w)\n",
    "    m_eff = rho.sum()\n",
    "    ipr = (U**4).sum(axis=0)                  # inverse participation ratio\n",
    "    eff_support = 1.0 / ipr                   # effektiv støtte\n",
    "    return dict(w=w, U=U, rho=rho, m_eff=m_eff, ipr=ipr, eff_supp=eff_support)\n",
    "\n",
    "def m_eff_blocks_from_G(G):\n",
    "    M  = evd_metrics(G)\n",
    "    BE = block_energy(M['U'], BLOCKS)        # (modes, H)\n",
    "    m_eff_b = (M['rho'][:, None] * BE).sum(axis=0)  # (H,)\n",
    "    return m_eff_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Forutsetter at du har disse =====\n",
    "W2_gauss_samps = tanh_fit['Gaussian tanh']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "W2_RHS_samps = tanh_fit['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "W2_DHS_samps = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "W2_DST_samps = tanh_fit['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "\n",
    "S = 4000\n",
    "\n",
    "# --- beregn m_eff per blokk for ALLE samples\n",
    "m_eff_blocks_GAUSS = np.zeros((S, H))\n",
    "m_eff_blocks_RHS   = np.zeros((S, H))\n",
    "m_eff_blocks_DHS   = np.zeros((S, H))\n",
    "m_eff_blocks_DST = np.zeros((S, H))\n",
    "for s in range(S):\n",
    "    m_eff_blocks_GAUSS[s] = m_eff_blocks_from_G(G_gauss[s])\n",
    "    m_eff_blocks_RHS[s] = m_eff_blocks_from_G(G_RHS[s])\n",
    "    m_eff_blocks_DHS[s]   = m_eff_blocks_from_G(G_DHS[s])\n",
    "    m_eff_blocks_DST[s]   = m_eff_blocks_from_G(G_DST[s])\n",
    "    \n",
    "\n",
    "# --- klargjør |W2| i samme form\n",
    "W2_GAUSS_flat = np.abs(np.atleast_2d(W2_gauss_samps).reshape(S, H))\n",
    "W2_RHS_flat   = np.abs(np.atleast_2d(W2_RHS_samps).reshape(S, H))\n",
    "W2_DHS_flat   = np.abs(np.atleast_2d(W2_DHS_samps).reshape(S, H))\n",
    "W2_DST_flat   = np.abs(np.atleast_2d(W2_DST_samps).reshape(S, H))\n",
    "\n",
    "# --- flate til 1D for scatter\n",
    "x_gau = m_eff_blocks_GAUSS.ravel()\n",
    "y_gau = W2_GAUSS_flat.ravel()\n",
    "x_rhs = m_eff_blocks_RHS.ravel()\n",
    "y_rhs = W2_RHS_flat.ravel()\n",
    "x_dhs = m_eff_blocks_DHS.ravel()\n",
    "y_dhs = W2_DHS_flat.ravel()\n",
    "x_dst = m_eff_blocks_DST.ravel()\n",
    "y_dst = W2_DST_flat.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_gau, y_gau, label=\"Gaussian\", s=8, alpha=0.35)\n",
    "plt.scatter(x_rhs, y_rhs, label=\"RHS\", s=8, alpha=0.35)\n",
    "plt.scatter(x_dhs, y_dhs, label=\"DHS\", s=8, alpha=0.35)\n",
    "plt.scatter(x_dst, y_dst, label=\"DST\", s=8, alpha=0.35)\n",
    "plt.xlabel(r\"$m_{\\mathrm{eff}}^{(b)}$\")\n",
    "plt.ylabel(r\"$|W_2|$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X, X_test, y, y_test = load_abalone_regression_data(standardized=False, frac=0.2)\n",
    "# Coerce everything to plain float64 NumPy arrays\n",
    "X      = np.asarray(X, dtype=float)\n",
    "X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "# y often comes as a (n,1) DataFrame/array — flatten to (n,)\n",
    "y      = np.asarray(y, dtype=float).reshape(-1)\n",
    "y_test = np.asarray(y_test, dtype=float).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import extract_model_draws, compute_shrinkage_for_W_block, shrinkage_eigs_and_df\n",
    "from utils.sparsity import local_prune_weights\n",
    "\n",
    "def compute_shrinkage_with_pruning(\n",
    "    X,\n",
    "    W_all, b_all, v_all,          # (D,H,p), (D,H), (D,H)\n",
    "    sigma_all, tau_w_all, tau_v_all,  # (D,), (D,), (D,)\n",
    "    lambda_all,                   # (D,H,p)\n",
    "    activation=\"tanh\",\n",
    "    return_mats=True,             # set False if you only want summaries\n",
    "    include_b1_in_Sigma: bool = True,\n",
    "    include_b2_in_Sigma: bool = True,\n",
    "    sparsity = 0.9\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop over draws and compute R=(P+S)^{-1}P per draw using your single-draw function.\n",
    "    Returns:\n",
    "      R_stack : (D, N, N) with N=H*p  (if return_mats=True, else None)\n",
    "      r_eigs  : (D, N)  sorted eigenvalues in [0,1]\n",
    "      df_eff  : (D,)    effective dof = tr(I-R) = N - tr(R)\n",
    "    \"\"\"\n",
    "    D, H, p = W_all.shape\n",
    "    N = H * p\n",
    "\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    S_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    P_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    G_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    shrink_stack= np.empty((D, N, N)) if return_mats else None\n",
    "    r_eigs  = np.empty((D, N))\n",
    "    df_eff  = np.empty(D)\n",
    "\n",
    "    for d in range(D):\n",
    "        mask = local_prune_weights(W_all[d], sparsity_level=sparsity)\n",
    "        W_pruned = mask[0]*W_all[d]\n",
    "        R, P, S, Sigma_y, _, _ = compute_shrinkage_for_W_block(\n",
    "            X=X,\n",
    "            W0=W_pruned,\n",
    "            b0=b_all[d],\n",
    "            v0=v_all[d],\n",
    "            noise=float(sigma_all[d]),\n",
    "            tau_w=float(tau_w_all[d]),\n",
    "            tau_v=float(tau_v_all[d]),\n",
    "            lambda_tilde=lambda_all[d],\n",
    "            activation=activation,\n",
    "            include_b1_in_Sigma=include_b1_in_Sigma,\n",
    "            include_b2_in_Sigma=include_b2_in_Sigma,\n",
    "        )\n",
    "        p = np.diag(P)                       \n",
    "        P_inv_sqrt = np.diag(1.0/np.sqrt(p))         \n",
    "        G = P_inv_sqrt @ S @ P_inv_sqrt \n",
    "        I = np.identity(N)\n",
    "        shrink_mat = np.linalg.inv(I + G)@G\n",
    "\n",
    "        if return_mats:\n",
    "            R_stack[d] = R\n",
    "            S_stack[d] = S\n",
    "            P_stack[d] = P\n",
    "            G_stack[d] = G\n",
    "            shrink_stack[d] = shrink_mat\n",
    "        \n",
    "        r, df = shrinkage_eigs_and_df(P, S)\n",
    "        r_eigs[d] = np.sort(r)\n",
    "        df_eff[d] = df\n",
    "\n",
    "    return R_stack, S_stack, P_stack, G_stack, shrink_stack, r_eigs, df_eff\n",
    "\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Gaussian tanh'\n",
    ")\n",
    "R_gauss, S_gauss, P_gauss, G_gauss, shrink_gauss, eigs_gauss, df_gauss = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with Gauss\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Regularized Horseshoe tanh'\n",
    ")\n",
    "\n",
    "R_RHS, S_RHS, P_RHS, G_RHS, shrink_RHS, eigs_RHS, df_eff_RHS = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with RHS\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Dirichlet Horseshoe tanh'\n",
    ")\n",
    "R_DHS, S_DHS, P_DHS, G_DHS, shrink_DHS, eigs_DHS, df_eff_DHS = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with DHS\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Dirichlet Student T tanh'\n",
    ")\n",
    "R_DST, S_DST, P_DST, G_DST, shrink_DST, eigs_DST, df_eff_DST = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with DST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Traces as distributions (df_eff = tr(R) vs total shrinkage = tr(I-R)) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Effective dof: trace of (I+G)^{-1}G per draw\n",
    "# tr_R_gauss = np.trace(shrink_G_gauss, axis1=1, axis2=2)\n",
    "# tr_R_RHS   = np.trace(shrink_G_RHS,   axis1=1, axis2=2)\n",
    "# tr_R_DHS   = np.trace(shrink_G_DHS,   axis1=1, axis2=2)\n",
    "# tr_R_DST   = np.trace(shrink_G_DST,   axis1=1, axis2=2)\n",
    "\n",
    "# If you also want “total shrinkage”, use your SP_inv_S_* stacks (I - R):\n",
    "tr_SPinvS_gauss = np.trace(shrink_gauss, axis1=1, axis2=2)\n",
    "tr_SPinvS_RHS   = np.trace(shrink_RHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DHS   = np.trace(shrink_DHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DST   = np.trace(shrink_DST,   axis1=1, axis2=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_SPinvS_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_SPinvS_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_SPinvS_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_SPinvS_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(r\"$tr((P+S)^{-1}S)$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below does not seem to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X, X_test, y, y_test = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "# Coerce everything to plain float64 NumPy arrays\n",
    "X      = np.asarray(X, dtype=float)\n",
    "X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "# y often comes as a (n,1) DataFrame/array — flatten to (n,)\n",
    "y      = np.asarray(y, dtype=float).reshape(-1)\n",
    "y_test = np.asarray(y_test, dtype=float).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import build_hidden_and_jacobian_W, build_Sigma_y, build_S, build_P_from_lambda_tau, shrinkage_matrix_stable, extract_model_draws\n",
    "\n",
    "def solve_psd_pinv(S, g, rtol=1e-10):\n",
    "    # symmetric eigendecomp\n",
    "    evals, Q = np.linalg.eigh(S)\n",
    "    # threshold small eigenvalues\n",
    "    tol = rtol * max(evals.max(), 1.0)\n",
    "    keep = evals > tol\n",
    "    if not np.any(keep):\n",
    "        return np.zeros_like(g)\n",
    "    inv_eigs = np.zeros_like(evals)\n",
    "    inv_eigs[keep] = 1.0 / evals[keep]\n",
    "    # S^+ g = Q diag(inv_eigs) Q^T g\n",
    "    return Q @ (inv_eigs * (Q.T @ g))\n",
    "\n",
    "# ----- Low-rank builder that includes biases exactly like build_Sigma_y -----\n",
    "def build_U(Phi_mat, tau_v, J_b1=None, J_b2=None, include_b1=True, include_b2=True):\n",
    "    cols = [np.sqrt(tau_v**2) * Phi_mat]          # (n, H)\n",
    "    if include_b1 and (J_b1 is not None):\n",
    "        cols.append(J_b1)                          # (n, H)\n",
    "    if include_b2 and (J_b2 is not None):\n",
    "        cols.append(J_b2.reshape(-1, 1))           # (n, 1)\n",
    "    return np.concatenate(cols, axis=1) if len(cols) > 1 else cols[0]  # (n, r)\n",
    "\n",
    "# ----- Woodbury apply: returns Σ_y^{-1} B without forming Σ_y -----\n",
    "def woodbury_apply(U, sigma2, B):\n",
    "    # U: (n, r), B: (n,) or (n, k)\n",
    "    n = U.shape[0]\n",
    "    B = B.reshape(n, -1)  # (n, k)\n",
    "    inv_sigma2 = 1.0 / sigma2\n",
    "    UtU = U.T @ U                        # (r, r)\n",
    "    A = np.eye(UtU.shape[0]) + inv_sigma2 * UtU\n",
    "    # solve A X = inv_sigma2 * U^T B for X\n",
    "    RHS = inv_sigma2 * (U.T @ B)         # (r, k)\n",
    "    X = np.linalg.solve(A, RHS)          # (r, k)\n",
    "    out = inv_sigma2 * (B - U @ X)       # (n, k)\n",
    "    return out if out.shape[1] > 1 else out.ravel()\n",
    "\n",
    "# ----- Fast mean with biases via Woodbury + your precomputed (R, P) -----\n",
    "def compute_linearized_mean_fast(\n",
    "    X, y,\n",
    "    W_1, b_1, W_2, b_2,\n",
    "    noise_all, tau_w_all, tau_v_all,\n",
    "    lambda_all,\n",
    "    R_all=None,                 # optional: (D, N, N) with R=(P+S)^{-1}P\n",
    "    shrink_PS_all=None,         # optional: (D, N, N) with (P+S)^{-1}S\n",
    "    P_all=None,            # (D, N) diagonal of P\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    "    return_mats=False,\n",
    "    D_lim = None\n",
    "):\n",
    "    D, H, p = W_1.shape\n",
    "    if D_lim is not None:\n",
    "        D=D_lim\n",
    "    else:\n",
    "        pass\n",
    "    N = H * p\n",
    "    n = y.shape[0]\n",
    "    y = np.asarray(y, float).reshape(n)\n",
    "\n",
    "    w_bar_stack = np.empty((D, N))\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "\n",
    "    for d in range(D):\n",
    "        Phi_mat, JW, Jb1, Jb2 = build_hidden_and_jacobian_W(\n",
    "            X, W_1[d], b_1[d], W_2[d], activation=activation\n",
    "        )\n",
    "\n",
    "        U = build_U(\n",
    "            Phi_mat, tau_v_all[d],\n",
    "            J_b1=Jb1, J_b2=Jb2,\n",
    "            include_b1=include_b1_in_Sigma,\n",
    "            include_b2=include_b2_in_Sigma,\n",
    "        )\n",
    "\n",
    "        w0_vec = W_1[d].reshape(-1)\n",
    "        y_star = y + (JW @ w0_vec) + (Jb1 @ b_1[d])\n",
    "        \n",
    "        Sigma_y = build_Sigma_y(Phi_mat,\n",
    "            tau_v=tau_v_all[d],\n",
    "            noise=noise_all[d],\n",
    "            J_b1=Jb1,\n",
    "            J_b2=Jb2,\n",
    "            include_b1=include_b1_in_Sigma,\n",
    "            include_b2=include_b2_in_Sigma\n",
    "        )\n",
    "        r_vec = np.linalg.solve(Sigma_y, y_star) \n",
    "        #r_vec = woodbury_apply(U, noise_all[d]**2, y_star)\n",
    "        g = JW.T @ r_vec\n",
    "        # z = P^{-1} g\n",
    "        if P_all is not None:\n",
    "            P_diag_all = np.diag(P_all[d])\n",
    "            z = g / P_diag_all[d]\n",
    "        else:\n",
    "            P = build_P_from_lambda_tau(lambda_all[d], tau_w=tau_w_all[d])\n",
    "            z = g / np.diag(P)\n",
    "\n",
    "        if R_all is not None:\n",
    "            bar_w = R_all[d] @ z\n",
    "            if return_mats:\n",
    "                R_stack[d] = R_all[d]\n",
    "        elif shrink_PS_all is not None:\n",
    "            # R z = z - shrink_PS z  (no need to build R)\n",
    "            bar_w = z - (shrink_PS_all[d] @ z)\n",
    "            if return_mats:\n",
    "                # If caller really wants R back, construct it efficiently:\n",
    "                R_tmp = -shrink_PS_all[d].copy()\n",
    "                np.fill_diagonal(R_tmp, 1.0 + np.diag(R_tmp))\n",
    "                R_stack[d] = R_tmp\n",
    "        else:\n",
    "            # Fallback (slow): compute R from scratch\n",
    "            P = build_P_from_lambda_tau(lambda_all[d], tau_w=tau_w_all[d])\n",
    "            S = build_S(JW, U @ U.T + (noise_all[d]**2) * np.eye(n))\n",
    "            R = shrinkage_matrix_stable(P, S, want_R=True)  # make your helper return R directly\n",
    "            bar_w = R @ z\n",
    "            if return_mats:\n",
    "                R_stack[d] = R\n",
    "\n",
    "        w_bar_stack[d] = bar_w\n",
    "\n",
    "    return (R_stack if return_mats else None), w_bar_stack\n",
    "\n",
    "\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(tanh_fit, model='Gaussian tanh')\n",
    "# Gaussian\n",
    "_, w_bar_gauss = compute_linearized_mean_fast(\n",
    "    X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde,\n",
    "    shrink_PS_all=shrink_PS_gauss, P_all=None, activation=\"tanh\", D_lim=100\n",
    ")\n",
    "\n",
    "print(\"Done Gauss\")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(tanh_fit, model='Regularized Horseshoe tanh')\n",
    "# RHS\n",
    "_, w_bar_RHS = compute_linearized_mean_fast(\n",
    "    X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde,\n",
    "    shrink_PS_all=shrink_PS_RHS, P_all=None, activation=\"tanh\", D_lim=100\n",
    ")\n",
    "\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(tanh_fit, model='Dirichlet Horseshoe tanh')\n",
    "# DHS\n",
    "_, w_bar_DHS = compute_linearized_mean_fast(\n",
    "    X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde,\n",
    "    shrink_PS_all=shrink_PS_DHS, P_all=None, activation=\"tanh\", D_lim=100\n",
    ")\n",
    "\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(tanh_fit, model='Dirichlet Student T tanh')\n",
    "\n",
    "# DST\n",
    "_, w_bar_DST = compute_linearized_mean_fast(\n",
    "    X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde,\n",
    "    shrink_PS_all=shrink_PS_DST, P_all=None, activation=\"tanh\", D_lim=100\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_compare(W_all, v_all, w_bar_stack, sort_key=\"abs_v\"):\n",
    "    \"\"\"\n",
    "    Align signs & permutations across draws before comparing linearized mean with posterior mean.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    W_all        : array-like, shape (D, H, p) or (D, p, H) or with stray singleton dims.\n",
    "    v_all        : array-like, shape (D, H) or (D, H, 1) or similar (length H per draw).\n",
    "    w_bar_stack  : array-like, shape (D, H*p) OR (D, H, p) OR (D, 1, H*p), etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W_fix        : (D, H, p)   sign/permutation aligned\n",
    "    v_fix        : (D, H)\n",
    "    wbar_fix     : (D, H, p)\n",
    "    summary      : dict with RMSE, Corr, CosSim, SignAgree (means vs means in aligned basis)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    W_all = np.asarray(W_all)\n",
    "    v_all = np.asarray(v_all)\n",
    "    w_bar_stack = np.asarray(w_bar_stack)\n",
    "\n",
    "    D = W_all.shape[0]\n",
    "\n",
    "    # --- infer H from v (source of truth) ---\n",
    "    v0 = np.squeeze(v_all[0]).ravel()\n",
    "    H = v0.size\n",
    "    if H == 0:\n",
    "        raise ValueError(\"v_all[0] seems empty; cannot infer H.\")\n",
    "    # infer p from w_bar_stack length\n",
    "    wb0 = np.squeeze(w_bar_stack[0]).ravel()\n",
    "    if wb0.size % H != 0:\n",
    "        # fallback: try infer p from W_all[0] after squeezing\n",
    "        W0 = np.squeeze(W_all[0])\n",
    "        if W0.ndim != 2:\n",
    "            # try to drop any singleton dims\n",
    "            W0 = W0.reshape([s for s in W0.shape if s != 1])\n",
    "        if W0.ndim != 2:\n",
    "            raise ValueError(f\"Cannot infer (H,p). v length={H}, but w_bar_stack[0] has {wb0.size} elems \"\n",
    "                             f\"and W_all[0] has shape {np.squeeze(W_all[0]).shape}.\")\n",
    "        h, p_candidate = W0.shape\n",
    "        if h != H and p_candidate == H:\n",
    "            p = h\n",
    "        else:\n",
    "            p = p_candidate\n",
    "    else:\n",
    "        p = wb0.size // H\n",
    "\n",
    "    N = H * p\n",
    "\n",
    "    # alloc outputs\n",
    "    W_fix = np.empty((D, H, p), dtype=float)\n",
    "    v_fix = np.empty((D, H), dtype=float)\n",
    "    wbar_fix = np.empty((D, H, p), dtype=float)\n",
    "\n",
    "    def coerce_W(Wd, H, p):\n",
    "        \"\"\"Return Wd as (H,p). Accepts (H,p), (p,H), or with singleton dims.\"\"\"\n",
    "        A = np.asarray(Wd, dtype=float)\n",
    "        A = np.squeeze(A)\n",
    "        if A.ndim == 2:\n",
    "            h, q = A.shape\n",
    "            if h == H and q == p:\n",
    "                return A\n",
    "            if h == p and q == H:\n",
    "                return A.T\n",
    "            # If one matches H, try reshape to (H, -1)\n",
    "            if h == H and h*q == H*p:\n",
    "                return A.reshape(H, p)\n",
    "            if q == H and h*q == H*p:\n",
    "                return A.T.reshape(H, p)\n",
    "            raise ValueError(f\"Cannot coerce W of shape {A.shape} to (H,p)=({H},{p}).\")\n",
    "        elif A.ndim == 3 and 1 in A.shape:\n",
    "            # squeeze singleton and recurse\n",
    "            return coerce_W(np.squeeze(A), H, p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected W ndim={A.ndim}, shape={A.shape}\")\n",
    "\n",
    "    def coerce_v(vd, H):\n",
    "        \"\"\"Return vd as (H,)\"\"\"\n",
    "        v = np.asarray(vd, dtype=float).squeeze().ravel()\n",
    "        if v.size != H:\n",
    "            raise ValueError(f\"v has size {v.size}, expected H={H}.\")\n",
    "        return v\n",
    "\n",
    "    def coerce_wbar_row(wbd, H, p):\n",
    "        \"\"\"Return wbar row as (H,p) from (N,) or already (H,p).\"\"\"\n",
    "        w = np.asarray(wbd, dtype=float).squeeze().ravel()\n",
    "        if w.size == H * p:\n",
    "            return w.reshape(H, p)\n",
    "        # already 2D?\n",
    "        W2 = np.asarray(wbd, dtype=float).squeeze()\n",
    "        if W2.ndim == 2 and W2.shape == (H, p):\n",
    "            return W2\n",
    "        raise ValueError(f\"w_bar row has {w.size} elems but H*p={H*p} and not (H,p).\")\n",
    "\n",
    "    for d in range(D):\n",
    "        # coerce shapes\n",
    "        Wd = coerce_W(W_all[d], H, p)          # (H,p)\n",
    "        vd = coerce_v(v_all[d], H)             # (H,)\n",
    "        wbd = coerce_wbar_row(w_bar_stack[d], H, p)\n",
    "\n",
    "        # 1) sign fix so v >= 0\n",
    "        s = np.sign(vd)\n",
    "        s[s == 0.0] = 1.0\n",
    "        Wd = Wd * s[:, None]\n",
    "        wbd = wbd * s[:, None]\n",
    "        vd = np.abs(vd)\n",
    "\n",
    "        # 2) permute units by a stable key\n",
    "        if sort_key == \"abs_v\":\n",
    "            idx = np.argsort(-vd)  # descending |v|\n",
    "        elif sort_key == \"abs_v_times_rownorm\":\n",
    "            idx = np.argsort(-(vd * np.linalg.norm(Wd, axis=1)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sort_key: {sort_key}\")\n",
    "\n",
    "        W_fix[d] = Wd[idx]\n",
    "        wbar_fix[d] = wbd[idx]\n",
    "        v_fix[d] = vd[idx]\n",
    "\n",
    "    # Compare means in aligned basis\n",
    "    w_post_mean = W_fix.reshape(D, -1).mean(axis=0)   # (N,)\n",
    "    w_lin_mean  = wbar_fix.reshape(D, -1).mean(axis=0)\n",
    "\n",
    "    rmse = float(np.sqrt(np.mean((w_lin_mean - w_post_mean)**2)))\n",
    "    corr = float(np.corrcoef(w_lin_mean, w_post_mean)[0, 1])\n",
    "    cos  = float(np.dot(w_lin_mean, w_post_mean) /\n",
    "                 (np.linalg.norm(w_lin_mean) * np.linalg.norm(w_post_mean)))\n",
    "    sign_agree = float(np.mean(np.sign(w_lin_mean) == np.sign(w_post_mean)))\n",
    "\n",
    "    summary = dict(RMSE=rmse, Corr=corr, CosSim=cos, SignAgree=sign_agree,\n",
    "                   H=H, p=p, N=N)\n",
    "    return W_fix, v_fix, wbar_fix, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: pick a \"MAP-like\" representative draw and plot MAP vs. \\bar{w} ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def select_map_like_index(W_fix: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Returns the index of the draw whose aligned W is closest (in Frobenius norm)\n",
    "    to the aligned posterior mean -- a robust MAP/medoid proxy.\n",
    "    W_fix: (D, H, p) aligned weights (output of align_and_compare)\n",
    "    \"\"\"\n",
    "    D = W_fix.shape[0]\n",
    "    mu = W_fix.reshape(D, -1).mean(axis=0)  # posterior mean in aligned basis\n",
    "    diffs = W_fix.reshape(D, -1) - mu[None, :]\n",
    "    d2 = np.einsum('di,di->d', diffs, diffs)  # squared distances\n",
    "    return int(np.argmin(d2))\n",
    "\n",
    "def plot_map_vs_barw(W_fix: np.ndarray, wbar_fix: np.ndarray, title: str = \"\", alpha=0.7):\n",
    "    \"\"\"\n",
    "    Overlay scatter: MAP-like draw's W (dots) vs the same draw's \\bar{w} (crosses).\n",
    "    Both arrays must be aligned: (D, H, p). We auto-pick a representative draw.\n",
    "    \"\"\"\n",
    "    D, H, p = W_fix.shape\n",
    "    idx = select_map_like_index(W_fix)  # representative draw\n",
    "    w_map = W_fix[idx].reshape(-1)\n",
    "    w_bar = wbar_fix[idx].reshape(-1)\n",
    "    \n",
    "    eps = 1e-1                          # Small threshold to see non-zero weights\n",
    "\n",
    "    x = np.arange(1, H*p + 1)\n",
    "    plt.figure(figsize=(10, 3.5), dpi=150)\n",
    "    plt.scatter(x, w_map, s=12, marker='o', label=\"MAP-like $w$\", alpha=alpha)\n",
    "    plt.scatter(x, w_bar, s=18, marker='x', label=r\"Linearized $\\bar{w}$\", alpha=alpha)\n",
    "\n",
    "    # light vertical guides between hidden units\n",
    "    for h in range(1, H):\n",
    "        plt.axvline(h*p + 0.5, color='0.85', lw=1, zorder=0)\n",
    "    \n",
    "    plt.axhline(eps, color='0.85', lw=1, zorder=0)\n",
    "    plt.axhline(-eps, color='0.85', lw=1, zorder=0)\n",
    "\n",
    "    plt.xlabel(\"parameter index (after alignment)\")\n",
    "    plt.ylabel(\"value\")\n",
    "    plt.title(title if title else \"MAP-like $w$ vs linearized $\\~w$\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_all_gauss = tanh_fit['Gaussian tanh']['posterior'].stan_variable(\"W_1\")[:100]\n",
    "v_all_gauss = tanh_fit['Gaussian tanh']['posterior'].stan_variable(\"W_L\")[:100]\n",
    "\n",
    "W_all_RHS = tanh_fit['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_1\")[:100]\n",
    "v_all_RHS = tanh_fit['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_L\")[:100]\n",
    "\n",
    "W_all_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_1\")[:100]\n",
    "v_all_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_L\")[:100]\n",
    "\n",
    "W_all_DST = tanh_fit['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_1\")[:100]\n",
    "v_all_DST = tanh_fit['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_L\")[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gaussian: align and plot ---\n",
    "W_fix_g, v_fix_g, wbar_fix_g, summary_g = align_and_compare(W_all_gauss, v_all_gauss, w_bar_gauss, sort_key=\"abs_v\")\n",
    "print(\"Gaussian summary:\", summary_g)\n",
    "plot_map_vs_barw(W_fix_g, wbar_fix_g, title=\"Gaussian prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Regularized Horseshoe: align and plot ---\n",
    "W_fix_r, v_fix_r, wbar_fix_r, summary_r = align_and_compare(W_all_RHS, v_all_RHS, w_bar_RHS, sort_key=\"abs_v\")\n",
    "print(\"RHS summary:\", summary_r)\n",
    "plot_map_vs_barw(W_fix_r, wbar_fix_r, title=\"RHS prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dirichlet Horseshoe & Dirichlet Student-t: align and plot ---\n",
    "W_fix_dhs, v_fix_dhs, wbar_fix_dhs, summary_dhs = align_and_compare(W_all_DHS, v_all_DHS, w_bar_DHS, sort_key=\"abs_v\")\n",
    "print(\"DHS summary:\", summary_dhs)\n",
    "plot_map_vs_barw(W_fix_dhs, wbar_fix_dhs, title=\"DHS prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_fix_dst, v_fix_dst, wbar_fix_dst, summary_dst = align_and_compare(W_all_DST, v_all_DST, w_bar_DST, sort_key=\"abs_v\")\n",
    "print(\"DST summary:\", summary_dst)\n",
    "plot_map_vs_barw(W_fix_dst, wbar_fix_dst, title=\"DST prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
