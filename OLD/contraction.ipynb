{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/contraction\"\n",
    "results_dir = \"results_contraction\"\n",
    "model_names = [\"Dirichlet Horseshoe\", \"Dirichlet Horseshoe full\"]\n",
    "\n",
    "all_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # â†’ \"config_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    print(full_config_path)\n",
    "    print(base_config_name)\n",
    "    fits = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir,\n",
    "        models=model_names,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    all_fits[base_config_name] = fits  # use clean key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_N_sigma(seed):\n",
    "    if seed == 1:\n",
    "        N=25\n",
    "    elif seed == 2:\n",
    "        N=50\n",
    "    elif seed == 3:\n",
    "        N=100\n",
    "    elif seed == 4:\n",
    "        N=200\n",
    "    sigma=1.0\n",
    "    return N, sigma\n",
    "\n",
    "def compute_rmse_results(seeds, models, all_fits, get_N_sigma):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'INT_N{N}_p8_sigma{sigma:.2f}_seed{seed}'\n",
    "        path = f\"datasets/contraction/{dataset_key}.npz\"\n",
    "\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                output_test = fit.stan_variable(\"output_test\")  # (S, N/5, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = output_test.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "\n",
    "            for i in range(S):\n",
    "                y_hat = output_test[i]\n",
    "                rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test.squeeze()) ** 2))\n",
    "                \n",
    "            posterior_mean = np.mean(output_test, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean.squeeze() - y_test.squeeze()) ** 2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 2, 3, 4]\n",
    "\n",
    "\n",
    "df_rmse, df_posterior_rmse = compute_rmse_results(\n",
    "    seeds, model_names, all_fits, get_N_sigma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the order for plotting\n",
    "model_order = [\"Dirichlet Horseshoe\", \"Dirichlet Horseshoe full\"]\n",
    "N_order = [25, 50, 100, 200]  # If you only have 25, 50, 100, adjust this\n",
    "\n",
    "# Filter to include only sigma = 1.0 (as before)\n",
    "df_plot = df_rmse[df_rmse[\"sigma\"] == 1.0].copy()\n",
    "\n",
    "# Optional: Only keep N values you actually have data for\n",
    "df_plot = df_plot[df_plot[\"N\"].isin([25, 50, 100, 200])]  # Add 200 if available\n",
    "\n",
    "# Create a composite x-axis grouping\n",
    "df_plot[\"N_str\"] = df_plot[\"N\"].astype(str)  # Make sure it's string for seaborn\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df_plot,\n",
    "    x=\"N_str\",\n",
    "    y=\"rmse\",\n",
    "    hue=\"model\",\n",
    "    hue_order=model_order,\n",
    "    order=[str(n) for n in N_order if n in df_plot[\"N\"].unique()],\n",
    ")\n",
    "\n",
    "plt.title(\"RMSE by Sample Size N and Model\")\n",
    "plt.xlabel(\"Sample Size N\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.ylim(0, 8)\n",
    "plt.grid(True, axis='y')\n",
    "\n",
    "plt.legend(title=\"Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1. RMSE Computation Function\n",
    "# -----------------------------\n",
    "\n",
    "def compute_rmse_results(seeds, N_vals, models, all_fits, sigma=1.0, M_func=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N = N_vals[seed]\n",
    "        dataset_key = f'INT_N{N}_p8_sigma{sigma:.2f}_seed{seed}'\n",
    "\n",
    "        try:\n",
    "            data = np.load(f\"datasets/contraction/{dataset_key}.npz\")\n",
    "            y_test = data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] Missing data: {dataset_key}\")\n",
    "            continue\n",
    "\n",
    "        M = M_func(N) if M_func else 1.0\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                output_test = fit.stan_variable(\"output_test\")\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Missing model/posterior: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = output_test.shape[0]\n",
    "            for i in range(S):\n",
    "                rmse = np.sqrt(np.mean((output_test[i].squeeze() - y_test.squeeze())**2)) / M\n",
    "                results.append({\n",
    "                    'seed': seed, 'N': N, 'model': model, 'rmse': rmse\n",
    "                })\n",
    "\n",
    "            posterior_mean = output_test.mean(axis=0)\n",
    "            pm_rmse = np.sqrt(np.mean((posterior_mean.squeeze() - y_test.squeeze())**2)) / M\n",
    "            posterior_means.append({\n",
    "                'seed': seed, 'N': N, 'model': model, 'posterior_mean_rmse': pm_rmse\n",
    "            })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_post = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_post\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Plotting Function\n",
    "# -----------------------------\n",
    "\n",
    "def plot_contraction(df_post, models, alpha=1.0, p=8, M_func=None):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "    Ns = sorted(df_post['N'].unique())\n",
    "\n",
    "    for model in models:\n",
    "        dfm = df_post[df_post['model'] == model]\n",
    "        means = dfm.groupby('N')['posterior_mean_rmse'].mean()\n",
    "        stds = dfm.groupby('N')['posterior_mean_rmse'].std()\n",
    "\n",
    "        ax.errorbar(means.index, means, yerr=stds, label=model, marker='o', capsize=3)\n",
    "\n",
    "    # Plot theoretical contraction curve\n",
    "    slope = -alpha / (2 * alpha + p)\n",
    "    ref_N = Ns[0]\n",
    "    ref_val = df_post[df_post['N'] == ref_N]['posterior_mean_rmse'].mean()\n",
    "\n",
    "    theory_x = np.array(Ns)\n",
    "    theory_y = ref_val * (theory_x / ref_N) ** slope\n",
    "\n",
    "    ax.plot(theory_x, theory_y, '--', color='gray', label=f'Theory: slope={slope:.2f}')\n",
    "\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Sample Size $N$ (log)')\n",
    "    ax.set_ylabel('Scaled Posterior Mean RMSE (log)')\n",
    "    ax.set_title('Empirical Posterior Contraction vs. Theoretical Rate')\n",
    "    ax.legend()\n",
    "    ax.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Example Setup & Execution\n",
    "# -----------------------------\n",
    "\n",
    "# Example inputs (adjust these to your context):\n",
    "N_vals = {1: 25, 2: 50, 3: 100, 4: 200, 5: 400}\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "models = [\"Dirichlet Horseshoe\", \"Dirichlet Horseshoe full\"]\n",
    "sigma = 1.0\n",
    "alpha = 1.0\n",
    "p = 8\n",
    "\n",
    "# Optional: define theoretical epsilon_n scaling\n",
    "def epsilon_n(N):\n",
    "    return N ** (-alpha / (2 * alpha + p))\n",
    "\n",
    "# Assume you have thi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute results:\n",
    "df_rmse, df_post = compute_rmse_results(\n",
    "    seeds=seeds,\n",
    "    N_vals=N_vals,\n",
    "    models=models,\n",
    "    all_fits=all_fits,\n",
    "    sigma=sigma,\n",
    "    M_func=epsilon_n  # Optional scaling\n",
    ")\n",
    "\n",
    "# Plot results:\n",
    "fig = plot_contraction(df_post, models=models, alpha=alpha, p=p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    if seed == 1:\n",
    "        N=25\n",
    "    elif seed == 2:\n",
    "        N=50\n",
    "    elif seed == 3:\n",
    "        N=100\n",
    "    elif seed == 4:\n",
    "        N=200\n",
    "    sigma=1.0\n",
    "    return N, sigma\n",
    "\n",
    "\n",
    "def get_all_convergence_diagnostics(all_fits):\n",
    "    diagnostics = []\n",
    "\n",
    "    for config_name, model_fits in all_fits.items():\n",
    "        for model_name, fit in model_fits.items():\n",
    "            try:\n",
    "                idata = az.from_cmdstanpy(fit['posterior'])\n",
    "                y_pred = fit['posterior'].stan_variable('output_test')\n",
    "                \n",
    "                path = f'datasets/contraction/{config_name}.npz'\n",
    "                try:\n",
    "                    data = np.load(path)\n",
    "                    y_test = data[\"y_test\"]\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"[SKIP] File not found: {path}\")\n",
    "                    continue\n",
    "                \n",
    "                S = y_pred.shape[0]\n",
    "                rmses = np.zeros(S)\n",
    "                for i in range(S):\n",
    "                   rmses[i] = np.sqrt(np.mean((y_pred[i].squeeze() - y_test.squeeze()) ** 2))\n",
    "\n",
    "                summary = az.summary(idata, var_names=[\"output\"], round_to=3)\n",
    "                \n",
    "                rhat = summary[\"r_hat\"]\n",
    "\n",
    "                ess_bulk = summary[\"ess_bulk\"]\n",
    "                ess_tail = summary[\"ess_tail\"]\n",
    "                \n",
    "                try:\n",
    "                    seed = int(config_name.split(\"_seed\")[-1])\n",
    "                    N, sigma = get_N_sigma(seed)\n",
    "                except:\n",
    "                    N, sigma = np.nan, np.nan\n",
    "\n",
    "                diagnostics.append({\n",
    "                    #\"config\": config_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"max_rhat\": rhat.max(),\n",
    "                    \"median_rhat\": rhat.median(),\n",
    "                    #\"p95_rhat\": rhat.quantile(0.95),\n",
    "                    \"rmse\": np.mean(rmses, axis=0),\n",
    "                    #\"min_ess_bulk\": ess_bulk.min(),\n",
    "                    \"median_ess_bulk\": ess_bulk.median(),\n",
    "                    #\"p05_ess_bulk\": ess_bulk.quantile(0.05),\n",
    "                    #\"min_ess_tail\": ess_tail.min(),\n",
    "                    \"median_ess_tail\": ess_tail.median(),\n",
    "                    \"N\": N,\n",
    "                    \"sigma\": sigma,\n",
    "                    #\"p05_ess_tail\": ess_tail.quantile(0.05),\n",
    "                    #\"n_divergent\": divergences\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                diagnostics.append({\n",
    "                    #\"config\": config_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"max_rhat\": np.nan,\n",
    "                    \"median_rhat\": np.nan,\n",
    "                    \"p95_rhat\": np.nan,\n",
    "                    \"min_ess_bulk\": np.nan,\n",
    "                    \"min_ess_tail\": np.nan,\n",
    "                    #\"n_divergent\": np.nan,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(diagnostics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_diagostic = get_all_convergence_diagnostics(all_fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_grouped = relu_diagostic.assign(row_index=lambda df: df.index) \\\n",
    "    .sort_values([\"model\", \"N\"]) \\\n",
    "    .drop(columns=\"row_index\") \\\n",
    "    .reset_index(drop=True)\n",
    "    \n",
    "relu_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-6*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
