{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = 1\n",
    "data_dir = f\"datasets/interactions\"\n",
    "results_dir_relu = \"results_relu_interaction\"\n",
    "results_dir_tanh = \"results_tanh_interaction\"\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "relu_fits = {}\n",
    "tanh_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # â†’ \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_N_sigma(seed):\n",
    "    N = 100 if seed in [1, 2, 3, 4] else 200\n",
    "    sigma = 1.0 if seed in [1, 2, 7, 8] else 3.0\n",
    "    if seed == 19:\n",
    "        N, sigma = 1000, 1.0\n",
    "    return N, sigma\n",
    "\n",
    "def compute_rmse_results(seeds, models, all_fits, get_N_sigma):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'INT_N{N}_p8_sigma{sigma:.2f}_seed{seed}'\n",
    "        path = f\"datasets/interactions/{dataset_key}.npz\"\n",
    "\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                output_test = fit.stan_variable(\"output_test\")  # (S, N/5, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = output_test.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "\n",
    "            for i in range(S):\n",
    "                y_hat = output_test[i]\n",
    "                rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test.squeeze()) ** 2))\n",
    "                \n",
    "            posterior_mean = np.mean(output_test, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean.squeeze() - y_test.squeeze()) ** 2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 2, 3, 4, 7, 8, 9, 10]#, 19]\n",
    "\n",
    "\n",
    "df_rmse_relu, df_posterior_rmse_relu = compute_rmse_results(\n",
    "    seeds, model_names_relu, relu_fits, get_N_sigma\n",
    ")\n",
    "\n",
    "\n",
    "df_rmse_tanh, df_posterior_rmse_tanh = compute_rmse_results(\n",
    "    seeds, model_names_tanh, tanh_fits, get_N_sigma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rmse_tanh[df_rmse_tanh['model'] == 'Dirichlet Horseshoe tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rmse_tanh[df_rmse_tanh['model'] == 'Regularized Horseshoe tanh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gauss = df_rmse_relu[df_rmse_relu['model'] == 'Gaussian']\n",
    "rmse_gauss = df_gauss[df_gauss['seed'] == 10]['rmse'].mean()\n",
    "\n",
    "df_rhs = df_rmse_relu[df_rmse_relu['model'] == 'Regularized Horseshoe']\n",
    "rmse_rhs = df_rhs[df_rhs['seed'] == 10]['rmse'].mean()\n",
    "\n",
    "df_dhs = df_rmse_relu[df_rmse_relu['model'] == 'Dirichlet Horseshoe']\n",
    "rmse_dhs = df_dhs[df_dhs['seed'] == 10]['rmse'].mean()\n",
    "\n",
    "df_dst = df_rmse_relu[df_rmse_relu['model'] == 'Dirichlet Student T']\n",
    "rmse_dst = df_dst[df_dst['seed'] == 10]['rmse'].mean()\n",
    "\n",
    "\n",
    "print(f\"RMSE Gaussian: {rmse_gauss:.3f}\")\n",
    "print(f\"RMSE Regularized Horseshoe: {rmse_rhs:.3f}\")\n",
    "print(f\"RMSE Dirichlet Horseshoe: {rmse_dhs:.3f}\")\n",
    "print(f\"RMSE Dirichlet Student T: {rmse_dst:.3f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "# Combine and tag activation\n",
    "df_relu = df_rmse_relu.copy()\n",
    "df_relu[\"activation\"] = \"ReLU\"\n",
    "\n",
    "df_tanh = df_rmse_tanh.copy()\n",
    "df_tanh[\"activation\"] = \"tanh\"\n",
    "\n",
    "df_all = pd.concat([df_relu, df_tanh])\n",
    "df_all[\"model\"] = df_all[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# Order of models and activations\n",
    "model_order = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "activation_order = [\"ReLU\", \"tanh\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 7), sharey=False)\n",
    "\n",
    "for i, N_val in enumerate([100, 200]):\n",
    "    for j, sigma_val in enumerate([1.0, 3.0]):\n",
    "        ax = axes[j, i]\n",
    "        df_plot = df_all[(df_all[\"N\"] == N_val) & (df_all[\"sigma\"] == sigma_val)].copy()\n",
    "\n",
    "        # Use model as x, activation as hue\n",
    "        sns.boxplot(\n",
    "            data=df_plot,\n",
    "            x=\"model\",\n",
    "            y=\"rmse\",\n",
    "            hue=\"activation\",\n",
    "            order=model_order,\n",
    "            hue_order=activation_order,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"N = {N_val}, Sigma = {sigma_val}\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"RMSE\")\n",
    "        if sigma_val == 1.0:\n",
    "            ax.set_ylim(0, 8)\n",
    "        else:\n",
    "            ax.set_ylim(0, 12)\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Only show legend on top left plot\n",
    "        if i != 0 or j != 0:\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "# Add shared legend at top center\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, title=\"Activation\", loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVERGENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    N = 100 if seed in [1, 2, 3, 4] else 200\n",
    "    sigma = 1.0 if seed in [1, 2, 7, 8] else 3.0\n",
    "    if seed == 19:\n",
    "        N, sigma = 1000, 1.0\n",
    "    return N, sigma\n",
    "\n",
    "def get_all_convergence_diagnostics(all_fits):\n",
    "    diagnostics = []\n",
    "\n",
    "    for config_name, model_fits in all_fits.items():\n",
    "        for model_name, fit in model_fits.items():\n",
    "            try:\n",
    "                idata = az.from_cmdstanpy(fit['posterior'])\n",
    "                y_pred = fit['posterior'].stan_variable('output_test')\n",
    "                \n",
    "                path = f'datasets/interactions/{config_name}.npz'\n",
    "                try:\n",
    "                    data = np.load(path)\n",
    "                    y_test = data[\"y_test\"]\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"[SKIP] File not found: {path}\")\n",
    "                    continue\n",
    "                \n",
    "                S = y_pred.shape[0]\n",
    "                rmses = np.zeros(S)\n",
    "                for i in range(S):\n",
    "                   rmses[i] = np.sqrt(np.mean((y_pred.squeeze() - y_test.squeeze()) ** 2))\n",
    "\n",
    "                summary = az.summary(idata, var_names=[\"output\"], round_to=3)\n",
    "                \n",
    "                rhat = summary[\"r_hat\"]\n",
    "\n",
    "                ess_bulk = summary[\"ess_bulk\"]\n",
    "                ess_tail = summary[\"ess_tail\"]\n",
    "                \n",
    "                try:\n",
    "                    seed = int(config_name.split(\"_seed\")[-1])\n",
    "                    N, sigma = get_N_sigma(seed)\n",
    "                except:\n",
    "                    N, sigma = np.nan, np.nan\n",
    "\n",
    "                diagnostics.append({\n",
    "                    #\"config\": config_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"max_rhat\": rhat.max(),\n",
    "                    \"median_rhat\": rhat.median(),\n",
    "                    #\"p95_rhat\": rhat.quantile(0.95),\n",
    "                    \"rmse\": np.mean(rmses, axis=0),\n",
    "                    #\"min_ess_bulk\": ess_bulk.min(),\n",
    "                    \"median_ess_bulk\": ess_bulk.median(),\n",
    "                    #\"p05_ess_bulk\": ess_bulk.quantile(0.05),\n",
    "                    #\"min_ess_tail\": ess_tail.min(),\n",
    "                    \"median_ess_tail\": ess_tail.median(),\n",
    "                    \"N\": N,\n",
    "                    \"sigma\": sigma,\n",
    "                    #\"p05_ess_tail\": ess_tail.quantile(0.05),\n",
    "                    #\"n_divergent\": divergences\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                diagnostics.append({\n",
    "                    #\"config\": config_name,\n",
    "                    \"model\": model_name,\n",
    "                    \"max_rhat\": np.nan,\n",
    "                    \"median_rhat\": np.nan,\n",
    "                    \"p95_rhat\": np.nan,\n",
    "                    \"min_ess_bulk\": np.nan,\n",
    "                    \"min_ess_tail\": np.nan,\n",
    "                    #\"n_divergent\": np.nan,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(diagnostics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_diagostic = get_all_convergence_diagnostics(relu_fits)\n",
    "tanh_diagostic = get_all_convergence_diagnostics(tanh_fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_grouped = relu_diagostic.assign(row_index=lambda df: df.index) \\\n",
    "    .sort_values([\"model\", \"row_index\"]) \\\n",
    "    .drop(columns=\"row_index\") \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "tanh_grouped = tanh_diagostic.assign(row_index=lambda df: df.index) \\\n",
    "    .sort_values([\"model\", \"row_index\"]) \\\n",
    "    .drop(columns=\"row_index\") \\\n",
    "    .reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_relu = relu_grouped.to_latex(index=False, float_format=\"%.3f\")\n",
    "latex_tanh = tanh_grouped.to_latex(index=False, float_format=\"%.3f\")\n",
    "#print(latex_relu)\n",
    "#print(latex_tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, remove \"tanh\" from model names in both DataFrames\n",
    "relu_diagostic = relu_diagostic.copy()\n",
    "relu_diagostic[\"model\"] = relu_diagostic[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "tanh_diagostic = tanh_diagostic.copy()\n",
    "tanh_diagostic[\"model\"] = tanh_diagostic[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# Create shared-axes figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True, sharey=True)\n",
    "\n",
    "# Plot ReLU\n",
    "sns.scatterplot(\n",
    "    data=relu_diagostic,\n",
    "    x=\"max_rhat\", y=\"rmse\",\n",
    "    hue=\"model\",\n",
    "    style=\"sigma\",\n",
    "    size=\"N\", sizes=(100, 300),\n",
    "    ax=axes[0],\n",
    "    legend=False\n",
    ")\n",
    "axes[0].set_title(\"ReLU\")\n",
    "axes[0].set_xlabel(r\"Max $\\hat{R}$\")\n",
    "axes[0].set_ylabel(\"RMSE\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot tanh and keep legend\n",
    "plot_obj = sns.scatterplot(\n",
    "    data=tanh_diagostic,\n",
    "    x=\"max_rhat\", y=\"rmse\",\n",
    "    hue=\"model\",\n",
    "    style=\"sigma\",\n",
    "    size=\"N\", sizes=(100, 300),\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(r\"$\\tanh$\")\n",
    "axes[1].set_xlabel(r\"Max $\\hat{R}$\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Extract and filter legend\n",
    "handles, labels = axes[1].get_legend_handles_labels()\n",
    "axes[1].legend_.remove()\n",
    "\n",
    "exclude_labels = {\"model\", \"N\", \"sigma\"}\n",
    "filtered = [(h, l) for h, l in zip(handles, labels) if l not in exclude_labels]\n",
    "# Filter out \"N =\" entries\n",
    "#filtered = [(h, l) for h, l in zip(handles, labels) if not l.startswith(\"N =\")]\n",
    "if filtered:\n",
    "    filtered_handles, filtered_labels = zip(*filtered)\n",
    "    #print(filtered_handles)\n",
    "    print(filtered_labels)\n",
    "    # Use fig.legend to add shared legend\n",
    "    fig.legend(\n",
    "        filtered_handles,\n",
    "        filtered_labels,\n",
    "        title=\"\",\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1),\n",
    "        ncol=4,\n",
    "    )\n",
    "\n",
    "# Layout adjustments\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.85)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARSITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_relu(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward pass for a single layer BNN.\n",
    "    \"\"\"\n",
    "    pre_act_1 = X @ W1 + b1.reshape(1, -1)\n",
    "    #pre_hidden += b1.reshape(1, -1)\n",
    "    post_act_1 = np.maximum(0, pre_act_1)\n",
    "    ouput = post_act_1 @ W2 + b2.reshape(1, -1)\n",
    "    return ouput\n",
    "\n",
    "def forward_pass_tanh(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward pass for a single layer BNN.\n",
    "    \"\"\"\n",
    "    pre_act_1 = X @ W1 + b1.reshape(1, -1)\n",
    "    #pre_hidden += b1.reshape(1, -1)\n",
    "    post_act_1 = np.tanh(pre_act_1)\n",
    "    ouput = post_act_1 @ W2 + b2.reshape(1, -1)\n",
    "    return ouput\n",
    "\n",
    "def compute_rmse_results(seeds, models, all_fits, get_N_sigma, forward_pass,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'INT_N{N}_p8_sigma{sigma:.2f}_seed{seed}'\n",
    "        path = f\"datasets/interactions/{dataset_key}.npz\"\n",
    "\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "            #print(y_test.shape)\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                W2 = W2_samples[i]\n",
    "\n",
    "                # Apply pruning mask if requested\n",
    "                if prune_fn is not None and sparsity > 0.0:\n",
    "                    masks = prune_fn([W1, W2], sparsity)\n",
    "                    W1 = W1 * masks[0]\n",
    "                    W2 = W2 * masks[1]\n",
    "\n",
    "                y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i])\n",
    "                y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "                rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "                \n",
    "            posterior_mean = np.mean(y_hats, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'sparsity': sparsity,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "seeds = [1, 2, 3, 4, 7, 8, 9, 10]\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    N = 100 if seed in [1, 2, 3, 4] else 200\n",
    "    sigma = 1.0 if seed in [1, 2, 7, 8] else 3.0\n",
    "    return N, sigma\n",
    "\n",
    "\n",
    "def global_prune_weights(weight_matrices, sparsity_level):\n",
    "    \"\"\"\n",
    "    Prune globally across multiple weight matrices.\n",
    "    \n",
    "    Args:\n",
    "        weight_matrices: List of numpy arrays (weight matrices).\n",
    "        sparsity_level: Float in [0, 1], fraction of weights to prune.\n",
    "\n",
    "    Returns:\n",
    "        List of binary masks with same shapes as weight_matrices.\n",
    "    \"\"\"\n",
    "    # Flatten all weights and concatenate\n",
    "    flat_weights = np.concatenate([w.flatten() for w in weight_matrices])\n",
    "    abs_weights = np.abs(flat_weights)\n",
    "    \n",
    "    # Determine number of weights to prune\n",
    "    total_weights = abs_weights.size\n",
    "    num_to_prune = int(np.floor(sparsity_level * total_weights))\n",
    "\n",
    "    # Get indices of smallest weights to prune\n",
    "    prune_indices = np.argpartition(abs_weights, num_to_prune)[:num_to_prune]\n",
    "    \n",
    "    # Create global mask\n",
    "    global_mask_flat = np.ones(total_weights, dtype=bool)\n",
    "    global_mask_flat[prune_indices] = False\n",
    "\n",
    "    # Split the global mask back into original shapes\n",
    "    masks = []\n",
    "    idx = 0\n",
    "    for w in weight_matrices:\n",
    "        size = w.size\n",
    "        mask = global_mask_flat[idx:idx + size].reshape(w.shape)\n",
    "        masks.append(mask.astype(float))\n",
    "        idx += size\n",
    "\n",
    "    return masks\n",
    "\n",
    "def local_prune_weights(weights, sparsity_level, index_to_prune=0):\n",
    "    \"\"\"\n",
    "    Apply pruning to only one weight matrix in a list, specified by index.\n",
    "\n",
    "    Parameters:\n",
    "    - weights: list of np.ndarray (e.g., [W1, W2])\n",
    "    - sparsity_level: fraction of weights to prune (0.0 to 1.0)\n",
    "    - index_to_prune: which weight matrix to prune in the list\n",
    "\n",
    "    Returns:\n",
    "    - list of masks (one for each weight matrix)\n",
    "    \"\"\"\n",
    "    masks = [np.ones_like(W) for W in weights]\n",
    "\n",
    "    W = weights[index_to_prune]\n",
    "    flat = np.abs(W.flatten())\n",
    "    num_to_prune = int(np.floor(sparsity_level * flat.size))\n",
    "\n",
    "    if num_to_prune > 0:\n",
    "        idx = np.argpartition(flat, num_to_prune)[:num_to_prune]\n",
    "        mask_flat = np.ones_like(flat, dtype=bool)\n",
    "        mask_flat[idx] = False\n",
    "        masks[index_to_prune] = mask_flat.reshape(W.shape).astype(float)\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse_relu, df_posterior_rmse_relu = {}, {}\n",
    "df_rmse_tanh, df_posterior_rmse_tanh = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    df_rmse_relu[sparsity], df_posterior_rmse_relu[sparsity] = compute_rmse_results(\n",
    "        seeds, model_names_relu, relu_fits, get_N_sigma, forward_pass_relu,\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    df_rmse_tanh[sparsity], df_posterior_rmse_tanh[sparsity] = compute_rmse_results(\n",
    "        seeds, model_names_tanh, tanh_fits, get_N_sigma, forward_pass_tanh,\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_rmse_full_relu = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_relu.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_rmse_full_tanh = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_tanh.items()],\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10), sharex=True, sharey=True)\n",
    "\n",
    "activation_data = [(\"ReLU\", df_rmse_full_relu), (\"tanh\", df_rmse_full_tanh)]\n",
    "\n",
    "for row_idx, (name, df) in enumerate(activation_data):\n",
    "    for col_idx, N_val in enumerate([100, 200]):\n",
    "        sigma = 1.0 if col_idx == 0 else 3.0\n",
    "        ax = axes[col_idx, row_idx]\n",
    "\n",
    "        sns.lineplot(\n",
    "            data=df[df['sigma'] == sigma],\n",
    "            x='sparsity', y='rmse',\n",
    "            hue='model', style='N', marker='o', errorbar=None, ax=ax\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"{name}, $\\sigma = {sigma}$\")\n",
    "        if col_idx == 0 or col_idx == 1:\n",
    "            ax.set_ylabel(\"RMSE\")\n",
    "        if row_idx == 0 or row_idx == 1:\n",
    "            ax.set_xlabel(\"Sparsity Level\")\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Remove individual legends\n",
    "        if ax.get_legend() is not None:\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "exclude_labels = {\"model\", \"N\"}\n",
    "filtered = [(h, l) for h, l in zip(handles, labels) if l not in exclude_labels]\n",
    "# Shared legend at bottom center\n",
    "if filtered:\n",
    "    filtered_handles, filtered_labels = zip(*filtered)\n",
    "    # Use fig.legend to add shared legend\n",
    "    fig.legend(\n",
    "        filtered_handles,\n",
    "        filtered_labels,\n",
    "        title=\"\",\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.05),\n",
    "        ncol=3,\n",
    "    )\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])  # Space at bottom for shared legend\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Clean model names\n",
    "df_rmse_full_relu = df_rmse_full_relu.copy()\n",
    "df_rmse_full_relu[\"model\"] = df_rmse_full_relu[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "df_rmse_full_tanh = df_rmse_full_tanh.copy()\n",
    "df_rmse_full_tanh[\"model\"] = df_rmse_full_tanh[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# Define consistent color palette\n",
    "custom_palette = {\n",
    "    \"Gaussian\": \"C0\",  # blue\n",
    "    \"Regularized Horseshoe\": \"C1\",  # orange\n",
    "    \"Dirichlet Horseshoe\": \"C2\",  # green\n",
    "    \"Dirichlet Student T\": \"C3\",  # red\n",
    "}\n",
    "\n",
    "# Set up plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10), sharex=True, sharey=True)\n",
    "\n",
    "activation_data = [(\"ReLU\", df_rmse_full_relu), (\"tanh\", df_rmse_full_tanh)]\n",
    "all_handles_labels = []\n",
    "\n",
    "# Plot\n",
    "for row_idx, (name, df) in enumerate(activation_data):\n",
    "    for col_idx, N_val in enumerate([100, 200]):\n",
    "        sigma = 1.0 if col_idx == 0 else 3.0\n",
    "        ax = axes[col_idx, row_idx]\n",
    "\n",
    "        plot = sns.lineplot(\n",
    "            data=df[df['sigma'] == sigma],\n",
    "            x='sparsity', y='rmse',\n",
    "            hue='model', style='N', marker='o', errorbar=None, ax=ax,\n",
    "            palette=custom_palette\n",
    "        )\n",
    "\n",
    "        # Capture legend handles before removing\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        all_handles_labels.extend(zip(handles, labels))\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "        ax.set_title(f\"{name}, $\\sigma = {sigma}$\")\n",
    "        if col_idx == 0 or col_idx == 1:\n",
    "            ax.set_ylabel(\"RMSE\")\n",
    "        if row_idx == 0 or row_idx == 1:\n",
    "            ax.set_xlabel(\"Sparsity Level\")\n",
    "        ax.grid(True)\n",
    "\n",
    "# Filter and sort legend\n",
    "legend_dict = OrderedDict()\n",
    "for handle, label in all_handles_labels:\n",
    "    if label not in {\"model\", \"N\"} and label not in legend_dict:\n",
    "        legend_dict[label] = handle\n",
    "\n",
    "desired_order = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "filtered = [(legend_dict[label], label) for label in desired_order if label in legend_dict]\n",
    "\n",
    "# Shared legend\n",
    "if filtered:\n",
    "    filtered_handles, filtered_labels = zip(*filtered)\n",
    "    fig.legend(\n",
    "        filtered_handles,\n",
    "        filtered_labels,\n",
    "        title=\"\",\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.05),\n",
    "        ncol=2,\n",
    "    )\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTIMODALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_fits['INT_N100_p8_sigma1.00_seed1']['Gaussian']['posterior'].stan_variable('output_test').squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === REQUIRED: Extract your posterior function samples as (num_samples, num_points) ===\n",
    "# Example (YOU must provide this):\n",
    "# f_matrix = relu_fits['INT_N100_p8_sigma1.00_seed1']['Gaussian']['posterior'].stan_variable('f')\n",
    "# Make sure f_matrix is a NumPy array of shape (4000, N)\n",
    "\n",
    "f_matrix = relu_fits['INT_N100_p8_sigma1.00_seed1']['Gaussian']['posterior'].stan_variable('output_test').squeeze()  # <--- REPLACE THIS WITH YOUR ACTUAL DATA\n",
    "\n",
    "# Step 1: t-SNE projection to 2D\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "f_2d = tsne.fit_transform(f_matrix)\n",
    "\n",
    "# Step 2: DBSCAN clustering to estimate modes\n",
    "db = DBSCAN(eps=2.5, min_samples=10).fit(f_2d)\n",
    "labels = db.labels_\n",
    "n_modes = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "# Step 3: Plot t-SNE with clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "palette = sns.color_palette(\"hsv\", len(set(labels)))\n",
    "for label in set(labels):\n",
    "    idx = labels == label\n",
    "    plt.scatter(f_2d[idx, 0], f_2d[idx, 1], s=10,\n",
    "                label=f'Mode {label}' if label != -1 else 'Noise', alpha=0.7)\n",
    "plt.title(f't-SNE of Posterior Samples (Estimated Modes: {n_modes})')\n",
    "plt.xlabel('Dim 1')\n",
    "plt.ylabel('Dim 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # Step 4: Density estimation over t-SNE space\n",
    "# kde = KernelDensity(bandwidth=0.5).fit(f_2d)\n",
    "# log_density = kde.score_samples(f_2d)\n",
    "\n",
    "# # Step 5: Plot KDE density landscape\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(f_2d[:, 0], f_2d[:, 1], c=log_density, cmap='viridis', s=10)\n",
    "# plt.title(\"Posterior Sample Density (KDE on t-SNE Projection)\")\n",
    "# plt.colorbar(label=\"Log Density\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_matrix = relu_fits['INT_N100_p8_sigma1.00_seed1']['Dirichlet Horseshoe']['posterior'].stan_variable('output_test').squeeze()  # <--- REPLACE THIS WITH YOUR ACTUAL DATA\n",
    "\n",
    "# Step 1: t-SNE projection to 2D\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
    "f_2d = tsne.fit_transform(f_matrix)\n",
    "\n",
    "# Step 2: DBSCAN clustering to estimate modes\n",
    "db = DBSCAN(eps=2.5, min_samples=10).fit(f_2d)\n",
    "labels = db.labels_\n",
    "n_modes = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "# Step 3: Plot t-SNE with clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "palette = sns.color_palette(\"hsv\", len(set(labels)))\n",
    "for label in set(labels):\n",
    "    idx = labels == label\n",
    "    plt.scatter(f_2d[idx, 0], f_2d[idx, 1], s=10,\n",
    "                label=f'Mode {label}' if label != -1 else 'Noise', alpha=0.7)\n",
    "plt.title(f't-SNE of Posterior Samples (Estimated Modes: {n_modes})')\n",
    "plt.xlabel('Dim 1')\n",
    "plt.ylabel('Dim 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
