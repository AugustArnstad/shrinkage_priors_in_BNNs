{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/moons/many\"\n",
    "results_dir_relu = \"results/classification/single_layer/relu/moons\"\n",
    "results_dir_tanh = \"results/classification/single_layer/tanh/moons\"\n",
    "#model_names_relu = [\"Dirichlet Horseshoe\"]\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "relu_fits = {}\n",
    "tanh_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/moons\"\n",
    "results_dir_relu = \"results/classification/single_layer/relu/moons\"\n",
    "results_dir_tanh = \"results/classification/single_layer/tanh/moons\"\n",
    "#model_names_relu = [\"Dirichlet Horseshoe\"]\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from utils.generate_data import generate_two_moons\n",
    "from utils.robust_utils import build_pytorch_model_from_stan_sample\n",
    "def evaluate_on_generated_test_set(\n",
    "    fits_dict,\n",
    "    input_dim,\n",
    "    hidden_dim,\n",
    "    output_dim,\n",
    "    N_test=1000,\n",
    "    sigma=0.2,\n",
    "    D=2,\n",
    "    seed=123,\n",
    "    sample_indices=range(0, 2000, 50),\n",
    "):\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "    _, X_test, _, y_test = generate_two_moons(N=N_test, sigma=sigma, test_size=0.99, D=D, seed=seed)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for model_name, model_entry in fits_dict.items():\n",
    "        fit = model_entry[\"posterior\"]\n",
    "\n",
    "        probs = []\n",
    "\n",
    "        for idx in sample_indices:\n",
    "            model = build_pytorch_model_from_stan_sample(\n",
    "                fit=fit,\n",
    "                sample_idx=idx,\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=output_dim,\n",
    "                task=\"classification\"\n",
    "            )\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(X_test_tensor)\n",
    "                prob = F.softmax(logits, dim=1).numpy()\n",
    "                probs.append(prob)\n",
    "\n",
    "        probs = np.stack(probs, axis=0)  # shape: (n_samples, N_test, 2)\n",
    "        mean_probs = np.mean(probs, axis=0)\n",
    "        y_pred_mode = np.argmax(mean_probs, axis=1) + 1\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred_mode)\n",
    "        nll = log_loss(y_test - 1, mean_probs)\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"nll\": nll\n",
    "        }\n",
    "    return results, X_test, y_test\n",
    "\n",
    "def extract_metadata_from_key(key):\n",
    "    \"\"\"Hent ut N og seed fra nøkkel som 'Moon_N100_p2_sigma0.20_seed1'\"\"\"\n",
    "    match = re.search(r\"N(\\d+)_p\\d+_sigma[\\d\\.]+_seed(\\d+)\", key)\n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2))\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def evaluate_all_models_on_generated_test_set(fits_dict, N_test=200, sample_indices=range(2000)):\n",
    "    all_results = []\n",
    "\n",
    "    for key, fits in fits_dict.items():\n",
    "        N, seed = extract_metadata_from_key(key)\n",
    "        if N is None:\n",
    "            continue\n",
    "\n",
    "        result, _, _ = evaluate_on_generated_test_set(\n",
    "            fits_dict=fits,\n",
    "            input_dim=2,\n",
    "            hidden_dim=16,\n",
    "            output_dim=2,\n",
    "            N_test=N_test,\n",
    "            sigma=0.2,\n",
    "            D=2,\n",
    "            seed=999,  # fast seed for fair test-set\n",
    "            sample_indices=sample_indices\n",
    "        )\n",
    "\n",
    "        for model_name, metrics in result.items():\n",
    "            all_results.append({\n",
    "                \"config\": key,\n",
    "                \"model\": model_name,\n",
    "                \"N\": N,\n",
    "                \"seed\": seed,\n",
    "                \"accuracy\": metrics[\"accuracy\"],\n",
    "                \"nll\": metrics[\"nll\"]\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = evaluate_all_models_on_generated_test_set(relu_fits, N_test=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df_results.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"accuracy\", \"mean\"),\n",
    "    acc_std=(\"accuracy\", \"std\"),\n",
    "    nll_mean=(\"nll\", \"mean\"),\n",
    "    nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary.to_latex(index=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=df_results, x=\"N\", y=\"accuracy\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "plt.title(\"Accuracy on generated test set\")\n",
    "plt.show()\n",
    "\n",
    "sns.pointplot(data=df_results, x=\"N\", y=\"nll\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "plt.title(\"Negative log-likelihood on generated test set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
