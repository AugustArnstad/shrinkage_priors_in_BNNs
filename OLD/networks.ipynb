{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = 1\n",
    "data_dir = f\"datasets/type_{data_config}\"\n",
    "results_dir_relu = \"results_relu\"\n",
    "results_dir_tanh = \"results_tanh\"\n",
    "model_names_relu = [\"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Gaussian\", \"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Gaussian tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "relu_fits = {}\n",
    "tanh_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"type_{data_config}/{base_config_name}\"  # â†’ \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def extract_posterior_means(fit, layer_structure):\n",
    "    \"\"\"\n",
    "    Extract posterior mean weights for all layers from a Stan model fit.\n",
    "\n",
    "    Parameters:\n",
    "        fit (CmdStanMCMC): Fitted Stan model containing sampled weights.\n",
    "        layer_structure (dict): Dictionary specifying parameter names and shapes.\n",
    "            Example structure:\n",
    "                {\n",
    "                    \"input_to_hidden\": {\"name\": \"data_to_hidden_mat\", \"shape\": (D, H)},\n",
    "                    \"hidden_to_output\": {\"name\": \"hidden_to_output\", \"shape\": (H, 1)},\n",
    "                    \"hidden_to_hidden\": {\n",
    "                        \"name\": \"hidden_hidden_weights\",\n",
    "                        \"shape\": [(H, H), (H, H)]\n",
    "                    }  # optional\n",
    "                }\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary mapping each parameter name to its posterior mean weight matrix.\n",
    "            Hidden-to-hidden layers are returned as a list of 2D arrays (one per layer).\n",
    "    \"\"\"\n",
    "\n",
    "    means = {}\n",
    "\n",
    "    # Input-to-hidden\n",
    "    param = layer_structure['input_to_hidden']['name']\n",
    "    shape = layer_structure['input_to_hidden']['shape']\n",
    "    means[param] = fit.stan_variable(param).mean(axis=0)#.reshape(shape)\n",
    "\n",
    "    # Optional hidden-to-hidden\n",
    "    if 'hidden_to_hidden' in layer_structure:\n",
    "        param = layer_structure['hidden_to_hidden']['name']\n",
    "        shapes = layer_structure['hidden_to_hidden']['shape']\n",
    "        raw = fit.stan_variable(param).mean(axis=0)\n",
    "        means[param] = [raw[i].reshape(shapes[i]) for i in range(len(shapes))]\n",
    "\n",
    "    # Hidden-to-output\n",
    "    param = layer_structure['hidden_to_output']['name']\n",
    "    shape = layer_structure['hidden_to_output']['shape']\n",
    "    means[param] = fit.stan_variable(param).mean(axis=0).reshape(shape)\n",
    "\n",
    "    return means\n",
    "\n",
    "def prune_weights(W, num_to_prune):\n",
    "    \"\"\"\n",
    "    Create a binary mask that keeps the largest (by absolute value) weights.\n",
    "    \"\"\"\n",
    "    flat = np.abs(W.flatten())\n",
    "    if num_to_prune == 0:\n",
    "        return np.ones_like(W)\n",
    "    idx = np.argpartition(flat, num_to_prune)[:num_to_prune]\n",
    "    mask_flat = np.ones_like(flat, dtype=bool)\n",
    "    mask_flat[idx] = False\n",
    "    return mask_flat.reshape(W.shape).astype(float)\n",
    "\n",
    "def plot_all_networks_subplots_activations(model_dicts, layer_sizes, node_activation_colors=None, activation_color_max=None, max_width=5.0, ncols=3, figsize_per_plot=(5, 4), signed_colors=False):\n",
    "    \"\"\"\n",
    "    Plot multiple neural networks as subplots, with edge thickness representing weight magnitude\n",
    "    and hidden node color intensity representing activation frequency.\n",
    "\n",
    "    Parameters:\n",
    "        model_dicts (dict): Dictionary mapping model names to weight dicts.\n",
    "                            Each weight dict must include:\n",
    "                            - 'W_1': input-to-hidden weights (P, H)\n",
    "                            - 'W_L': hidden-to-output weights (H, 1)\n",
    "                            - optionally 'W_internal': list of (H, H) hidden-hidden weights\n",
    "        layer_sizes (list[int]): List of node counts for each layer (e.g. [8, 16, 1]).\n",
    "        node_activation_colors (dict): Maps model name to array of activation frequencies for hidden nodes.\n",
    "        max_width (float): Maximum line width for strongest edge. Default is 5.0.\n",
    "        ncols (int): Number of subplot columns. Default is 3.\n",
    "        figsize_per_plot (tuple): Base figure size per subplot (width, height).\n",
    "        signed_colors (bool): If True, positive weights are red and negative weights are blue.\n",
    "\n",
    "    Returns:\n",
    "        fig, edge_widths: Matplotlib figure and list of edge widths for the last model.\n",
    "    \"\"\"\n",
    "\n",
    "    n_models = len(model_dicts)\n",
    "    nrows = int(np.ceil(n_models / ncols))\n",
    "    figsize = (figsize_per_plot[0] * ncols, figsize_per_plot[1] * nrows)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax in axes[n_models:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for idx, (title, weight_dict) in enumerate(model_dicts.items()):\n",
    "        G = nx.DiGraph()\n",
    "        pos = {}\n",
    "        node_ids_per_layer = []\n",
    "        node_colors = []\n",
    "\n",
    "        # Add nodes\n",
    "        for layer_idx, size in enumerate(layer_sizes):\n",
    "            nodes = []\n",
    "            y_coords = np.linspace(size - 1, 0, size) - (size - 1) / 2\n",
    "            for i in range(size):\n",
    "                nid = f\"L{layer_idx}_{i}\"\n",
    "                G.add_node(nid)\n",
    "                pos[nid] = (layer_idx, y_coords[i])\n",
    "                nodes.append(nid)\n",
    "\n",
    "                # Set node color\n",
    "                # if node_activation_colors and layer_idx == 1:\n",
    "                #     act_freqs = node_activation_colors.get(title, np.zeros(size))\n",
    "                #     color_val = np.clip(act_freqs[i], 0.0, 1.0)  # Ensure valid range\n",
    "                #     color = plt.cm.winter(color_val)\n",
    "                if node_activation_colors and layer_idx == 1:\n",
    "                    act_freqs = node_activation_colors.get(title, np.zeros(size))\n",
    "                    scale = activation_color_max if activation_color_max is not None else 1.0\n",
    "                    color_val = np.clip(act_freqs[i] / scale, 0.0, 1.0)  # Normalize globally\n",
    "                    color = plt.cm.winter(color_val)\n",
    "                else:\n",
    "                    color = 'lightgray'\n",
    "                node_colors.append(color)\n",
    "\n",
    "            node_ids_per_layer.append(nodes)\n",
    "\n",
    "        edge_colors = []\n",
    "        edge_widths = []\n",
    "\n",
    "        # Function to add edges from W\n",
    "        def add_edges(W, in_nodes, out_nodes):\n",
    "            for j, out_node in enumerate(out_nodes):\n",
    "                for i, in_node in enumerate(in_nodes):\n",
    "                    w = W[i, j]\n",
    "                    G.add_edge(in_node, out_node, weight=abs(w))\n",
    "                    edge_colors.append('red' if w >= 0 else 'blue')\n",
    "                    edge_widths.append(abs(w))\n",
    "\n",
    "        # Input-to-hidden\n",
    "        add_edges(weight_dict['W_1'], node_ids_per_layer[0], node_ids_per_layer[1])\n",
    "\n",
    "        # Hidden-to-hidden\n",
    "        if 'W_internal' in weight_dict:\n",
    "            for l in range(len(weight_dict['W_internal'])):\n",
    "                add_edges(weight_dict['W_internal'][l], node_ids_per_layer[l+1], node_ids_per_layer[l+2])\n",
    "\n",
    "        # Hidden-to-output\n",
    "        add_edges(weight_dict['W_L'], node_ids_per_layer[-2], node_ids_per_layer[-1])\n",
    "\n",
    "        #labels = {nid: nid for nid in G.nodes}\n",
    "        #nx.draw_networkx_labels(G, pos, labels=labels, ax=axes[idx], font_size=8)\n",
    "\n",
    "        edge_widths = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "\n",
    "        nx.draw(G, pos, ax=axes[idx], node_color=node_colors,\n",
    "                edge_color=edge_colors if signed_colors else 'red',\n",
    "                width=edge_widths, with_labels=False,\n",
    "                node_size=400, arrows=False)\n",
    "\n",
    "        axes[idx].set_title(title, fontsize=10)\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, edge_widths\n",
    "\n",
    "def get_pruned_mean_weights(fit, layer_structure, sparsity_level):\n",
    "    \"\"\"\n",
    "    Extract posterior mean weights from a single model and prune the input layer.\n",
    "\n",
    "    Parameters:\n",
    "        fit : CmdStanMCMC\n",
    "        layer_structure : dict\n",
    "            Structure of layers with param names and shapes.\n",
    "        sparsity_level : float\n",
    "            Proportion of weights to prune from input-to-hidden layer.\n",
    "\n",
    "    Returns:\n",
    "        dict: pruned mean weights for plotting.\n",
    "    \"\"\"\n",
    "    means = extract_posterior_means(fit, layer_structure)\n",
    "    # Prune input-to-hidden layer only (you can generalize if needed)\n",
    "    W1 = means['W_1']\n",
    "    P, H = W1.shape\n",
    "    num_to_prune = int(np.floor(sparsity_level * P * H))\n",
    "    mask = prune_weights(W1, num_to_prune)\n",
    "    W1_pruned = W1 * mask\n",
    "\n",
    "    means['W_1'] = W1_pruned\n",
    "    return means\n",
    "\n",
    "def extract_all_pruned_means(fits, layer_structure, sparsity_level):\n",
    "    \"\"\"\n",
    "    Apply pruning to all fits and return dict of pruned weight dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "        fits : dict of {model_name: {\"posterior\": CmdStanMCMC}}\n",
    "        layer_structure : as used in plotting\n",
    "        sparsity_level : float\n",
    "\n",
    "    Returns:\n",
    "        dict: {model_name: {pruned weights}}\n",
    "    \"\"\"\n",
    "    model_means = {}\n",
    "    for name, fit_dict in fits.items():\n",
    "        pruned_means = get_pruned_mean_weights(fit_dict[\"posterior\"], layer_structure, sparsity_level)\n",
    "        model_means[name] = pruned_means\n",
    "    return model_means\n",
    "\n",
    "def compute_activation_frequency(fit, x_train):\n",
    "    W1 = fit['posterior'].stan_variable('W_1').mean(axis=0)\n",
    "    b1 = fit['posterior'].stan_variable('hidden_bias').mean(axis=0)\n",
    "    pre_act = x_train @ W1 + b1\n",
    "    post_act = np.maximum(0, pre_act)\n",
    "    return (post_act > 0).mean(axis=0)  # shape: (H,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/type_1/GAM_N200_p8_sigma3.00_seed9.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_activation_colors = {\n",
    "    model_name: compute_activation_frequency(fit, x_train)\n",
    "    for model_name, fit in relu_fits['GAM_N200_p8_sigma3.00_seed9'].items()\n",
    "}\n",
    "\n",
    "# Flatten and find the global maximum\n",
    "all_freqs = np.concatenate(list(node_activation_colors.values()))\n",
    "global_max = all_freqs.max()\n",
    "print(global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "P = 8\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "layer_sizes = [P] + [H]*L + [out_nodes]\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "# Example: 90% sparsity\n",
    "\n",
    "sparsity_level = 0.0\n",
    "\n",
    "#model_means = extract_all_posterior_means(relu_fits['GAM_N100_p8_sigma1.00_seed1'], layer_structure)\n",
    "\n",
    "pruned_model_means = extract_all_pruned_means(relu_fits['GAM_N200_p8_sigma3.00_seed9'], layer_structure, sparsity_level)\n",
    "\n",
    "#p2, widths_1 = plot_all_networks_subplots(model_means, layer_sizes, signed_colors=False)\n",
    "p1, widths_1 = plot_all_networks_subplots_activations(pruned_model_means, layer_sizes, node_activation_colors, activation_color_max=global_max, signed_colors=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
