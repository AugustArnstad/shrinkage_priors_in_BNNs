{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/abalone\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/abalone\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/abalone\"\n",
    "\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "\n",
    "\n",
    "full_config_path = \"abalone_N1670_p8\"\n",
    "relu_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_relu,\n",
    "    models=model_names_relu,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from properscoring import crps_ensemble\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORTANT: this y_test must correspond to the same test set used to make `output_test` in Stan,\n",
    "# otherwise scores wonâ€™t be comparable.\n",
    "from utils.generate_data import load_abalone_regression_data\n",
    "X_train, X_test, y_train, y_test = load_abalone_regression_data(standardized=False, frac=0.5)\n",
    "\n",
    "rows = []\n",
    "for model_name, model_entry in relu_fit.items():\n",
    "    post = model_entry[\"posterior\"]\n",
    "\n",
    "    y_samps = post.stan_variable(\"output_test\").squeeze(-1)\n",
    "\n",
    "    y_mean = y_samps.mean(axis=0)                                   # (n_test,)\n",
    "    rmse_post_mean = float(np.sqrt(mean_squared_error(y_test, y_mean)))\n",
    "\n",
    "    per_draw_rmse = np.sqrt(((y_samps - y_test[None, :])**2).mean(axis=1))  # (S,)\n",
    "    rmse_draw_mean = float(per_draw_rmse.mean())\n",
    "\n",
    "    crps = float(np.mean(crps_ensemble(y_test, y_samps.T)))\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE_posterior_mean\": rmse_post_mean,\n",
    "        \"RMSE_mean_over_draws\": rmse_draw_mean,\n",
    "        \"CRPS\": crps,\n",
    "        \"n_draws\": y_samps.shape[0]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values(\"RMSE_posterior_mean\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 utils/run_all_regression_models.py --model dirichlet_horseshoe_tanh --output_dir results/regression/single_layer/tanh/friedman/small_a --N 100 && python3 utils/run_all_regression_models.py --model dirichlet_student_t_tanh --output_dir results/regression/single_layer/tanh/friedman/small_a --N 100 && python3 utils/run_all_regression_models.py --model regularized_horseshoe_tanh --output_dir results/regression/single_layer/tanh/friedman/small_a --N 100 && python3 utils/run_all_regression_models.py --model beta_horseshoe_tanh --output_dir results/regression/single_layer/tanh/friedman/small_a --N 100 && python3 utils/run_all_regression_models.py --model beta_student_t_tanh --output_dir results/regression/single_layer/tanh/friedman/small_a --N 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from properscoring import crps_ensemble\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.generate_data import load_abalone_regression_data\n",
    "X_train, X_test, y_train, y_test = load_abalone_regression_data(standardized=False, frac=0.5)\n",
    "\n",
    "rows = []\n",
    "for model_name, model_entry in tanh_fit.items():\n",
    "    post = model_entry[\"posterior\"]\n",
    "\n",
    "    y_samps = post.stan_variable(\"output_test\").squeeze(-1)\n",
    "\n",
    "    y_mean = y_samps.mean(axis=0)                                   # (n_test,)\n",
    "    rmse_post_mean = float(np.sqrt(mean_squared_error(y_test, y_mean)))\n",
    "\n",
    "    per_draw_rmse = np.sqrt(((y_samps - y_test[None, :])**2).mean(axis=1))  # (S,)\n",
    "    rmse_draw_mean = float(per_draw_rmse.mean())\n",
    "\n",
    "    crps = float(np.mean(crps_ensemble(y_test, y_samps.T)))\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE_posterior_mean\": rmse_post_mean,\n",
    "        \"RMSE_mean_over_draws\": rmse_draw_mean,\n",
    "        \"CRPS\": crps,\n",
    "        \"n_draws\": y_samps.shape[0]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values(\"RMSE_posterior_mean\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from utils.generate_data import load_abalone_regression_data\n",
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "def evaluate_posterior_on_multiple_testsets(\n",
    "    relu_fit,\n",
    "    models,\n",
    "    frac,\n",
    "    forward_pass,\n",
    "    n_testsets=5,\n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    for test_id in range(n_testsets):\n",
    "        _, X_test, _, y_test = load_abalone_regression_data(\n",
    "            standardized=False,\n",
    "            frac=frac,\n",
    "            random_state=42 + test_id\n",
    "        )\n",
    "\n",
    "        X_test_np = X_test.to_numpy()\n",
    "        y_test_np = y_test.reshape(-1)\n",
    "\n",
    "        for model in models:\n",
    "            fit = relu_fit[model][\"posterior\"]\n",
    "\n",
    "            W1_samples = fit.stan_variable(\"W_1\")\n",
    "            W2_samples = fit.stan_variable(\"W_L\")\n",
    "            b1_samples = fit.stan_variable(\"hidden_bias\")\n",
    "            b2_samples = fit.stan_variable(\"output_bias\")\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            y_hats = np.zeros((S, y_test_np.shape[0]))\n",
    "            rmse = np.zeros((S))\n",
    "\n",
    "            for i in range(S):\n",
    "                y_hat = forward_pass(\n",
    "                    X_test_np,\n",
    "                    W1_samples[i],\n",
    "                    np.asarray(b1_samples[i]).reshape(-1),\n",
    "                    W2_samples[i],\n",
    "                    np.asarray(b2_samples[i]).reshape(-1),\n",
    "                )\n",
    "                y_hats[i] = y_hat.squeeze()\n",
    "                #rmse[i] = np.sqrt(mean_squared_error(y_test_np, y_hats[i]))\n",
    "                \n",
    "            \n",
    "            y_mean = y_hats.mean(axis=0)\n",
    "            posterior_rmse = np.sqrt(mean_squared_error(y_test_np, y_mean))\n",
    "\n",
    "            rows.append({\n",
    "                \"model\": model,\n",
    "                \"test_set\": test_id,\n",
    "                \"posterior_rmse\": posterior_rmse,\n",
    "                #\"mean_rmse\": rmse.mean(axis=0)\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # ðŸ”¹ THIS is the only new part\n",
    "    df_mean = (\n",
    "        df.groupby(\"model\", as_index=False)[\"posterior_rmse\"]\n",
    "          .mean()\n",
    "          .rename(columns={\"posterior_rmse\": \"mean_rmse_over_testsets\"})\n",
    "    )\n",
    "\n",
    "    return df_mean, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(tanh_fit.keys())  # e.g. [\"Gaussian\", \"Regularized Horseshoe\", ...]\n",
    "df_results, df = evaluate_posterior_on_multiple_testsets(\n",
    "    relu_fit=tanh_fit,\n",
    "    models=models,\n",
    "    frac=0.5,        # YOU control this\n",
    "    forward_pass=forward_pass_tanh,\n",
    "    n_testsets=5,\n",
    ")\n",
    "\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = results_df.to_latex(index=False, float_format=\"%.4f\", column_format=\"lcc\", caption=\"RMSE and CRPS per model.\", label=\"tab:rmse_crps\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "def compute_sparse_rmse_results_abalone(models, all_fits, forward_pass, frac,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "    for model in models:\n",
    "        try:\n",
    "            fit = all_fits[model]['posterior']\n",
    "            W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "            W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "            b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "            b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "        except KeyError:\n",
    "            print(f\"[SKIP] Model or posterior not found:\")\n",
    "            continue\n",
    "\n",
    "        S = W1_samples.shape[0]\n",
    "        rmses = np.zeros(S)\n",
    "        #print(y_test.shape)\n",
    "        _, X_test, _, y_test = load_abalone_regression_data(standardized=False, frac=frac)\n",
    "        y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "        for i in range(S):\n",
    "            W1 = W1_samples[i]\n",
    "            W2 = W2_samples[i]\n",
    "\n",
    "            # Apply pruning mask if requested\n",
    "            if prune_fn is not None and sparsity > 0.0:\n",
    "                masks = prune_fn([W1, W2], sparsity)\n",
    "                W1 = W1 * masks[0]\n",
    "                #W2 = W2 * masks[1]\n",
    "\n",
    "            y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i])\n",
    "            y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "            rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "            \n",
    "        posterior_mean = np.mean(y_hats, axis=0)\n",
    "        posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "        posterior_means.append({\n",
    "            'model': model,\n",
    "            'sparsity': sparsity,\n",
    "            'posterior_mean_rmse': posterior_mean_rmse\n",
    "        })\n",
    "\n",
    "        for i in range(S):\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'rmse': rmses[i]\n",
    "            })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "df_rmse_relu, df_posterior_rmse_relu = {}, {}\n",
    "df_rmse_tanh, df_posterior_rmse_tanh = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    df_rmse_relu[sparsity], df_posterior_rmse_relu[sparsity] = compute_sparse_rmse_results_abalone(\n",
    "        models = model_names_relu,\n",
    "        all_fits = relu_fit, \n",
    "        forward_pass = forward_pass_relu,\n",
    "        frac=0.1,\n",
    "        sparsity=sparsity, \n",
    "        prune_fn=local_prune_weights\n",
    "    )\n",
    "\n",
    "    df_rmse_tanh[sparsity], df_posterior_rmse_tanh[sparsity] = compute_sparse_rmse_results_abalone(\n",
    "        models = model_names_tanh,\n",
    "        all_fits = tanh_fit, \n",
    "        forward_pass = forward_pass_tanh,\n",
    "        frac=0.1,\n",
    "        sparsity=sparsity, \n",
    "        prune_fn=local_prune_weights\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "df_rmse_full_relu = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_relu.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_rmse_full_tanh = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_tanh.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Plot (simplified version)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "custom_palette = {\n",
    "    \"Gaussian\": \"C0\",\n",
    "    \"Regularized Horseshoe\": \"C1\",\n",
    "    \"Dirichlet Horseshoe\": \"C2\",\n",
    "    \"Dirichlet Student T\": \"C3\",\n",
    "    \"Beta Horseshoe\": \"C4\",\n",
    "    \"Beta Student T\": \"C5\",\n",
    "}\n",
    "abbr = {\n",
    "    \"Gaussian\": \"Gauss\",\n",
    "    \"Regularized Horseshoe\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DST\",\n",
    "    \"Beta Horseshoe\": \"BHS\",\n",
    "    \"Beta Student T\": \"BST\",\n",
    "    #\"Pred CP\": \"PCP\"\n",
    "}\n",
    "# Clean names\n",
    "df_rmse_full_relu[\"model\"] = df_rmse_full_relu[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "df_rmse_full_tanh[\"model\"] = df_rmse_full_tanh[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharex=True, sharey=True)\n",
    "activation_data = [(\"ReLU\", df_rmse_full_relu), (\"tanh\", df_rmse_full_tanh)]\n",
    "\n",
    "for ax, (name, df) in zip(axes, activation_data):\n",
    "    df[\"model_abbr\"] = df[\"model\"].map(lambda m: abbr.get(m, m))\n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x='sparsity', y='rmse',\n",
    "        hue='model_abbr', marker='o', errorbar=None, ax=ax,\n",
    "        markersize=12,\n",
    "        #palette=custom_palette,\n",
    "        palette={abbr[k]: v for k, v in custom_palette.items() if k in df[\"model\"].unique()},\n",
    "        hue_order=[abbr[m] for m in sorted(df[\"model\"].unique(), key=lambda x: list(custom_palette).index(x) if x in custom_palette else 999)],\n",
    "    )\n",
    "    \n",
    "    ax.set_title(name, fontsize = 15)\n",
    "    ax.set_xlabel(\"Sparsity level\", fontsize = 15)\n",
    "    ax.set_ylabel(\"RMSE\", fontsize = 15)\n",
    "    ax.grid(True)\n",
    "    ax.legend(title=\"Model\", loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures_for_use_in_paper/abalone_sparsity.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"tau\")\n",
    "lambda_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"lambda_data\")\n",
    "xi_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"phi_data\")\n",
    "W1_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_BHS = tanh_fit['Beta Horseshoe tanh']['posterior'].stan_variable(\"tau\")\n",
    "lambda_BHS = tanh_fit['Beta Horseshoe tanh']['posterior'].stan_variable(\"lambda_data\")\n",
    "xi_BHS = tanh_fit['Beta Horseshoe tanh']['posterior'].stan_variable(\"phi_data\")\n",
    "W1_BHS = tanh_fit['Beta Horseshoe tanh']['posterior'].stan_variable(\"W_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove pathological tiny values\n",
    "tau_DHS_clipped = tau_DHS[tau_DHS > 1e-10]\n",
    "tau_BHS_clipped = tau_BHS[tau_BHS > 1e-10]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Log-histogram\n",
    "axes[0].hist(np.log(tau_DHS_clipped), bins=60, density=True, label=\"DHS\")\n",
    "axes[0].hist(np.log(tau_BHS_clipped), bins=60, density=True, label=\"BHS\")\n",
    "axes[0].set_title(\"log(tau)\")\n",
    "axes[0].set_xlabel(\"log(tau)\")\n",
    "\n",
    "# ECDF\n",
    "tau_sorted_DHS = np.sort(tau_DHS_clipped)\n",
    "ecdf_DHS = np.arange(1, len(tau_sorted_DHS)+1) / len(tau_sorted_DHS)\n",
    "\n",
    "tau_sorted_BHS = np.sort(tau_BHS_clipped)\n",
    "ecdf_BHS = np.arange(1, len(tau_sorted_BHS)+1) / len(tau_sorted_BHS)\n",
    "\n",
    "axes[1].plot(tau_sorted_DHS, ecdf_DHS, label=\"DHS\")\n",
    "axes[1].plot(tau_sorted_BHS, ecdf_BHS, label=\"BHS\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_title(\"ECDF of tau\")\n",
    "axes[1].set_xlabel(\"tau\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_flat_DHS = lambda_DHS.reshape(-1)\n",
    "lambda_flat_DHS = lambda_flat_DHS[lambda_flat_DHS > 1e-12]\n",
    "lo, hi = np.quantile(lambda_flat_DHS, [0.01, 0.99])\n",
    "lambda_clip_DHS = lambda_flat_DHS[(lambda_flat_DHS >= lo) & (lambda_flat_DHS <= hi)]\n",
    "\n",
    "lambda_flat_BHS = lambda_BHS.reshape(-1)\n",
    "lambda_flat_BHS = lambda_flat_BHS[lambda_flat_BHS > 1e-12]\n",
    "lo, hi = np.quantile(lambda_flat_BHS, [0.01, 0.99])\n",
    "lambda_clip_BHS = lambda_flat_BHS[(lambda_flat_BHS >= lo) & (lambda_flat_BHS <= hi)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Log-histogram\n",
    "axes[0].hist(np.log(lambda_clip_DHS), bins=60, density=True, label=\"DHS\", alpha=0.5)\n",
    "axes[0].hist(np.log(lambda_clip_BHS), bins=60, density=True, label=\"BHS\", alpha=0.5)\n",
    "axes[0].set_title(\"log(lambda)\")\n",
    "axes[0].set_xlabel(\"log(lambda)\")\n",
    "\n",
    "# ECDF\n",
    "lam_sorted_DHS = np.sort(lambda_flat_DHS)\n",
    "ecdf_DHS = np.arange(1, len(lam_sorted_DHS)+1) / len(lam_sorted_DHS)\n",
    "lam_sorted_BHS = np.sort(lambda_flat_BHS)\n",
    "ecdf_BHS = np.arange(1, len(lam_sorted_BHS)+1) / len(lam_sorted_BHS)\n",
    "\n",
    "axes[1].plot(lam_sorted_DHS, ecdf_DHS, label=\"DHS\")\n",
    "axes[1].plot(lam_sorted_BHS, ecdf_BHS, label=\"BHS\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_title(\"ECDF of lambda\")\n",
    "axes[1].set_xlabel(\"lambda\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xi_max_DHS = xi_DHS.max(axis=2).reshape(-1)\n",
    "xi_max_BHS = xi_BHS.max(axis=2).reshape(-1)\n",
    "\n",
    "\n",
    "xi_eff_DHS = 1.0 / np.sum(xi_DHS**2, axis=2)\n",
    "xi_eff_DHS = xi_eff_DHS.reshape(-1)\n",
    "\n",
    "xi_eff_BHS = 1.0 / np.sum(xi_BHS**2, axis=2)\n",
    "xi_eff_BHS = xi_eff_BHS.reshape(-1)\n",
    "\n",
    "def topk_mass(xi, k):\n",
    "    xi_sorted = np.sort(xi, axis=2)[:, :, ::-1]\n",
    "    return xi_sorted[:, :, :k].sum(axis=2)\n",
    "\n",
    "top3_DHS = topk_mass(xi_DHS, k=3).reshape(-1)\n",
    "top3_BHS = topk_mass(xi_BHS, k=3).reshape(-1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Log-histogram\n",
    "axes[0].hist(xi_max_DHS, bins=60, density=True, label=\"DHS\", alpha=0.5)\n",
    "axes[0].hist(xi_max_BHS, bins=60, density=True, label=\"BHS\", alpha=0.5)\n",
    "axes[0].set_title(\"Largest xi\")\n",
    "axes[0].set_xlabel(\"max_i xi_{j,i}\")\n",
    "\n",
    "# Log-histogram\n",
    "#axes[1].hist(xi_eff_DHS, bins=60, density=True, label=\"DHS\")\n",
    "axes[1].hist(xi_eff_BHS, bins=60, density=True, label=\"BHS\")\n",
    "axes[1].set_title(\"effective #inputs\")\n",
    "axes[1].set_xlabel(\"Dirichlet concentration per unit\")\n",
    "\n",
    "# Log-histogram\n",
    "axes[2].hist(top3_DHS, bins=60, density=True, label=\"DHS\")\n",
    "#axes[2].hist(top3_BHS, bins=60, density=True, label=\"BHS\", alpha=0.5)\n",
    "axes[2].set_title(\"mass in top-3 inputs\")\n",
    "axes[2].set_xlabel(\"Top-3 concentration\")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you saved lambda_tilde in Stan, extract that instead.\n",
    "\n",
    "var_eff_DHS = (\n",
    "    tau_DHS[:, None, None]**2\n",
    "    * lambda_DHS\n",
    "    * xi_DHS\n",
    ")\n",
    "\n",
    "var_flat_DHS = var_eff_DHS.reshape(-1)\n",
    "var_flat_DHS = var_flat_DHS[var_flat_DHS > 1e-16]\n",
    "\n",
    "# Log-scale ECDF\n",
    "v_sorted_DHS = np.sort(var_flat_DHS)\n",
    "ecdf_DHS = np.arange(1, len(v_sorted_DHS)+1) / len(v_sorted_DHS)\n",
    "\n",
    "var_eff_BHS = (\n",
    "    tau_BHS[:, None, None]**2\n",
    "    * lambda_BHS\n",
    "    * xi_DHS\n",
    ")\n",
    "\n",
    "var_flat_BHS = var_eff_BHS.reshape(-1)\n",
    "var_flat_BHS = var_flat_BHS[var_flat_BHS > 1e-16]\n",
    "\n",
    "# Log-scale ECDF\n",
    "v_sorted_BHS = np.sort(var_flat_BHS)\n",
    "ecdf_BHS = np.arange(1, len(v_sorted_BHS)+1) / len(v_sorted_BHS)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(v_sorted_DHS, ecdf_DHS)\n",
    "plt.plot(v_sorted_BHS, ecdf_BHS)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"effective variance\")\n",
    "plt.title(\"ECDF of weight variances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_flat_DHS = W1_DHS.reshape(-1)\n",
    "W1_flat_DHS = W1_flat_DHS[W1_flat_DHS > 1e-12]\n",
    "lo, hi = np.quantile(W1_flat_DHS, [0.01, 0.99])\n",
    "W1_clip_DHS = W1_flat_DHS[(W1_flat_DHS >= lo) & (W1_flat_DHS <= hi)]\n",
    "\n",
    "W1_flat_BHS = W1_BHS.reshape(-1)\n",
    "W1_flat_BHS = W1_flat_BHS[W1_flat_BHS > 1e-12]\n",
    "lo, hi = np.quantile(W1_flat_BHS, [0.01, 0.99])\n",
    "W1_clip_BHS = W1_flat_BHS[(W1_flat_BHS >= lo) & (W1_flat_BHS <= hi)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Log-histogram\n",
    "axes[0].hist((W1_clip_DHS), bins=60, density=True, label=\"DHS\", alpha=0.5)\n",
    "axes[0].hist((W1_clip_BHS), bins=60, density=True, label=\"BHS\", alpha=0.5)\n",
    "axes[0].set_title(\"log(W1)\")\n",
    "axes[0].set_xlabel(\"log(W1)\")\n",
    "\n",
    "# ECDF\n",
    "w_sorted_DHS = np.sort(W1_flat_DHS)\n",
    "ecdf_DHS = np.arange(1, len(w_sorted_DHS)+1) / len(w_sorted_DHS)\n",
    "w_sorted_BHS = np.sort(W1_flat_BHS)\n",
    "ecdf_BHS = np.arange(1, len(w_sorted_BHS)+1) / len(w_sorted_BHS)\n",
    "\n",
    "axes[1].plot(w_sorted_DHS, ecdf_DHS, label=\"DHS\")\n",
    "axes[1].plot(w_sorted_BHS, ecdf_BHS, label=\"BHS\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_title(\"ECDF of w1\")\n",
    "axes[1].set_xlabel(\"w1\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X_train, X_test, y_train, y_test = load_abalone_regression_data(standardized=False, frac=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "\n",
    "model_preds = {}\n",
    "for model_name in model_names:\n",
    "    preds = relu_fit[model_name]['posterior'].stan_variable(\"output_test_rng\")\n",
    "    model_preds[model_name] = {\n",
    "        \"mean\": np.mean(preds, axis=0).squeeze(-1),\n",
    "        \"std\": np.std(preds, axis=0).squeeze(-1),\n",
    "        \"lower_95\": np.percentile(preds, 2.5, axis=0).squeeze(-1),\n",
    "        \"upper_95\": np.percentile(preds, 97.5, axis=0).squeeze(-1),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Get global limits for consistent scaling\n",
    "all_preds = np.concatenate([model_preds[m][\"mean\"] for m in model_names])\n",
    "y_min = min(y_test.min(), all_preds.min())\n",
    "y_max = max(y_test.max(), all_preds.max())\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    ax = axs[i]\n",
    "    sc = ax.scatter(\n",
    "        y_test, model_preds[model_name][\"mean\"],\n",
    "        c=model_preds[model_name][\"std\"], cmap='viridis', alpha=0.7\n",
    "    )\n",
    "    ax.plot([y_min, y_max], [y_min, y_max], 'k--', lw=1)\n",
    "    ax.set_title(f\"{model_name}\")\n",
    "    ax.set_xlabel(\"True y\")\n",
    "    ax.set_ylabel(\"Predicted mean\")\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(y_min, y_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "cbar = fig.colorbar(sc, ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label(\"Predictive Std Dev\")\n",
    "plt.suptitle(\"Regression Predictions with Uncertainty\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = {}\n",
    "for model_name in model_names:\n",
    "    lower = model_preds[model_name][\"lower_95\"]\n",
    "    upper = model_preds[model_name][\"upper_95\"]\n",
    "    inside = (y_test >= lower) & (y_test <= upper)\n",
    "    coverage[model_name] = np.mean(inside)\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(coverage.keys(), coverage.values())\n",
    "plt.axhline(0.95, color='red', linestyle='--', label='Ideal Coverage (95%)')\n",
    "plt.ylabel(\"Proportion Covered\")\n",
    "plt.title(\"Coverage of 95% Predictive Intervals\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis='y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for model_name in model_names:\n",
    "    errors = np.abs(model_preds[model_name][\"mean\"] - y_test)\n",
    "    stds = model_preds[model_name][\"std\"]\n",
    "    plt.scatter(stds, errors, label=model_name, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Predictive Std Dev\")\n",
    "plt.ylabel(\"Absolute Error\")\n",
    "plt.title(\"Error vs. Uncertainty\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    pred = model_preds[model_name][\"mean\"]\n",
    "    mae = np.mean(np.abs(pred - y_test))\n",
    "    rmse = np.sqrt(np.mean((pred - y_test) ** 2))\n",
    "    width = np.mean(model_preds[model_name][\"upper_95\"] - model_preds[model_name][\"lower_95\"])\n",
    "    print(f\"{model_name}: MAE = {mae:.3f}, RMSE = {rmse:.3f}, Avg Interval Width = {width:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "model_preds = {}\n",
    "for model_name in model_names:\n",
    "    preds = tanh_fit[model_name]['posterior'].stan_variable(\"output_test_rng\")\n",
    "    model_preds[model_name] = {\n",
    "        \"mean\": np.mean(preds, axis=0).squeeze(-1),\n",
    "        \"std\": np.std(preds, axis=0).squeeze(-1),\n",
    "        \"lower_95\": np.percentile(preds, 2.5, axis=0).squeeze(-1),\n",
    "        \"upper_95\": np.percentile(preds, 97.5, axis=0).squeeze(-1),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    ax = axs[i]\n",
    "    sc = ax.scatter(y_test, model_preds[model_name][\"mean\"],\n",
    "                    c=model_preds[model_name][\"std\"], cmap='viridis', alpha=0.7)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=1)\n",
    "    ax.set_title(f\"{model_name}\")\n",
    "    ax.set_xlabel(\"True y\")\n",
    "    ax.set_ylabel(\"Predicted mean\")\n",
    "    ax.grid(True)\n",
    "    \n",
    "cbar = fig.colorbar(sc, ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label(\"Predictive Std Dev\")\n",
    "plt.suptitle(\"Regression Predictions with Uncertainty\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = {}\n",
    "for model_name in model_names:\n",
    "    lower = model_preds[model_name][\"lower_95\"]\n",
    "    upper = model_preds[model_name][\"upper_95\"]\n",
    "    inside = (y_test >= lower) & (y_test <= upper)\n",
    "    coverage[model_name] = np.mean(inside)\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.bar(coverage.keys(), coverage.values())\n",
    "plt.axhline(0.95, color='red', linestyle='--', label='Ideal Coverage (95%)')\n",
    "plt.ylabel(\"Proportion Covered\")\n",
    "plt.title(\"Coverage of 95% Predictive Intervals\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis='y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for model_name in model_names:\n",
    "    errors = np.abs(model_preds[model_name][\"mean\"] - y_test)\n",
    "    stds = model_preds[model_name][\"std\"]\n",
    "    plt.scatter(stds, errors, label=model_name, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Predictive Std Dev\")\n",
    "plt.ylabel(\"Absolute Error\")\n",
    "plt.title(\"Error vs. Uncertainty\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    pred = model_preds[model_name][\"mean\"]\n",
    "    mae = np.mean(np.abs(pred - y_test))\n",
    "    rmse = np.sqrt(np.mean((pred - y_test) ** 2))\n",
    "    width = np.mean(model_preds[model_name][\"upper_95\"] - model_preds[model_name][\"lower_95\"])\n",
    "    print(f\"{model_name}: MAE = {mae:.3f}, RMSE = {rmse:.3f}, Avg Interval Width = {width:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POSTERIOR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from utils.generate_data import load_abalone_regression_data\n",
    "X_train, _, _, _ = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "\n",
    "P = 8\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "\n",
    "def build_single_draw_weights(fits, layer_structure, draw_idx):\n",
    "    \"\"\"Return {model: {'W_1': (P,H), 'W_L': (H,O)}} for ONE draw.\"\"\"\n",
    "    out = {}\n",
    "    for name, fd in fits.items():\n",
    "        fit = fd[\"posterior\"]\n",
    "        W1 = fit.stan_variable(layer_structure['input_to_hidden']['name'])[draw_idx]\n",
    "        WL = fit.stan_variable(layer_structure['hidden_to_output']['name'])[draw_idx]\n",
    "        WL = WL.reshape(layer_structure['hidden_to_output']['shape'])\n",
    "        out[name] = {\"W_1\": W1, \"W_L\": WL}\n",
    "    return out\n",
    "\n",
    "def scale_W1_for_plot(model_means, mode='global'):\n",
    "    \"\"\"\n",
    "    Skalerer alle W_1 til [-1, 1] for rettferdig sammenligning av edge-tykkelser.\n",
    "\n",
    "    mode:\n",
    "      - 'global' : Ã©n felles skala over alle modeller (mest sammenlignbar)\n",
    "      - 'per_model': egen skala per modell (uavhengig sammenligning)\n",
    "      - 'per_node' : skalerer hver kolonne (node) separat til [-1,1]\n",
    "\n",
    "    Returnerer: scaled_model_means (samme struktur som input), scale_info\n",
    "    \"\"\"\n",
    "    scaled = {}\n",
    "    if mode == 'global':\n",
    "        gmax = max(np.abs(m['W_1']).max() for m in model_means.values())\n",
    "        gmax = max(gmax, 1e-12)\n",
    "        for name, m in model_means.items():\n",
    "            W1s = m['W_1'] / gmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'global', 'scale': gmax}\n",
    "\n",
    "    elif mode == 'per_model':\n",
    "        for name, m in model_means.items():\n",
    "            s = max(np.abs(m['W_1']).max(), 1e-12)\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = m['W_1'] / s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_model'}\n",
    "\n",
    "    elif mode == 'per_node':\n",
    "        for name, m in model_means.items():\n",
    "            W1 = m['W_1'].copy()\n",
    "            P, H = W1.shape\n",
    "            for h in range(H):\n",
    "                colmax = max(np.abs(W1[:, h]).max(), 1e-12)\n",
    "                W1[:, h] = W1[:, h] / colmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_node'}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'global', 'per_model', or 'per_node'\")\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian\": \"Gauss\",\n",
    "    \"Regularized Horseshoe\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DST\",\n",
    "    #\"Pred CP\": \"PCP\"\n",
    "}\n",
    "\n",
    "def plot_models_with_activations(model_means, layer_sizes,\n",
    "                                 activations=None, activation_color_max=None,\n",
    "                                 ncols=3, figsize_per_plot=(5,4), signed_colors=False, feature_names=None):\n",
    "    \"\"\"\n",
    "    model_means: dict {model_name: {'W_1':(P,H), 'W_L':(H,O), optional 'W_internal':[...]} }\n",
    "    layer_sizes: f.eks [P, H, O] eller [P, H, H, O] ved internlag\n",
    "    activations: dict {model_name: (H,)} â€“ aktiveringsfrekvens kun for fÃ¸rste skjulte lag\n",
    "    activation_color_max: global maks for skalering av farger (hvis None brukes 1.0)\n",
    "    \"\"\"\n",
    "    names = list(model_means.keys())\n",
    "    n_models = len(names)\n",
    "    nrows = int(np.ceil(n_models / ncols))\n",
    "    figsize = (figsize_per_plot[0] * ncols, figsize_per_plot[1] * nrows)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    if nrows * ncols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Skru av blanke akser\n",
    "    for ax in axes[n_models:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax, name in zip(axes, names):\n",
    "        weights = model_means[name]\n",
    "        G = nx.DiGraph()\n",
    "        pos, nodes_per_layer, node_colors = {}, [], []\n",
    "\n",
    "        # Noder med posisjon og farge\n",
    "        for li, size in enumerate(layer_sizes):\n",
    "            ids = []\n",
    "            ycoords = np.linspace(size - 1, 0, size) - (size - 1) / 2\n",
    "            for i in range(size):\n",
    "                nid = f\"L{li}_{i}\"\n",
    "                G.add_node(nid)\n",
    "                pos[nid] = (li, ycoords[i])\n",
    "                ids.append(nid)\n",
    "                if li == 0 and feature_names is not None:\n",
    "                    ax.text(pos[nid][0]-0.12, pos[nid][1], feature_names[i],\n",
    "                            ha='right', va='center', fontsize=8)\n",
    "\n",
    "                if activations is not None and li == 1:  # kun fÃ¸rste skjulte lag\n",
    "                    #a = activations.get(name, np.zeros(size))\n",
    "                    a = activations.get(name, np.zeros(size))\n",
    "                    a = np.asarray(a).ravel()   # <-- flater til 1D array\n",
    "                    scale = activation_color_max if activation_color_max is not None else 1.0\n",
    "                    val = float(np.clip(a[i] / max(scale, 1e-12), 0.0, 1.0))\n",
    "                    color = plt.cm.winter(val)\n",
    "                else:\n",
    "                    color = 'lightblue'\n",
    "                node_colors.append(color)\n",
    "\n",
    "            nodes_per_layer.append(ids)\n",
    "\n",
    "        edge_colors, edge_widths = [], []\n",
    "\n",
    "        def add_edges(W, inn, ut):\n",
    "            for j, out_n in enumerate(ut):\n",
    "                for i, in_n in enumerate(inn):\n",
    "                    w = float(W[i, j])\n",
    "                    G.add_edge(in_n, out_n, weight=abs(w))\n",
    "                    edge_colors.append('red' if w >= 0 else 'blue')\n",
    "                    edge_widths.append(abs(w))\n",
    "\n",
    "        # input -> hidden(1)\n",
    "        add_edges(weights['W_1'], nodes_per_layer[0], nodes_per_layer[1])\n",
    "\n",
    "        # ev. internlag\n",
    "        if 'W_internal' in weights:\n",
    "            for l, Win in enumerate(weights['W_internal']):\n",
    "                add_edges(Win, nodes_per_layer[l+1], nodes_per_layer[l+2])\n",
    "\n",
    "        # siste hidden -> output\n",
    "        add_edges(weights['W_L'], nodes_per_layer[-2], nodes_per_layer[-1])\n",
    "\n",
    "        nx.draw(G, pos, ax=ax,\n",
    "                node_color=node_colors,\n",
    "                edge_color=(edge_colors if signed_colors else 'red'),\n",
    "                width=[G[u][v]['weight'] for u,v in G.edges()],\n",
    "                with_labels=False, node_size=400, arrows=False)\n",
    "\n",
    "        ax.set_title(abbr[name], fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def compute_hidden_activation(fit_dict, x_train, draw_idx):\n",
    "    fit = fit_dict['posterior']\n",
    "    W1 = fit.stan_variable('W_1')[draw_idx, :, :]          # (P,H)\n",
    "    try:\n",
    "        b1 = fit.stan_variable('hidden_bias')[draw_idx, :] # (H,)\n",
    "    except Exception:\n",
    "        b1 = np.zeros(W1.shape[1])\n",
    "    # tanh i [-1,1]\n",
    "    a_full = np.tanh(x_train @ W1 + b1)             # (H,)\n",
    "    a=np.mean(a_full, axis=0)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velg en observasjon Ã¥ \"lyse opp\" nodefargene med\n",
    "obs_idx = 3\n",
    "draw_idx = 69 #pick_draw_idx(prior_fits, seed=42)      # one common draw across models\n",
    "prior_draws = build_single_draw_weights(relu_fit, layer_structure, draw_idx)\n",
    "\n",
    "# 1) Beregn aktivasjoner for ALLE modellene\n",
    "activations = {}\n",
    "for name, fd in relu_fit.items():\n",
    "    a = compute_hidden_activation(fd, X_train, draw_idx)\n",
    "    activations[name] = np.abs(a)      \n",
    "\n",
    "# 2) Skaler vekter for plotting (som fÃ¸r)\n",
    "scaled, _ = scale_W1_for_plot(prior_draws, mode='per_model')\n",
    "\n",
    "# 3) Kall plottet med aktivasjoner\n",
    "# Siden tanh âˆˆ [-1,1] og vi bruker |a|, sÃ¥ sett activation_color_max=1.0\n",
    "fig = plot_models_with_activations(\n",
    "    scaled,\n",
    "    layer_sizes=[P, H, out_nodes],\n",
    "    activations=None,\n",
    "    activation_color_max=1.0,\n",
    "    ncols=2,\n",
    "    feature_names = None\n",
    ")\n",
    "plt.savefig(\"figures_for_use_in_paper/abalone_network_relu.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "def mean_abs(arr):  # arr: (S, ...)\n",
    "    return np.mean(np.abs(np.asarray(arr)), axis=0)\n",
    "\n",
    "def nid_single_hidden(posterior, W1_name=\"W_1\", WL_name_candidates=(\"W_L\",\"W_2\")):\n",
    "    \"\"\"\n",
    "    posterior: CmdStanMCMC-objekt\n",
    "    W_1: shape (S, P, H)  (input -> hidden), som du har\n",
    "    W_L: shape (S, H) eller (S, H, O)  (hidden -> output)\n",
    "    \"\"\"\n",
    "    W1_samps = posterior.stan_variable(W1_name)          # (S, P, H)\n",
    "    # Finn navn for siste lag\n",
    "    for nm in WL_name_candidates:\n",
    "        try:\n",
    "            WL_samps = posterior.stan_variable(nm)       # (S, H) eller (S, H, O)\n",
    "            break\n",
    "        except Exception:\n",
    "            WL_samps = None\n",
    "    if WL_samps is None:\n",
    "        raise ValueError(\"Fant ikke siste-lag-vekter (prÃ¸v Ã¥ angi riktig navn i WL_name_candidates).\")\n",
    "\n",
    "    # Posterior plug-in: gjennomsnitt av absoluttverdier\n",
    "    W1_abs = mean_abs(W1_samps)                          # (P, H)\n",
    "    WL_abs = mean_abs(WL_samps)                          # (H,) eller (H, O)\n",
    "\n",
    "    # z^(1): aggregert node-innflytelse (sum over outputs hvis flere)\n",
    "    if WL_abs.ndim == 1:\n",
    "        z1 = WL_abs                                      # (H,)\n",
    "    else:\n",
    "        z1 = WL_abs.sum(axis=1)                          # (H,)\n",
    "\n",
    "    P, H = W1_abs.shape\n",
    "\n",
    "    # Main effects: Ï‰({j}) = Î£_i z_i * |W1[j,i]|\n",
    "    omega_main = (W1_abs * z1[None, :]).sum(axis=1)      # (P,)\n",
    "\n",
    "    # Pairwise: Ï‰({j,k}) = Î£_i z_i * min(|W1[j,i]|, |W1[k,i]|)\n",
    "    omega_pair = np.zeros((P, P))\n",
    "    for j, k in combinations(range(P), 2):\n",
    "        mins = np.minimum(W1_abs[j, :], W1_abs[k, :])    # (H,)\n",
    "        omega = np.dot(z1, mins)                         # skalar\n",
    "        omega_pair[j, k] = omega_pair[k, j] = omega\n",
    "\n",
    "    return z1, omega_main, omega_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = relu_fit['Dirichlet Student T']['posterior'] \n",
    "z1, omega_main, omega_pair = nid_single_hidden(post) # W_1=(S,P,H), W_L/(W_2)=(S,H[,O]) # Eksempler: # - topp 10 viktigste noder etter z1: \n",
    "top_nodes = np.argsort(-z1) # - topp 10 viktigste features (main effects): \n",
    "top_feats = np.argsort(-omega_main) # - sterkeste parvise interaksjoner: \n",
    "P = omega_pair.shape[0] \n",
    "pairs = [(j, k, omega_pair[j, k]) for j in range(P) for k in range(j+1, P)] \n",
    "top_pairs = sorted(pairs, key=lambda t: -t[2])[:10]\n",
    "\n",
    "res = np.array(omega_main/(np.sum(omega_main)))\n",
    "print(np.round(res, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "def gini(v):\n",
    "    v = np.asarray(v, float)\n",
    "    v = np.abs(v)\n",
    "    if np.all(v == 0): return 0.0\n",
    "    v = np.sort(v)\n",
    "    n = v.size\n",
    "    cum = np.cumsum(v)\n",
    "    return (n + 1 - 2 * (cum.sum() / cum[-1])) / n\n",
    "\n",
    "def topk_share(v, k):\n",
    "    v = np.asarray(v, float)\n",
    "    tot = v.sum()\n",
    "    if tot == 0: return 0.0\n",
    "    idx = np.argsort(-v)[:k]\n",
    "    return v[idx].sum() / tot\n",
    "\n",
    "def summarize_main(omega_main):\n",
    "    tot = omega_main.sum()\n",
    "    mx  = omega_main.max() if omega_main.size else 0.0\n",
    "    return {\n",
    "        \"Gini(main)\": gini(omega_main),\n",
    "        \"Top1(main)\": topk_share(omega_main, 1),\n",
    "        \"Top3(main)\": topk_share(omega_main, 3),\n",
    "        \"Top5(main)\": topk_share(omega_main, 5),\n",
    "        \"#â‰¥10%max(main)\": int((omega_main >= 0.10 * mx).sum()),\n",
    "        \"Total(main)\": tot,\n",
    "    }\n",
    "\n",
    "def summarize_pairs(omega_pair):\n",
    "    # ta Ã¸vre trekant uten diagonal\n",
    "    P = omega_pair.shape[0]\n",
    "    tri = [omega_pair[j, k] for j in range(P) for k in range(j+1, P)]\n",
    "    tri = np.asarray(tri, float)\n",
    "    tot = tri.sum()\n",
    "    mx  = tri.max() if tri.size else 0.0\n",
    "    return {\n",
    "        \"Gini(pair)\": gini(tri),\n",
    "        \"Top5(pair)\": topk_share(tri, 5),\n",
    "        \"Top10(pair)\": topk_share(tri, 10),\n",
    "        \"#â‰¥10%max(pair)\": int((tri >= 0.10 * mx).sum()),\n",
    "        \"Total(pair)\": tot,\n",
    "    }\n",
    "\n",
    "models = [\n",
    "    (\"Gaussian tanh\",                tanh_fit['Gaussian tanh']['posterior']),\n",
    "    (\"Regularized Horseshoe tanh\",   tanh_fit['Regularized Horseshoe tanh']['posterior']),\n",
    "    (\"Dirichlet Horseshoe tanh\",     tanh_fit['Dirichlet Horseshoe tanh']['posterior']),\n",
    "    (\"Dirichlet Student T tanh\",     tanh_fit['Dirichlet Student T tanh']['posterior']),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, post in models:\n",
    "    z1, omega_main, omega_pair = nid_single_hidden(post)\n",
    "    m = summarize_main(omega_main)\n",
    "    p = summarize_pairs(omega_pair)\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        **m,\n",
    "        **p,\n",
    "        \"Median z1\": np.median(z1),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"Model\",\n",
    "    \"Gini(main)\", \"Top1(main)\", \"Top3(main)\", \"Top5(main)\", \"#â‰¥10%max(main)\", \"Total(main)\",\n",
    "    \"Gini(pair)\", \"Top5(pair)\", \"Top10(pair)\", \"#â‰¥10%max(pair)\", \"Total(pair)\",\n",
    "    \"Median z1\",\n",
    "])\n",
    "\n",
    "# Kort og ryddig LaTeX\n",
    "print(df.to_latex(index=False, float_format=\"%.3f\", escape=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute effective number of nonzero parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_kappa(fit, q, node_idx, input_idx, model_type='gaussian'):\n",
    "    \"\"\"\n",
    "    Returnerer arrays per trekning (S,): kappa_original, b_j, kappa_tilde\n",
    "    \"\"\"\n",
    "    q_hp = np.asarray(q)[:, node_idx, input_idx]  # (S,)\n",
    "\n",
    "    if model_type == 'gaussian':\n",
    "        tau = 1.0\n",
    "        lam = 1.0\n",
    "        c_sq = 1.0\n",
    "        phi = 1.0\n",
    "    else:\n",
    "        tau = fit.stan_variable(\"tau\")                          # (S,)\n",
    "        c_sq = fit.stan_variable(\"c_sq\")[:, node_idx]           # (S,)\n",
    "\n",
    "        if model_type == 'rhs':\n",
    "            lam = fit.stan_variable(\"lambda_tilde\")[:, node_idx, input_idx]       # (S,)\n",
    "            phi = 1.0\n",
    "        elif model_type == 'dhs' or 'dst':\n",
    "            lam = fit.stan_variable(\"lambda_tilde_data\")[:, node_idx, input_idx]  # (S,)\n",
    "            phi = fit.stan_variable(\"phi_data\")[:, node_idx, input_idx]           # (S,)\n",
    "        # elif model_type == 'dst':\n",
    "        #     lam = fit.stan_variable(\"lambda_tilde_data\")[:, node_idx, input_idx]       # (S,)\n",
    "        #     phi = fit.stan_variable(\"phi_data\")[:, node_idx, input_idx]           # (S,)\n",
    "        else:\n",
    "            raise ValueError(\"model_type mÃ¥ vÃ¦re 'gaussian', 'rhs', 'dhs' eller 'dst'.\")\n",
    "\n",
    "    kappa_original = 1.0 / (1.0 + q_hp * (tau**2) * (lam**2) * (phi))  # (S,)\n",
    "    b_j            = 1.0 / (1.0 + q_hp * (tau**2) * c_sq * (phi))      # (S,)\n",
    "    kappa_tilde    = (1.0 - b_j) * kappa_original + b_j                # (S,)\n",
    "\n",
    "    return kappa_original, b_j, kappa_tilde\n",
    "\n",
    "\n",
    "def compute_kappa_per_input(fit, q, node_idx, model_type='gaussian'):\n",
    "    \"\"\"\n",
    "    For en gitt node: beregn per input p\n",
    "      - E[1 - kappa_tilde]_p\n",
    "      - E[(1 - kappa)(1 - b_j)]_p\n",
    "    og returnÃ©r begge som (P,) + totalsum som skalarer.\n",
    "    \"\"\"\n",
    "    S, H, P = np.asarray(q).shape\n",
    "    mean_1_minus_kappa_tilde = np.zeros(P)\n",
    "    mean_prod_identity       = np.zeros(P)\n",
    "\n",
    "    for p in range(P):\n",
    "        kappa, b_j, kappa_tilde = compute_kappa(fit, q, node_idx=node_idx, input_idx=p, model_type=model_type)\n",
    "\n",
    "        # Riktig aggregering: gjennomsnitt av uttrykket per trekning\n",
    "        mean_1_minus_kappa_tilde[p] = np.mean(1.0 - kappa_tilde)\n",
    "        mean_prod_identity[p]       = np.mean((1.0 - kappa) * (1.0 - b_j))\n",
    "\n",
    "    # SummÃ©r over inputs (antall ikke-null-vekter inn i noden i forventning)\n",
    "    total_meff_tilde = np.sum(mean_1_minus_kappa_tilde)\n",
    "    total_meff_check = np.sum(mean_prod_identity)\n",
    "\n",
    "    return total_meff_tilde, total_meff_check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------- 1) Simuler X ----------\n",
    "def simulate_X(n, P, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.uniform(0.0, 1.0, size=(n, P))\n",
    "\n",
    "# ---------- 2) Aktivering og derivert ----------\n",
    "def get_activation(activation=\"tanh\"):\n",
    "    if activation == \"tanh\":\n",
    "        phi = np.tanh\n",
    "        def dphi(a): return 1.0 - np.tanh(a)**2\n",
    "    elif activation == \"relu\":\n",
    "        def phi(a): return np.maximum(0.0, a)\n",
    "        def dphi(a): return (a > 0.0).astype(a.dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    return phi, dphi\n",
    "\n",
    "# ---------- 3) Hovedfunksjon: q for alle trekk ----------\n",
    "def compute_q_for_fit(cmdstan_mcmc, N=1000, activation=\"tanh\", seed=1, output_index=0, X=None):\n",
    "    \"\"\"\n",
    "    Beregn q_{ell, j} for fÃ¸rste-lagsvektene for hver trekk (draw).\n",
    "    Returnerer:\n",
    "      q_draws:  (n_draws, H, P)\n",
    "      q_mean:   (H, P)  â€“ gjennomsnitt over trekk\n",
    "      X:        (N, P)  â€“ datasettet brukt i beregningen\n",
    "    \"\"\"\n",
    "    # Hent ut variabler fra Stan\n",
    "    W1_all = cmdstan_mcmc.stan_variable(\"W_1\")            # (draws, P, H)\n",
    "    WL_all = cmdstan_mcmc.stan_variable(\"W_L\")             # (draws, H, O)\n",
    "    hb_all = cmdstan_mcmc.stan_variable(\"hidden_bias\")     # (draws, L, H)\n",
    "    sigma_all = cmdstan_mcmc.stan_variable(\"sigma\")        # (draws,)\n",
    "    Wint_all = cmdstan_mcmc.stan_variable(\"W_internal\")    # (draws, max(L-1,1), H, H)\n",
    "\n",
    "    draws, P, H = W1_all.shape\n",
    "    O = WL_all.shape[2]\n",
    "    L = hb_all.shape[1]\n",
    "\n",
    "    if O == 0:\n",
    "        raise ValueError(\"W_L has zero output nodes. Expected at least 1.\")\n",
    "    if output_index < 0 or output_index >= O:\n",
    "        raise ValueError(f\"output_index {output_index} out of range 0..{O-1}\")\n",
    "\n",
    "    if X is None:\n",
    "        X = simulate_X(N, P, seed=seed)\n",
    "\n",
    "    X_sq = X**2\n",
    "    phi, dphi = get_activation(activation)\n",
    "\n",
    "    q_draws = np.empty((draws, H, P), dtype=float)\n",
    "\n",
    "    for s in range(draws):\n",
    "        W1 = W1_all[s]            # (P, H)\n",
    "        WL = WL_all[s]            # (H, O)\n",
    "        hb = hb_all[s]            # (L, H)\n",
    "        Wints = Wint_all[s]       # (max(L-1,1), H, H)\n",
    "        sigma = float(sigma_all[s])\n",
    "\n",
    "        # ----- Forward pass -----\n",
    "        a_list = []\n",
    "        h_list = []\n",
    "\n",
    "        a = X @ W1 + hb[0]        # (N, H)\n",
    "        h = phi(a)\n",
    "        a_list.append(a); h_list.append(h)\n",
    "\n",
    "        for l in range(1, L):\n",
    "            Wl = Wints[l-1]       # (H, H)\n",
    "            a = h @ Wl + hb[l]    # (N, H)\n",
    "            h = phi(a)\n",
    "            a_list.append(a); h_list.append(h)\n",
    "\n",
    "        # ----- Backward: delta_L = d f / d a^(L) -----\n",
    "        # lineÃ¦r utgang: df/dh^(L) = WL[:, output_index]\n",
    "        v = WL[:, output_index]           # (H,)\n",
    "        delta = dphi(a_list[-1]) * v      # (N, H), broadcast over N\n",
    "\n",
    "        # Bakover gjennom skjulte lag\n",
    "        for l in range(L-2, -1, -1):\n",
    "            Wnext = Wints[l]              # (H, H) â€“ brukes bare hvis L>1\n",
    "            delta = (delta @ Wnext.T) * dphi(a_list[l]) if L > 1 else delta\n",
    "\n",
    "        delta1 = delta  # (N, H) == âˆ‚f/âˆ‚a^(1)\n",
    "\n",
    "        # ----- q: (1/sigma^2) * sum_i (delta1[i,ell]^2 * X[i,j]^2) -----\n",
    "        D_sq = delta1**2                  # (N, H)\n",
    "        Q = (X_sq.T @ D_sq) / (sigma**2)  # (P, H)\n",
    "        q_draws[s] = Q.T                  # (H, P)\n",
    "\n",
    "    q_mean = q_draws.mean(axis=0)         # (H, P)\n",
    "    return q_draws, q_mean, X\n",
    "\n",
    "# ---------- 4) Eksempelbruk ----------\n",
    "\n",
    "\n",
    "posterior_q_dhs, _, _ = compute_q_for_fit(\n",
    "    tanh_fit['Dirichlet Horseshoe tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = X_train     \n",
    ")\n",
    "\n",
    "posterior_q_dst, _, _ = compute_q_for_fit(\n",
    "    tanh_fit['Dirichlet Student T tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = X_train      \n",
    ")\n",
    "\n",
    "posterior_q_rhs, _, _ = compute_q_for_fit(\n",
    "    tanh_fit['Regularized Horseshoe tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = X_train      \n",
    ")\n",
    "\n",
    "posterior_q_gauss, _, _ = compute_q_for_fit(\n",
    "    tanh_fit['Gaussian tanh']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='tanh',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = X_train      \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 16\n",
    "gauss_fit = tanh_fit['Gaussian tanh']['posterior']\n",
    "rhs_fit = tanh_fit['Regularized Horseshoe tanh']['posterior']\n",
    "dhs_fit = tanh_fit['Dirichlet Horseshoe tanh']['posterior']\n",
    "dst_fit = tanh_fit['Dirichlet Student T tanh']['posterior']\n",
    "\n",
    "nonzero = np.zeros((4, nodes))\n",
    "\n",
    "for i in range(nodes):\n",
    "    meff_tilde_g, _ = compute_kappa_per_input(\n",
    "        gauss_fit, posterior_q_gauss, node_idx=i, model_type='gaussian'\n",
    "    )\n",
    "    meff_tilde_rhs, _ = compute_kappa_per_input(\n",
    "        rhs_fit, posterior_q_rhs, node_idx=i, model_type='rhs'\n",
    "    )\n",
    "    meff_tilde_dhs, _ = compute_kappa_per_input(\n",
    "        dhs_fit, posterior_q_dhs, node_idx=i, model_type='dhs'\n",
    "    )\n",
    "    meff_tilde_dst, _ = compute_kappa_per_input(\n",
    "        dst_fit, posterior_q_dst, node_idx=i, model_type='dst'\n",
    "    )\n",
    "    nonzero[0, i] = meff_tilde_g\n",
    "    nonzero[1, i] = meff_tilde_rhs\n",
    "    nonzero[2, i] = meff_tilde_dhs\n",
    "    nonzero[3, i] = meff_tilde_dst\n",
    "    \n",
    "print(\"Posterior number of nonzero weights: \", np.sum(nonzero, axis=1))\n",
    "print(\"Posterior percentage of nonzero weights: \", np.sum(nonzero, axis=1)/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------- 1) Simuler X ----------\n",
    "def simulate_X(n, P, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.uniform(0.0, 1.0, size=(n, P))\n",
    "\n",
    "# ---------- 2) Aktivering og derivert ----------\n",
    "def get_activation(activation=\"tanh\"):\n",
    "    if activation == \"tanh\":\n",
    "        phi = np.tanh\n",
    "        def dphi(a): return 1.0 - np.tanh(a)**2\n",
    "    elif activation == \"relu\":\n",
    "        def phi(a): return np.maximum(0.0, a)\n",
    "        def dphi(a): \n",
    "            a = np.asarray(a)\n",
    "            return (a > 0.0).astype(a.dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    return phi, dphi\n",
    "\n",
    "# ---------- 3) Hovedfunksjon: q for alle trekk ----------\n",
    "def compute_q_for_fit(cmdstan_mcmc, N=1000, activation=\"tanh\", seed=1, output_index=0, X=None):\n",
    "    \"\"\"\n",
    "    Beregn q_{ell, j} for fÃ¸rste-lagsvektene for hver trekk (draw).\n",
    "    Returnerer:\n",
    "      q_draws:  (n_draws, H, P)\n",
    "      q_mean:   (H, P)  â€“ gjennomsnitt over trekk\n",
    "      X:        (N, P)  â€“ datasettet brukt i beregningen\n",
    "    \"\"\"\n",
    "    # Hent ut variabler fra Stan\n",
    "    W1_all = cmdstan_mcmc.stan_variable(\"W_1\")            # (draws, P, H)\n",
    "    WL_all = cmdstan_mcmc.stan_variable(\"W_L\")             # (draws, H, O)\n",
    "    hb_all = cmdstan_mcmc.stan_variable(\"hidden_bias\")     # (draws, L, H)\n",
    "    sigma_all = cmdstan_mcmc.stan_variable(\"sigma\")        # (draws,)\n",
    "    Wint_all = cmdstan_mcmc.stan_variable(\"W_internal\")    # (draws, max(L-1,1), H, H)\n",
    "\n",
    "    draws, P, H = W1_all.shape\n",
    "    O = WL_all.shape[2]\n",
    "    L = hb_all.shape[1]\n",
    "\n",
    "    if O == 0:\n",
    "        raise ValueError(\"W_L has zero output nodes. Expected at least 1.\")\n",
    "    if output_index < 0 or output_index >= O:\n",
    "        raise ValueError(f\"output_index {output_index} out of range 0..{O-1}\")\n",
    "\n",
    "    if X is None:\n",
    "        X = simulate_X(N, P, seed=seed)\n",
    "\n",
    "    X_sq = X**2\n",
    "    phi, dphi = get_activation(activation)\n",
    "\n",
    "    q_draws = np.empty((draws, H, P), dtype=float)\n",
    "\n",
    "    for s in range(draws):\n",
    "        W1 = W1_all[s]            # (P, H)\n",
    "        WL = WL_all[s]            # (H, O)\n",
    "        hb = hb_all[s]            # (L, H)\n",
    "        Wints = Wint_all[s]       # (max(L-1,1), H, H)\n",
    "        sigma = float(sigma_all[s])\n",
    "\n",
    "        # ----- Forward pass -----\n",
    "        a_list = []\n",
    "        h_list = []\n",
    "\n",
    "        a = X @ W1 + hb[0]        # (N, H)\n",
    "        h = phi(a)\n",
    "        a_list.append(a); h_list.append(h)\n",
    "\n",
    "        for l in range(1, L):\n",
    "            Wl = Wints[l-1]       # (H, H)\n",
    "            a = h @ Wl + hb[l]    # (N, H)\n",
    "            h = phi(a)\n",
    "            a_list.append(a); h_list.append(h)\n",
    "\n",
    "        # ----- Backward: delta_L = d f / d a^(L) -----\n",
    "        # lineÃ¦r utgang: df/dh^(L) = WL[:, output_index]\n",
    "        v = WL[:, output_index]           # (H,)\n",
    "        delta = dphi(a_list[-1]) * v      # (N, H), broadcast over N\n",
    "\n",
    "        # Bakover gjennom skjulte lag\n",
    "        for l in range(L-2, -1, -1):\n",
    "            Wnext = Wints[l]              # (H, H) â€“ brukes bare hvis L>1\n",
    "            delta = (delta @ Wnext.T) * dphi(a_list[l]) if L > 1 else delta\n",
    "\n",
    "        delta1 = delta  # (N, H) == âˆ‚f/âˆ‚a^(1)\n",
    "\n",
    "        # ----- q: (1/sigma^2) * sum_i (delta1[i,ell]^2 * X[i,j]^2) -----\n",
    "        D_sq = delta1**2                  # (N, H)\n",
    "        Q = (X_sq.T @ D_sq) / (sigma**2)  # (P, H)\n",
    "        q_draws[s] = Q.T                  # (H, P)\n",
    "\n",
    "    q_mean = q_draws.mean(axis=0)         # (H, P)\n",
    "    return q_draws, q_mean, X\n",
    "\n",
    "# ---------- 4) Eksempelbruk ----------\n",
    "\n",
    "\n",
    "posterior_q_dhs, _, _ = compute_q_for_fit(\n",
    "    relu_fit['Dirichlet Horseshoe']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='relu',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = X_train     \n",
    ")\n",
    "\n",
    "posterior_q_dst, _, _ = compute_q_for_fit(\n",
    "    relu_fit['Dirichlet Student T']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='relu',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = X_train      \n",
    ")\n",
    "\n",
    "posterior_q_rhs, _, _ = compute_q_for_fit(\n",
    "    relu_fit['Regularized Horseshoe']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='relu',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = X_train      \n",
    ")\n",
    "\n",
    "posterior_q_gauss, _, _ = compute_q_for_fit(\n",
    "    relu_fit['Gaussian']['posterior'],\n",
    "    N=1000,             \n",
    "    activation='relu',  \n",
    "    seed=123,\n",
    "    output_index=0,\n",
    "    X = X_train      \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 16\n",
    "gauss_fit = relu_fit['Gaussian']['posterior']\n",
    "rhs_fit = relu_fit['Regularized Horseshoe']['posterior']\n",
    "dhs_fit = relu_fit['Dirichlet Horseshoe']['posterior']\n",
    "dst_fit = relu_fit['Dirichlet Student T']['posterior']\n",
    "\n",
    "nonzero = np.zeros((4, nodes))\n",
    "\n",
    "for i in range(nodes):\n",
    "    meff_tilde_g, _ = compute_kappa_per_input(\n",
    "        gauss_fit, posterior_q_gauss, node_idx=i, model_type='gaussian'\n",
    "    )\n",
    "    meff_tilde_rhs, _ = compute_kappa_per_input(\n",
    "        rhs_fit, posterior_q_rhs, node_idx=i, model_type='rhs'\n",
    "    )\n",
    "    meff_tilde_dhs, _ = compute_kappa_per_input(\n",
    "        dhs_fit, posterior_q_dhs, node_idx=i, model_type='dhs'\n",
    "    )\n",
    "    meff_tilde_dst, _ = compute_kappa_per_input(\n",
    "        dst_fit, posterior_q_dst, node_idx=i, model_type='dst'\n",
    "    )\n",
    "    nonzero[0, i] = meff_tilde_g\n",
    "    nonzero[1, i] = meff_tilde_rhs\n",
    "    nonzero[2, i] = meff_tilde_dhs\n",
    "    nonzero[3, i] = meff_tilde_dst\n",
    "    \n",
    "print(\"Posterior number of nonzero weights: \", np.sum(nonzero, axis=1))\n",
    "print(\"Posterior percentage of nonzero weights: \", np.sum(nonzero, axis=1)/128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SHAPLEY VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X_train, X_test, y_train, y_test = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.robust_utils import build_pytorch_model_from_stan_sample\n",
    "import torch, numpy as np, shap\n",
    "\n",
    "fit = tanh_fit['Dirichlet Student T tanh']['posterior']\n",
    "\n",
    "P = X_train.shape[1]\n",
    "H=16\n",
    "\n",
    "# 1) Build the torch model from one Stan draw\n",
    "model = build_pytorch_model_from_stan_sample(\n",
    "    fit, sample_idx=69, input_dim=P, hidden_dim=H,\n",
    "    output_dim=1, task=\"regression\", activation=torch.tanh\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# 2) Wrap a predict function that takes a numpy array and returns numpy\n",
    "def predict_numpy(X_np):\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.tensor(X_np, dtype=torch.float32)\n",
    "        y = model(X_t).cpu().numpy()\n",
    "    return y\n",
    "\n",
    "# 3) Choose a small background (50â€“200 rows) for SHAPâ€™s reference\n",
    "feature_names = list(X_train.columns)\n",
    "X_bg   = X_train.sample(n=3341, random_state=0).to_numpy(dtype=float)\n",
    "X_eval = X_test.sample(n=836, random_state=1).to_numpy(dtype=float)  # what you want SHAP for\n",
    "\n",
    "# 4) KernelSHAP\n",
    "explainer = shap.KernelExplainer(predict_numpy, X_bg)\n",
    "shap_vals = explainer.shap_values(X_eval)   # shape: (n_eval, P)\n",
    "\n",
    "# 5) Global importance (mean |SHAP|)\n",
    "mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "order = np.argsort(mean_abs)[::-1]\n",
    "for j in order:\n",
    "    print(f\"{feature_names[j]:16s}  {mean_abs[j]:.4f}\")\n",
    "    \n",
    "shap.summary_plot(shap_vals, X_eval, feature_names=feature_names, show=False)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.robust_utils import build_pytorch_model_from_stan_sample\n",
    "import torch, numpy as np, shap\n",
    "\n",
    "fit = relu_fit['Dirichlet Student T']['posterior']\n",
    "\n",
    "P = X_train.shape[1]\n",
    "H=16\n",
    "\n",
    "# 1) Build the torch model from one Stan draw\n",
    "model = build_pytorch_model_from_stan_sample(\n",
    "    fit, sample_idx=69, input_dim=P, hidden_dim=H,\n",
    "    output_dim=1, task=\"regression\", activation=torch.relu\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# 2) Wrap a predict function that takes a numpy array and returns numpy\n",
    "def predict_numpy(X_np):\n",
    "    with torch.no_grad():\n",
    "        X_t = torch.tensor(X_np, dtype=torch.float32)\n",
    "        y = model(X_t).cpu().numpy()\n",
    "    return y\n",
    "\n",
    "# 3) Choose a small background (50â€“200 rows) for SHAPâ€™s reference\n",
    "feature_names = list(X_train.columns)\n",
    "X_bg   = X_train.sample(n=3341, random_state=0).to_numpy(dtype=float)\n",
    "X_eval = X_test.sample(n=836, random_state=1).to_numpy(dtype=float)  # what you want SHAP for\n",
    "\n",
    "# 4) KernelSHAP\n",
    "explainer = shap.KernelExplainer(predict_numpy, X_bg)\n",
    "shap_vals = explainer.shap_values(X_eval)   # shape: (n_eval, P)\n",
    "\n",
    "# 5) Global importance (mean |SHAP|)\n",
    "mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "order = np.argsort(mean_abs)[::-1]\n",
    "for j in order:\n",
    "    print(f\"{feature_names[j]:16s}  {mean_abs[j]:.4f}\")\n",
    "\n",
    "shap.summary_plot(shap_vals, X_eval, feature_names=feature_names, show=False)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corr = pd.DataFrame(X_train, columns=X_train.columns).drop(columns=[\"Sex\"]).corr()\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0.92)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING ALIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
