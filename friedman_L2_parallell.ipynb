{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman/many\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]#, \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "#model_names_tanh_L2 = [\"Dirichlet Horseshoe tanh L2\", \"Dirichlet Student T tanh L2\", \"Beta Horseshoe tanh L2\", \"Beta Student T tanh L2\"]\n",
    "\n",
    "tanh_fits = {}\n",
    "tanh_fits_L2 = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    # tanh_fit_L2 = get_model_fits(\n",
    "    #     config=full_config_path,\n",
    "    #     results_dir=results_dir_tanh,\n",
    "    #     models=model_names_tanh_L2,\n",
    "    #     include_prior=False,\n",
    "    # )\n",
    "    \n",
    "    tanh_fits[base_config_name] = tanh_fit\n",
    "    #tanh_fits_L2[base_config_name] = tanh_fit_L2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]#, \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    # tanh_fit_L2 = get_model_fits(\n",
    "    #     config=full_config_path,\n",
    "    #     results_dir=results_dir_tanh,\n",
    "    #     models=model_names_tanh_L2,\n",
    "    #     include_prior=False,\n",
    "    # )\n",
    "    \n",
    "    tanh_fits[base_config_name] = tanh_fit\n",
    "    #tanh_fits_L2[base_config_name] = tanh_fit_L2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman_correlated/many\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "model_names_tanh_corr = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]#, \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "#model_names_tanh_L2_corr = [\"Dirichlet Horseshoe tanh L2\", \"Dirichlet Student T tanh L2\", \"Beta Horseshoe tanh L2\", \"Beta Student T tanh L2\"]\n",
    "\n",
    "tanh_fits_corr = {}\n",
    "tanh_fits_L2_corr = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    # tanh_fit_L2_corr = get_model_fits(\n",
    "    #     config=full_config_path,\n",
    "    #     results_dir=results_dir_tanh,\n",
    "    #     models=model_names_tanh_L2_corr,\n",
    "    #     include_prior=False,\n",
    "    # )\n",
    "    \n",
    "    tanh_fits_corr[base_config_name] = tanh_fit_corr\n",
    "    # tanh_fits_L2_corr[base_config_name] = tanh_fit_L2_corr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman_correlated\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "model_names_tanh_corr = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    # tanh_fit_L2_corr = get_model_fits(\n",
    "    #     config=full_config_path,\n",
    "    #     results_dir=results_dir_tanh,\n",
    "    #     models=model_names_tanh_L2_corr,\n",
    "    #     include_prior=False,\n",
    "    # )\n",
    "    \n",
    "    tanh_fits_corr[base_config_name] = tanh_fit_corr\n",
    "    #tanh_fits_L2_corr[base_config_name] = tanh_fit_L2_corr\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.generate_data import generate_Friedman_data, generate_correlated_Friedman_data\n",
    "\n",
    "_FRIEDMAN_KEY = re.compile(r\"Friedman_N(\\d+)_p\\d+_sigma([\\d.]+)_seed(\\d+)\")\n",
    "\n",
    "def extract_friedman_metadata(key: str):\n",
    "    \"\"\"\n",
    "    Parse 'Friedman_N{N}_p10_sigma{sigma}_seed{seed}' -> (N:int, sigma:float, seed:int)\n",
    "    Returns (None, None, None) if it doesn't match.\n",
    "    \"\"\"\n",
    "    m = _FRIEDMAN_KEY.search(key)\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "    N = int(m.group(1))\n",
    "    sigma = float(m.group(2))\n",
    "    seed = int(m.group(3))\n",
    "    return N, sigma, seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_tanh\n",
    "\n",
    "def forward_pass_tanh(X, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Forward pass for a single layer BNN.\n",
    "    \"\"\"\n",
    "    pre_act_1 = X @ W1 + b1.reshape(1, -1)\n",
    "    #pre_hidden += b1.reshape(1, -1)\n",
    "    post_act_1 = np.tanh(pre_act_1)\n",
    "    ouput = post_act_1 @ W2 + b2.reshape(1, -1)\n",
    "    return ouput\n",
    "\n",
    "\n",
    "def forward_pass_tanh_L2(X, W1, b1, W2, b2, WL, bL):\n",
    "    \"\"\"\n",
    "    Forward pass for a single layer BNN.\n",
    "    \"\"\"\n",
    "    pre_act_1 = X @ W1 + b1.reshape(1, -1)\n",
    "    post_act_1 = np.tanh(pre_act_1)\n",
    "    pre_act_2 = post_act_1 @ W2 + b2.reshape(1, -1)\n",
    "    post_act_2 = np.tanh(pre_act_2)\n",
    "    ouput = post_act_2 @ WL + bL.reshape(1, -1)\n",
    "    return ouput\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# You already have these somewhere:\n",
    "# from your_data_module import generate_Friedman_data, generate_correlated_Friedman_data\n",
    "\n",
    "\n",
    "def _find_run_key(fits_by_run, *, N, D, sigma, seed, correlated=False):\n",
    "    \"\"\"\n",
    "    Tries to find the right key inside e.g. tanh_fits for the requested (N, D, sigma, seed).\n",
    "    Works even if your sigma formatting differs a bit (e.g. 1, 1.0, 1.00).\n",
    "    \"\"\"\n",
    "    # Common naming you showed: Friedman_N100_p10_sigma1.00_seed3\n",
    "    # If you also have \"CorrelatedFriedman\" etc, we try both patterns.\n",
    "    sigma_strs = [\n",
    "        f\"{sigma}\",\n",
    "        f\"{sigma:.1f}\",\n",
    "        f\"{sigma:.2f}\",\n",
    "        f\"{sigma:.3f}\",\n",
    "    ]\n",
    "    candidates = []\n",
    "    for s in sigma_strs:\n",
    "        if correlated:\n",
    "            candidates.append(f\"CorrelatedFriedman_N{N}_p{D}_sigma{s}_seed{seed}\")\n",
    "        candidates.append(f\"Friedman_N{N}_p{D}_sigma{s}_seed{seed}\")\n",
    "\n",
    "    for k in candidates:\n",
    "        if k in fits_by_run:\n",
    "            return k\n",
    "\n",
    "    # If exact match not found, do a regex search (robust to extra prefixes/suffixes)\n",
    "    # Example matches: \"...Friedman_N100_p10_sigma1.00_seed3...\"\n",
    "    sig_pat = \"|\".join(re.escape(s) for s in sigma_strs)\n",
    "    base = r\"Friedman\" if not correlated else r\"(?:CorrelatedFriedman|Friedman)\"\n",
    "    pat = re.compile(\n",
    "        rf\"{base}_N{N}_p{D}_sigma(?:{sig_pat})_seed{seed}\"\n",
    "    )\n",
    "    matches = [k for k in fits_by_run.keys() if pat.search(k)]\n",
    "    if len(matches) == 1:\n",
    "        return matches[0]\n",
    "    if len(matches) > 1:\n",
    "        # Prefer the shortest/most exact-looking key\n",
    "        matches = sorted(matches, key=len)\n",
    "        return matches[0]\n",
    "\n",
    "    raise KeyError(\n",
    "        f\"Could not find a run in fits_by_run for N={N}, D={D}, sigma={sigma}, seed={seed}, correlated={correlated}.\\n\"\n",
    "        f\"Example expected key: 'Friedman_N{N}_p{D}_sigma{sigma:.2f}_seed{seed}'.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run,            # <-- pass tanh_fits here\n",
    "    models,\n",
    "    layers,\n",
    "    forward_pass,\n",
    "    *,\n",
    "    correlated=False,\n",
    "    sigma=5.0,\n",
    "    D=10,\n",
    "    N_train=100,\n",
    "    Ns=(2000,),             # <-- run multiple N's if you want\n",
    "    n_testsets=5,\n",
    "    seeds=(3,),\n",
    "    test_seed_base=123,\n",
    "):\n",
    "    \"\"\"\n",
    "    - You pass tanh_fits (a dict keyed by run-name like 'Friedman_N100_p10_sigma1.00_seed3')\n",
    "    - For each (N, seed), we:\n",
    "        1) generate n_testsets independent test sets (seed = test_seed_base + test_id)\n",
    "        2) evaluate posterior mean prediction RMSE on each test set\n",
    "        3) return:\n",
    "           df_mean: mean RMSE over testsets, *separately* per seed (not merged)\n",
    "           df_raw : per-testset RMSE rows\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for N in Ns:\n",
    "        for seed in seeds:\n",
    "            run_key = _find_run_key(\n",
    "                fits_by_run, N=N_train, D=D, sigma=sigma, seed=seed, correlated=correlated\n",
    "            )\n",
    "            fits = fits_by_run[run_key]  # <-- this is what you previously indexed manually\n",
    "\n",
    "            for test_id in range(n_testsets):\n",
    "                # Make test sets genuinely different:\n",
    "                data_seed = test_seed_base + test_id\n",
    "\n",
    "                if correlated:\n",
    "                    _, X_test, y_train_raw, y_test_raw = generate_correlated_Friedman_data(\n",
    "                        N=N, D=D, sigma=sigma, test_size=0.2, seed=data_seed, standardize_y=False\n",
    "                    )\n",
    "                else:\n",
    "                    _, X_test, y_train_raw, y_test_raw = generate_Friedman_data(\n",
    "                        N=N, D=D, sigma=sigma, test_size=0.2, seed=data_seed, standardize_y=False\n",
    "                    )\n",
    "\n",
    "                y_train_mean = y_train_raw.mean()\n",
    "                y_train_std = y_train_raw.std()\n",
    "\n",
    "                y_test = (y_test_raw - y_train_mean) / y_train_std\n",
    "                y_test_np = y_test.reshape(-1)\n",
    "\n",
    "                for model in models:\n",
    "                    fit = fits[model][\"posterior\"]\n",
    "\n",
    "                    W1_samples = fit.stan_variable(\"W_1\")         # (S, P, H)\n",
    "                    if layers == 2:\n",
    "                        W2_samples = fit.stan_variable(\"W_2\")     # (S, H, H) or similar\n",
    "                    WL_samples = fit.stan_variable(\"W_L\")         # (S, H, O)\n",
    "                    b_samples = fit.stan_variable(\"hidden_bias\")  # (S, L, H)\n",
    "                    b1_samples = b_samples[:, 0, :]\n",
    "                    if layers == 2:\n",
    "                        b2_samples = b_samples[:, 1, :]\n",
    "                    bL_samples = fit.stan_variable(\"output_bias\") # (S, O)\n",
    "\n",
    "                    S = W1_samples.shape[0]\n",
    "                    y_hats = np.zeros((S, y_test_np.shape[0]))\n",
    "\n",
    "                    for i in range(S):\n",
    "                        W1 = W1_samples[i]\n",
    "                        WL = WL_samples[i]\n",
    "\n",
    "                        if layers == 1:\n",
    "                            y_hat = forward_pass(X_test, W1, b1_samples[i], WL, bL_samples[i])\n",
    "                        else:\n",
    "                            W2 = W2_samples[i]\n",
    "                            y_hat = forward_pass(X_test, W1, b1_samples[i], W2, b2_samples[i], WL, bL_samples[i])\n",
    "\n",
    "                        y_hats[i] = y_hat.squeeze()\n",
    "\n",
    "                    post_mean = y_hats.mean(axis=0)\n",
    "                    posterior_rmse_std = np.sqrt(mean_squared_error(y_test_np, post_mean))\n",
    "                    posterior_rmse_rawscale = posterior_rmse_std * y_train_std\n",
    "\n",
    "                    rows.append({\n",
    "                        \"run_key\": run_key,\n",
    "                        \"model\": model,\n",
    "                        \"N\": N,\n",
    "                        \"D\": D,\n",
    "                        \"sigma\": sigma,\n",
    "                        \"correlated\": correlated,\n",
    "                        \"seed\": seed,\n",
    "                        \"test_set\": test_id,\n",
    "                        \"posterior_rmse\": posterior_rmse_rawscale,\n",
    "                    })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Mean over testsets, BUT kept separate for each (model, seed, N, ...)\n",
    "    group_cols = [\"model\", \"seed\", \"N\", \"D\", \"sigma\", \"correlated\"]\n",
    "    df_mean = (\n",
    "        df.groupby(group_cols, as_index=False)[\"posterior_rmse\"]\n",
    "          .mean()\n",
    "          .rename(columns={\"posterior_rmse\": \"mean_rmse_over_testsets (scaled)\"})\n",
    "          .sort_values(group_cols)\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return df_mean, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_N100, df_raw_N100 = evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run=tanh_fits,\n",
    "    models=list(tanh_fits['Friedman_N100_p10_sigma1.00_seed1'].keys()),\n",
    "    layers=1,\n",
    "    forward_pass=forward_pass_tanh,\n",
    "    correlated=False,\n",
    "    sigma=1.00,\n",
    "    D=10,\n",
    "    N_train = 100,\n",
    "    Ns=[5000],\n",
    "    n_testsets=5,\n",
    "    seeds=[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_N200, df_raw_N200 = evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run=tanh_fits,\n",
    "    models=list(tanh_fits['Friedman_N200_p10_sigma1.00_seed2'].keys()),\n",
    "    layers=1,\n",
    "    forward_pass=forward_pass_tanh,\n",
    "    correlated=False,\n",
    "    sigma=1.00,\n",
    "    D=10,\n",
    "    N_train = 200,\n",
    "    Ns=[5000],\n",
    "    n_testsets=5,\n",
    "    seeds=[2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_N500, df_raw_N500 = evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run=tanh_fits,\n",
    "    models=list(tanh_fits['Friedman_N500_p10_sigma1.00_seed11'].keys()),\n",
    "    layers=1,\n",
    "    forward_pass=forward_pass_tanh,\n",
    "    correlated=False,\n",
    "    sigma=1.00,\n",
    "    D=10,\n",
    "    N_train = 500,\n",
    "    Ns=[5000],\n",
    "    n_testsets=5,\n",
    "    seeds=[11],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_N100_corr, df_raw_N100_corr = evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run=tanh_fits_corr,\n",
    "    models=list(tanh_fits_corr['Friedman_N100_p10_sigma1.00_seed1'].keys()),\n",
    "    layers=1,\n",
    "    forward_pass=forward_pass_tanh,\n",
    "    correlated=True,\n",
    "    sigma=1.00,\n",
    "    D=10,\n",
    "    N_train = 100,\n",
    "    Ns=[500],\n",
    "    n_testsets=1,\n",
    "    seeds=[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_N200_corr, df_raw_N200_corr = evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run=tanh_fits_corr,\n",
    "    models=list(tanh_fits_corr['Friedman_N200_p10_sigma1.00_seed6'].keys()),\n",
    "    layers=1,\n",
    "    forward_pass=forward_pass_tanh,\n",
    "    correlated=True,\n",
    "    sigma=1.00,\n",
    "    D=10,\n",
    "    N_train = 200,\n",
    "    Ns=[5000],\n",
    "    n_testsets=5,\n",
    "    seeds=[6],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_N500_corr, df_raw_N500_corr = evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run=tanh_fits_corr,\n",
    "    models=list(tanh_fits_corr['Friedman_N500_p10_sigma1.00_seed11'].keys()),\n",
    "    layers=1,\n",
    "    forward_pass=forward_pass_tanh,\n",
    "    correlated=True,\n",
    "    sigma=1.00,\n",
    "    D=10,\n",
    "    N_train = 500,\n",
    "    Ns=[5000],\n",
    "    n_testsets=5,\n",
    "    seeds=[11],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# def compare_models(\n",
    "#     uncorr_by_N,\n",
    "#     corr_by_N,\n",
    "#     *,\n",
    "#     rmse_col=\"mean_rmse_over_testsets (scaled)\",\n",
    "#     model_col=\"model\",\n",
    "#     sigma=1.0,\n",
    "#     abbr=None,\n",
    "#     model_offsets=None,\n",
    "#     legend_loc=\"upper right\",\n",
    "# ):\n",
    "#     abbr = abbr or {}\n",
    "#     model_offsets = model_offsets or {}\n",
    "\n",
    "#     # Fixed color palette per model (C1–C6)\n",
    "#     color_cycle = [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\"]\n",
    "#     model_colors = {\n",
    "#         m: color_cycle[i % len(color_cycle)]\n",
    "#         for i, m in enumerate(model_offsets.keys())\n",
    "#     }\n",
    "\n",
    "#     def base_name(m):\n",
    "#         for k in model_offsets:\n",
    "#             if str(m).startswith(k):\n",
    "#                 return k\n",
    "#         return str(m)\n",
    "\n",
    "#     def short_name(base):\n",
    "#         return abbr.get(base, base)\n",
    "\n",
    "#     def plot_panel(ax, title, byN, floor=True):\n",
    "#         Ns = sorted(byN.keys())\n",
    "#         x_pos = {N: i for i, N in enumerate(Ns)}  # categorical positions\n",
    "\n",
    "#         handles_by_base = {}\n",
    "\n",
    "#         for N in Ns:\n",
    "#             df = byN[N]\n",
    "#             for _, row in df.iterrows():\n",
    "#                 base = base_name(row[model_col])\n",
    "#                 label = short_name(base)\n",
    "#                 color = model_colors.get(base, \"black\")\n",
    "#                 offset = model_offsets.get(base, 0.0)\n",
    "\n",
    "#                 sc = ax.scatter(\n",
    "#                     x_pos[N] + offset,\n",
    "#                     row[rmse_col],\n",
    "#                     s=70,\n",
    "#                     color=color,\n",
    "#                 )\n",
    "\n",
    "#                 # Store exactly one handle per model base\n",
    "#                 if base not in handles_by_base:\n",
    "#                     handles_by_base[base] = sc\n",
    "\n",
    "#         # if floor:\n",
    "#         #     ax.axhline(\n",
    "#         #         sigma, linestyle=\"--\", color=\"black\", alpha=0.7, label=\"Noise floor\"\n",
    "#         #     )\n",
    "#         if floor:\n",
    "#             noise_handle = ax.axhline(sigma, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "\n",
    "\n",
    "#         ax.set_title(title)\n",
    "#         ax.set_xlabel(\"N_train\")\n",
    "#         ax.set_xticks(list(x_pos.values()))\n",
    "#         ax.set_xticklabels([str(N) for N in Ns])\n",
    "#         ax.grid(True, axis=\"y\", alpha=0.35)\n",
    "\n",
    "#         # Build legend in fixed order\n",
    "#         handles = []\n",
    "#         labels = []\n",
    "#         for base in legend_order:\n",
    "#             if base in handles_by_base:\n",
    "#                 handles.append(handles_by_base[base])\n",
    "#                 labels.append(short_name(base))\n",
    "\n",
    "#         # # Add noise floor last (optional)\n",
    "#         # if floor:\n",
    "#         #     handles.append(ax.lines[-1])\n",
    "#         #     labels.append(\"Noise floor\")\n",
    "#         if floor and noise_handle is not None:\n",
    "#             handles.append(noise_handle)\n",
    "#             labels.append(\"Noise floor\")\n",
    "\n",
    "#         ax.legend(handles, labels, loc=legend_loc, frameon=True)\n",
    "\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "#     plot_panel(axes[0], \"Uncorrelated\", uncorr_by_N)\n",
    "#     plot_panel(axes[1], \"Correlated\", corr_by_N, floor = False)\n",
    "#     axes[0].set_ylabel(\"Posterior mean RMSE (scaled back)\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"figures_for_use_in_paper/friedman_RMSE_tanh.pdf\", bbox_inches=\"tight\")\n",
    "#     plt.show()\n",
    "#     return fig, axes\n",
    "\n",
    "def compare_models(\n",
    "    uncorr_by_N,\n",
    "    corr_by_N,\n",
    "    *,\n",
    "    rmse_col=\"mean_rmse_over_testsets (scaled)\",\n",
    "    model_col=\"model\",\n",
    "    sigma=1.0,\n",
    "    abbr=None,\n",
    "    model_offsets=None,\n",
    "    legend_order=None,\n",
    "    legend_loc=\"upper right\",\n",
    "    show_noise_floor_left=True,\n",
    "    show_noise_floor_right=False,\n",
    "    debug_unknown_models=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots two panels (uncorrelated / correlated) with jittered x-positions per model.\n",
    "    Fixes the \"mysterious black dot\" by:\n",
    "      1) clearing the axes inside each panel (defensive against state leakage),\n",
    "      2) building the legend ONLY from the scatter handles we created,\n",
    "      3) adding the noise-floor legend handle only if we actually created it,\n",
    "      4) optionally flagging models that don't match model_offsets keys (they would be black).\n",
    "    \"\"\"\n",
    "    abbr = abbr or {}\n",
    "    model_offsets = model_offsets or {}\n",
    "    legend_order = legend_order or list(model_offsets.keys())\n",
    "\n",
    "    # Fixed color palette per model\n",
    "    color_cycle = [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\"]\n",
    "    model_colors = {\n",
    "        m: color_cycle[i % len(color_cycle)]\n",
    "        for i, m in enumerate(model_offsets.keys())\n",
    "    }\n",
    "\n",
    "    def base_name(m):\n",
    "        s = str(m)\n",
    "        for k in model_offsets:\n",
    "            if s.startswith(k):\n",
    "                return k\n",
    "        return s\n",
    "\n",
    "    def short_name(base):\n",
    "        return abbr.get(base, base)\n",
    "\n",
    "    def plot_panel(ax, title, byN, floor=True):\n",
    "        # Defensive: ensure no artists from previous plots live on this axes\n",
    "        ax.clear()\n",
    "\n",
    "        Ns = sorted(byN.keys())\n",
    "        x_pos = {N: i for i, N in enumerate(Ns)}  # categorical positions\n",
    "\n",
    "        handles_by_base = {}\n",
    "        noise_handle = None\n",
    "\n",
    "        for N in Ns:\n",
    "            df = byN[N]\n",
    "            for _, row in df.iterrows():\n",
    "                base = base_name(row[model_col])\n",
    "\n",
    "                if debug_unknown_models and base not in model_offsets:\n",
    "                    print(f\"[WARN] Unknown base '{base}' from model '{row[model_col]}'\")\n",
    "\n",
    "                color = model_colors.get(base, \"black\")\n",
    "                offset = model_offsets.get(base, 0.0)\n",
    "\n",
    "                sc = ax.scatter(\n",
    "                    x_pos[N] + offset,\n",
    "                    row[rmse_col],\n",
    "                    s=70,\n",
    "                    color=color,\n",
    "                )\n",
    "\n",
    "                # Store exactly one handle per model base for the legend\n",
    "                if base not in handles_by_base:\n",
    "                    handles_by_base[base] = sc\n",
    "\n",
    "        if floor:\n",
    "            noise_handle = ax.axhline(sigma, linestyle=\"--\", color=\"black\", alpha=0.7)\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"N_train\")\n",
    "        ax.set_xticks(list(x_pos.values()))\n",
    "        ax.set_xticklabels([str(N) for N in Ns])\n",
    "        ax.grid(True, axis=\"y\", alpha=0.35)\n",
    "\n",
    "        # Build legend ONLY from what we plotted\n",
    "        handles = []\n",
    "        labels = []\n",
    "        for base in legend_order:\n",
    "            if base in handles_by_base:\n",
    "                handles.append(handles_by_base[base])\n",
    "                labels.append(short_name(base))\n",
    "\n",
    "        if floor and noise_handle is not None:\n",
    "            handles.append(noise_handle)\n",
    "            labels.append(\"Noise floor\")\n",
    "\n",
    "        ax.legend(handles, labels, loc=legend_loc, frameon=True)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "    plot_panel(axes[0], \"Uncorrelated\", uncorr_by_N, floor=show_noise_floor_left)\n",
    "    plot_panel(axes[1], \"Correlated\", corr_by_N, floor=show_noise_floor_right)\n",
    "\n",
    "    axes[0].set_ylabel(\"Posterior mean RMSE (scaled back)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "uncorr_by_N = {100: df_mean_N100, 200: df_mean_N200, 500: df_mean_N500}\n",
    "corr_by_N   = {100: df_mean_N100_corr, 200: df_mean_N200_corr, 500: df_mean_N500_corr}\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian\": \"Gauss\",\n",
    "    \"Regularized Horseshoe\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DST\",\n",
    "    # \"Beta Horseshoe\": \"BHS\",\n",
    "    # \"Beta Student T\": \"BST\",\n",
    "}\n",
    "\n",
    "model_offsets = {\n",
    "    \"Gaussian\": -0.12,\n",
    "    \"Regularized Horseshoe\": -0.04,\n",
    "    \"Dirichlet Horseshoe\": +0.04,\n",
    "    \"Dirichlet Student T\": +0.12,\n",
    "    # \"Beta Horseshoe\": +0.12,\n",
    "    # \"Beta Student T\": +0.22,\n",
    "}\n",
    "\n",
    "legend_order = [\n",
    "    \"Gaussian\",\n",
    "    \"Regularized Horseshoe\",\n",
    "    \"Dirichlet Horseshoe\",\n",
    "    \"Dirichlet Student T\",\n",
    "    # \"Beta Horseshoe\",\n",
    "    # \"Beta Student T\",\n",
    "]\n",
    "\n",
    "\n",
    "fig, axes = compare_models(\n",
    "    uncorr_by_N,\n",
    "    corr_by_N,\n",
    "    abbr=abbr,\n",
    "    model_offsets=model_offsets,\n",
    "    sigma=1.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_N100_corr_L2, df_raw_N100_corr_L2 = evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run=tanh_fits_L2_corr,\n",
    "    models=list(tanh_fits_L2_corr['Friedman_N100_p10_sigma1.00_seed2'].keys()),\n",
    "    layers=2,\n",
    "    forward_pass=forward_pass_tanh_L2,\n",
    "    correlated=True,\n",
    "    sigma=1.00,\n",
    "    D=10,\n",
    "    N_train = 100,\n",
    "    Ns=[5000],\n",
    "    n_testsets=5,\n",
    "    seeds=[2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_N200_corr_L2, df_raw_N200_corr_L2 = evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run=tanh_fits_L2_corr,\n",
    "    models=list(tanh_fits_L2_corr['Friedman_N200_p10_sigma1.00_seed7'].keys()),\n",
    "    layers=2,\n",
    "    forward_pass=forward_pass_tanh_L2,\n",
    "    correlated=True,\n",
    "    sigma=1.00,\n",
    "    D=10,\n",
    "    N_train = 200,\n",
    "    Ns=[5000],\n",
    "    n_testsets=5,\n",
    "    seeds=[7],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_N500_corr_L2, df_raw_N500_corr_L2 = evaluate_posterior_on_multiple_testsets(\n",
    "    fits_by_run=tanh_fits_L2_corr,\n",
    "    models=list(tanh_fits_L2_corr['Friedman_N500_p10_sigma1.00_seed12'].keys()),\n",
    "    layers=2,\n",
    "    forward_pass=forward_pass_tanh_L2,\n",
    "    correlated=True,\n",
    "    sigma=1.00,\n",
    "    D=10,\n",
    "    N_train = 500,\n",
    "    Ns=[5000],\n",
    "    n_testsets=5,\n",
    "    seeds=[12],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_by_N = {100: df_mean_N100_corr, 200: df_mean_N200_corr, 500: df_mean_N500_corr}\n",
    "corr_by_N_L2   = {100: df_mean_N100_corr_L2, 200: df_mean_N200_corr_L2, 500: df_mean_N500_corr_L2}\n",
    "\n",
    "fig, axes = compare_models(\n",
    "    corr_by_N,\n",
    "    corr_by_N_L2,\n",
    "    abbr=abbr,\n",
    "    model_offsets=model_offsets,\n",
    "    sigma=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# posterior mean prediction on standardized scale (most likely)\n",
    "output_mean_std_DHS = (\n",
    "    tanh_fits_corr['Friedman_N100_p10_sigma1.00_seed1']\n",
    "    ['Dirichlet Horseshoe tanh']['posterior']\n",
    "    .stan_variable(\"output_test\")\n",
    "    .mean(axis=0)\n",
    ").reshape(-1)\n",
    "\n",
    "output_mean_std_BHS = (\n",
    "    tanh_fits_corr['Friedman_N100_p10_sigma1.00_seed1']\n",
    "    ['Beta Horseshoe tanh']['posterior']\n",
    "    .stan_variable(\"output_test\")\n",
    "    .mean(axis=0)\n",
    ").reshape(-1)\n",
    "\n",
    "_, X_test, y_train_raw, y_test_raw = generate_correlated_Friedman_data(\n",
    "    N=100, D=10, sigma=1.0, test_size=0.2, seed=1, standardize_y=False\n",
    ")\n",
    "\n",
    "y_train_raw = y_train_raw.reshape(-1)\n",
    "y_test_raw  = y_test_raw.reshape(-1)\n",
    "\n",
    "y_mean = y_train_raw.mean()\n",
    "y_std  = y_train_raw.std()\n",
    "\n",
    "# IMPORTANT: fully invert standardization if output is on standardized scale\n",
    "y_pred_raw_DHS = y_mean + y_std * output_mean_std_DHS\n",
    "y_pred_raw_BHS = y_mean + y_std * output_mean_std_BHS\n",
    "\n",
    "# ---- sanity checks (very helpful) ----\n",
    "print(\"len(output_mean_std):\", len(output_mean_std_DHS))\n",
    "print(\"len(y_train_raw):\", len(y_train_raw), \"len(y_test_raw):\", len(y_test_raw))\n",
    "\n",
    "# If your \"output\" is for TRAINING points, it should match len(y_train_raw) (80 here).\n",
    "# If it matches 20, it’s probably test output. If it matches 100, it’s for all data.\n",
    "# Pick the corresponding y_true.\n",
    "if len(output_mean_std_DHS) == len(y_train_raw):\n",
    "    y_true_raw = y_train_raw\n",
    "    title_suffix = \"train\"\n",
    "elif len(output_mean_std_DHS) == len(y_test_raw):\n",
    "    y_true_raw = y_test_raw\n",
    "    title_suffix = \"test\"\n",
    "else:\n",
    "    # fallback: truncate to min length (not ideal, but avoids crashing)\n",
    "    # n = min(len(output_mean_std_DHS), len(y_train_raw))\n",
    "    # y_true_raw = y_train_raw[:n]\n",
    "    # y_pred_raw = y_pred_raw[:n]\n",
    "    print(f\"ERROR\")\n",
    "\n",
    "# ---- plot 1: pred vs true (scatter with y=x line) ----\n",
    "plt.figure()\n",
    "plt.scatter(y_true_raw, y_pred_raw_DHS, alpha=0.7, label=\"DHS\", color=\"Orange\")\n",
    "lo = min(y_true_raw.min(), y_pred_raw_DHS.min())\n",
    "hi = max(y_true_raw.max(), y_pred_raw_DHS.max())\n",
    "plt.scatter(y_true_raw, y_pred_raw_BHS, alpha=0.7, label=\"BHS\", color=\"Green\")\n",
    "plt.plot([lo, hi], [lo, hi])  # identity line\n",
    "plt.xlabel(\"True y (raw)\")\n",
    "plt.ylabel(\"Predicted y (raw)\")\n",
    "plt.title(f\"Pred vs True ({title_suffix})\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# ---- plot 2: index plot (both series over observations) ----\n",
    "plt.figure()\n",
    "idx = np.arange(len(y_true_raw))\n",
    "plt.scatter(idx, y_true_raw, label=\"true\", alpha=0.8)\n",
    "plt.scatter(idx, y_pred_raw_DHS, label=\"pred DHS\", alpha=0.8)\n",
    "plt.scatter(idx, y_pred_raw_BHS, label=\"pred BHS\", alpha=0.8)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"y (raw)\")\n",
    "plt.title(f\"True and Pred over index ({title_suffix})\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_tanh, local_prune_weights\n",
    "\n",
    "def local_prune_weights(weights, sparsity_level, index_to_prune=0):\n",
    "    \"\"\"\n",
    "    Apply pruning to only one weight matrix in a list, specified by index.\n",
    "\n",
    "    Parameters:\n",
    "    - weights: list of np.ndarray (e.g., [W1, W2])\n",
    "    - sparsity_level: fraction of weights to prune (0.0 to 1.0)\n",
    "    - index_to_prune: which weight matrix to prune in the list\n",
    "\n",
    "    Returns:\n",
    "    - list of masks (one for each weight matrix)\n",
    "    \"\"\"\n",
    "    masks = [np.ones_like(W) for W in weights]\n",
    "\n",
    "    W = weights[index_to_prune]\n",
    "    flat = np.abs(W.flatten())\n",
    "    num_to_prune = int(np.floor(sparsity_level * flat.size))\n",
    "\n",
    "    if num_to_prune > 0:\n",
    "        idx = np.argpartition(flat, num_to_prune)[:num_to_prune]\n",
    "        mask_flat = np.ones_like(flat, dtype=bool)\n",
    "        mask_flat[idx] = False\n",
    "        masks[index_to_prune] = mask_flat.reshape(W.shape).astype(float)\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "def forward_pass_tanh_L2(X, W1, b1, W2, b2, WL, bL):\n",
    "    \"\"\"\n",
    "    Forward pass for a single layer BNN.\n",
    "    \"\"\"\n",
    "    pre_act_1 = X @ W1 + b1.reshape(1, -1)\n",
    "    post_act_1 = np.tanh(pre_act_1)\n",
    "    pre_act_2 = post_act_1 @ W2 + b2.reshape(1, -1)\n",
    "    post_act_2 = np.tanh(pre_act_2)\n",
    "    ouput = post_act_2 @ WL + bL.reshape(1, -1)\n",
    "    return ouput\n",
    "\n",
    "\n",
    "def compute_sparse_rmse_results(seeds, models, all_fits, get_N_sigma, forward_pass, correlated, layers=1,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "        # path = f\"datasets/{folder}/{dataset_key}.npz\"\n",
    "\n",
    "        # try:\n",
    "        #     data = np.load(path)\n",
    "        #     X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "        # except FileNotFoundError:\n",
    "        #     print(f\"[SKIP] File not found: {path}\")\n",
    "        #     continue\n",
    "        \n",
    "        if correlated:\n",
    "            _, X_test, y_train_raw, y_test_raw = generate_correlated_Friedman_data(\n",
    "                N=5000, D=10, sigma=sigma, test_size=0.2, seed=123, standardize_y=False\n",
    "            )\n",
    "        else:\n",
    "            _, X_test, y_train_raw, y_test_raw = generate_Friedman_data(\n",
    "                N=5000, D=10, sigma=sigma, test_size=0.2, seed=123, standardize_y=False\n",
    "            )\n",
    "            \n",
    "        y_train_mean = y_train_raw.mean()\n",
    "        y_train_std = y_train_raw.std()\n",
    "\n",
    "        y_test = (y_test_raw - y_train_mean) / y_train_std\n",
    "        y_test = y_test.reshape(-1)\n",
    "\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                if layers == 2:\n",
    "                    W2_samples = fit.stan_variable(\"W_2\")           # (S, P, H)\n",
    "                WL_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b_samples = fit.stan_variable(\"hidden_bias\")   # (S, L, H)\n",
    "                b1_samples = b_samples[:, 0, :]\n",
    "                if layers == 2:\n",
    "                    b2_samples = b_samples[:, 1, :]\n",
    "                bL_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "            #print(y_test.shape)\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                if layers == 2:\n",
    "                    W2 = W2_samples[i]\n",
    "                WL = WL_samples[i]\n",
    "\n",
    "                # Apply pruning mask if requested\n",
    "                if prune_fn is not None and sparsity > 0.0:\n",
    "                    mask_W1 = prune_fn([W1, WL], sparsity)\n",
    "                    if layers == 2:\n",
    "                        mask_W2 = prune_fn([W1, W2], sparsity, index_to_prune=1)\n",
    "                    W1 = W1 * mask_W1[0]\n",
    "                    if layers == 2:\n",
    "                        W2 = W2 * mask_W2[1]\n",
    "                    # if i == 0:\n",
    "                    #     print(W2)\n",
    "                if layers == 1:\n",
    "                    y_hat = forward_pass(X_test, W1, b1_samples[i], WL, bL_samples[i])\n",
    "                    y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "                    rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "                else:\n",
    "                    y_hat = forward_pass(X_test, W1, b1_samples[i], W2, b2_samples[i], WL, bL_samples[i])\n",
    "                    y_hats[i] = y_hat.squeeze()\n",
    "                    rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "\n",
    "                \n",
    "            posterior_mean = np.mean(y_hats, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse,\n",
    "                'posterior_mean_rmse_scaled': posterior_mean_rmse*y_train_std\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'sparsity': sparsity,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "\n",
    "seeds = [1]\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    if seed in [1, 3, 4, 5, 6]:\n",
    "        N=100\n",
    "        sigma=1.00\n",
    "    elif seed in [2, 7, 8, 9, 10]:\n",
    "        N=200\n",
    "        sigma=1.00\n",
    "    else:\n",
    "        N=500\n",
    "        sigma=1.00\n",
    "    return N, sigma\n",
    "\n",
    "def get_N_sigma_correlated(seed):\n",
    "    if seed in [1, 2, 3, 4, 5]:\n",
    "        N=100\n",
    "        sigma=1.00\n",
    "    elif seed in [6, 7, 8, 9, 10]:\n",
    "        N=200\n",
    "        sigma=1.00\n",
    "    else:\n",
    "        N=500\n",
    "        sigma=1.00\n",
    "    return N, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse_sparse_tanh, df_posterior_rmse_sparse_tanh = {}, {}\n",
    "df_rmse_sparse_tanh_L2, df_posterior_rmse_sparse_tanh_L2 = {}, {}\n",
    "df_rmse_sparse_tanh_correlated, df_posterior_rmse_sparse_tanh_correlated = {}, {}\n",
    "df_rmse_sparse_tanh_correlated_L2, df_posterior_rmse_sparse_tanh_correlated_L2 = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    df_rmse_sparse_tanh[sparsity], df_posterior_rmse_sparse_tanh[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_tanh, tanh_fits, get_N_sigma, forward_pass_tanh, correlated = False,\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )\n",
    "    \n",
    "    # df_rmse_sparse_tanh_L2[sparsity], df_posterior_rmse_sparse_tanh_L2[sparsity] = compute_sparse_rmse_results(\n",
    "    #     seeds, model_names_tanh_L2, tanh_fits_L2, get_N_sigma, forward_pass_tanh_L2, correlated = False, layers=2,\n",
    "    #     sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    # )\n",
    "    \n",
    "    # df_rmse_sparse_tanh_correlated[sparsity], df_posterior_rmse_sparse_tanh_correlated[sparsity] = compute_sparse_rmse_results(\n",
    "    #     seeds, model_names_tanh, tanh_fits_corr, get_N_sigma_correlated, forward_pass_tanh, correlated = True,\n",
    "    #     sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    # )\n",
    "    \n",
    "    # df_rmse_sparse_tanh_correlated_L2[sparsity], df_posterior_rmse_sparse_tanh_correlated_L2[sparsity] = compute_sparse_rmse_results(\n",
    "    #     seeds, model_names_tanh_L2, tanh_fits_L2_corr, get_N_sigma_correlated, forward_pass_tanh_L2, correlated = True, layers=2,\n",
    "    #     sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dict: sparsity -> dataframe\n",
    "# df_posterior_rmse_sparse_tanh_correlated_L2 = {...}\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Stack into one long df\n",
    "# ----------------------------\n",
    "dfs = []\n",
    "for sparsity, df in df_posterior_rmse_sparse_tanh.items():\n",
    "    tmp = df.copy()\n",
    "    tmp[\"sparsity\"] = float(sparsity)\n",
    "    dfs.append(tmp)\n",
    "\n",
    "df_long = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Ensure numeric types\n",
    "df_long[\"N\"] = pd.to_numeric(df_long[\"N\"])\n",
    "df_long[\"sparsity\"] = pd.to_numeric(df_long[\"sparsity\"])\n",
    "df_long[\"posterior_mean_rmse_scaled\"] = pd.to_numeric(df_long[\"posterior_mean_rmse_scaled\"])\n",
    "\n",
    "# Optional: average over seeds (and anything else) *within each (N, model, sparsity)*\n",
    "df_plot = (\n",
    "    df_long\n",
    "    .groupby([\"N\", \"model\", \"sparsity\"], as_index=False)[\"posterior_mean_rmse_scaled\"]\n",
    "    .mean()\n",
    "    .sort_values([\"N\", \"model\", \"sparsity\"])\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Plot: separate panel per N\n",
    "# ----------------------------\n",
    "Ns = sorted(df_plot[\"N\"].unique())\n",
    "n_panels = len(Ns)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=n_panels,\n",
    "    figsize=(5 * n_panels, 4),\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "# Make axes iterable even if only one panel\n",
    "if n_panels == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, N in zip(axes, Ns):\n",
    "    gN = df_plot[df_plot[\"N\"] == N]\n",
    "\n",
    "    for model, g in gN.groupby(\"model\"):\n",
    "        if model == \"Gaussian tanh\":\n",
    "            continue\n",
    "        g = g.sort_values(\"sparsity\")\n",
    "        ax.plot(\n",
    "            g[\"sparsity\"],\n",
    "            g[\"posterior_mean_rmse_scaled\"],\n",
    "            marker=\"o\",\n",
    "            label=model,\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"N = {N}\")\n",
    "    ax.set_xlabel(\"Sparsity\")\n",
    "    ax.grid(True)\n",
    "\n",
    "axes[0].set_ylabel(\"Scaled posterior mean RMSE\")\n",
    "\n",
    "# Put one legend for the whole figure (outside)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(1.02, 0.5))\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.show()  # <-- intentionally NOT called\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dict: sparsity -> dataframe\n",
    "# df_posterior_rmse_sparse_tanh_correlated_L2 = {...}\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Stack into one long df\n",
    "# ----------------------------\n",
    "dfs = []\n",
    "for sparsity, df in df_posterior_rmse_sparse_tanh_correlated.items():\n",
    "    tmp = df.copy()\n",
    "    tmp[\"sparsity\"] = float(sparsity)\n",
    "    dfs.append(tmp)\n",
    "\n",
    "df_long = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Ensure numeric types\n",
    "df_long[\"N\"] = pd.to_numeric(df_long[\"N\"])\n",
    "df_long[\"sparsity\"] = pd.to_numeric(df_long[\"sparsity\"])\n",
    "df_long[\"posterior_mean_rmse_scaled\"] = pd.to_numeric(df_long[\"posterior_mean_rmse_scaled\"])\n",
    "\n",
    "# Optional: average over seeds (and anything else) *within each (N, model, sparsity)*\n",
    "df_plot = (\n",
    "    df_long\n",
    "    .groupby([\"N\", \"model\", \"sparsity\"], as_index=False)[\"posterior_mean_rmse_scaled\"]\n",
    "    .mean()\n",
    "    .sort_values([\"N\", \"model\", \"sparsity\"])\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Plot: separate panel per N\n",
    "# ----------------------------\n",
    "Ns = sorted(df_plot[\"N\"].unique())\n",
    "n_panels = len(Ns)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=n_panels,\n",
    "    figsize=(5 * n_panels, 4),\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "# Make axes iterable even if only one panel\n",
    "if n_panels == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, N in zip(axes, Ns):\n",
    "    gN = df_plot[df_plot[\"N\"] == N]\n",
    "\n",
    "    for model, g in gN.groupby(\"model\"):\n",
    "        if model == \"Gaussian tanh\":\n",
    "            continue\n",
    "        g = g.sort_values(\"sparsity\")\n",
    "        ax.plot(\n",
    "            g[\"sparsity\"],\n",
    "            g[\"posterior_mean_rmse_scaled\"],\n",
    "            marker=\"o\",\n",
    "            label=model,\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"N = {N}\")\n",
    "    ax.set_xlabel(\"Sparsity\")\n",
    "    ax.grid(True)\n",
    "\n",
    "axes[0].set_ylabel(\"Scaled posterior mean RMSE\")\n",
    "\n",
    "# Put one legend for the whole figure (outside)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"center left\", bbox_to_anchor=(1.02, 0.5))\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.show()  # <-- intentionally NOT called\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- 1. Stack dict into one long DataFrame ----\n",
    "dfs = []\n",
    "for sparsity, df in df_posterior_rmse_sparse_tanh_correlated_L2.items():\n",
    "    df_tmp = df.copy()\n",
    "    df_tmp[\"sparsity\"] = sparsity\n",
    "    dfs.append(df_tmp)\n",
    "\n",
    "df_long = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# ---- 2. Sort for clean plotting ----\n",
    "df_long = df_long.sort_values([\"model\", \"sparsity\"])\n",
    "\n",
    "# ---- 3. Plot: RMSE vs sparsity, one curve per model ----\n",
    "plt.figure()\n",
    "\n",
    "for model, g in df_long.groupby(\"model\"):\n",
    "    if model == \"Gaussian tanh\":\n",
    "            continue\n",
    "    plt.plot(\n",
    "        g[\"sparsity\"],\n",
    "        g[\"posterior_mean_rmse_scaled\"],\n",
    "        marker=\"o\",\n",
    "        label=model,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Sparsity\")\n",
    "plt.ylabel(\"Scaled posterior mean RMSE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# plt.show()  # <-- deliberately NOT called\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
