{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"datasets/friedman_correlated/Friedman_N{500}_p10_sigma{1:.2f}_seed{11}.npz\"\n",
    "data = np.load(path)\n",
    "y_train = data[\"y_train\"].squeeze()\n",
    "print(np.var(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman/many\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/friedman\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]#, \"Pred CP tanh\"]\n",
    "\n",
    "\n",
    "relu_fits = {}\n",
    "tanh_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/friedman\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]#, \"Pred CP tanh\"]\n",
    "\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman_correlated/many\"\n",
    "results_dir_relu_correlated = \"results/regression/single_layer/relu/friedman_correlated\"\n",
    "results_dir_tanh_correlated = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "relu_fits_correlated = {}\n",
    "tanh_fits_correlated = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit_correlated = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu_correlated,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit_correlated = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh_correlated,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    relu_fits_correlated[base_config_name] = relu_fit_correlated  # use clean key\n",
    "    tanh_fits_correlated[base_config_name] = tanh_fit_correlated  # use clean key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman_correlated\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/friedman_correlated\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit_correlated = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    tanh_fit_correlated = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    relu_fits_correlated[base_config_name] = relu_fit_correlated  # use clean key\n",
    "    tanh_fits_correlated[base_config_name] = tanh_fit_correlated  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from properscoring import crps_ensemble\n",
    "from scores.probability import crps_for_ensemble\n",
    "\n",
    "_FRIEDMAN_KEY = re.compile(r\"Friedman_N(\\d+)_p\\d+_sigma([\\d.]+)_seed(\\d+)\")\n",
    "\n",
    "def extract_friedman_metadata(key: str):\n",
    "    \"\"\"\n",
    "    Parse 'Friedman_N{N}_p10_sigma{sigma}_seed{seed}' -> (N:int, sigma:float, seed:int)\n",
    "    Returns (None, None, None) if it doesn't match.\n",
    "    \"\"\"\n",
    "    m = _FRIEDMAN_KEY.search(key)\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "    N = int(m.group(1))\n",
    "    sigma = float(m.group(2))\n",
    "    seed = int(m.group(3))\n",
    "    return N, sigma, seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_from_fits(all_fits, model_names=None, folder=\"friedman\"):\n",
    "    \"\"\"\n",
    "    Iterate over all dataset keys in `all_fits` (e.g., relu_fits or tanh_fits).\n",
    "    For each model in `model_names` (or all models found if None), compute:\n",
    "      - RMSE for each posterior draw\n",
    "      - RMSE of the posterior mean predictor\n",
    "\n",
    "    Returns:\n",
    "        df_rmse: long DF with one row per posterior draw.\n",
    "        df_posterior_rmse: one row per model/dataset with posterior-mean RMSE.\n",
    "    \"\"\"\n",
    "    rmse_rows = []\n",
    "    post_mean_rows = []\n",
    "\n",
    "    for dataset_key, model_dict in all_fits.items():\n",
    "        N, sigma, seed = extract_friedman_metadata(dataset_key)\n",
    "        if N is None:\n",
    "            # Skip non-Friedman entries if any\n",
    "            continue\n",
    "\n",
    "        \n",
    "        try:\n",
    "            path = f\"datasets/{folder}/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # shape (N_test,)\n",
    "        except FileNotFoundError:\n",
    "            path = f\"datasets/{folder}/many/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # shape (N_test,)\n",
    "            #print(f\"[SKIP] y_test not found: {path}\")\n",
    "            #continue\n",
    "\n",
    "        # Choose which models to evaluate\n",
    "        models_to_eval = model_names or list(model_dict.keys())\n",
    "\n",
    "        for model in models_to_eval:\n",
    "            # Some entries may be missing\n",
    "            entry = model_dict.get(model, None)\n",
    "            if not entry or \"posterior\" not in entry:\n",
    "                print(f\"[SKIP] Missing posterior: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            fit = entry[\"posterior\"]\n",
    "\n",
    "            # Expecting (S, N_test, 1) or (S, N_test)\n",
    "            output_test = fit.stan_variable(\"output_test\")\n",
    "            if output_test.ndim == 3 and output_test.shape[-1] == 1:\n",
    "                preds = output_test[..., 0]  # (S, N_test)\n",
    "            elif output_test.ndim == 2:\n",
    "                preds = output_test  # (S, N_test)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected output_test shape {output_test.shape} for {dataset_key} -> {model}\")\n",
    "\n",
    "            # Per-sample RMSE\n",
    "            sq_err = (preds - y_test[None, :])**2  # (S, N_test)\n",
    "            rmse_per_sample = np.sqrt(np.mean(sq_err, axis=1))  # (S,)\n",
    "\n",
    "            for s_idx, rmse in enumerate(rmse_per_sample):\n",
    "                rmse_rows.append({\n",
    "                    \"dataset_key\": dataset_key,\n",
    "                    \"model\": model,\n",
    "                    \"N\": N,\n",
    "                    \"sigma\": sigma,\n",
    "                    \"seed\": seed,\n",
    "                    \"sample_idx\": s_idx,\n",
    "                    \"rmse\": float(rmse)\n",
    "                })\n",
    "\n",
    "            # Posterior-mean RMSE\n",
    "            posterior_mean = preds.mean(axis=0)  # (N_test,)\n",
    "            post_mean_rmse = float(np.sqrt(np.mean((posterior_mean - y_test)**2)))\n",
    "            post_mean_rows.append({\n",
    "                \"dataset_key\": dataset_key,\n",
    "                \"model\": model,\n",
    "                \"N\": N,\n",
    "                \"sigma\": sigma,\n",
    "                \"seed\": seed,\n",
    "                \"posterior_mean_rmse\": post_mean_rmse\n",
    "            })\n",
    "\n",
    "    df_rmse = pd.DataFrame(rmse_rows)\n",
    "    df_posterior_rmse = pd.DataFrame(post_mean_rows)\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "\n",
    "def compute_crps_from_fits(all_fits, model_names=None, folder=\"friedman\"):\n",
    "    \"\"\"\n",
    "    Compute CRPS per dataset/model using all posterior predictive samples.\n",
    "\n",
    "    Returns:\n",
    "        df_crps: one row per dataset/model with mean CRPS.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for dataset_key, model_dict in all_fits.items():\n",
    "        N, sigma, seed = extract_friedman_metadata(dataset_key)\n",
    "        if N is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            path = f\"datasets/{folder}/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # shape (N_test,)\n",
    "        except FileNotFoundError:\n",
    "            path = f\"datasets/{folder}/many/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # shape (N_test,)\n",
    "            #print(f\"[SKIP] y_test not found: {path}\")\n",
    "            #continue\n",
    "\n",
    "        models_to_eval = model_names or list(model_dict.keys())\n",
    "\n",
    "        for model in models_to_eval:\n",
    "            entry = model_dict.get(model, None)\n",
    "            if not entry or \"posterior\" not in entry:\n",
    "                print(f\"[SKIP] Missing posterior: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            fit = entry[\"posterior\"]\n",
    "            output_test = fit.stan_variable(\"output_test\")\n",
    "\n",
    "            # Expecting (S, N_test, 1) or (S, N_test)\n",
    "            if output_test.ndim == 3 and output_test.shape[-1] == 1:\n",
    "                preds = output_test[..., 0]  # (S, N_test)\n",
    "            elif output_test.ndim == 2:\n",
    "                preds = output_test  # (S, N_test)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected output_test shape {output_test.shape} for {dataset_key} -> {model}\")\n",
    "\n",
    "            # crps_ensemble expects shape (N_test, S)\n",
    "            crps_point = crps_ensemble(y_test, preds.T)  # (N_test,)\n",
    "            rows.append({\n",
    "                \"dataset_key\": dataset_key,\n",
    "                \"model\": model,\n",
    "                \"N\": N,\n",
    "                \"sigma\": sigma,\n",
    "                \"seed\": seed,\n",
    "                \"crps\": float(crps_point.mean())\n",
    "            })\n",
    "\n",
    "    df_crps = pd.DataFrame(rows)\n",
    "    return df_crps\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "def compute_crps_from_fits_distributional(\n",
    "    all_fits,\n",
    "    model_names=None,\n",
    "    folder=\"friedman\",\n",
    "    return_pointwise=True,\n",
    "    add_obs_summaries=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes CRPS per test point using posterior predictive ensemble.\n",
    "    Optionally returns pointwise CRPS (distribution across test points),\n",
    "    plus per-dataset/model summaries (mean/median/quantiles over test points).\n",
    "\n",
    "    Returns:\n",
    "        df_pointwise (optional): one row per (dataset_key, model, test_idx) with crps_i\n",
    "        df_summary: one row per (dataset_key, model) with summary stats over test points\n",
    "    \"\"\"\n",
    "    rows_pointwise = []\n",
    "    rows_summary = []\n",
    "\n",
    "    for dataset_key, model_dict in all_fits.items():\n",
    "        N, sigma, seed = extract_friedman_metadata(dataset_key)\n",
    "        if N is None:\n",
    "            continue\n",
    "\n",
    "        # load y_test\n",
    "        path1 = f\"datasets/{folder}/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "        path2 = f\"datasets/{folder}/many/Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}.npz\"\n",
    "        try:\n",
    "            data = np.load(path1)\n",
    "        except FileNotFoundError:\n",
    "            data = np.load(path2)\n",
    "        y_test = data[\"y_test\"].squeeze()  # (N_test,)\n",
    "\n",
    "        models_to_eval = model_names or list(model_dict.keys())\n",
    "\n",
    "        for model in models_to_eval:\n",
    "            entry = model_dict.get(model, None)\n",
    "            if not entry or \"posterior\" not in entry:\n",
    "                print(f\"[SKIP] Missing posterior: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            fit = entry[\"posterior\"]\n",
    "            output_test = fit.stan_variable(\"output_test\")\n",
    "\n",
    "            # (S, N_test, 1) or (S, N_test)\n",
    "            if output_test.ndim == 3 and output_test.shape[-1] == 1:\n",
    "                preds = output_test[..., 0]  # (S, N_test)\n",
    "            elif output_test.ndim == 2:\n",
    "                preds = output_test  # (S, N_test)\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected output_test shape {output_test.shape} for {dataset_key} -> {model}\")\n",
    "\n",
    "            # per test point CRPS, using full posterior predictive ensemble\n",
    "            #crps_i = crps_ensemble(y_test, preds.T)  # shape (N_test,)\n",
    "            fcst = xr.DataArray(\n",
    "                preds, \n",
    "                dims=[\"ensemble_member\", \"test_index\"]\n",
    "            )\n",
    "            obs = xr.DataArray(\n",
    "                y_test, \n",
    "                dims=[\"test_index\"]\n",
    "            )\n",
    "\n",
    "            # compute CRPS for each test point\n",
    "            crps_da = crps_for_ensemble(\n",
    "                fcst,\n",
    "                obs,\n",
    "                ensemble_member_dim=\"ensemble_member\",\n",
    "                preserve_dims=\"test_index\",\n",
    "                method=\"fair\"\n",
    "            )\n",
    "\n",
    "            crps_i = crps_da.values\n",
    "\n",
    "            # store pointwise distribution across test points\n",
    "            if return_pointwise:\n",
    "                for i, val in enumerate(crps_i):\n",
    "                    rows_pointwise.append({\n",
    "                        \"dataset_key\": dataset_key,\n",
    "                        \"model\": model,\n",
    "                        \"N\": N,\n",
    "                        \"sigma\": sigma,\n",
    "                        \"seed\": seed,\n",
    "                        \"test_idx\": i,\n",
    "                        \"y_true\": float(y_test[i]),\n",
    "                        \"crps\": float(val),\n",
    "                    })\n",
    "\n",
    "            # store summaries over test points (distributional summaries!)\n",
    "            if add_obs_summaries:\n",
    "                rows_summary.append({\n",
    "                    \"dataset_key\": dataset_key,\n",
    "                    \"model\": model,\n",
    "                    \"N\": N,\n",
    "                    \"sigma\": sigma,\n",
    "                    \"seed\": seed,\n",
    "                    \"crps_mean\": float(np.mean(crps_i)),\n",
    "                    \"crps_median\": float(np.median(crps_i)),\n",
    "                    \"crps_q10\": float(np.quantile(crps_i, 0.10)),\n",
    "                    \"crps_q90\": float(np.quantile(crps_i, 0.90)),\n",
    "                    \"crps_std_over_test\": float(np.std(crps_i, ddof=1)) if len(crps_i) > 1 else 0.0,\n",
    "                })\n",
    "\n",
    "    df_pointwise = pd.DataFrame(rows_pointwise) if return_pointwise else None\n",
    "    df_summary = pd.DataFrame(rows_summary)\n",
    "\n",
    "    return df_pointwise, df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ReLU models\n",
    "df_rmse_relu, df_posterior_rmse_relu = compute_rmse_from_fits(\n",
    "    relu_fits, model_names_relu  # or None to use all found\n",
    ")\n",
    "# df_crps_relu = compute_crps_from_fits_distributional(\n",
    "#     relu_fits, model_names_relu\n",
    "# )\n",
    "\n",
    "df_rmse_relu_correlated, df_posterior_rmse_relu_correlated = compute_rmse_from_fits(\n",
    "    relu_fits_correlated, model_names_relu, folder = \"friedman_correlated\"\n",
    ")\n",
    "# df_crps_relu_correlated = compute_crps_from_fits_distributional(\n",
    "#     relu_fits_correlated, model_names_relu, folder = \"friedman_correlated\"\n",
    "# )\n",
    "\n",
    "# Evaluate tanh models\n",
    "df_rmse_tanh, df_posterior_rmse_tanh = compute_rmse_from_fits(\n",
    "    tanh_fits, model_names_tanh\n",
    ")\n",
    "# df_crps_tanh = compute_crps_from_fits_distributional(\n",
    "#     tanh_fits, model_names_tanh\n",
    "# )\n",
    "\n",
    "df_rmse_tanh_correlated, df_posterior_rmse_tanh_correlated = compute_rmse_from_fits(\n",
    "    tanh_fits_correlated, model_names_tanh, folder = \"friedman_correlated\"\n",
    ")\n",
    "# df_crps_tanh_correlated = compute_crps_from_fits_distributional(\n",
    "#     tanh_fits_correlated, model_names_tanh, folder = \"friedman_correlated\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dataset = df_crps_relu[0][df_crps_relu[0]['dataset_key'] == 'Friedman_N500_p10_sigma1.00_seed11']\n",
    "gauss = first_dataset[first_dataset['model']=='Gaussian']\n",
    "rhs = first_dataset[first_dataset['model']=='Regularized Horseshoe']\n",
    "dhs = first_dataset[first_dataset['model']=='Dirichlet Horseshoe']\n",
    "dst = first_dataset[first_dataset['model']=='Dirichlet Student T']\n",
    "\n",
    "print(np.mean(gauss['crps']))\n",
    "print(\"\\n\", np.mean(rhs['crps']))\n",
    "print(\"\\n\", np.mean(dhs['crps']))\n",
    "print(\"\\n\", np.mean(dst['crps']))\n",
    "print(\"\\n\", np.median(gauss['crps']))\n",
    "print(\"\\n\", np.median(rhs['crps']))\n",
    "print(\"\\n\", np.median(dhs['crps']))\n",
    "print(\"\\n\", np.median(dst['crps']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(gauss['crps'], label=\"Gauss\", alpha=0.5)\n",
    "plt.hist(rhs['crps'], label=\"RHS\", alpha=0.5)\n",
    "plt.hist(dhs['crps'], label=\"DHS\", alpha=0.5)\n",
    "plt.hist(dst['crps'], label=\"DST\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_crps_pointwise_relu, df_crps_summary_relu = compute_crps_from_fits_distributional(\n",
    "#     relu_fits, model_names_relu, folder=\"friedman\"\n",
    "# )\n",
    "\n",
    "# df_crps_pointwise_tanh, df_crps_summary_tanh = compute_crps_from_fits_distributional(\n",
    "#     tanh_fits, model_names_tanh, folder=\"friedman\"\n",
    "# )\n",
    "\n",
    "# df_crps_pointwise_relu_corr, df_crps_summary_relu_corr = compute_crps_from_fits_distributional(\n",
    "#     relu_fits_correlated, model_names_relu, folder=\"friedman_correlated\"\n",
    "# )\n",
    "\n",
    "# df_crps_pointwise_tanh_corr, df_crps_summary_tanh_corr = compute_crps_from_fits_distributional(\n",
    "#     tanh_fits_correlated, model_names_tanh, folder=\"friedman_correlated\"\n",
    "# )\n",
    "\n",
    "# df_crps_pointwise_relu[\"setting\"] = \"Original\"\n",
    "# df_crps_pointwise_tanh[\"setting\"] = \"Original\"\n",
    "# df_crps_pointwise_relu_corr[\"setting\"] = \"Correlated\"\n",
    "# df_crps_pointwise_tanh_corr[\"setting\"] = \"Correlated\"\n",
    "# df_crps_relu[0][\"setting\"] = \"Original\"\n",
    "# df_crps_tanh[0][\"setting\"] = \"Original\"\n",
    "# df_crps_relu_correlated[0][\"setting\"] = \"Correlated\"\n",
    "# df_crps_tanh_correlated[0][\"setting\"] = \"Correlated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # --- combine into one long df ---\n",
    "# df_crps_relu[0][\"activation\"] = \"ReLU\"\n",
    "# df_crps_tanh[0][\"activation\"] = \"tanh\"\n",
    "# df_crps_relu_correlated[0][\"activation\"] = \"ReLU\"\n",
    "# df_crps_tanh_correlated[0][\"activation\"] = \"tanh\"\n",
    "\n",
    "# df_crps_all = pd.concat(\n",
    "#     [df_crps_relu[0], df_crps_tanh[0],\n",
    "#      df_crps_relu_correlated[0], df_crps_tanh_correlated[0]],\n",
    "#     ignore_index=True\n",
    "# )\n",
    "\n",
    "# optional: make model names consistent if they include \" tanh\"/\" ReLU\" suffixes\n",
    "# df_crps_all[\"model_clean\"] = (\n",
    "#     df_crps_all[\"model\"]\n",
    "#       .str.replace(\" tanh\", \"\", regex=False)\n",
    "#       .str.replace(\" ReLU\", \"\", regex=False)\n",
    "# )\n",
    "\n",
    "# # nice ordering (edit as you like)\n",
    "# model_order = [\"Gauss\", \"RHS\", \"DHS\", \"DST\"]\n",
    "# model_order = [m for m in model_order if m in df_crps_all[\"model\"].unique()]\n",
    "\n",
    "# #df_crps_all[\"model_clean\"] = pd.Categorical(df_crps_all[\"model_clean\"],\n",
    "# #                                            categories=model_order, ordered=True)\n",
    "# df_crps_all[\"setting\"] = pd.Categorical(df_crps_all[\"setting\"],\n",
    "#                                         categories=[\"Original\", \"Correlated\"], ordered=True)\n",
    "# df_crps_all[\"activation\"] = pd.Categorical(df_crps_all[\"activation\"],\n",
    "#                                           categories=[\"ReLU\", \"tanh\"], ordered=True)\n",
    "\n",
    "# # --- JOINT/FACET plot: distributions of pointwise CRPS ---\n",
    "# g = sns.catplot(\n",
    "#     data=df_crps_all,\n",
    "#     x=\"model\", y=\"crps\",\n",
    "#     row=\"setting\", col=\"N\",\n",
    "#     hue=\"activation\",\n",
    "#     kind=\"violin\",\n",
    "#     inner=\"quartile\",\n",
    "#     cut=0,\n",
    "#     height=3.0, aspect=1.15,\n",
    "#     sharey=True\n",
    "# )\n",
    "\n",
    "# g.set_axis_labels(\"\", \"Pointwise CRPS\")\n",
    "# g.set_titles(row_template=\"{row_name}\", col_template=\"N = {col_name}\")\n",
    "\n",
    "# # rotate x labels + bump fontsize\n",
    "# for ax in g.axes.flatten():\n",
    "#     ax.tick_params(axis=\"x\", rotation=20, labelsize=11)\n",
    "#     ax.grid(True, axis=\"y\", alpha=0.25)\n",
    "\n",
    "# # put legend on top\n",
    "# sns.move_legend(g, \"upper center\", bbox_to_anchor=(0.5, 1.03), ncol=2, frameon=False, title=\"Activation\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = df_rmse_relu[df_rmse_relu['dataset_key']=='Friedman_N500_p10_sigma1.00_seed15']\n",
    "rmse_gaussian_model_1 = model_1[model_1['model']=='Gaussian']['rmse']\n",
    "rmse_RHS_model_1 = model_1[model_1['model']=='Regularized Horseshoe']['rmse']\n",
    "rmse_DHS_model_1 = model_1[model_1['model']=='Dirichlet Horseshoe']['rmse']\n",
    "rmse_DST_model_1 = model_1[model_1['model']=='Dirichlet Student T']['rmse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "test_stat, p_value = ttest_rel(rmse_RHS_model_1, rmse_DST_model_1)\n",
    "\n",
    "print(\"t =\", test_stat)\n",
    "print(\"p =\", p_value)\n",
    "\n",
    "wilcox_stat, wilcox_p_value = wilcoxon(rmse_RHS_model_1, rmse_DST_model_1)\n",
    "\n",
    "print(\"Wilcox stat =\", wilcox_stat)\n",
    "print(\"Wilcox p =\", wilcox_p_value)\n",
    "\n",
    "\n",
    "dA_std = (np.array(rmse_RHS_model_1) - np.array(rmse_gaussian_model_1)) / np.array(rmse_gaussian_model_1)\n",
    "dB_std = (np.array(rmse_DST_model_1) - np.array(rmse_gaussian_model_1)) / np.array(rmse_gaussian_model_1)\n",
    "\n",
    "standardized_test_stat, standardized_p_value = ttest_rel(dA_std, dB_std)\n",
    "standardized_wilcox_test_stat, standardized_wilcox_p_value = wilcoxon(dA_std, dB_std)\n",
    "\n",
    "print(\"stat =\", standardized_test_stat)\n",
    "print(\"p =\", standardized_p_value)\n",
    "\n",
    "print(\"Wilcox stat =\", standardized_wilcox_test_stat)\n",
    "print(\"Wilcox p =\", standardized_wilcox_p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_relu = df_rmse_relu.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"rmse\", \"mean\"),\n",
    "    acc_std=(\"rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()\n",
    "\n",
    "#print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_posterior_relu = df_posterior_rmse_relu.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"posterior_mean_rmse\", \"mean\"),\n",
    "    acc_std=(\"posterior_mean_rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_relu_correlated = df_rmse_relu_correlated.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"rmse\", \"mean\"),\n",
    "    acc_std=(\"rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_posterior_relu_correlated = df_posterior_rmse_relu_correlated.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"posterior_mean_rmse\", \"mean\"),\n",
    "    acc_std=(\"posterior_mean_rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tanh = df_rmse_tanh.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"rmse\", \"mean\"),\n",
    "    acc_std=(\"rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_posterior_tanh = df_posterior_rmse_tanh.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"posterior_mean_rmse\", \"mean\"),\n",
    "    acc_std=(\"posterior_mean_rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_tanh_correlated = df_rmse_tanh_correlated.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"rmse\", \"mean\"),\n",
    "    acc_std=(\"rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_posterior_tanh_correlated = df_posterior_rmse_tanh_correlated.groupby([\"model\", \"N\"]).agg(\n",
    "    acc_mean=(\"posterior_mean_rmse\", \"mean\"),\n",
    "    acc_std=(\"posterior_mean_rmse\", \"std\"),\n",
    "    #nll_mean=(\"nll\", \"mean\"),\n",
    "    #nll_std=(\"nll\", \"std\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_relu.to_latex(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "print(summary_relu_correlated.to_latex(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "print(summary_tanh.to_latex(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "print(summary_tanh_correlated.to_latex(index=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_posterior_relu.to_latex(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "print(summary_posterior_relu_correlated.to_latex(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "print(summary_posterior_tanh.to_latex(index=False, float_format=\"%.3f\"))\n",
    "\n",
    "print(summary_posterior_tanh_correlated.to_latex(index=False, float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = df_rmse_relu.assign(activation=\"ReLU\", setting=\"Original\")\n",
    "df2 = df_rmse_tanh.assign(activation=\"Tanh\", setting=\"Original\")\n",
    "df3 = df_rmse_relu_correlated.assign(activation=\"ReLU\", setting=\"Correlated\")\n",
    "df4 = df_rmse_tanh_correlated.assign(activation=\"Tanh\", setting=\"Correlated\")\n",
    "\n",
    "df_all = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "\n",
    "df1_pm = df_posterior_rmse_relu.assign(activation=\"ReLU\", setting=\"Original\")\n",
    "df2_pm = df_posterior_rmse_tanh.assign(activation=\"Tanh\", setting=\"Original\")\n",
    "df3_pm = df_posterior_rmse_relu_correlated.assign(activation=\"ReLU\", setting=\"Correlated\")\n",
    "df4_pm = df_posterior_rmse_tanh_correlated.assign(activation=\"Tanh\", setting=\"Correlated\")\n",
    "\n",
    "df_all_pm = pd.concat([df1_pm, df2_pm, df3_pm, df4_pm], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# --- prepare data ---\n",
    "df = df_all.copy()\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian\": \"Gauss\",\n",
    "    \"Regularized Horseshoe\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DST\",    \n",
    "    \"Beta Horseshoe\": \"BHS\",\n",
    "    \"Beta Student T\": \"BST\",\n",
    "}\n",
    "\n",
    "# unify model names across activations (strip \" tanh\")\n",
    "df[\"model_clean\"] = df[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# summary stats per (setting, N, model, activation)\n",
    "summary = (\n",
    "    df.groupby([\"setting\", \"N\", \"model_clean\", \"activation\"], as_index=False)[\"rmse\"]\n",
    "      .agg(mean=\"mean\", std=\"std\")\n",
    ")\n",
    "\n",
    "# plotting order\n",
    "settings = [\"Original\", \"Correlated\"]\n",
    "Ns = [50, 100, 200, 500]\n",
    "models = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "\n",
    "# visuals\n",
    "markers = {\"ReLU\": \"o\", \"Tanh\": \"^\"}            # shapes\n",
    "offsets = {\"ReLU\": -0.12, \"Tanh\": +0.12}        # side-by-side jitter on x\n",
    "model_offsets = {\n",
    "    \"Gaussian\": -0.08,\n",
    "    \"Regularized Horseshoe\": -0.05,\n",
    "    \"Dirichlet Horseshoe\": -0.02,\n",
    "    \"Dirichlet Student T\": +0.02,\n",
    "    \"Beta Horseshoe\": +0.05,\n",
    "    \"Beta Student T\": +0.08,\n",
    "}\n",
    "palette_list = plt.get_cmap(\"tab10\").colors\n",
    "palette = {m: palette_list[i] for i, m in enumerate(models)}\n",
    "\n",
    "# map N to base x positions and add offsets for activation\n",
    "xbase = {N: i for i, N in enumerate(Ns)}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 7), sharey=True)\n",
    "\n",
    "for ax, setting in zip(axes, settings):\n",
    "    sub = summary[summary[\"setting\"] == setting]\n",
    "    # plot each model+activation with errorbars, without lines\n",
    "    for m in models:\n",
    "        for act in [\"ReLU\", \"Tanh\"]:\n",
    "            g = sub[(sub[\"model_clean\"] == m) & (sub[\"activation\"] == act)]\n",
    "            if g.empty:\n",
    "                continue\n",
    "            #xs = [xbase[n] + offsets[act] for n in g[\"N\"]]\n",
    "            xs = [xbase[n] + offsets[act] + model_offsets[m] for n in g[\"N\"]]\n",
    "\n",
    "            ax.errorbar(\n",
    "                xs, g[\"mean\"], yerr=g[\"std\"],\n",
    "                fmt=markers[act], markersize=10,\n",
    "                linestyle=\"none\", capsize=3,\n",
    "                color=palette[m], markeredgecolor=\"black\"\n",
    "            )\n",
    "\n",
    "    ax.set_title(f\"{setting}\", fontsize=15)\n",
    "    ax.set_xticks(range(len(Ns)))\n",
    "    ax.set_xticklabels(Ns, fontsize=15)\n",
    "    ax.set_xlabel(\"N\", fontsize=15)\n",
    "    ax.set_ylabel(\"RMSE\", fontsize=15)\n",
    "    ax.tick_params(axis='y', labelsize=15)\n",
    "    ax.grid()\n",
    "\n",
    "# --- legends ---\n",
    "model_handles = [\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"none\",\n",
    "        color=palette[m],\n",
    "        markeredgecolor=\"black\",\n",
    "        markersize=12,\n",
    "        label=abbr.get(m, m)   # <- use abbreviation\n",
    "    )\n",
    "    for m in models\n",
    "]\n",
    "\n",
    "# activation legend (shapes)\n",
    "activation_handles = [\n",
    "    Line2D([0], [0], marker=markers[\"ReLU\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=12, label=\"ReLU\"),\n",
    "    Line2D([0], [0], marker=markers[\"Tanh\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=12, label=\"Tanh\"),\n",
    "]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(\n",
    "        handles=model_handles + activation_handles,\n",
    "        title=None,\n",
    "        loc=\"upper right\",\n",
    "        frameon=False,\n",
    "        ncol=1,\n",
    "        fontsize = 14\n",
    "    )\n",
    "plt.tight_layout(rect=(0, 0, 1, 1))\n",
    "#plt.grid()\n",
    "plt.savefig(\"figures_for_use_in_paper/friedman_RMSE_with_beta.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "# --- prepare data ---\n",
    "df = df_all_pm.copy()\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian\": \"Gauss\",\n",
    "    \"Regularized Horseshoe\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DST\",\n",
    "}\n",
    "\n",
    "# unify model names across activations (strip \" tanh\")\n",
    "df[\"model_clean\"] = df[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# summary stats per (setting, N, model, activation)\n",
    "summary = (\n",
    "    df.groupby([\"setting\", \"N\", \"model_clean\", \"activation\"], as_index=False)[\"posterior_mean_rmse\"]\n",
    "      .agg(mean=\"mean\", std=\"std\")\n",
    ")\n",
    "# plotting order\n",
    "settings = [\"Original\", \"Correlated\"]\n",
    "Ns = [50, 100, 200, 500]\n",
    "models = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Pred CP\"]\n",
    "\n",
    "# visuals\n",
    "markers = {\"ReLU\": \"o\", \"Tanh\": \"^\"}            # shapes\n",
    "offsets = {\"ReLU\": -0.12, \"Tanh\": +0.12}        # side-by-side jitter on x\n",
    "model_offsets = {\n",
    "    \"Gaussian\": -0.06,\n",
    "    #\"Pred CP\": -0.03,\n",
    "    \"Regularized Horseshoe\": -0.02,\n",
    "    \"Dirichlet Horseshoe\": +0.02,\n",
    "    \"Dirichlet Student T\": +0.06,\n",
    "}\n",
    "palette_list = plt.get_cmap(\"tab10\").colors\n",
    "palette = {m: palette_list[i] for i, m in enumerate(models)}\n",
    "\n",
    "# map N to base x positions and add offsets for activation\n",
    "xbase = {N: i for i, N in enumerate(Ns)}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "for ax, setting in zip(axes, settings):\n",
    "    sub = summary[summary[\"setting\"] == setting]\n",
    "    # plot each model+activation with errorbars, without lines\n",
    "    for m in models:\n",
    "        for act in [\"ReLU\", \"Tanh\"]:\n",
    "            g = sub[(sub[\"model_clean\"] == m) & (sub[\"activation\"] == act)]\n",
    "            if g.empty:\n",
    "                continue\n",
    "            #xs = [xbase[n] + offsets[act] for n in g[\"N\"]]\n",
    "            xs = [xbase[n] + offsets[act] + model_offsets[m] for n in g[\"N\"]]\n",
    "            \n",
    "            ax.plot(\n",
    "                xs, g[\"mean\"],\n",
    "                marker=markers[act],\n",
    "                markersize=7,\n",
    "                linestyle=\"none\",\n",
    "                color=palette[m],\n",
    "                markeredgecolor=\"black\",\n",
    "            )\n",
    "\n",
    "\n",
    "    ax.set_title(f\"{setting}\")\n",
    "    ax.set_xticks(range(len(Ns)))\n",
    "    ax.set_xticklabels(Ns)\n",
    "    ax.set_xlabel(\"N\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.grid()\n",
    "\n",
    "# --- legends ---\n",
    "model_handles = [\n",
    "    Line2D(\n",
    "        [0], [0],\n",
    "        marker=\"o\",\n",
    "        linestyle=\"none\",\n",
    "        color=palette[m],\n",
    "        markeredgecolor=\"black\",\n",
    "        markersize=7,\n",
    "        label=abbr.get(m, m)   # <- use abbreviation\n",
    "    )\n",
    "    for m in models\n",
    "]\n",
    "\n",
    "# activation legend (shapes)\n",
    "activation_handles = [\n",
    "    Line2D([0], [0], marker=markers[\"ReLU\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=7, label=\"ReLU\"),\n",
    "    Line2D([0], [0], marker=markers[\"Tanh\"], linestyle=\"none\", color=\"black\",\n",
    "           markersize=7, label=\"Tanh\"),\n",
    "]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend(\n",
    "        handles=model_handles + activation_handles,\n",
    "        title=None,\n",
    "        loc=\"upper right\",\n",
    "        frameon=False,\n",
    "        ncol=1\n",
    "    )\n",
    "plt.tight_layout(rect=(0, 0, 1, 1))\n",
    "#plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=df_rmse_relu\n",
    ", x=\"N\", y=\"rmse\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "sns.pointplot(data=df_rmse_tanh, x=\"N\", y=\"rmse\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "plt.title(\"RMSE relu\")\n",
    "plt.show()\n",
    "\n",
    "sns.pointplot(data=df_rmse_relu_correlated\n",
    ", x=\"N\", y=\"rmse\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "sns.pointplot(data=df_rmse_tanh_correlated, x=\"N\", y=\"rmse\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "plt.title(\"RMSE relu\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=df_posterior_rmse_relu, x=\"N\", y=\"posterior_mean_rmse\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "plt.title(\"RMSE relu\")\n",
    "plt.show()\n",
    "\n",
    "sns.pointplot(data=df_posterior_rmse_tanh, x=\"N\", y=\"posterior_mean_rmse\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "plt.title(\"RMSE tanh\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=df_posterior_rmse_relu_correlated, x=\"N\", y=\"posterior_mean_rmse\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "plt.title(\"RMSE relu\")\n",
    "plt.show()\n",
    "\n",
    "sns.pointplot(data=df_posterior_rmse_tanh_correlated, x=\"N\", y=\"posterior_mean_rmse\", hue=\"model\", dodge=True, errorbar=\"sd\")\n",
    "plt.title(\"RMSE tanh\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
