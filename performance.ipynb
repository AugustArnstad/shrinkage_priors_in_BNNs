{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman/many\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/friedman/full_regularization\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman/full_regularization\"\n",
    "model_names_relu = [\"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Dirichlet Student T tanh\"]\n",
    "#model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "#model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "relu_fits = {}\n",
    "tanh_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    # tanh_fit = get_model_fits(\n",
    "    #     config=full_config_path,\n",
    "    #     results_dir=results_dir_tanh,\n",
    "    #     models=model_names_tanh,\n",
    "    #     include_prior=False,\n",
    "    # )\n",
    "    \n",
    "\n",
    "    relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    #tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman/many\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/friedman/full_regularization\"\n",
    "model_names_relu = [\"Dirichlet Student T\"]\n",
    "\n",
    "\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    relu_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_relu,\n",
    "        models=model_names_relu,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "    relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from properscoring import crps_ensemble\n",
    "\n",
    "seed_to_N = {\n",
    "    1: 100, 3: 100, 4: 100, 5: 100, 6: 100,\n",
    "    2: 200, 7: 200, 8: 200, 9: 200, 10: 200,\n",
    "    11: 500, 12: 500, 13: 500, 14: 500, 15: 500\n",
    "}\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    if seed not in seed_to_N:\n",
    "        raise ValueError(f\"Unsupported seed: {seed}\")\n",
    "    return seed_to_N[seed], 1\n",
    "\n",
    "\n",
    "def compute_rmse_results(seeds, models, all_fits, get_N_sigma):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "        path = f\"datasets/friedman/{dataset_key}.npz\"\n",
    "\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                output_test = fit.stan_variable(\"output_test\")  # (S, N/5, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = output_test.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "\n",
    "            for i in range(S):\n",
    "                y_hat = output_test[i]\n",
    "                rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test.squeeze()) ** 2))\n",
    "                \n",
    "            posterior_mean = np.mean(output_test, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean.squeeze() - y_test.squeeze()) ** 2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "\n",
    "\n",
    "def compute_crps_results(seeds, models, all_fits, get_N_sigma):\n",
    "    \"\"\"\n",
    "    Compute CRPS for each model and dataset using full posterior predictive samples.\n",
    "\n",
    "    Returns:\n",
    "        df_crps: DataFrame with model-level CRPS scores.\n",
    "    \"\"\"\n",
    "    model_crps_list = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "        path = f\"datasets/friedman/{dataset_key}.npz\"\n",
    "\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            y_test = data[\"y_test\"].squeeze()  # shape (N_test,)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                output_test = fit.stan_variable(\"output_test\")  # (S, N_test, 1)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S, N_test, output_dim = output_test.shape\n",
    "            assert output_dim == 1, f\"Expected output dim = 1, got shape {output_test.shape}\"\n",
    "\n",
    "            preds = output_test.squeeze(-1)  # (S, N_test)\n",
    "\n",
    "            crps_per_point = crps_ensemble(y_test, preds.T)  # shape (N_test,)\n",
    "            crps_mean = crps_per_point.mean()\n",
    "\n",
    "            model_crps_list.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'crps': crps_mean\n",
    "            })\n",
    "\n",
    "    df_crps = pd.DataFrame(model_crps_list)\n",
    "\n",
    "    return df_crps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "df_rmse_relu, df_posterior_rmse_relu = compute_rmse_results(\n",
    "    seeds, model_names_relu, relu_fits, get_N_sigma\n",
    ")\n",
    "\n",
    "# df_rmse_tanh, df_posterior_rmse_tanh = compute_rmse_results(\n",
    "#     seeds, model_names_tanh, tanh_fits, get_N_sigma\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 2]\n",
    "\n",
    "df_crps_relu = compute_crps_results(\n",
    "    seeds, model_names_relu, relu_fits, get_N_sigma\n",
    ")\n",
    "\n",
    "df_crps_tanh = compute_crps_results(\n",
    "    seeds, model_names_tanh, tanh_fits, get_N_sigma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crps_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crps_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "# Combine and tag activation\n",
    "df_relu = df_rmse_relu.copy()\n",
    "df_relu[\"activation\"] = \"ReLU\"\n",
    "\n",
    "df_tanh = df_rmse_tanh.copy()\n",
    "df_tanh[\"activation\"] = \"tanh\"\n",
    "\n",
    "df_all = pd.concat([df_relu, df_tanh])\n",
    "df_all[\"model\"] = df_all[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# Order of models and activations\n",
    "model_order = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "activation_order = [\"ReLU\", \"tanh\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7), sharey=True)\n",
    "\n",
    "for i, N_val in enumerate([100, 200]):\n",
    "    ax = axes[i]\n",
    "    df_plot = df_all[(df_all[\"N\"] == N_val)].copy()\n",
    "\n",
    "    # Use model as x, activation as hue\n",
    "    sns.boxplot(\n",
    "        data=df_plot,\n",
    "        x=\"model\",\n",
    "        y=\"rmse\",\n",
    "        hue=\"activation\",\n",
    "        order=model_order,\n",
    "        hue_order=activation_order,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"N = {N_val}\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "# Add shared legend at top center\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, title=\"Activation\", loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def compute_true_y_std_from_Xtrain(X_train, sigma, seed):\n",
    "    x0, x1, x2, x3, x4 = X_train[:, 0], X_train[:, 1], X_train[:, 2], X_train[:, 3], X_train[:, 4]\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5 * x4\n",
    "    )\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = y_clean + rng.normal(0, sigma, size=len(X_train))\n",
    "    return y.std()\n",
    "\n",
    "def get_y_std_map_from_X(data_dir=\"datasets/friedman\"):\n",
    "    y_stds = {}\n",
    "    for path in glob.glob(f\"{data_dir}/*.npz\"):\n",
    "        data = np.load(path)\n",
    "        X_train = data[\"X_train\"]\n",
    "        \n",
    "        # Parse metadata from filename\n",
    "        fname = path.split(\"/\")[-1].replace(\".npz\", \"\")\n",
    "        parts = fname.split(\"_\")\n",
    "        N = int(parts[1][1:])\n",
    "        sigma = float(parts[3][5:])\n",
    "        seed = int(parts[4][4:])\n",
    "\n",
    "        y_std = compute_true_y_std_from_Xtrain(X_train, sigma, seed)\n",
    "        y_stds[fname] = y_std\n",
    "    return y_stds\n",
    "\n",
    "# Step 2: Add original-scale RMSE to your DataFrame\n",
    "def add_original_rmse(df, y_std_map):\n",
    "    df = df.copy()\n",
    "    df[\"dataset_key\"] = df.apply(\n",
    "        lambda row: f\"Friedman_N{row.N}_p10_sigma{row.sigma:.2f}_seed{row.seed}\", axis=1\n",
    "    )\n",
    "    df[\"rmse_original\"] = df.apply(\n",
    "        lambda row: row.rmse * y_std_map.get(row.dataset_key, np.nan), axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "y_std_map = get_y_std_map_from_X(\"datasets/friedman\")\n",
    "df_rmse_with_orig_relu = add_original_rmse(df_rmse_relu, y_std_map)\n",
    "df_rmse_with_orig_tanh = add_original_rmse(df_rmse_tanh, y_std_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "# Combine and tag activation\n",
    "df_relu = df_rmse_with_orig_relu.copy()\n",
    "df_relu[\"activation\"] = \"ReLU\"\n",
    "\n",
    "df_tanh = df_rmse_with_orig_tanh.copy()\n",
    "df_tanh[\"activation\"] = \"tanh\"\n",
    "\n",
    "df_all = pd.concat([df_relu, df_tanh])\n",
    "df_all[\"model\"] = df_all[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# Order of models and activations\n",
    "model_order = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "activation_order = [\"ReLU\", \"tanh\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7), sharey=True)\n",
    "\n",
    "for i, N_val in enumerate([100, 200]):\n",
    "    ax = axes[i]\n",
    "    df_plot = df_all[(df_all[\"N\"] == N_val)].copy()\n",
    "\n",
    "    # Use model as x, activation as hue\n",
    "    sns.boxplot(\n",
    "        data=df_plot,\n",
    "        x=\"model\",\n",
    "        y=\"rmse_original\",\n",
    "        hue=\"activation\",\n",
    "        order=model_order,\n",
    "        hue_order=activation_order,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"N = {N_val}\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "# Add shared legend at top center\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, title=\"Activation\", loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
