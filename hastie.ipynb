{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/hastie\"\n",
    "results_dir_tanh = \"results/hastie\"\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "full_config_path = \"hastie_N80_p20\"\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from properscoring import crps_ensemble\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# IMPORTANT: this y_test must correspond to the same test set used to make `output_test` in Stan,\n",
    "# otherwise scores won’t be comparable.\n",
    "from utils.generate_data import generate_latent_hastie_data\n",
    "X_train, X_test, y_train, y_test = generate_latent_hastie_data(n=100, p=20)\n",
    "\n",
    "rows = []\n",
    "for model_name, model_entry in tanh_fit.items():\n",
    "    post = model_entry[\"posterior\"]\n",
    "\n",
    "    # (S, n_test)\n",
    "    y_samps = post.stan_variable(\"output_test\").squeeze(-1)\n",
    "\n",
    "    # Optional: limit to first S draws if desired\n",
    "    # S = min(4000, y_samps.shape[0])\n",
    "    # y_samps = y_samps[:S]\n",
    "\n",
    "    # Posterior-mean predictions and RMSE\n",
    "    y_mean = y_samps.mean(axis=0)                                   # (n_test,)\n",
    "    rmse_post_mean = float(np.sqrt(mean_squared_error(y_test, y_mean)))\n",
    "\n",
    "    # Per-draw RMSEs and their mean\n",
    "    per_draw_rmse = np.sqrt(((y_samps - y_test[None, :])**2).mean(axis=1))  # (S,)\n",
    "    rmse_draw_mean = float(per_draw_rmse.mean())\n",
    "\n",
    "    # CRPS across the ensemble (expects shape (n_test, S))\n",
    "    crps = float(np.mean(crps_ensemble(y_test, y_samps.T)))\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE_posterior_mean\": rmse_post_mean,\n",
    "        \"RMSE_mean_over_draws\": rmse_draw_mean,\n",
    "        \"CRPS\": crps,\n",
    "        \"n_draws\": y_samps.shape[0]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values(\"RMSE_posterior_mean\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def make_latent_data_sec54(n, p, d=5, r_theta=1.0, sigma_xi=0.0, random_state=42, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Section 5.4 latent model (Hastie–Montanari–Rosset–Tibshirani):\n",
    "      X = Z W^T + U,   y = Z θ + ξ\n",
    "      z_i ~ N(0, I_d), u_ij ~ N(0, 1), ξ_i ~ N(0, σ_ξ^2)\n",
    "    Rows w_j of W satisfy ||w_j|| = 1.               [Fig. 5/6 setup]\n",
    "    Population mapping to linear model:\n",
    "      Σ = I_p + W W^T,   β = W (I + W^T W)^{-1} θ.   [eqs. (26)-(27)]\n",
    "    Returns: X (n×p), y (n,), W (p×d), theta (d,), beta_true (p,), Sigma (p×p)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Random W with unit-norm rows (||w_j||=1)\n",
    "    W = rng.normal(size=(p, d))\n",
    "    W /= np.linalg.norm(W, axis=1, keepdims=True) + 1e-12  # enforce ||w_j||=1\n",
    "\n",
    "    # Latent Z, feature noise U, label noise ξ\n",
    "    Z = rng.normal(size=(n, d))\n",
    "    U = rng.normal(size=(n, p))\n",
    "    xi = rng.normal(scale=sigma_xi, size=n)\n",
    "\n",
    "    # Signal vector θ with ||θ|| = r_theta\n",
    "    theta = rng.normal(size=d)\n",
    "    theta *= r_theta / (np.linalg.norm(theta) + 1e-12)\n",
    "\n",
    "    # Data\n",
    "    X = Z @ W.T + U\n",
    "    y = Z @ theta + xi\n",
    "\n",
    "    # Population quantities for risk\n",
    "    Sigma = np.eye(p) + W @ W.T\n",
    "    beta_true = W @ np.linalg.solve(np.eye(d) + W.T @ W, theta)  # β = W (I + W^T W)^(-1) θ\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, Z, y_train, W, theta, beta_true, Sigma\n",
    "\n",
    "X_train, Z, y_train, W, theta, beta_true, Sigma = make_latent_data_sec54(100, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bayes_rmse_floor(W, theta, sigma_xi):\n",
    "    Ainv = np.linalg.inv(np.eye(W.shape[1]) + W.T @ W)         # (I + W^T W)^{-1}\n",
    "    mse = float(theta.T @ Ainv @ theta) + sigma_xi**2\n",
    "    return np.sqrt(mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_rmse_floor(W, theta, sigma_xi=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import generate_latent_hastie_data\n",
    "def compute_sparse_rmse_results_hastie(models, all_fits, forward_pass,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "    _, X_test, _, y_test = generate_latent_hastie_data(n=100, p=20)\n",
    "    for model in models:\n",
    "        try:\n",
    "            fit = all_fits[model]['posterior']\n",
    "            W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "            W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "            b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "            b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "        except KeyError:\n",
    "            print(f\"[SKIP] Model or posterior not found:\")\n",
    "            continue\n",
    "\n",
    "        S = W1_samples.shape[0]\n",
    "        rmses = np.zeros(S)\n",
    "        #print(y_test.shape)\n",
    "        y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "        for i in range(S):\n",
    "            W1 = W1_samples[i]\n",
    "            W2 = W2_samples[i]\n",
    "\n",
    "            # Apply pruning mask if requested\n",
    "            if prune_fn is not None and sparsity > 0.0:\n",
    "                masks = prune_fn([W1, W2], sparsity)\n",
    "                W1 = W1 * masks[0]\n",
    "                #W2 = W2 * masks[1]\n",
    "\n",
    "            y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i])\n",
    "            y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "            rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "            \n",
    "        posterior_mean = np.mean(y_hats, axis=0)\n",
    "        posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "        posterior_means.append({\n",
    "            'model': model,\n",
    "            'sparsity': sparsity,\n",
    "            'posterior_mean_rmse': posterior_mean_rmse\n",
    "        })\n",
    "\n",
    "        for i in range(S):\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'rmse': rmses[i]\n",
    "            })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "df_rmse_tanh, df_posterior_rmse_tanh = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "\n",
    "    df_rmse_tanh[sparsity], df_posterior_rmse_tanh[sparsity] = compute_sparse_rmse_results_hastie(\n",
    "        models = model_names_tanh,\n",
    "        all_fits = tanh_fit, \n",
    "        forward_pass = forward_pass_tanh,\n",
    "        sparsity=sparsity, \n",
    "        prune_fn=local_prune_weights\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---- Anta df_rmse_tanh er definert som i spørsmålet ----\n",
    "# df_rmse_tanh: Dict[float, DataFrame(model, sparsity, rmse)]\n",
    "\n",
    "# ---- Slå sammen alle DataFrames og legg til eksplisitt sparsity-kolonne ----\n",
    "# Hvis de enkelte DF allerede har korrekt 'sparsity', bruker vi den.\n",
    "# Ellers setter vi sparsity fra dictionary-key for robusthet.\n",
    "frames = []\n",
    "for sp, df in df_rmse_tanh.items():\n",
    "    df = df.copy()\n",
    "    # Sikre at sparsity-kolonnen matcher key (skriver inn hvis avvik)\n",
    "    df['sparsity'] = float(sp)\n",
    "    frames.append(df)\n",
    "\n",
    "df_all = pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "# Rydding: sikre dtypes og sortering\n",
    "df_all['sparsity'] = df_all['sparsity'].astype(float)\n",
    "df_all['rmse'] = pd.to_numeric(df_all['rmse'], errors='coerce')\n",
    "df_all = df_all.dropna(subset=['rmse', 'sparsity', 'model'])\n",
    "\n",
    "# Valgfritt: sortér modellnavn som kategorisk for konsistent plottrekkefølge\n",
    "models_order = sorted(df_all['model'].unique())\n",
    "df_all['model'] = pd.Categorical(df_all['model'], categories=models_order, ordered=True)\n",
    "\n",
    "# ---- Oppsummeringsstatistikk: mean, std, n, 95% CI ----\n",
    "summary = (\n",
    "    df_all.groupby(['model', 'sparsity'])\n",
    "    .agg(n=('rmse', 'size'),\n",
    "         mean_rmse=('rmse', 'mean'),\n",
    "         std_rmse=('rmse', 'std'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Unngå deling på null\n",
    "summary['sem'] = summary['std_rmse'] / summary['n'].replace(0, np.nan).pow(0.5)\n",
    "# 95% CI med normaltilnærming: mean ± 1.96 * SEM\n",
    "summary['ci95'] = 1.96 * summary['sem']\n",
    "summary['ymin'] = summary['mean_rmse'] - summary['ci95']\n",
    "summary['ymax'] = summary['mean_rmse'] + summary['ci95']\n",
    "\n",
    "# ---- Plot-stil ----\n",
    "sns.set_context('talk')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# ---- Figur 1: Linjeplot av mean RMSE vs sparsity, farget per modell, med CI ----\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Linjer\n",
    "sns.lineplot(\n",
    "    data=summary.sort_values(['model', 'sparsity']),\n",
    "    x='sparsity', y='mean_rmse', hue='model',\n",
    "    linewidth=2.5, marker='o', markersize=7\n",
    ")\n",
    "# Errorbars (CI)\n",
    "for _, row in summary.iterrows():\n",
    "    plt.plot([row['sparsity'], row['sparsity']], [row['ymin'], row['ymax']],\n",
    "             color=sns.color_palette()[models_order.index(row['model'])], lw=2)\n",
    "\n",
    "plt.title('RMSE vs sparsity per modell (mean ± 95% CI)')\n",
    "plt.xlabel('Sparsity')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(title='Modell', loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "# ---- Figur 2: Boxplot av RMSE fordelt per sparsity, delt (hue) på modell ----\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    data=df_all,\n",
    "    x='sparsity', y='rmse', hue='model',\n",
    "    showfliers=False,  # skjul outliers for ryddigere helhetsinntrykk\n",
    "    linewidth=1.2\n",
    ")\n",
    "sns.stripplot(\n",
    "    data=df_all.sample(min(len(df_all), 2000), random_state=42),  # vis et utvalg punkter, ikke alt\n",
    "    x='sparsity', y='rmse', hue='model',\n",
    "    dodge=True, size=2, alpha=0.25, palette='dark'\n",
    ")\n",
    "# Fjern duplisert legend fra stripplot\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles[:len(models_order)], labels[:len(models_order)], title='Modell', loc='best')\n",
    "\n",
    "plt.title('RMSE-fordeling per sparsity og modell (box + punkter)')\n",
    "plt.xlabel('Sparsity')\n",
    "plt.ylabel('RMSE')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POSTERIOR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from utils.generate_data import generate_latent_hastie_data\n",
    "X_train, _, _, _ = generate_latent_hastie_data(n=100, p=20)\n",
    "\n",
    "P = 20\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "\n",
    "def build_single_draw_weights(fits, layer_structure, draw_idx):\n",
    "    \"\"\"Return {model: {'W_1': (P,H), 'W_L': (H,O)}} for ONE draw.\"\"\"\n",
    "    out = {}\n",
    "    for name, fd in fits.items():\n",
    "        fit = fd[\"posterior\"]\n",
    "        W1 = fit.stan_variable(layer_structure['input_to_hidden']['name'])[draw_idx]\n",
    "        WL = fit.stan_variable(layer_structure['hidden_to_output']['name'])[draw_idx]\n",
    "        WL = WL.reshape(layer_structure['hidden_to_output']['shape'])\n",
    "        out[name] = {\"W_1\": W1, \"W_L\": WL}\n",
    "    return out\n",
    "\n",
    "def scale_W1_for_plot(model_means, mode='global'):\n",
    "    \"\"\"\n",
    "    Skalerer alle W_1 til [-1, 1] for rettferdig sammenligning av edge-tykkelser.\n",
    "\n",
    "    mode:\n",
    "      - 'global' : én felles skala over alle modeller (mest sammenlignbar)\n",
    "      - 'per_model': egen skala per modell (uavhengig sammenligning)\n",
    "      - 'per_node' : skalerer hver kolonne (node) separat til [-1,1]\n",
    "\n",
    "    Returnerer: scaled_model_means (samme struktur som input), scale_info\n",
    "    \"\"\"\n",
    "    scaled = {}\n",
    "    if mode == 'global':\n",
    "        gmax = max(np.abs(m['W_1']).max() for m in model_means.values())\n",
    "        gmax = max(gmax, 1e-12)\n",
    "        for name, m in model_means.items():\n",
    "            W1s = m['W_1'] / gmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'global', 'scale': gmax}\n",
    "\n",
    "    elif mode == 'per_model':\n",
    "        for name, m in model_means.items():\n",
    "            s = max(np.abs(m['W_1']).max(), 1e-12)\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = m['W_1'] / s\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_model'}\n",
    "\n",
    "    elif mode == 'per_node':\n",
    "        for name, m in model_means.items():\n",
    "            W1 = m['W_1'].copy()\n",
    "            P, H = W1.shape\n",
    "            for h in range(H):\n",
    "                colmax = max(np.abs(W1[:, h]).max(), 1e-12)\n",
    "                W1[:, h] = W1[:, h] / colmax\n",
    "            out = {k: v for k, v in m.items()}\n",
    "            out['W_1'] = W1\n",
    "            scaled[name] = out\n",
    "        return scaled, {'mode': 'per_node'}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'global', 'per_model', or 'per_node'\")\n",
    "#feature_names = list(X_train.columns)\n",
    "def plot_models_with_activations(model_means, layer_sizes,\n",
    "                                 activations=None, activation_color_max=None,\n",
    "                                 ncols=3, figsize_per_plot=(5,4), signed_colors=False, feature_names=None):\n",
    "    \"\"\"\n",
    "    model_means: dict {model_name: {'W_1':(P,H), 'W_L':(H,O), optional 'W_internal':[...]} }\n",
    "    layer_sizes: f.eks [P, H, O] eller [P, H, H, O] ved internlag\n",
    "    activations: dict {model_name: (H,)} – aktiveringsfrekvens kun for første skjulte lag\n",
    "    activation_color_max: global maks for skalering av farger (hvis None brukes 1.0)\n",
    "    \"\"\"\n",
    "    names = list(model_means.keys())\n",
    "    n_models = len(names)\n",
    "    nrows = int(np.ceil(n_models / ncols))\n",
    "    figsize = (figsize_per_plot[0] * ncols, figsize_per_plot[1] * nrows)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "    if nrows * ncols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Skru av blanke akser\n",
    "    for ax in axes[n_models:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax, name in zip(axes, names):\n",
    "        weights = model_means[name]\n",
    "        G = nx.DiGraph()\n",
    "        pos, nodes_per_layer, node_colors = {}, [], []\n",
    "\n",
    "        # Noder med posisjon og farge\n",
    "        for li, size in enumerate(layer_sizes):\n",
    "            ids = []\n",
    "            ycoords = np.linspace(size - 1, 0, size) - (size - 1) / 2\n",
    "            for i in range(size):\n",
    "                nid = f\"L{li}_{i}\"\n",
    "                G.add_node(nid)\n",
    "                pos[nid] = (li, ycoords[i])\n",
    "                ids.append(nid)\n",
    "                if li == 0 and feature_names is not None:\n",
    "                    ax.text(pos[nid][0]-0.12, pos[nid][1], feature_names[i],\n",
    "                            ha='right', va='center', fontsize=8)\n",
    "\n",
    "                if activations is not None and li == 1:  # kun første skjulte lag\n",
    "                    #a = activations.get(name, np.zeros(size))\n",
    "                    a = activations.get(name, np.zeros(size))\n",
    "                    a = np.asarray(a).ravel()   # <-- flater til 1D array\n",
    "                    scale = activation_color_max if activation_color_max is not None else 1.0\n",
    "                    val = float(np.clip(a[i] / max(scale, 1e-12), 0.0, 1.0))\n",
    "                    color = plt.cm.winter(val)\n",
    "                else:\n",
    "                    color = 'lightgray'\n",
    "                node_colors.append(color)\n",
    "\n",
    "            nodes_per_layer.append(ids)\n",
    "\n",
    "        edge_colors, edge_widths = [], []\n",
    "\n",
    "        def add_edges(W, inn, ut):\n",
    "            for j, out_n in enumerate(ut):\n",
    "                for i, in_n in enumerate(inn):\n",
    "                    w = float(W[i, j])\n",
    "                    G.add_edge(in_n, out_n, weight=abs(w))\n",
    "                    edge_colors.append('red' if w >= 0 else 'blue')\n",
    "                    edge_widths.append(abs(w))\n",
    "\n",
    "        # input -> hidden(1)\n",
    "        add_edges(weights['W_1'], nodes_per_layer[0], nodes_per_layer[1])\n",
    "\n",
    "        # ev. internlag\n",
    "        if 'W_internal' in weights:\n",
    "            for l, Win in enumerate(weights['W_internal']):\n",
    "                add_edges(Win, nodes_per_layer[l+1], nodes_per_layer[l+2])\n",
    "\n",
    "        # siste hidden -> output\n",
    "        add_edges(weights['W_L'], nodes_per_layer[-2], nodes_per_layer[-1])\n",
    "\n",
    "        nx.draw(G, pos, ax=ax,\n",
    "                node_color=node_colors,\n",
    "                edge_color=(edge_colors if signed_colors else 'red'),\n",
    "                width=[G[u][v]['weight'] for u,v in G.edges()],\n",
    "                with_labels=False, node_size=400, arrows=False)\n",
    "\n",
    "        ax.set_title(name, fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def compute_hidden_activation(fit_dict, x_train, draw_idx):\n",
    "    fit = fit_dict['posterior']\n",
    "    W1 = fit.stan_variable('W_1')[draw_idx, :, :]          # (P,H)\n",
    "    try:\n",
    "        b1 = fit.stan_variable('hidden_bias')[draw_idx, :] # (H,)\n",
    "    except Exception:\n",
    "        b1 = np.zeros(W1.shape[1])\n",
    "    # tanh i [-1,1]\n",
    "    a_full = np.tanh(x_train @ W1 + b1)             # (H,)\n",
    "    a=np.mean(a_full, axis=0)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velg en observasjon å \"lyse opp\" nodefargene med\n",
    "obs_idx = 3\n",
    "draw_idx = 69 #pick_draw_idx(prior_fits, seed=42)      # one common draw across models\n",
    "prior_draws = build_single_draw_weights(tanh_fit, layer_structure, draw_idx)\n",
    "\n",
    "# 1) Beregn aktivasjoner for ALLE modellene\n",
    "activations = {}\n",
    "for name, fd in tanh_fit.items():\n",
    "    a = compute_hidden_activation(fd, X_train, draw_idx)\n",
    "    activations[name] = np.abs(a)      \n",
    "\n",
    "# 2) Skaler vekter for plotting (som før)\n",
    "scaled, _ = scale_W1_for_plot(prior_draws, mode='per_model')\n",
    "\n",
    "# 3) Kall plottet med aktivasjoner\n",
    "# Siden tanh ∈ [-1,1] og vi bruker |a|, så sett activation_color_max=1.0\n",
    "fig = plot_models_with_activations(\n",
    "    scaled,\n",
    "    layer_sizes=[P, H, out_nodes],\n",
    "    activations=activations,\n",
    "    activation_color_max=1.0,\n",
    "    ncols=2,\n",
    "    feature_names = None #feature_names\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "def mean_abs(arr):  # arr: (S, ...)\n",
    "    return np.mean(np.abs(np.asarray(arr)), axis=0)\n",
    "\n",
    "def nid_single_hidden(posterior, W1_name=\"W_1\", WL_name_candidates=(\"W_L\",\"W_2\")):\n",
    "    \"\"\"\n",
    "    posterior: CmdStanMCMC-objekt\n",
    "    W_1: shape (S, P, H)  (input -> hidden), som du har\n",
    "    W_L: shape (S, H) eller (S, H, O)  (hidden -> output)\n",
    "    \"\"\"\n",
    "    W1_samps = posterior.stan_variable(W1_name)          # (S, P, H)\n",
    "    # Finn navn for siste lag\n",
    "    for nm in WL_name_candidates:\n",
    "        try:\n",
    "            WL_samps = posterior.stan_variable(nm)       # (S, H) eller (S, H, O)\n",
    "            break\n",
    "        except Exception:\n",
    "            WL_samps = None\n",
    "    if WL_samps is None:\n",
    "        raise ValueError(\"Fant ikke siste-lag-vekter (prøv å angi riktig navn i WL_name_candidates).\")\n",
    "\n",
    "    # Posterior plug-in: gjennomsnitt av absoluttverdier\n",
    "    W1_abs = mean_abs(W1_samps)                          # (P, H)\n",
    "    WL_abs = mean_abs(WL_samps)                          # (H,) eller (H, O)\n",
    "\n",
    "    # z^(1): aggregert node-innflytelse (sum over outputs hvis flere)\n",
    "    if WL_abs.ndim == 1:\n",
    "        z1 = WL_abs                                      # (H,)\n",
    "    else:\n",
    "        z1 = WL_abs.sum(axis=1)                          # (H,)\n",
    "\n",
    "    P, H = W1_abs.shape\n",
    "\n",
    "    # Main effects: ω({j}) = Σ_i z_i * |W1[j,i]|\n",
    "    omega_main = (W1_abs * z1[None, :]).sum(axis=1)      # (P,)\n",
    "\n",
    "    # Pairwise: ω({j,k}) = Σ_i z_i * min(|W1[j,i]|, |W1[k,i]|)\n",
    "    omega_pair = np.zeros((P, P))\n",
    "    for j, k in combinations(range(P), 2):\n",
    "        mins = np.minimum(W1_abs[j, :], W1_abs[k, :])    # (H,)\n",
    "        omega = np.dot(z1, mins)                         # skalar\n",
    "        omega_pair[j, k] = omega_pair[k, j] = omega\n",
    "\n",
    "    return z1, omega_main, omega_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = tanh_fit['Gaussian tanh']['posterior'] \n",
    "z1, omega_main, omega_pair = nid_single_hidden(post) # W_1=(S,P,H), W_L/(W_2)=(S,H[,O]) # Eksempler: # - topp 10 viktigste noder etter z1: \n",
    "top_nodes = np.argsort(-z1) # - topp 10 viktigste features (main effects): \n",
    "top_feats = np.argsort(-omega_main) # - sterkeste parvise interaksjoner: \n",
    "P = omega_pair.shape[0] \n",
    "pairs = [(j, k, omega_pair[j, k]) for j in range(P) for k in range(j+1, P)] \n",
    "top_pairs = sorted(pairs, key=lambda t: -t[2])[:10]\n",
    "\n",
    "res = np.array(omega_main/(np.sum(omega_main)))\n",
    "print(np.round(res, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "def gini(v):\n",
    "    v = np.asarray(v, float)\n",
    "    v = np.abs(v)\n",
    "    if np.all(v == 0): return 0.0\n",
    "    v = np.sort(v)\n",
    "    n = v.size\n",
    "    cum = np.cumsum(v)\n",
    "    return (n + 1 - 2 * (cum.sum() / cum[-1])) / n\n",
    "\n",
    "def topk_share(v, k):\n",
    "    v = np.asarray(v, float)\n",
    "    tot = v.sum()\n",
    "    if tot == 0: return 0.0\n",
    "    idx = np.argsort(-v)[:k]\n",
    "    return v[idx].sum() / tot\n",
    "\n",
    "def summarize_main(omega_main):\n",
    "    tot = omega_main.sum()\n",
    "    mx  = omega_main.max() if omega_main.size else 0.0\n",
    "    return {\n",
    "        \"Gini(main)\": gini(omega_main),\n",
    "        \"Top1(main)\": topk_share(omega_main, 1),\n",
    "        \"Top3(main)\": topk_share(omega_main, 3),\n",
    "        \"Top5(main)\": topk_share(omega_main, 5),\n",
    "        \"#≥10%max(main)\": int((omega_main >= 0.10 * mx).sum()),\n",
    "        \"Total(main)\": tot,\n",
    "    }\n",
    "\n",
    "def summarize_pairs(omega_pair):\n",
    "    # ta øvre trekant uten diagonal\n",
    "    P = omega_pair.shape[0]\n",
    "    tri = [omega_pair[j, k] for j in range(P) for k in range(j+1, P)]\n",
    "    tri = np.asarray(tri, float)\n",
    "    tot = tri.sum()\n",
    "    mx  = tri.max() if tri.size else 0.0\n",
    "    return {\n",
    "        \"Gini(pair)\": gini(tri),\n",
    "        \"Top5(pair)\": topk_share(tri, 5),\n",
    "        \"Top10(pair)\": topk_share(tri, 10),\n",
    "        \"#≥10%max(pair)\": int((tri >= 0.10 * mx).sum()),\n",
    "        \"Total(pair)\": tot,\n",
    "    }\n",
    "\n",
    "models = [\n",
    "    (\"Gaussian tanh\",                tanh_fit['Gaussian tanh']['posterior']),\n",
    "    (\"Regularized Horseshoe tanh\",   tanh_fit['Regularized Horseshoe tanh']['posterior']),\n",
    "    (\"Dirichlet Horseshoe tanh\",     tanh_fit['Dirichlet Horseshoe tanh']['posterior']),\n",
    "    (\"Dirichlet Student T tanh\",     tanh_fit['Dirichlet Student T tanh']['posterior']),\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for name, post in models:\n",
    "    z1, omega_main, omega_pair = nid_single_hidden(post)\n",
    "    m = summarize_main(omega_main)\n",
    "    p = summarize_pairs(omega_pair)\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        **m,\n",
    "        **p,\n",
    "        \"Median z1\": np.median(z1),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\n",
    "    \"Model\",\n",
    "    \"Gini(main)\", \"Top1(main)\", \"Top3(main)\", \"Top5(main)\", \"#≥10%max(main)\", \"Total(main)\",\n",
    "    \"Gini(pair)\", \"Top5(pair)\", \"Top10(pair)\", \"#≥10%max(pair)\", \"Total(pair)\",\n",
    "    \"Median z1\",\n",
    "])\n",
    "\n",
    "# Kort og ryddig LaTeX\n",
    "print(df.to_latex(index=False, float_format=\"%.3f\", escape=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SHAPLEY VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import generate_latent_hastie_data\n",
    "X_train, X_test, y_train, y_test = generate_latent_hastie_data(n=500, p=20)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Suppose X_train has shape (n, p)\n",
    "X_train_pd = pd.DataFrame(X_train, columns=[f\"X{i+1}\" for i in range(X_train.shape[1])])\n",
    "X_test_pd = pd.DataFrame(X_test, columns=[f\"X{i+1}\" for i in range(X_train.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# W: (p×d), theta: (d,)\n",
    "alignment = W @ theta   # shape (p,), equals cos(angle) since norms = 1\n",
    "\n",
    "df_alignment = pd.DataFrame({\n",
    "    \"feature\": [f\"X{j+1}\" for j in range(W.shape[0])],\n",
    "    \"alignment\": alignment,\n",
    "    \"abs_alignment\": np.abs(alignment)\n",
    "}).sort_values(\"abs_alignment\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.robust_utils import build_pytorch_model_from_stan_sample\n",
    "\n",
    "models_to_eval = [\n",
    "    \"Gaussian tanh\",\n",
    "    \"Regularized Horseshoe tanh\",\n",
    "    \"Dirichlet Horseshoe tanh\",\n",
    "    \"Dirichlet Student T tanh\"\n",
    "]\n",
    "\n",
    "H = 16                       # hidden dim\n",
    "feature_names = [f\"X{i+1}\" for i in range(P)]\n",
    "\n",
    "# SHAP background & evaluation subsets (reuse across models for fairness)\n",
    "X_bg   = X_train_pd.sample(n=80, random_state=0).to_numpy(float)\n",
    "X_eval = X_test_pd.sample(n=40, random_state=1).to_numpy(float)\n",
    "\n",
    "results = {}   # store mean SHAP vectors per model\n",
    "\n",
    "for model_name in models_to_eval:\n",
    "\n",
    "    print(f\"\\n=== Evaluating SHAP for model: {model_name} ===\")\n",
    "\n",
    "    fit = tanh_fit[model_name]['posterior']\n",
    "\n",
    "    # Build one representative sample from posterior\n",
    "    model = build_pytorch_model_from_stan_sample(\n",
    "        fit, sample_idx=69, input_dim=P, hidden_dim=H,\n",
    "        output_dim=1, task=\"regression\", activation=torch.tanh\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    def predict_numpy(X_np):\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.tensor(X_np, dtype=torch.float32)\n",
    "            y = model(X_t).cpu().numpy()\n",
    "        return y\n",
    "\n",
    "    explainer = shap.KernelExplainer(predict_numpy, X_bg)\n",
    "    shap_vals = explainer.shap_values(X_eval)  # shape = (n_eval, P)\n",
    "\n",
    "    mean_abs_shap = np.abs(shap_vals).mean(axis=0)  # global importance\n",
    "\n",
    "    results[model_name] = mean_abs_shap\n",
    "\n",
    "    print(f\"Top features for {model_name}:\")\n",
    "    order = np.argsort(mean_abs_shap)[::-1]\n",
    "    for j in order[:10]:\n",
    "        print(f\"  {feature_names[j]:16s}  SHAP={mean_abs_shap[j]:.4f}   align={np.abs(alignment[j]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for model_name, shap_imp in results.items():\n",
    "    plt.scatter(np.abs(alignment), shap_imp, label=model_name, alpha=0.7)\n",
    "    \n",
    "plt.scatter(np.abs(alignment), np.abs(beta_true), label=\"β_true vs alignment\")\n",
    "plt.xlabel(\"True feature-signal alignment\")\n",
    "plt.ylabel(\"SHAP importance mean\")\n",
    "plt.title(\"SHAP vs True Latent Alignment\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def alignment_metrics(abs_alignment, shap_imp):\n",
    "    x = abs_alignment.reshape(-1, 1)\n",
    "    y = shap_imp.reshape(-1)\n",
    "\n",
    "    # Pearson & Spearman\n",
    "    pear = pearsonr(abs_alignment, y)[0]\n",
    "    spear = spearmanr(abs_alignment, y).correlation\n",
    "\n",
    "    # Linear fit (no intercept or with intercept — try both)\n",
    "    lr0 = LinearRegression(fit_intercept=False).fit(x, y)\n",
    "    r2_0 = lr0.score(x, y)\n",
    "    slope0 = float(lr0.coef_[0])\n",
    "\n",
    "    lr1 = LinearRegression(fit_intercept=True).fit(x, y)\n",
    "    r2_1 = lr1.score(x, y)\n",
    "    slope1 = float(lr1.coef_[0])\n",
    "    intercept1 = float(lr1.intercept_)\n",
    "\n",
    "    return {\n",
    "        \"pearson\": pear,\n",
    "        \"spearman\": spear,\n",
    "        \"R2_no_intercept\": r2_0,\n",
    "        \"slope_no_intercept\": slope0,\n",
    "        \"R2_with_intercept\": r2_1,\n",
    "        \"slope_with_intercept\": slope1,\n",
    "        \"intercept\": intercept1,\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for model_name, shap_imp in results.items():\n",
    "    m = alignment_metrics(np.abs(alignment), shap_imp)\n",
    "    m[\"model\"] = model_name\n",
    "    rows.append(m)\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(\"model\").sort_values(\"pearson\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_true = W @ np.linalg.inv(np.eye(W.shape[1]) + W.T @ W) @ theta\n",
    "summary_df.loc[\"β_true (theory)\"] = alignment_metrics(np.abs(alignment), np.abs(beta_true))\n",
    "print(summary_df.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corr = pd.DataFrame(X_train_pd, columns=X_train_pd.columns).drop(columns=[\"X6\", \"X7\", \"X8\", \"X9\", \"X10\", \"X11\", \"X12\", \"X13\", \"X14\", \"X15\", \"X16\", \"X17\", \"X18\", \"X19\", \"X20\"]).corr()\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING ALIGNMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
