{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_relu = \"results/classification/single_layer/relu/breastcancer\"\n",
    "results_dir_tanh = \"results/classification/single_layer/tanh/breastcancer\"\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "\n",
    "full_config_path = \"breast_cancer_N455_p30\"\n",
    "\n",
    "relu_fits = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_relu,\n",
    "    models=model_names_relu,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "tanh_fits = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_breast_cancer_data\n",
    "X_train, X_test, y_train, y_test, *_ = load_breast_cancer_data(\n",
    "    test_size=0.2, standardize=False, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def expected_calibration_error(probs, y_true, n_bins=15, strategy=\"uniform\"):\n",
    "    \"\"\"\n",
    "    Compute ECE (and MCE) using the standard 'confidence' binning approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probs : array, shape (n_samples, n_classes) or (n_samples,)\n",
    "        Predicted probabilities per class (multiclass) or positive-class probs (binary).\n",
    "    y_true : array, shape (n_samples,)\n",
    "        True labels as integers in [0, n_classes-1].\n",
    "    n_bins : int\n",
    "        Number of bins in [0, 1].\n",
    "    strategy : {'uniform', 'quantile'}\n",
    "        'uniform' uses equal-width bins; 'quantile' uses equal-mass bins based on confidences.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ece : float\n",
    "        Expected Calibration Error (weighted average |acc - conf|).\n",
    "    mce : float\n",
    "        Maximum Calibration Error (max |acc - conf| across bins).\n",
    "    bin_stats : dict\n",
    "        Per-bin counts, accuracy, confidence, and edges.\n",
    "    \"\"\"\n",
    "    probs = np.asarray(probs)\n",
    "    y_true = np.asarray(y_true)\n",
    "\n",
    "    # Convert to confidences (max class prob) and predicted labels\n",
    "    if probs.ndim == 1 or (probs.ndim == 2 and probs.shape[1] == 1):\n",
    "        # Binary: probs is P(y=1). Turn into confidences wrt predicted label.\n",
    "        p1 = probs.ravel()\n",
    "        y_hat = (p1 >= 0.5).astype(int)\n",
    "        conf = np.where(y_hat == 1, p1, 1.0 - p1)\n",
    "    else:\n",
    "        y_hat = probs.argmax(axis=1)\n",
    "        conf = probs.max(axis=1)\n",
    "\n",
    "    n = len(y_true)\n",
    "    if strategy == \"uniform\":\n",
    "        edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    elif strategy == \"quantile\":\n",
    "        # Use unique quantiles to avoid duplicate edges when many equal confidences\n",
    "        quantiles = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        edges = np.unique(np.quantile(conf, quantiles))\n",
    "        # Ensure we still cover [0,1]\n",
    "        edges[0], edges[-1] = 0.0, 1.0\n",
    "    else:\n",
    "        raise ValueError(\"strategy must be 'uniform' or 'quantile'\")\n",
    "\n",
    "    ece = 0.0\n",
    "    mce = 0.0\n",
    "    bin_counts, bin_accs, bin_confs = [], [], []\n",
    "\n",
    "    for b in range(len(edges) - 1):\n",
    "        lo, hi = edges[b], edges[b + 1]\n",
    "        # Include left, exclude right except for final bin\n",
    "        if b < len(edges) - 2:\n",
    "            mask = (conf >= lo) & (conf < hi)\n",
    "        else:\n",
    "            mask = (conf >= lo) & (conf <= hi)\n",
    "\n",
    "        count = int(mask.sum())\n",
    "        if count == 0:\n",
    "            bin_counts.append(0)\n",
    "            bin_accs.append(np.nan)\n",
    "            bin_confs.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        acc_b = (y_hat[mask] == y_true[mask]).mean()\n",
    "        conf_b = conf[mask].mean()\n",
    "        gap = abs(acc_b - conf_b)\n",
    "\n",
    "        weight = count / n\n",
    "        ece += weight * gap\n",
    "        mce = max(mce, gap)\n",
    "\n",
    "        bin_counts.append(count)\n",
    "        bin_accs.append(acc_b)\n",
    "        bin_confs.append(conf_b)\n",
    "\n",
    "    bin_stats = {\n",
    "        \"counts\": np.array(bin_counts),\n",
    "        \"acc\": np.array(bin_accs),\n",
    "        \"conf\": np.array(bin_confs),\n",
    "        \"edges\": np.array(edges),\n",
    "    }\n",
    "    return float(ece), float(mce), bin_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model_names = list(tanh_fits.keys())\n",
    "results = []\n",
    "\n",
    "for model in model_names:\n",
    "    posterior = tanh_fits[model]['posterior']\n",
    "    \n",
    "    # Predicted class labels from posterior samples: majority vote\n",
    "    pred_samples = posterior.stan_variable(\"pred_test\")                 # shape: [n_samples, n_test]\n",
    "    majority_preds = mode(pred_samples, axis=0, keepdims=False).mode.flatten()\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, majority_preds)\n",
    "\n",
    "    # Mean predictive probabilities across posterior samples\n",
    "    pred_probs = posterior.stan_variable(\"prob_test\")                   # shape: [n_samples, n_test, n_classes]\n",
    "    mean_probs = pred_probs.mean(axis=0)                                 # shape: [n_test, n_classes]\n",
    "\n",
    "    # Negative log-likelihood (log loss)\n",
    "    y_test_adj = y_test - 1  # ensure labels are {0,1} for binary\n",
    "    nll = log_loss(y_test_adj, mean_probs, labels=[0, 1])\n",
    "\n",
    "    # ECE / MCE (you can change n_bins or strategy)\n",
    "    ece, mce, _ = expected_calibration_error(mean_probs, y_test_adj, n_bins=15, strategy=\"uniform\")\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model,\n",
    "        \"Accuracy\": acc,\n",
    "        \"NLL\": nll,\n",
    "        \"ECE\": ece,\n",
    "        \"MCE\": mce\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = results_df.to_latex(index=False, float_format=\"%.4f\", column_format=\"lcc\", caption=\"Accuracy and NLL per model.\", label=\"tab:accuracy_nll\")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Of 114 observations,\", np.round(114*results_df['Accuracy'][0], 3), \"were classified correctly by the\", results_df['Model'][0], \"model \\n\")\n",
    "print(\"Of 114 observations,\", np.round(114*results_df['Accuracy'][1], 3), \"were classified correctly by the\", results_df['Model'][1], \"model \\n\")\n",
    "print(\"Of 114 observations,\", np.round(114*results_df['Accuracy'][2], 3), \"were classified correctly by the\", results_df['Model'][2], \"model \\n\")\n",
    "print(\"Of 114 observations,\", np.round(114*results_df['Accuracy'][3], 3), \"were classified correctly by the\", results_df['Model'][3], \"model \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "\n",
    "W_1 = tanh_fits['Gaussian tanh']['posterior'].stan_variable(\"W_1\")[0, :, :]   # (30, 16)\n",
    "W_2 = tanh_fits['Gaussian tanh']['posterior'].stan_variable(\"W_L\")[0, :, :]   # (16, 2)\n",
    "b_1 = tanh_fits['Gaussian tanh']['posterior'].stan_variable(\"hidden_bias\")[0, :]  # (16,)\n",
    "b_2 = tanh_fits['Gaussian tanh']['posterior'].stan_variable(\"output_bias\")[0, :]  # (2,)\n",
    "\n",
    "# Konverter DataFrame til tensor\n",
    "X = torch.tensor(X_test_df.to_numpy(), dtype=torch.float32)  # (114, 30)\n",
    "\n",
    "# Stan-vekter til tensor\n",
    "W1 = torch.tensor(W_1, dtype=torch.float32)   # (30, 16)\n",
    "b1 = torch.tensor(b_1.squeeze(), dtype=torch.float32)  # (16,)\n",
    "W2 = torch.tensor(W_2, dtype=torch.float32)   # (16, 2)\n",
    "b2 = torch.tensor(b_2, dtype=torch.float32)   # (2,)\n",
    "\n",
    "# Definer nettverket\n",
    "class StanNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation=torch.tanh):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.activation(self.linear1(x))\n",
    "        logits = self.linear2(h)\n",
    "        return logits  # matcher Stan sin `output`\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return F.softmax(logits, dim=1)  # matcher Stan sin `prob_test`\n",
    "\n",
    "    def predict(self, x):\n",
    "        probs = self.predict_proba(x)\n",
    "        return torch.argmax(probs, dim=1)  # matcher Stan sin `pred_test`\n",
    "\n",
    "# Bygg og kopier vektene\n",
    "model = StanNNClassifier(input_dim=30, hidden_dim=16, output_dim=2, activation=torch.tanh)\n",
    "with torch.no_grad():\n",
    "    model.linear1.weight.copy_(W1.T)   # (16, 30)\n",
    "    model.linear1.bias.copy_(b1)       # (16,)\n",
    "    model.linear2.weight.copy_(W2.T)   # (2, 16)\n",
    "    model.linear2.bias.copy_(b2)       # (2,)\n",
    "\n",
    "# === Test forward ===\n",
    "logits = model(X)                # (114, 2), Stan: `output_test`\n",
    "probs = model.predict_proba(X)   # (114, 2), Stan: `prob_test`\n",
    "preds = model.predict(X)         # (114,),   Stan: `pred_test`\n",
    "\n",
    "#print(\"logits shape:\", logits.shape)\n",
    "#print(\"probs shape :\", probs)\n",
    "#print(\"preds shape :\", preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_probs = tanh_fits['Gaussian tanh']['posterior'].stan_variable(\"prob_test\")[0, :, :]\n",
    "stan_probs_torch = torch.tensor(stan_probs, dtype=torch.float32)\n",
    "\n",
    "print(\"Max diff:\", (probs - stan_probs_torch).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from utils.robust_utils import estimate_robustness_over_test_set\n",
    "import torch\n",
    "# === Settings ===\n",
    "epsilons = [0.01, 0.05, 0.1, 0.25]\n",
    "#epsilons = [0.1, 0.5, 1.0, 2.5]\n",
    "scales = [0.01, 0.05, 0.1, 0.5, 1.0, 5.0]\n",
    "sample_indices = range(0, 4000, 800)\n",
    "input_dim = X_test.shape[1]\n",
    "hidden_dim = 16\n",
    "output_dim = 2\n",
    "p_norm = 2\n",
    "\n",
    "# === Subset test set ===\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "y_test_s = pd.Series(y_test) - 1  # Ensure labels in {0,1}\n",
    "test_subset = X_test_df.sample(frac=1.0, random_state=42)\n",
    "subset_indices = test_subset.index\n",
    "y_subset = y_test_s.loc[subset_indices]\n",
    "\n",
    "# === Run robustness for each model ===\n",
    "all_results = []\n",
    "\n",
    "for model_name, fit_entry in tanh_fits.items():\n",
    "    for epsilon in epsilons:\n",
    "        for scale in scales:\n",
    "            delta = scale * epsilon\n",
    "\n",
    "            df_result = estimate_robustness_over_test_set(\n",
    "                x_test=test_subset,\n",
    "                y_test=y_subset,\n",
    "                fits_dict={model_name: fit_entry},  # Wrap as dict\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=output_dim,\n",
    "                sample_indices=sample_indices,\n",
    "                epsilon=epsilon,\n",
    "                delta=delta,\n",
    "                p_norm=p_norm,\n",
    "                activation=torch.tanh\n",
    "            )\n",
    "\n",
    "            df_result[\"epsilon\"] = epsilon\n",
    "            df_result[\"delta\"] = delta\n",
    "            df_result[\"scale\"] = round(scale, 2)\n",
    "            df_result[\"model\"] = model_name\n",
    "            all_results.append(df_result.copy())\n",
    "\n",
    "# === Combine results ===\n",
    "df_robust = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# === Unpack 'robustness' dictionary column into separate columns ===\n",
    "df_flat = pd.concat(\n",
    "    [df_robust.drop(columns=[\"robustness\"]),\n",
    "     df_robust[\"robustness\"].apply(pd.Series)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Add derived columns ===\n",
    "df_flat[\"1-p1\"] = (1.0 - df_flat[\"p1\"]).round(5)\n",
    "df_flat[\"1-p2\"] = (1.0 - df_flat[\"p2\"]).round(5)\n",
    "\n",
    "# Resulting DataFrame: df_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure model name consistency and ordering\n",
    "model_order = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "#model_order = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "#Ns = sorted(df_flat[\"N\"].unique())\n",
    "\n",
    "# Global color scale (consistent across all plots)\n",
    "vmin = df_flat[\"1-p1\"].min()\n",
    "vmax = df_flat[\"1-p1\"].max()\n",
    "\n",
    "# short_names = {\n",
    "#     \"Gaussian\": \"Gauss\",\n",
    "#     \"Regularized Horseshoe\": \"RHS\",\n",
    "#     \"Dirichlet Horseshoe\": \"DHS\",\n",
    "#     \"Dirichlet Student T\": \"DS-T\",\n",
    "# }\n",
    "\n",
    "short_names = {\n",
    "    \"Gaussian tanh\": \"Gauss\",\n",
    "    \"Regularized Horseshoe tanh\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe tanh\": \"DHS\",\n",
    "    \"Dirichlet Student T tanh\": \"DS-T\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8), sharex=True, sharey=True)\n",
    "\n",
    "for j, model in enumerate(model_order):\n",
    "    row, col = divmod(j, 2)\n",
    "    ax = axes[row, col]\n",
    "    df_model = df_flat[df_flat[\"model\"] == model]\n",
    "\n",
    "    # Use pivot_table to handle duplicates\n",
    "    heatmap_df = df_model.pivot_table(\n",
    "        index=\"scale\", columns=\"epsilon\", values=\"1-p1\", aggfunc=\"mean\"\n",
    "    )\n",
    "\n",
    "    sns.heatmap(\n",
    "        heatmap_df.sort_index(ascending=False),\n",
    "        annot=True, fmt=\".2f\", cmap=\"RdYlGn\", ax=ax,\n",
    "        cbar=False, vmin=vmin, vmax=vmax\n",
    "    )\n",
    "\n",
    "    #ax.set_title(model, fontsize=13)\n",
    "    ax.set_title(short_names[model], fontsize=13)\n",
    "    ax.set_xlabel(r\"$\\varepsilon$\", fontsize=12)\n",
    "    ax.set_ylabel(r\"$\\delta / \\varepsilon$\", fontsize=12)\n",
    "\n",
    "# Adjust layout to leave space for colorbar\n",
    "plt.tight_layout(rect=[0, 0, 0.93, 0.95])\n",
    "\n",
    "# Add colorbar on the right\n",
    "cbar_ax = fig.add_axes([0.94, 0.25, 0.015, 0.5])  # [left, bottom, width, height]\n",
    "norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdYlGn\", norm=norm)\n",
    "sm.set_array([])\n",
    "fig.colorbar(sm, cax=cbar_ax, label='$1 - p_1$')\n",
    "\n",
    "fig.suptitle(f\"Softmax Shift Robustness (1 - $p_1$)\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assuming df is your big dataframe\n",
    "summary = (\n",
    "    df_flat.groupby(\"model\")[\"1-p2\"]\n",
    "      .agg([\"mean\", \"std\", \"min\", \"max\"])\n",
    "      .reset_index()\n",
    "      .round(3)\n",
    ")\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure correct model order for consistency\n",
    "model_order = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "palette = sns.color_palette(\"Set2\", n_colors=4)\n",
    "\n",
    "# # Optional: choose one N (or loop if needed)\n",
    "# N_select = 100\n",
    "# df_plot = df_flat[df_flat[\"N\"] == N_select]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(\n",
    "    data=df_flat,\n",
    "    x=\"epsilon\",\n",
    "    y=\"1-p2\",\n",
    "    hue=\"model\",\n",
    "    hue_order=model_order,\n",
    "    palette=palette,\n",
    "    cut=0,\n",
    "    inner=\"quartile\",\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "plt.title(f\"Label Invariance Probability ($1 - p_2$) across models\")\n",
    "plt.xlabel(r\"Adversarial strength $\\varepsilon$\")\n",
    "plt.ylabel(r\"$1-p_2$\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Model\", bbox_to_anchor=(0.02, 0.05))\n",
    "plt.tight_layout()\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "entropy_data = {}  # store outputs\n",
    "\n",
    "for model in model_names:\n",
    "    probs = relu_fits[model][\"posterior\"].stan_variable(\"prob_test\")  # shape: (S, N_test, 2)\n",
    "\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    predictive_entropy = -np.sum(mean_probs * np.log(mean_probs + 1e-12), axis=1)\n",
    "\n",
    "    sample_entropies = -np.sum(probs * np.log(probs + 1e-12), axis=2)\n",
    "    expected_entropy = np.mean(sample_entropies, axis=0)\n",
    "\n",
    "    mutual_information = predictive_entropy - expected_entropy\n",
    "\n",
    "    y_pred = np.argmax(mean_probs, axis=1) + 1  # add 1 only if y_test is 1/2\n",
    "    correct = (y_pred == y_test)\n",
    "\n",
    "    # Store everything\n",
    "    entropy_data[model] = {\n",
    "        \"entropy\": predictive_entropy,\n",
    "        \"mutual_info\": mutual_information,\n",
    "        \"correct\": correct\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, model in enumerate(model_names):\n",
    "    ax = axs[i]\n",
    "    entropy = entropy_data[model][\"entropy\"]\n",
    "    mi = entropy_data[model][\"mutual_info\"]\n",
    "    correct = entropy_data[model][\"correct\"]\n",
    "\n",
    "    ax.scatter(entropy[correct], mi[correct], color='green', alpha=0.5, label='Correct')\n",
    "    ax.scatter(entropy[~correct], mi[~correct], color='red', alpha=0.5, label='Incorrect')\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel(\"Predictive Entropy\")\n",
    "    ax.set_ylabel(\"Mutual Information\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# Legend only once\n",
    "axs[0].legend()\n",
    "plt.suptitle(\"Mutual Information vs Entropy Colored by Prediction Correctness\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_names:\n",
    "    correct = entropy_data[model][\"correct\"]\n",
    "    acc = np.mean(correct)\n",
    "    avg_entropy = np.mean(entropy_data[model][\"entropy\"])\n",
    "    avg_mi = np.mean(entropy_data[model][\"mutual_info\"])\n",
    "    print(f\"{model}: Accuracy={acc:.3f}, Avg Entropy={avg_entropy:.3f}, Avg MI={avg_mi:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "entropy_data = {}  # store outputs\n",
    "\n",
    "for model in model_names:\n",
    "    probs = tanh_fits[model][\"posterior\"].stan_variable(\"prob_test\")  # shape: (S, N_test, 2)\n",
    "\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    predictive_entropy = -np.sum(mean_probs * np.log(mean_probs + 1e-12), axis=1)\n",
    "\n",
    "    sample_entropies = -np.sum(probs * np.log(probs + 1e-12), axis=2)\n",
    "    expected_entropy = np.mean(sample_entropies, axis=0)\n",
    "\n",
    "    mutual_information = predictive_entropy - expected_entropy\n",
    "\n",
    "    y_pred = np.argmax(mean_probs, axis=1) + 1  # add 1 only if y_test is 1/2\n",
    "    correct = (y_pred == y_test)\n",
    "\n",
    "    # Store everything\n",
    "    entropy_data[model] = {\n",
    "        \"entropy\": predictive_entropy,\n",
    "        \"mutual_info\": mutual_information,\n",
    "        \"correct\": correct\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, model in enumerate(model_names):\n",
    "    ax = axs[i]\n",
    "    entropy = entropy_data[model][\"entropy\"]\n",
    "    mi = entropy_data[model][\"mutual_info\"]\n",
    "    correct = entropy_data[model][\"correct\"]\n",
    "\n",
    "    ax.scatter(entropy[correct], mi[correct], color='green', alpha=0.5, label='Correct')\n",
    "    ax.scatter(entropy[~correct], mi[~correct], color='red', alpha=0.5, label='Incorrect')\n",
    "\n",
    "    ax.set_title(model)\n",
    "    ax.set_xlabel(\"Predictive Entropy\")\n",
    "    ax.set_ylabel(\"Mutual Information\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# Legend only once\n",
    "axs[0].legend()\n",
    "plt.suptitle(\"Mutual Information vs Entropy Colored by Prediction Correctness\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_names:\n",
    "    correct = entropy_data[model][\"correct\"]\n",
    "    acc = np.mean(correct)\n",
    "    avg_entropy = np.mean(entropy_data[model][\"entropy\"])\n",
    "    avg_mi = np.mean(entropy_data[model][\"mutual_info\"])\n",
    "    print(f\"{model}: Accuracy={acc:.3f}, Avg Entropy={avg_entropy:.3f}, Avg MI={avg_mi:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame first\n",
    "df = pd.DataFrame({\n",
    "    \"Entropy\": entropy_data[\"Dirichlet Student T tanh\"][\"entropy\"],\n",
    "    \"MI\": entropy_data[\"Dirichlet Student T tanh\"][\"mutual_info\"],\n",
    "    \"Correct\": entropy_data[\"Dirichlet Student T tanh\"][\"correct\"]\n",
    "})\n",
    "\n",
    "# Plot\n",
    "sns.jointplot(\n",
    "    data=df,\n",
    "    x=\"Entropy\",\n",
    "    y=\"MI\",\n",
    "    hue=\"Correct\",\n",
    "    kind=\"scatter\",\n",
    "    palette={True: \"green\", False: \"red\"},\n",
    "    alpha=0.5,\n",
    "    marginal_kws=dict(fill=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing kappa matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_breast_cancer_data\n",
    "X, X_test, y, y_test, *_ = load_breast_cancer_data(\n",
    "    test_size=0.2, standardize=False, random_state=42\n",
    ")\n",
    "\n",
    "X      = np.asarray(X, dtype=float)\n",
    "X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "# y often comes as a (n,1) DataFrame/array — flatten to (n,)\n",
    "y      = np.asarray(y, dtype=float).reshape(-1)\n",
    "y_test = np.asarray(y_test, dtype=float).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Callable, List\n",
    "\n",
    "# ============================================================\n",
    "# Activations\n",
    "# ============================================================\n",
    "\n",
    "def get_activation(activation: str = \"tanh\") -> Tuple[Callable, Callable]:\n",
    "    if activation == \"tanh\":\n",
    "        phi = np.tanh\n",
    "        def dphi(a): return 1.0 - np.tanh(a)**2\n",
    "    elif activation == \"relu\":\n",
    "        def phi(a): return np.maximum(0.0, a)\n",
    "        def dphi(a): return (a > 0.0).astype(a.dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    return phi, dphi\n",
    "\n",
    "# ============================================================\n",
    "# Extraction (Gaussian prior, 2 outputs)\n",
    "# ============================================================\n",
    "\n",
    "def extract_gaussian_two_output_draws(fit_dict, model_name: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      W_all   : (D,H,p)\n",
    "      b1_all  : (D,H)\n",
    "      V_all   : (D,H,2)\n",
    "      b2_all  : (D,2)\n",
    "      tau_w   : (D,)   (defaults to ones if missing)\n",
    "      tau_v   : (D,)   (defaults to ones if missing)\n",
    "      lam_eff : (D,H,p)  (all ones for Gaussian prior)\n",
    "    \"\"\"\n",
    "    post = fit_dict[model_name]['posterior']\n",
    "\n",
    "    def _stan_var_or_none(name):\n",
    "        try:\n",
    "            return np.asarray(post.stan_variable(name))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # --- hidden_bias => define H from this (robust) ---\n",
    "    b1_raw = _stan_var_or_none(\"hidden_bias\")\n",
    "    if b1_raw is None:\n",
    "        raise ValueError(\"Missing 'hidden_bias' in posterior.\")\n",
    "    b1_all = np.squeeze(b1_raw)  # handles (D,1,H) -> (D,H)\n",
    "    if b1_all.ndim == 1:\n",
    "        # (D,) -> treat as (D,1)\n",
    "        b1_all = b1_all[:, None]\n",
    "    if b1_all.ndim != 2:\n",
    "        raise ValueError(f\"Unexpected hidden_bias shape after squeeze: {b1_raw.shape} -> {b1_all.shape}\")\n",
    "    D, H_true = b1_all.shape\n",
    "\n",
    "    # --- W_1 -> coerce to (D,H,p) using H_true ---\n",
    "    W_1 = _stan_var_or_none(\"W_1\")\n",
    "    if W_1 is None:\n",
    "        raise ValueError(\"Missing 'W_1' in posterior.\")\n",
    "    if W_1.ndim != 3 or W_1.shape[0] != D:\n",
    "        raise ValueError(f\"Unexpected W_1 shape: {W_1.shape}, D={D}\")\n",
    "\n",
    "    if W_1.shape[1] == H_true:\n",
    "        # (D,H,p)\n",
    "        W_all = W_1\n",
    "    elif W_1.shape[2] == H_true:\n",
    "        # (D,p,H) -> transpose to (D,H,p)\n",
    "        W_all = np.transpose(W_1, (0, 2, 1))\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot match H from hidden_bias (H={H_true}) to W_1 shape {W_1.shape}\")\n",
    "\n",
    "    _, H_check, p = W_all.shape\n",
    "    assert H_check == H_true, \"H mismatch after coercion.\"\n",
    "\n",
    "    # --- W_L (output weights) -> coerce to (D,H,2) using H_true ---\n",
    "    W_L = _stan_var_or_none(\"W_L\")\n",
    "    if W_L is None:\n",
    "        raise ValueError(\"Missing 'W_L' in posterior.\")\n",
    "    if W_L.ndim != 3 or W_L.shape[0] != D:\n",
    "        raise ValueError(f\"Unexpected W_L shape: {W_L.shape}, D={D}\")\n",
    "\n",
    "    if W_L.shape[1] == H_true and W_L.shape[2] == 2:\n",
    "        V_all = W_L\n",
    "    elif W_L.shape[1] == 2 and W_L.shape[2] == H_true:\n",
    "        # (D,2,H) -> (D,H,2)\n",
    "        V_all = np.transpose(W_L, (0, 2, 1))\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot coerce W_L shape {W_L.shape} to (D,H=~{H_true},2)\")\n",
    "\n",
    "    # --- output_bias -> (D,2) ---\n",
    "    b2_raw = _stan_var_or_none(\"output_bias\")\n",
    "    if b2_raw is None:\n",
    "        raise ValueError(\"Missing 'output_bias' in posterior.\")\n",
    "    b2_all = np.squeeze(b2_raw)\n",
    "    if b2_all.ndim == 1:\n",
    "        b2_all = b2_all[:, None]\n",
    "    if b2_all.shape != (D, 2):\n",
    "        raise ValueError(f\"Expected output_bias (D,2), got {b2_raw.shape} -> {b2_all.shape}\")\n",
    "\n",
    "    # --- tau_w (global for input weights) ---\n",
    "    tau = _stan_var_or_none(\"tau\") or _stan_var_or_none(\"tau_w\")\n",
    "    tau_w = np.ones(D) if tau is None else np.squeeze(tau).reshape(D)\n",
    "\n",
    "    # --- tau_v (global for output weights); default 1 ---\n",
    "    tau_v_arr = _stan_var_or_none(\"tau_v\")\n",
    "    if tau_v_arr is None:\n",
    "        tau_v = np.ones(D)\n",
    "    else:\n",
    "        tau_v = np.squeeze(tau_v_arr)\n",
    "        if tau_v.ndim == 0:\n",
    "            tau_v = np.full(D, float(tau_v))\n",
    "        elif tau_v.ndim == 1 and tau_v.shape[0] == D:\n",
    "            pass\n",
    "        elif tau_v.ndim == 2 and tau_v.shape == (D, 2):\n",
    "            # reduce per draw to a single scalar (RMS across classes)\n",
    "            tau_v = np.sqrt(np.mean(tau_v**2, axis=1))\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected tau_v shape: {tau_v_arr.shape}\")\n",
    "\n",
    "    # Gaussian prior => lambda_eff = ones\n",
    "    lam_eff = np.ones((D, H_true, p))\n",
    "\n",
    "    return W_all, b1_all, V_all, b2_all, tau_w, tau_v, lam_eff\n",
    "\n",
    "# ============================================================\n",
    "# Linearization: multi-output (C=2)\n",
    "# ============================================================\n",
    "\n",
    "def build_hidden_and_jacobian_W_multi(\n",
    "    X: np.ndarray,          # (n,p)\n",
    "    W0: np.ndarray,         # (H,p)\n",
    "    b0: np.ndarray,         # (H,)\n",
    "    V0: np.ndarray,         # (H,2)\n",
    "    activation: str = \"tanh\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      Phi_mat : (n,H)\n",
    "      JW_list : [ (n,H*p), (n,H*p) ]  (per-class Jacobians)\n",
    "      Jb1_list: [ (n,H), (n,H) ]      (per-class hidden-bias Jacobians)\n",
    "      Jb2_vec : (n,)  (df_c/db2_c = 1)\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    H, pW = W0.shape\n",
    "    assert pW == p\n",
    "    assert V0.shape == (H, 2), \"Assume exactly two outputs.\"\n",
    "\n",
    "    phi, dphi = get_activation(activation)\n",
    "    A = X @ W0.T + b0[None, :]  # (n,H)\n",
    "    Phi_mat = phi(A)\n",
    "    dphiA   = dphi(A)\n",
    "\n",
    "    JW_list: List[np.ndarray] = []\n",
    "    Jb1_list: List[np.ndarray] = []\n",
    "    for c in range(2):\n",
    "        v_c = V0[:, c]  # (H,)\n",
    "        # Build (n, H*p) Jacobian block for class c\n",
    "        blocks = []\n",
    "        for h in range(H):\n",
    "            block_h = (v_c[h] * dphiA[:, [h]]) * X  # (n,p)\n",
    "            blocks.append(block_h.reshape(n, p))\n",
    "        JW_c = np.hstack(blocks)  # (n,H*p)\n",
    "        JW_list.append(JW_c)\n",
    "\n",
    "        # Hidden-bias Jacobian for class c: (n,H)\n",
    "        Jb1_c = dphiA * v_c[None, :]\n",
    "        Jb1_list.append(Jb1_c)\n",
    "\n",
    "    Jb2_vec = np.ones(n)  # df_c/db2_c = 1\n",
    "    return Phi_mat, JW_list, Jb1_list, Jb2_vec\n",
    "\n",
    "# ============================================================\n",
    "# Prior blocks and shrinkage operators\n",
    "# ============================================================\n",
    "\n",
    "def build_P_from_lambda_tau(lambda_eff: np.ndarray, tau_w: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Gaussian prior -> lambda_eff == ones. We keep the same interface.\n",
    "    P = diag( 1 / (tau_w^2 * lambda_eff) ). Here lambda_eff ~ 1, so P ~ I / tau_w^2.\n",
    "    Returns (H*p, H*p) diagonal.\n",
    "    \"\"\"\n",
    "    lam_vec = lambda_eff.reshape(-1).astype(float)\n",
    "    diagP = 1.0 / ( (tau_w**2) * lam_vec )\n",
    "    return np.diag(diagP)\n",
    "\n",
    "def build_Sigma_y_multi(\n",
    "    Phi_mat: np.ndarray,   # (n,H)\n",
    "    tau_v: float,          # scalar per draw (already reduced if needed)\n",
    "    Jb1_list: List[np.ndarray],  # two items, (n,H) each\n",
    "    Jb2_vec: np.ndarray,   # (n,)\n",
    "    include_b1: bool = True,\n",
    "    include_b2: bool = True,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Base covariance for the linearized outputs, aggregated over the two classes.\n",
    "    No Gaussian noise term (classification).\n",
    "    Σ_y = (τ_v^2 + τ_v^2) ΦΦ^T + sum_c J_b1,c J_b1,c^T + 2 * 11^T [if include_b2].\n",
    "    \"\"\"\n",
    "    n = Phi_mat.shape[0]\n",
    "    # Two outputs; sum of tau_v^2 across classes => 2 * tau_v^2 if scalar-per-draw\n",
    "    tau_v2_sum = 2.0 * float(tau_v)**2\n",
    "    Sigma_y = tau_v2_sum * (Phi_mat @ Phi_mat.T)  # (n,n)\n",
    "\n",
    "    if include_b1 and (Jb1_list is not None):\n",
    "        for Jb1_c in Jb1_list:\n",
    "            Sigma_y = Sigma_y + Jb1_c @ Jb1_c.T\n",
    "\n",
    "    if include_b2 and (Jb2_vec is not None):\n",
    "        Sigma_y = Sigma_y + 2.0 * np.outer(Jb2_vec, Jb2_vec)\n",
    "\n",
    "    return Sigma_y\n",
    "\n",
    "def build_S_multi(JW_list: List[np.ndarray], Sigma_y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    S = sum_c J_W_c^T Σ_y^{-1} J_W_c  (Hp x Hp)\n",
    "    \"\"\"\n",
    "    S = None\n",
    "    for JW_c in JW_list:\n",
    "        X_c = np.linalg.solve(Sigma_y, JW_c)  # (n, Hp)\n",
    "        S_c = JW_c.T @ X_c\n",
    "        S = S_c if S is None else (S + S_c)\n",
    "    return S\n",
    "\n",
    "def shrinkage_matrix_stable(P: np.ndarray, S: np.ndarray, jitter: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    R = (P+S)^{-1} P  computed stably via whitening:\n",
    "    R = P^{1/2} (I + P^{-1/2} S P^{-1/2})^{-1} P^{1/2}, but implemented with solves.\n",
    "    \"\"\"\n",
    "    d = np.diag(P).astype(float)\n",
    "    eps = 1e-12\n",
    "    d = np.clip(d, eps, np.finfo(float).max)\n",
    "    Phalf = np.diag(np.sqrt(d))\n",
    "    Pinvhalf = np.diag(1.0 / np.sqrt(d))\n",
    "\n",
    "    M = Pinvhalf @ S @ Pinvhalf\n",
    "    if jitter > 0:\n",
    "        M = M + jitter * np.eye(M.shape[0])\n",
    "\n",
    "    I = np.eye(M.shape[0])\n",
    "    L = np.linalg.cholesky(I + M)\n",
    "    Z = np.linalg.solve(L, Phalf)\n",
    "    W = np.linalg.solve(L.T, Z)\n",
    "    R = Pinvhalf @ W\n",
    "    R = 0.5 * (R + R.T)  # symmetrize\n",
    "    return R\n",
    "\n",
    "def shrinkage_eigs_and_df(P: np.ndarray, S: np.ndarray):\n",
    "    \"\"\"\n",
    "    In P-whitened coordinates: M = P^{-1/2} S P^{-1/2}, eigenvals mu>=0\n",
    "    r = 1/(1+mu) in (0,1]; df_eff = sum mu/(1+mu) = tr(I-R)\n",
    "    \"\"\"\n",
    "    d = np.diag(P).astype(float)\n",
    "    eps = 1e-12\n",
    "    Pinvhalf = np.diag(1.0 / np.sqrt(np.maximum(d, eps)))\n",
    "    M = Pinvhalf @ S @ Pinvhalf\n",
    "    mu = np.linalg.eigvalsh(M)\n",
    "    r = 1.0 / (1.0 + mu)\n",
    "    df_eff = np.sum(1.0 - r)\n",
    "    return r, df_eff\n",
    "\n",
    "# ============================================================\n",
    "# Single-draw and multi-draw drivers\n",
    "# ============================================================\n",
    "\n",
    "def compute_shrinkage_for_draw_gaussian_two_output(\n",
    "    X: np.ndarray,\n",
    "    W0: np.ndarray, b0: np.ndarray, V0: np.ndarray,  # (H,p), (H,), (H,2)\n",
    "    tau_w: float, tau_v: float,                       # scalars\n",
    "    activation: str = \"tanh\",\n",
    "    include_b1_in_Sigma: bool = True,\n",
    "    include_b2_in_Sigma: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      R, P, S, Sigma_y, JW_list, Phi_mat\n",
    "    \"\"\"\n",
    "    Phi_mat, JW_list, Jb1_list, Jb2_vec = build_hidden_and_jacobian_W_multi(\n",
    "        X, W0, b0, V0, activation=activation\n",
    "    )\n",
    "    Sigma_y = build_Sigma_y_multi(\n",
    "        Phi_mat=Phi_mat,\n",
    "        tau_v=tau_v,\n",
    "        Jb1_list=Jb1_list,\n",
    "        Jb2_vec=Jb2_vec,\n",
    "        include_b1=include_b1_in_Sigma,\n",
    "        include_b2=include_b2_in_Sigma,\n",
    "    )\n",
    "    # Gaussian prior => lambda_eff ones\n",
    "    lambda_eff = np.ones_like(W0)\n",
    "    P = build_P_from_lambda_tau(lambda_eff, tau_w=float(tau_w))\n",
    "    S = build_S_multi(JW_list, Sigma_y)\n",
    "    R = shrinkage_matrix_stable(P, S)\n",
    "    return R, P, S, Sigma_y, JW_list, Phi_mat\n",
    "\n",
    "def compute_shrinkage_gaussian_two_output(\n",
    "    X: np.ndarray,\n",
    "    W_all: np.ndarray, b1_all: np.ndarray, V_all: np.ndarray,   # (D,H,p), (D,H), (D,H,2)\n",
    "    tau_w_all: np.ndarray, tau_v_all: np.ndarray,               # (D,), (D,)\n",
    "    activation: str = \"tanh\",\n",
    "    return_mats: bool = True,\n",
    "    include_b1_in_Sigma: bool = True,\n",
    "    include_b2_in_Sigma: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop draws and compute shrinkage on the W-block:\n",
    "    Returns:\n",
    "      R_stack, S_stack, P_stack, G_stack, shrink_stack, r_eigs, df_eff\n",
    "    \"\"\"\n",
    "    D, H, p = W_all.shape\n",
    "    N = H * p\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    S_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    P_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    G_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    shrink_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    r_eigs = np.empty((D, N))\n",
    "    df_eff = np.empty(D)\n",
    "\n",
    "    for d in range(D):\n",
    "        R, P, S, Sigma_y, _, _ = compute_shrinkage_for_draw_gaussian_two_output(\n",
    "            X=X,\n",
    "            W0=W_all[d], b0=b1_all[d], V0=V_all[d],\n",
    "            tau_w=float(tau_w_all[d]),\n",
    "            tau_v=float(tau_v_all[d]),\n",
    "            activation=activation,\n",
    "            include_b1_in_Sigma=include_b1_in_Sigma,\n",
    "            include_b2_in_Sigma=include_b2_in_Sigma,\n",
    "        )\n",
    "        # Whitened S for diagnostics\n",
    "        pvec = np.diag(P)\n",
    "        P_inv_sqrt = np.diag(1.0 / np.sqrt(pvec))\n",
    "        G = P_inv_sqrt @ S @ P_inv_sqrt\n",
    "        I = np.identity(N)\n",
    "        shrink_mat = np.linalg.inv(I + G) @ G\n",
    "\n",
    "        if return_mats:\n",
    "            R_stack[d] = R\n",
    "            S_stack[d] = S\n",
    "            P_stack[d] = P\n",
    "            G_stack[d] = G\n",
    "            shrink_stack[d] = shrink_mat\n",
    "\n",
    "        r, df = shrinkage_eigs_and_df(P, S)\n",
    "        r_eigs[d] = np.sort(r)\n",
    "        df_eff[d] = df\n",
    "\n",
    "    return R_stack, S_stack, P_stack, G_stack, shrink_stack, r_eigs, df_eff\n",
    "\n",
    "# ============================================================\n",
    "# Example usage (assuming you have X, y, and a fit dict like `tanh_fit`)\n",
    "# ============================================================\n",
    "W, b1, V, b2, tau_w, tau_v, _ = extract_gaussian_two_output_draws(\n",
    "    tanh_fits, model_name='Gaussian tanh'\n",
    ")\n",
    "R_gauss, S_gauss, P_gauss, G_gauss, shrink_gauss, eigs_gauss, df_gauss = compute_shrinkage_gaussian_two_output(\n",
    "    X, W, b1, V, tau_w, tau_v,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"Done (Gaussian, 2 outputs).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
