{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_priors = \"results/priors/single_layer/tanh/friedman\"\n",
    "\n",
    "prior_names = [\"Dirichlet Horseshoe\", \"Regularized Horseshoe\", \"Dirichlet Student T\", \"Gaussian\"]\n",
    "\n",
    "\n",
    "prior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "prior_N200_fits = get_model_fits(\n",
    "    config=\"Friedman_N200_p10_sigma1.00_seed2\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "prior_N500_fits = get_model_fits(\n",
    "    config=\"Friedman_N500_p10_sigma1.00_seed11\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose the hidden node index to inspect\n",
    "node_idx = 1  # e.g., first hidden node\n",
    "\n",
    "def extract_W_for_node(cmdstan_mcmc, var_name=\"W_1\", node=0):\n",
    "    \"\"\"Return array shape (n_draws, P) of weights feeding into a given node.\"\"\"\n",
    "    W = cmdstan_mcmc.stan_variable(var_name)   # shape (n_draws, P, H)\n",
    "    return W[:, :, node]                       # (n_draws, P)\n",
    "\n",
    "# Grab arrays for each prior from your dict\n",
    "W_gauss = extract_W_for_node(prior_N500_fits['Gaussian']['posterior'], node=node_idx)\n",
    "W_reg_hs = extract_W_for_node(prior_N500_fits['Regularized Horseshoe']['posterior'], node=node_idx)\n",
    "W_dir_hs = np.sqrt(10)*extract_W_for_node(prior_N500_fits['Dirichlet Horseshoe']['posterior'], node=node_idx)\n",
    "W_dir_st = np.sqrt(10)*extract_W_for_node(prior_N500_fits['Dirichlet Student T']['posterior'], node=node_idx)\n",
    "#W_dir_gam = extract_W_for_node(prior_N500_fits['Dirichlet Gamma']['Dirichlet Gamma']['posterior'], node=node_idx)\n",
    "\n",
    "models = {\n",
    "    \"Gauss\": W_gauss,\n",
    "    \"RHS\": W_reg_hs,\n",
    "    \"DHS\": W_dir_hs,\n",
    "    \"DS-T\": W_dir_st,\n",
    "    #\"DG\": W_dir_gam\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_profile(W):\n",
    "    # sort |w| per draw, descending; then average by rank\n",
    "    sorted_abs = np.sort(np.abs(W), axis=1)[:, ::-1]         # (n_draws, P)\n",
    "    mean_rank = sorted_abs.mean(axis=0)\n",
    "    q05 = np.quantile(sorted_abs, 0.05, axis=0)\n",
    "    q95 = np.quantile(sorted_abs, 0.95, axis=0)\n",
    "    return mean_rank, q05, q95\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "for name, W in models.items():\n",
    "    mean_rank, q05, q95 = rank_profile(W)\n",
    "    x = np.arange(1, W.shape[1]+1)\n",
    "    plt.plot(x, mean_rank, marker='o', label=name)\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Average |w|\")\n",
    "plt.title(f\"Average magnitude by rank\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_curve(W):\n",
    "    sq = W**2\n",
    "    shares = sq / sq.sum(axis=1, keepdims=True)              # per-draw normalization\n",
    "    # Average of top-k shares ≈ cumsum of mean ordered shares\n",
    "    ordered = np.sort(shares, axis=1)[:, ::-1]\n",
    "    return ordered.mean(axis=0).cumsum()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "for name, W in models.items():\n",
    "    c = topk_curve(W)\n",
    "    plt.plot(np.arange(1, len(c)+1), c, marker='.', label = name)\n",
    "#plt.axhline(0.9, ls='--', lw=1, label='90%')\n",
    "#plt.axhline(0.95, ls='--', lw=1, label='95%')\n",
    "plt.xlabel(\"Rank\"); plt.ylabel(\"Expected share\")\n",
    "plt.title(f\"Cumulative share of squared weights\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winner_freq(W):\n",
    "    winners = np.argmax(np.abs(W), axis=1)            # index of largest |w| per draw\n",
    "    P = W.shape[1]\n",
    "    counts = np.bincount(winners, minlength=P)\n",
    "    return counts / counts.sum()\n",
    "\n",
    "freq_df = pd.DataFrame({name: winner_freq(W) for name, W in models.items()})\n",
    "freq_df.index = [f\"input_{i}\" for i in range(freq_df.shape[0])]\n",
    "print(freq_df.round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(v):\n",
    "    v = np.sort(v)\n",
    "    n = v.size\n",
    "    return (np.sum((2*np.arange(1, n+1) - n - 1) * v)) / (n * v.sum())\n",
    "\n",
    "def gini_over_draws(W):\n",
    "    sq = W**2\n",
    "    shares = sq / sq.sum(axis=1, keepdims=True)\n",
    "    return np.apply_along_axis(gini, 1, shares)\n",
    "\n",
    "for name, W in models.items():\n",
    "    g = gini_over_draws(W)\n",
    "    print(f\"{name}: mean Gini = {g.mean():.3f}   (0=uniform, 1=one-hot)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "def shares_from_W(W):\n",
    "    \"\"\"Row-normalized squared weights (per draw).\"\"\"\n",
    "    sq = W**2\n",
    "    return sq / (sq.sum(axis=1, keepdims=True) + EPS)\n",
    "\n",
    "# --- Shannon entropy and friends ---\n",
    "def shannon_entropy_over_draws(W):\n",
    "    \"\"\"\n",
    "    Natural-log entropy per draw.\n",
    "    Range: [0, ln P]. 0 if one-hot, ln P if uniform.\n",
    "    \"\"\"\n",
    "    p = shares_from_W(W)\n",
    "    return -(p * np.log(p + EPS)).sum(axis=1)\n",
    "\n",
    "def norm_entropy_over_draws(W):\n",
    "    \"\"\"\n",
    "    Normalized sparsity-style measure in [0,1]:\n",
    "    0=uniform, 1=one-hot.\n",
    "    \"\"\"\n",
    "    H = shannon_entropy_over_draws(W)\n",
    "    P = W.shape[1]\n",
    "    return 1.0 - H / (np.log(P) + EPS)\n",
    "\n",
    "def perplexity_over_draws(W):\n",
    "    \"\"\"\n",
    "    Effective count via entropy: exp(H) in [1, P].\n",
    "    \"\"\"\n",
    "    H = shannon_entropy_over_draws(W)\n",
    "    return np.exp(H)\n",
    "\n",
    "\n",
    "# --- Convenience: summarize per model ---\n",
    "def summarize_entropy_kl(models):\n",
    "    rows = []\n",
    "    for name, W in models.items():\n",
    "        H = shannon_entropy_over_draws(W)\n",
    "        Hn = 1 - H / (np.log(W.shape[1]) + EPS)\n",
    "        PPX = np.exp(H)\n",
    "\n",
    "        def m_ci(x):\n",
    "            return np.mean(x), np.quantile(x, 0.05), np.quantile(x, 0.95)\n",
    "\n",
    "        H_m, H_l, H_u = m_ci(H)\n",
    "        Hn_m, Hn_l, Hn_u = m_ci(Hn)\n",
    "        PPX_m, PPX_l, PPX_u = m_ci(PPX)\n",
    "\n",
    "        rows.append({\n",
    "            \"model\": name,\n",
    "            \"H (nats) mean\": H_m, \"H p05\": H_l, \"H p95\": H_u,\n",
    "            \"H_norm mean\": Hn_m, \"H_norm p05\": Hn_l, \"H_norm p95\": Hn_u,\n",
    "            \"Perplexity mean\": PPX_m, \"Perplexity p05\": PPX_l, \"Perplexity p95\": PPX_u,\n",
    "        })\n",
    "    return pd.DataFrame(rows).set_index(\"model\")\n",
    "\n",
    "# Example: print a compact summary table\n",
    "summary_df = summarize_entropy_kl(models)\n",
    "print(summary_df[[\n",
    "    \"H_norm mean\", \"Perplexity mean\"\n",
    "]].round(3))\n",
    "\n",
    "# Optional: quick visual comparing normalized entropy & KL across models\n",
    "plt.figure(figsize=(6.5, 3.8))\n",
    "x = np.arange(len(models))\n",
    "bar_w = 0.35\n",
    "Hn_means = [norm_entropy_over_draws(W).mean() for W in models.values()]\n",
    "plt.bar(x - bar_w/2, Hn_means, width=bar_w, label=\"1 - H/ln P\")\n",
    "plt.xticks(x, list(models.keys()), rotation=15)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Normalized (0=uniform, 1=one-hot)\")\n",
    "plt.title(f\"Entropy & KL sparsity — node {node_idx}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# generic: rank→uniform columnwise (works for any matrix A)\n",
    "def empirical_copula_cols(A):\n",
    "    n, P = A.shape\n",
    "    U = np.zeros_like(A, dtype=float)\n",
    "    for j in range(P):\n",
    "        U[:, j] = rankdata(A[:, j], method=\"average\") / (n + 1.0)\n",
    "    return U\n",
    "\n",
    "def normalize_rows(W, mode=\"none\", eps=1e-12):\n",
    "    if mode == \"none\":\n",
    "        return W\n",
    "    if mode == \"l2\":\n",
    "        norms = np.linalg.norm(W, axis=1, keepdims=True)\n",
    "        return W / np.maximum(norms, eps)\n",
    "    if mode == \"mad\":  # robust alternativ\n",
    "        med = np.median(W, axis=1, keepdims=True)\n",
    "        mad = np.median(np.abs(W - med), axis=1, keepdims=True)\n",
    "        return (W - med) / np.maximum(mad, eps)\n",
    "    raise ValueError(\"mode must be 'none', 'l2', or 'mad'\")\n",
    "\n",
    "def tail_dependence_curve_from_matrix(A, i=0, j=1, u_grid=None):\n",
    "    if u_grid is None:\n",
    "        u_grid = np.linspace(0.80, 0.99, 25)\n",
    "    U = empirical_copula_cols(A)\n",
    "    ui, uj = U[:, i], U[:, j]\n",
    "    lam = np.array([np.mean(uj[ui > u] > u) if np.any(ui > u) else np.nan for u in u_grid])\n",
    "    return u_grid, lam\n",
    "\n",
    "def plot_tail_dependence(models, i=0, j=1, u_grid=None):\n",
    "    if u_grid is None:\n",
    "        u_grid = np.linspace(0.80, 0.99, 25)\n",
    "\n",
    "    curves = {}\n",
    "    ymax = 0.0\n",
    "    for name, W in models.items():\n",
    "        #Wn = normalize_rows(W, mode=\"l2\")\n",
    "        A = np.abs(W)  # use |w|\n",
    "        u, lam = tail_dependence_curve_from_matrix(A, i=i, j=j, u_grid=u_grid)\n",
    "        curves[name] = (u, lam)\n",
    "        ymax = max(ymax, np.nanmax(lam))\n",
    "    baseline = 1.0 - u_grid\n",
    "    ymax = max(ymax, baseline.max())\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(5, 5), sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "    i = 0\n",
    "    for ax, (name, (u, lam)) in zip(axes, curves.items()):\n",
    "        ax.plot(u, lam, marker='o', ms=3, lw=1)\n",
    "        ax.plot(u_grid, baseline, linestyle='--', lw=1.2, label='(1 - u)')\n",
    "        ax.set_title(name)\n",
    "        ax.set_ylim(0, min(1.0, ymax * 1.05))\n",
    "        ax.set_xlim(0.8, 1)\n",
    "        ax.set_xticks([0.8, 0.9, 1.0])\n",
    "        ax.grid(True, linewidth=0.4, alpha=0.4)\n",
    "        if i > 1:\n",
    "            ax.set_xlabel(\"u\")\n",
    "        if i == 0 or i == 2:\n",
    "            ax.set_ylabel(r\"$\\lambda_U(u) = P(U_j>u \\mid U_i>u)$\")\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "        i += 1\n",
    "\n",
    "    #fig.suptitle(f\"Upper-tail dependence — |w|, inputs {i} vs {j}\", y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "plot_tail_dependence(models, i=0, j=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import default_rng\n",
    "# assumes empirical_copula_cols() is already defined above\n",
    "# assumes shares_from_W(W) is already defined in your session\n",
    "\n",
    "def _gaussian_shares_baseline_lambdaU(P, i=0, j=1, u_grid=None, N=200_000, seed=0):\n",
    "    rng = default_rng(seed)\n",
    "    if u_grid is None:\n",
    "        u_grid = np.linspace(0.80, 0.99, 25)\n",
    "    alpha = np.full(P, 0.5, dtype=float)\n",
    "    p_base = rng.dirichlet(alpha, size=N)    # Gaussian-shares baseline\n",
    "    U = empirical_copula_cols(p_base)\n",
    "    ui, uj = U[:, i], U[:, j]\n",
    "    lam_base = np.array([np.mean(uj[ui > u] > u) if np.any(ui > u) else np.nan for u in u_grid])\n",
    "    return u_grid, lam_base\n",
    "\n",
    "def tail_dependence_on_shares_gauss_baseline(models, i=0, j=1, u_grid=None, N=200_000, seed=0):\n",
    "    if u_grid is None:\n",
    "        u_grid = np.linspace(0.80, 0.99, 25)\n",
    "\n",
    "    P = next(iter(models.values())).shape[1]\n",
    "    u_base, lam_base = _gaussian_shares_baseline_lambdaU(P, i=i, j=j, u_grid=u_grid, N=N, seed=seed)\n",
    "\n",
    "    curves = {}\n",
    "    ymax = np.nanmax(lam_base)\n",
    "    for name, W in models.items():\n",
    "        p = shares_from_W(W)           # your existing function\n",
    "        U = empirical_copula_cols(p)\n",
    "        ui, uj = U[:, i], U[:, j]\n",
    "        lam = np.array([np.mean(uj[ui > u] > u) if np.any(ui > u) else np.nan for u in u_grid])\n",
    "        curves[name] = lam\n",
    "        ymax = max(ymax, np.nanmax(lam))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(5, 5), sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "    i = 0\n",
    "    for ax, (name, lam) in zip(axes, curves.items()):\n",
    "        ax.plot(u_grid, lam, marker='o', ms=3, lw=1)\n",
    "        ax.plot(u_base, lam_base, ls='--', lw=1.2, label='Dir(½)')\n",
    "        ax.set_title(name)\n",
    "        ax.set_ylim(0, min(1.0, ymax * 1.05))\n",
    "        ax.set_xlim(0.8, 1)\n",
    "        ax.grid(True, linewidth=0.4, alpha=0.4)\n",
    "        if i > 1:\n",
    "            ax.set_xlabel(\"u\")\n",
    "        if i == 0 or i == 2:\n",
    "            ax.set_ylabel(r\"$\\lambda_U(u) = P(U_j>u \\mid U_i>u)$\")\n",
    "        ax.legend(fontsize=8, loc='upper right')\n",
    "        i += 1\n",
    "\n",
    "    #fig.suptitle(r\"Upper-tail dependence of $\\frac{w^2}{\\sum_k w_k^2}$ — (baseline = Gaussian ⇒ Dir(½))\", y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Example:\n",
    "tail_dependence_on_shares_gauss_baseline(models, i=0, j=1, N=200_000, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_priors = \"results/tweak_tau\"\n",
    "\n",
    "prior_names = [\"Regularized Horseshoe\"]\n",
    "\n",
    "\n",
    "p0_1_fit = get_model_fits(\n",
    "    config=\"p0_1\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "p0_2_fit = get_model_fits(\n",
    "    config=\"p0_2\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "p0_4_fit = get_model_fits(\n",
    "    config=\"p0_4\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "p0_8_fit = get_model_fits(\n",
    "    config=\"p0_8\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "p0_9_fit = get_model_fits(\n",
    "    config=\"p0_9\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "results_dir_priors_other = \"results/priors/single_layer/tanh/friedman\"\n",
    "\n",
    "\n",
    "prior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_priors_other,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/friedman/Friedman_N100_p10_sigma1.00_seed1.npz\"\n",
    "data = np.load(path)\n",
    "X = data['X_train']\n",
    "y = data['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import extract_model_draws, compute_shrinkage\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    p0_1_fit, model='Regularized Horseshoe'\n",
    ")\n",
    "\n",
    "R_p01, S_p01, P_p01, G_p01, shrink_p01, eigs_p01, df_eff_p01 = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with p0_1\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    p0_2_fit, model='Regularized Horseshoe'\n",
    ")\n",
    "\n",
    "R_p02, S_p02, P_p02, G_p02, shrink_p02, eigs_p02, df_eff_p02 = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with p0_2\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    p0_4_fit, model='Regularized Horseshoe'\n",
    ")\n",
    "\n",
    "R_p04, S_p04, P_p04, G_p04, shrink_p04, eigs_p04, df_eff_p04 = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with p0_4\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    p0_8_fit, model='Regularized Horseshoe'\n",
    ")\n",
    "\n",
    "R_p08, S_p08, P_p08, G_p08, shrink_p08, eigs_p08, df_eff_p08 = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with p0_8\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    p0_9_fit, model='Regularized Horseshoe'\n",
    ")\n",
    "\n",
    "R_p09, S_p09, P_p09, G_p09, shrink_p09, eigs_p09, df_eff_p09 = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with p0_9\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    prior_N100_fits, model='Regularized Horseshoe'\n",
    ")\n",
    "\n",
    "R_other, S_other, P_other, G_other, shrink_other, eigs_other, df_eff_other = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with other\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP_inv_S_p01 = np.eye(16*10)[:, :] - R_p01\n",
    "SP_inv_S_p02 = np.eye(16*10)[:, :] - R_p02\n",
    "SP_inv_S_p04 = np.eye(16*10)[:, :] - R_p04\n",
    "SP_inv_S_p08 = np.eye(16*10)[:, :] - R_p08\n",
    "SP_inv_S_p09 = np.eye(16*10)[:, :] - R_p09\n",
    "\n",
    "SP_inv_S_other = np.eye(16*10)[:, :] - R_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you also want “total shrinkage”, use your SP_inv_S_* stacks (I - R):\n",
    "tr_SPinvS_p01 = np.trace(SP_inv_S_p01, axis1=1, axis2=2)\n",
    "tr_SPinvS_p02   = np.trace(SP_inv_S_p02,   axis1=1, axis2=2)\n",
    "tr_SPinvS_p04   = np.trace(SP_inv_S_p04,   axis1=1, axis2=2)\n",
    "tr_SPinvS_p08   = np.trace(SP_inv_S_p08,   axis1=1, axis2=2)\n",
    "tr_SPinvS_p09   = np.trace(SP_inv_S_p09,   axis1=1, axis2=2)\n",
    "\n",
    "tr_SPinvS_other   = np.trace(SP_inv_S_other,   axis1=1, axis2=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_SPinvS_p01, bins=bins, alpha=0.5, label=\"p0=1\")\n",
    "plt.hist(tr_SPinvS_p02,   bins=bins, alpha=0.5, label=\"p0=2\")\n",
    "plt.hist(tr_SPinvS_p04,   bins=bins, alpha=0.5, label=\"p0=4\")\n",
    "plt.hist(tr_SPinvS_p08,   bins=bins, alpha=0.5, label=\"p0=8\")\n",
    "plt.hist(tr_SPinvS_p09,   bins=bins, alpha=0.5, label=\"p0=9\")\n",
    "plt.hist(tr_SPinvS_other,   bins=bins, alpha=0.5, label=\"other\")\n",
    "plt.xlabel(r\"$tr((P+S)^{-1}S)$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
