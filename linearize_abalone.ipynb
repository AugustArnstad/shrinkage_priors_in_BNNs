{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/abalone\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/abalone\"\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "full_config_path = \"abalone_N3341_p8\"\n",
    "\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X, X_test, y, y_test = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "# Coerce everything to plain float64 NumPy arrays\n",
    "X      = np.asarray(X, dtype=float)\n",
    "X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "# y often comes as a (n,1) DataFrame/array — flatten to (n,)\n",
    "y      = np.asarray(y, dtype=float).reshape(-1)\n",
    "y_test = np.asarray(y_test, dtype=float).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import cholesky, solve\n",
    "from utils.kappa_matrix import shrinkage_matrix_stable\n",
    "\n",
    "def build_operators_from_PS(P, S):\n",
    "    \"\"\"\n",
    "    P, S: arrays of shape (S, d, d), SPD per sample.\n",
    "    Returns:\n",
    "      G        : P^{-1/2} S P^{-1/2}\n",
    "      shrink_PS: (P+S)^{-1} S\n",
    "      shrink_G : (I+G)^{-1} G\n",
    "    \"\"\"\n",
    "    S_, d, _ = P.shape\n",
    "    G         = np.empty_like(P, dtype=np.float64)\n",
    "    shrink_PS = np.empty_like(P, dtype=np.float64)\n",
    "    shrink_G  = np.empty_like(P, dtype=np.float64)\n",
    "\n",
    "    I = np.eye(d)\n",
    "\n",
    "    for s in range(S_):\n",
    "        Ps = P[s]; Ss = S[s]\n",
    "\n",
    "        # --- G = P^{-1/2} S P^{-1/2} via Cholesky (Ps = C C^T) -> C^{-T} S C^{-1}\n",
    "        C = cholesky(Ps)            # upper-triangular by NumPy convention\n",
    "        # temp = C^{-1}^T S\n",
    "        temp = solve(C.T, Ss)#, assume_a='sym')    # solves C^T X = S  -> X = C^{-T} S\n",
    "        Gs   = solve(C, temp.T)#, assume_a='sym').T  # solves C Y^T = temp^T -> Y = C^{-1} temp\n",
    "        G[s] = Gs\n",
    "\n",
    "        # # --- (P+S)^{-1} S\n",
    "        Rs = shrinkage_matrix_stable(Ps, Ss)\n",
    "        # A = Ps + Ss\n",
    "        # L = cholesky(A)\n",
    "        # # Solve A X = S  (two triangular solves)\n",
    "        # Y = solve(L, Ss)#, lower=False)           # L X = S  (NumPy returns upper L; set lower=False)\n",
    "        # X = solve(L.T, Y)#, lower=True)          # L^T X = Y\n",
    "        # shrink_PS[s] = X\n",
    "        shrink_PS[s] = np.eye(Ps.shape[0]) - Rs\n",
    "        \n",
    "\n",
    "        # --- (I+G)^{-1} G\n",
    "        B = I + Gs\n",
    "        LB = cholesky(B)\n",
    "        YB = solve(LB, Gs)#, lower=False)\n",
    "        XB = solve(LB.T, YB)#, lower=True)\n",
    "        shrink_G[s] = XB\n",
    "\n",
    "    return G, shrink_PS, shrink_G\n",
    "\n",
    "\n",
    "# Example usage after reloading a saved NPZ:\n",
    "dat = np.load(\"Abalone_matrices/Gaussian_PS.npz\")\n",
    "P_gauss, S_gauss = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_gauss, shrink_PS_gauss, shrink_G_gauss = build_operators_from_PS(P_gauss, S_gauss)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Regularized_Horseshoe_PS.npz\")\n",
    "P_RHS, S_RHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_RHS, shrink_PS_RHS, shrink_G_RHS = build_operators_from_PS(P_RHS, S_RHS)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_Horseshoe_PS.npz\")\n",
    "P_DHS, S_DHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DHS, shrink_PS_DHS, shrink_G_DHS = build_operators_from_PS(P_DHS, S_DHS)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_StudentT_PS.npz\")\n",
    "P_DST, S_DST = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DST, shrink_PS_DST, shrink_G_DST = build_operators_from_PS(P_DST, S_DST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import cholesky, solve, eigvalsh, norm\n",
    "\n",
    "from utils.kappa_matrix import (\n",
    "    build_hidden_and_jacobian_W, build_Sigma_y, build_S,\n",
    "    build_P_from_lambda_tau, shrinkage_matrix_stable, extract_model_draws\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Small utilities\n",
    "# =========================\n",
    "\n",
    "def solve_psd_pinv(S, g, rtol=1e-10):\n",
    "    \"\"\" Moore–Penrose for PSD S times vector g. \"\"\"\n",
    "    evals, Q = np.linalg.eigh(S)\n",
    "    tol = rtol * max(evals.max(), 1.0)\n",
    "    keep = evals > tol\n",
    "    if not np.any(keep):\n",
    "        return np.zeros_like(g)\n",
    "    inv_eigs = np.zeros_like(evals)\n",
    "    inv_eigs[keep] = 1.0 / evals[keep]\n",
    "    return Q @ (inv_eigs * (Q.T @ g))\n",
    "\n",
    "def vec_w1(W1_draw, H, p, vec_order=\"unit-major\"):\n",
    "    \"\"\"\n",
    "    Return w1 vector consistent with how J_w columns are ordered.\n",
    "    - 'unit-major' (default): weights are grouped by hidden unit (row-major flatten) -> shape (H,p) -> .reshape(-1)\n",
    "    - 'feature-major': weights grouped by feature (Fortran/column-major) -> .reshape(-1, order='F')\n",
    "    \"\"\"\n",
    "    W = np.asarray(W1_draw).reshape(H, p)\n",
    "    if vec_order == \"unit-major\":\n",
    "        return W.reshape(-1)  # C-order\n",
    "    elif vec_order == \"feature-major\":\n",
    "        return W.reshape(-1, order='F')\n",
    "    else:\n",
    "        raise ValueError(\"vec_order must be 'unit-major' or 'feature-major'\")\n",
    "\n",
    "# ----- Low-rank builder that mirrors build_Sigma_y exactly -----\n",
    "def build_U(Phi_mat, tau_v, J_b1=None, J_b2=None, include_b1=True, include_b2=True):\n",
    "    cols = [np.sqrt(tau_v**2) * Phi_mat]          # (n, H)\n",
    "    if include_b1 and (J_b1 is not None):\n",
    "        cols.append(J_b1)                          # (n, H)\n",
    "    if include_b2 and (J_b2 is not None):\n",
    "        cols.append(J_b2.reshape(-1, 1))           # (n, 1)\n",
    "    return np.concatenate(cols, axis=1) if len(cols) > 1 else cols[0]  # (n, r)\n",
    "\n",
    "# ----- Woodbury apply: returns Σ_y^{-1} B without forming Σ_y -----\n",
    "def woodbury_apply(U, sigma2, B):\n",
    "    # U: (n, r), B: (n,) or (n, k)\n",
    "    n = U.shape[0]\n",
    "    B = B.reshape(n, -1)  # (n, k)\n",
    "    inv_sigma2 = 1.0 / sigma2\n",
    "    UtU = U.T @ U                        # (r, r)\n",
    "    A = np.eye(UtU.shape[0]) + inv_sigma2 * UtU\n",
    "    RHS = inv_sigma2 * (U.T @ B)         # (r, k)\n",
    "    X = np.linalg.solve(A, RHS)          # (r, k)\n",
    "    out = inv_sigma2 * (B - U @ X)       # (n, k)\n",
    "    return out if out.shape[1] > 1 else out.ravel()\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError, eigvalsh, norm, solve\n",
    "\n",
    "# ---------- robust PSD→SPD helpers ----------\n",
    "def _sym(A):\n",
    "    return 0.5 * (A + A.T)\n",
    "\n",
    "def _min_eig(A):\n",
    "    try:\n",
    "        return float(np.min(eigvalsh(_sym(A))))\n",
    "    except LinAlgError:\n",
    "        # fall back if eig fails due to NaNs\n",
    "        return np.nan\n",
    "\n",
    "def safe_cholesky(A, name=\"A\", jitter0=None, max_tries=8, verbose=True):\n",
    "    \"\"\"\n",
    "    Cholesky with automatic diagonal jitter. Returns (L, jitter_used).\n",
    "    A must be symmetric (we symmetrize defensively).\n",
    "    Jitter schedule: jitter0 * 10^k, k=0..max_tries-1\n",
    "    Default jitter0 = 1e-12 * trace(A)/n, or 1e-12 if trace<=0.\n",
    "    \"\"\"\n",
    "    A = _sym(np.asarray(A, float))\n",
    "    n = A.shape[0]\n",
    "\n",
    "    if jitter0 is None:\n",
    "        tr = float(np.trace(A))\n",
    "        jitter0 = 1e-12 * (tr / n if tr > 0 else 1.0)\n",
    "\n",
    "    # Try without jitter first\n",
    "    try:\n",
    "        L = np.linalg.cholesky(A)\n",
    "        return L, 0.0\n",
    "    except LinAlgError:\n",
    "        pass\n",
    "\n",
    "    # Escalate jitter\n",
    "    jitter = jitter0\n",
    "    for k in range(max_tries):\n",
    "        A_jit = A + jitter * np.eye(n)\n",
    "        try:\n",
    "            L = np.linalg.cholesky(A_jit)\n",
    "            if verbose:\n",
    "                mine = _min_eig(A)\n",
    "                print(f\"[safe_cholesky] {name}: added jitter={jitter:.2e} \"\n",
    "                      f\"(min eig before={mine:.2e})\")\n",
    "            return L, jitter\n",
    "        except LinAlgError:\n",
    "            jitter *= 10.0\n",
    "\n",
    "    # Final attempt with eigen clip (last resort)\n",
    "    w, U = np.linalg.eigh(A)\n",
    "    floor = max(1e-15, 1e-12 * np.max(w))\n",
    "    w_clipped = np.clip(w, floor, None)\n",
    "    A_proj = (U * w_clipped) @ U.T\n",
    "    try:\n",
    "        L = np.linalg.cholesky(A_proj)\n",
    "        if verbose:\n",
    "            print(f\"[safe_cholesky] {name}: eigen-floor to {floor:.2e}\")\n",
    "        return L, -floor  # indicate eigen-floor used\n",
    "    except LinAlgError as e:\n",
    "        raise LinAlgError(f\"Cholesky failed for {name} even after jitter & clip.\") from e\n",
    "\n",
    "# ---------- drop-in replacements for your helpers ----------\n",
    "def cholesky_powers(P, name=\"P\", verbose=True):\n",
    "    \"\"\"\n",
    "    Returns (P_half, P_mhalf, jitter_used).\n",
    "    P_half is lower-tri L s.t. P≈L L^T, P_mhalf≈L^{-T}.\n",
    "    \"\"\"\n",
    "    L, jit = safe_cholesky(P, name=name, verbose=verbose)\n",
    "    I = np.eye(P.shape[0])\n",
    "    P_half = L\n",
    "    P_mhalf = solve(L.T, I)  # L^T X = I -> X = L^{-T}\n",
    "    return P_half, P_mhalf, jit\n",
    "\n",
    "def shrink_from_PS(P, S, verbose=True):\n",
    "    \"\"\"(P+S)^{-1} S using robust Cholesky on (P+S).\"\"\"\n",
    "    A = _sym(P + S)\n",
    "    L, jit = safe_cholesky(A, name=\"P+S\", verbose=verbose)\n",
    "    Y = solve(L, _sym(S))\n",
    "    return solve(L.T, Y)\n",
    "\n",
    "def shrink_from_G(G, verbose=True):\n",
    "    \"\"\"(I+G)^{-1} G using robust Cholesky on (I+G).\"\"\"\n",
    "    B = _sym(np.eye(G.shape[0]) + G)\n",
    "    L, jit = safe_cholesky(B, name=\"I+G\", verbose=verbose)\n",
    "    Y = solve(L, _sym(G))\n",
    "    return solve(L.T, Y)\n",
    "\n",
    "def whiten_G_from_PS(P, S, verbose=True):\n",
    "    \"\"\"G = P^{-1/2} S P^{-1/2} with robust (approx) inverse sqrt via Cholesky.\"\"\"\n",
    "    # P_half = L, P_mhalf = L^{-T}\n",
    "    P_half, P_mhalf, jit = cholesky_powers(_sym(P), name=\"P\", verbose=verbose)\n",
    "    # G = L^{-T} S L^{-1}  (we compute as solve(L, S) then solve(L.T, ...))\n",
    "    X = solve(P_half, _sym(S))\n",
    "    G = solve(P_half.T, X)\n",
    "    return _sym(G)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Stepwise checks (one draw)\n",
    "# =========================\n",
    "\n",
    "def check_step_1_rebuild_S(\n",
    "    X, y,\n",
    "    W1_d, b1_d, W2_d, b2_d,\n",
    "    sigma_d, tau_v_d,\n",
    "    S_stored_d,\n",
    "    include_b1_in_Sigma=True, include_b2_in_Sigma=True,\n",
    "    activation=\"tanh\",\n",
    "    vec_order=\"unit-major\",\n",
    "    dense_crosscheck=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Rebuild J_w and Σ_y^{-1} via Woodbury. Compare Ŝ = J^T Σ^{-1} J to stored S.\n",
    "    Prints metrics and returns dict with errors and objects needed for later steps.\n",
    "    \"\"\"\n",
    "    Phi, Jw, Jb1, Jb2 = build_hidden_and_jacobian_W(\n",
    "        X, W1_d, b1_d, W2_d, activation=activation\n",
    "    )\n",
    "\n",
    "    H, p = W1_d.shape\n",
    "    U = build_U(Phi, tau_v_d, J_b1=Jb1, J_b2=Jb2,\n",
    "                include_b1=include_b1_in_Sigma, include_b2=include_b2_in_Sigma)\n",
    "\n",
    "    # Σ_y^{-1} J_w\n",
    "    inv_Sigma_J = woodbury_apply(U, sigma_d**2, Jw)  # (n, Hp)\n",
    "    S_hat = Jw.T @ inv_Sigma_J                       # (Hp, Hp)\n",
    "\n",
    "    # Compare\n",
    "    S_d = S_stored_d\n",
    "    rel_err = norm(S_hat - S_d, 'fro') / max(norm(S_d, 'fro'), 1e-16)\n",
    "    max_abs = np.max(np.abs(S_hat - S_d))\n",
    "\n",
    "    dense_rel = None\n",
    "    if dense_crosscheck:\n",
    "        Sigma_y = build_Sigma_y(\n",
    "            Phi, tau_v=tau_v_d, noise=sigma_d,\n",
    "            J_b1=Jb1, J_b2=Jb2,\n",
    "            include_b1=include_b1_in_Sigma, include_b2=include_b2_in_Sigma\n",
    "        )\n",
    "        Xsol = np.linalg.solve(Sigma_y, Jw)\n",
    "        S_dense = Jw.T @ Xsol\n",
    "        dense_rel = norm(S_dense - S_d, 'fro') / max(norm(S_d, 'fro'), 1e-16)\n",
    "\n",
    "    print(f\"[Step1]  ‖Ŝ−S‖/‖S‖ = {rel_err:.3e}   |Ŝ−S|_∞ = {max_abs:.3e}\"\n",
    "          + (\"\" if dense_rel is None else f\"   (dense={dense_rel:.3e})\"))\n",
    "\n",
    "    return dict(\n",
    "        Phi=Phi, Jw=Jw, Jb1=Jb1, Jb2=Jb2, U=U,\n",
    "        S_hat=S_hat, S_d=S_d,\n",
    "        rel_err_S=rel_err, max_abs_S=max_abs,\n",
    "        H=H, p=p\n",
    "    )\n",
    "\n",
    "def _sym(A):\n",
    "    return 0.5*(A + A.T)\n",
    "\n",
    "def spectral_shrink_from_G(G, clip_floor=0.0):\n",
    "    \"\"\"\n",
    "    Return (I+G)^{-1} G computed spectrally as U diag(w/(1+w)) U^T.\n",
    "    Optionally clip tiny negative eigenvalues (round-off) to 'clip_floor' (>=0).\n",
    "    \"\"\"\n",
    "    w, U = np.linalg.eigh(_sym(G))\n",
    "    if clip_floor is not None:\n",
    "        w = np.clip(w, clip_floor, None)  # keep PSD\n",
    "    s = w / (1.0 + w)                    # w/(1+w) is well-behaved at w≈0\n",
    "    return (U * s) @ U.T\n",
    "\n",
    "def check_step_2_shrinkage_identity(P_d, S_d, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Verify (P+S)^{-1}S == P^{-1/2} (I+G)^{-1}G P^{1/2} using spectral shrinkage for the G-branch.\n",
    "    \"\"\"\n",
    "    # whiten\n",
    "    P_d = _sym(P_d); S_d = _sym(S_d)\n",
    "    # P^{-1/2} via Cholesky (with tiny jitter if needed)\n",
    "    # (we reuse your safe Cholesky from earlier)\n",
    "    P_half, P_mhalf, _ = cholesky_powers(P_d, name=\"P\", verbose=False)\n",
    "    # G = P^{-1/2} S P^{-1/2}\n",
    "    G = _sym(P_mhalf @ S_d @ P_mhalf.T)\n",
    "\n",
    "    # Original-basis shrinkage via (P+S)^{-1} S (triangular solves are fine here)\n",
    "    shrink_PS = shrink_from_PS(P_d, S_d, verbose=False)\n",
    "\n",
    "    # Whitened shrinkage via spectral formula (no solves / no eigen-floor on I+G)\n",
    "    shrink_G_spec = spectral_shrink_from_G(G, clip_floor=0.0)\n",
    "\n",
    "    # Map between bases\n",
    "    shrink_from_G_in_orig = P_mhalf @ shrink_G_spec @ P_half           # P^{-1/2} * (...) * P^{1/2}\n",
    "    shrink_PS_in_white    = P_half  @ shrink_PS      @ P_mhalf         # P^{1/2}  * (...) * P^{-1/2}\n",
    "\n",
    "    ok_orig  = np.allclose(shrink_PS,          shrink_from_G_in_orig, rtol=tol, atol=1e-10)\n",
    "    ok_white = np.allclose(shrink_PS_in_white, shrink_G_spec,         rtol=tol, atol=1e-10)\n",
    "\n",
    "    diff_orig  = float(np.max(np.abs(shrink_PS - shrink_from_G_in_orig)))\n",
    "    diff_white = float(np.max(np.abs(shrink_PS_in_white - shrink_G_spec)))\n",
    "\n",
    "    print(f\"[Step2]  shrink match (orig)={ok_orig}  max|Δ|={diff_orig:.2e}   \"\n",
    "          f\"(white)={ok_white}  max|Δ|={diff_white:.2e}\")\n",
    "\n",
    "    return dict(\n",
    "        shrink_PS=shrink_PS,\n",
    "        shrink_G=shrink_G_spec,\n",
    "        ok_orig=bool(ok_orig),  diff_orig=diff_orig,\n",
    "        ok_white=bool(ok_white), diff_white=diff_white\n",
    "    )\n",
    "\n",
    "\n",
    "def check_step_3_barw_two_forms(\n",
    "    y, Jw, U, sigma_d,\n",
    "    P_d, S_d, shrink_PS=None,\n",
    "    W1_d=None, b1_d=None, vec_order=\"unit-major\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare bar w via:\n",
    "      A: (P+S)^{-1} g\n",
    "      B: (I - (P+S)^{-1}S) P^{-1} g\n",
    "    where g = J^T Σ^{-1} y*,  y* = y + J w0 + J_b1 b1\n",
    "    \"\"\"\n",
    "    # y*\n",
    "    if (W1_d is None) or (b1_d is None):\n",
    "        raise ValueError(\"Provide W1_d and b1_d for y*.\")\n",
    "    H, p = W1_d.shape\n",
    "    w0_vec = vec_w1(W1_d, H, p, vec_order=vec_order)\n",
    "    y_star = y + (Jw @ w0_vec) + (U[:, -1]*0.0 if b1_d is None else 0.0)  # placeholder; J_b1 handled below\n",
    "\n",
    "    # We need J_b1 for the shift, but it’s already absorbed in U when forming Σ^{-1} (no harm to add explicitly):\n",
    "    # If you prefer explicit, pass Jb1 in and add (Jb1 @ b1_d) here. Otherwise, rely on y being the raw target.\n",
    "    # Safer: accept Jb1 as None here and add it outside if needed.\n",
    "\n",
    "    # g = J^T Σ_y^{-1} y*\n",
    "    r = woodbury_apply(U, sigma_d**2, y_star)\n",
    "    g = Jw.T @ r\n",
    "\n",
    "    # Shrinkage pieces\n",
    "    if shrink_PS is None:\n",
    "        shrink_PS = shrink_from_PS(P_d, S_d)\n",
    "    I = np.eye(P_d.shape[0])\n",
    "\n",
    "    # A:\n",
    "    barw_A = solve(P_d + S_d, g)\n",
    "\n",
    "    # B:\n",
    "    P_diag = np.diag(P_d)\n",
    "    z = g / P_diag\n",
    "    barw_B = (I - shrink_PS) @ z\n",
    "\n",
    "    diff = np.max(np.abs(barw_A - barw_B))\n",
    "    rel = norm(barw_A - barw_B) / max(norm(barw_A), 1e-16)\n",
    "\n",
    "    print(f\"[Step3]  barw forms:  max|Δ| = {diff:.3e}   rel = {rel:.3e}\")\n",
    "    return dict(barw_A=barw_A, barw_B=barw_B, max_abs=float(diff), rel=float(rel))\n",
    "\n",
    "# =========================\n",
    "# Full fast mean (many draws)\n",
    "# =========================\n",
    "\n",
    "def compute_linearized_mean_fast_fixed(\n",
    "    X, y,\n",
    "    W_1, b_1, W_2, b_2,\n",
    "    noise_all, tau_v_all,\n",
    "    P_all=None, S_all=None, shrink_PS_all=None,  # prefer supplying stored arrays\n",
    "    lambda_all=None, tau_w_all=None,             # only used if P_all is None (fallback)\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    "    vec_order=\"unit-major\",\n",
    "    use_woodbury=True,\n",
    "    return_mats=False,\n",
    "    D_lim=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Correct, basis-consistent, high-performance linearized mean.\n",
    "    Prefers supplied P_all, S_all, shrink_PS_all (from your NPZs).\n",
    "    \"\"\"\n",
    "    D, H, p = W_1.shape\n",
    "    if D_lim is not None:\n",
    "        D = D_lim\n",
    "    N = H * p\n",
    "    n = y.shape[0]\n",
    "    y = np.asarray(y, float).reshape(n)\n",
    "\n",
    "    w_bar_stack = np.empty((D, N))\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "\n",
    "    for d in range(D):\n",
    "        Phi, Jw, Jb1, Jb2 = build_hidden_and_jacobian_W(\n",
    "            X, W_1[d], b_1[d], W_2[d], activation=activation\n",
    "        )\n",
    "        U = build_U(\n",
    "            Phi, tau_v_all[d],\n",
    "            J_b1=Jb1, J_b2=Jb2,\n",
    "            include_b1=include_b1_in_Sigma, include_b2=include_b2_in_Sigma\n",
    "        )\n",
    "\n",
    "        w0_vec = vec_w1(W_1[d], H, p, vec_order=vec_order)\n",
    "        y_star = y + (Jw @ w0_vec) + (Jb1 @ b_1[d])\n",
    "\n",
    "        if use_woodbury:\n",
    "            r_vec = woodbury_apply(U, noise_all[d]**2, y_star)\n",
    "        else:\n",
    "            Sigma_y = build_Sigma_y(\n",
    "                Phi, tau_v=tau_v_all[d], noise=noise_all[d],\n",
    "                J_b1=Jb1, J_b2=Jb2,\n",
    "                include_b1=include_b1_in_Sigma, include_b2=include_b2_in_Sigma\n",
    "            )\n",
    "            r_vec = solve(Sigma_y, y_star)\n",
    "\n",
    "        g = Jw.T @ r_vec\n",
    "\n",
    "        # P and S for this draw\n",
    "        if P_all is not None:\n",
    "            P_d = P_all[d]\n",
    "            P_diag = np.diag(P_d)\n",
    "        else:\n",
    "            # Fallback: rebuild P_d (make sure this exactly matches your NPZ recipe!)\n",
    "            if (lambda_all is None) or (tau_w_all is None):\n",
    "                raise ValueError(\"Need P_all or (lambda_all, tau_w_all) to build P.\")\n",
    "            P_d = build_P_from_lambda_tau(lambda_all[d], tau_w=tau_w_all[d])\n",
    "            P_diag = np.diag(P_d)\n",
    "\n",
    "        if S_all is not None:\n",
    "            S_d = S_all[d]\n",
    "        else:\n",
    "            # Fallback S: S = J^T Σ^{-1} J  (Woodbury path for Σ^{-1})\n",
    "            inv_Sigma_J = woodbury_apply(U, noise_all[d]**2, Jw)\n",
    "            S_d = Jw.T @ inv_Sigma_J\n",
    "\n",
    "        # Shrinkage\n",
    "        if shrink_PS_all is not None:\n",
    "            shrink_PS_d = shrink_PS_all[d]\n",
    "        else:\n",
    "            shrink_PS_d = shrink_from_PS(P_d, S_d)\n",
    "\n",
    "        # bar w\n",
    "        z = g / P_diag\n",
    "        bar_w = z - (shrink_PS_d @ z)  # (I - shrink_PS) z\n",
    "\n",
    "        if return_mats:\n",
    "            R_tmp = -shrink_PS_d.copy()\n",
    "            np.fill_diagonal(R_tmp, 1.0 + np.diag(R_tmp))\n",
    "            R_stack[d] = R_tmp\n",
    "\n",
    "        w_bar_stack[d] = bar_w\n",
    "\n",
    "    return (R_stack if return_mats else None), w_bar_stack\n",
    "\n",
    "# =========================\n",
    "# One-call driver for a draw\n",
    "# =========================\n",
    "\n",
    "def debug_linearization_once(\n",
    "    X, y,\n",
    "    W1_d, b1_d, W2_d, b2_d,\n",
    "    sigma_d, tau_v_d,\n",
    "    P_d, S_d,\n",
    "    include_b1_in_Sigma=True, include_b2_in_Sigma=True,\n",
    "    activation=\"tanh\",\n",
    "    vec_order=\"unit-major\",\n",
    "    dense_S_check=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Run all three checks for a single draw index, printing compact metrics.\n",
    "    Returns a dict with all intermediate pieces.\n",
    "    \"\"\"\n",
    "    # Step 1: S reconstruction\n",
    "    step1 = check_step_1_rebuild_S(\n",
    "        X, y,\n",
    "        W1_d, b1_d, W2_d, b2_d,\n",
    "        sigma_d, tau_v_d, S_stored_d=S_d,\n",
    "        include_b1_in_Sigma=include_b1_in_Sigma, include_b2_in_Sigma=include_b2_in_Sigma,\n",
    "        activation=activation,\n",
    "        vec_order=vec_order,\n",
    "        dense_crosscheck=dense_S_check\n",
    "    )\n",
    "\n",
    "    # Step 2: shrinkage identity (with conjugation)\n",
    "    step2 = check_step_2_shrinkage_identity(P_d, S_d, tol=1e-8)\n",
    "\n",
    "    # Step 3: two forms of bar w\n",
    "    # Reuse U and Jw from step1; pass shrink_PS from step2 path\n",
    "    out3 = check_step_3_barw_two_forms(\n",
    "        y, step1[\"Jw\"], step1[\"U\"], sigma_d,\n",
    "        P_d, S_d, shrink_PS=shrink_from_PS(P_d, S_d),\n",
    "        W1_d=W1_d, b1_d=b1_d, vec_order=vec_order\n",
    "    )\n",
    "\n",
    "    return dict(step1=step1, step2=step2, step3=out3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(\n",
    "    tanh_fit, model='Gaussian tanh'\n",
    ")\n",
    "\n",
    "_, w_bar_gauss = compute_linearized_mean_fast_fixed(\n",
    "    X, y,\n",
    "    W1, b1, W2, b2,\n",
    "    sigma, tau_v,\n",
    "    P_all=P_gauss, S_all=S_gauss,  # and shrink_PS_all=shrink_PS_DHS if you have it\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    "    vec_order=\"unit-major\",\n",
    "    use_woodbury=True,\n",
    "    D_lim=None\n",
    ")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(\n",
    "    tanh_fit, model='Regularized Horseshoe tanh'\n",
    ")\n",
    "\n",
    "_, w_bar_RHS = compute_linearized_mean_fast_fixed(\n",
    "    X, y,\n",
    "    W1, b1, W2, b2,\n",
    "    sigma, tau_v,\n",
    "    P_all=P_RHS, S_all=S_RHS,  # and shrink_PS_all=shrink_PS_DHS if you have it\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    "    vec_order=\"unit-major\",\n",
    "    use_woodbury=True,\n",
    "    D_lim=None\n",
    ")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(\n",
    "    tanh_fit, model='Dirichlet Horseshoe tanh'\n",
    ")\n",
    "\n",
    "_, w_bar_DHS = compute_linearized_mean_fast_fixed(\n",
    "    X, y,\n",
    "    W1, b1, W2, b2,\n",
    "    sigma, tau_v,\n",
    "    P_all=P_DHS, S_all=S_DHS,  # and shrink_PS_all=shrink_PS_DHS if you have it\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    "    vec_order=\"unit-major\",\n",
    "    use_woodbury=True,\n",
    "    D_lim=None\n",
    ")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(\n",
    "    tanh_fit, model='Dirichlet Student T tanh'\n",
    ")\n",
    "\n",
    "_, w_bar_DST = compute_linearized_mean_fast_fixed(\n",
    "    X, y,\n",
    "    W1, b1, W2, b2,\n",
    "    sigma, tau_v,\n",
    "    P_all=P_DST, S_all=S_DST,  # and shrink_PS_all=shrink_PS_DHS if you have it\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    "    vec_order=\"unit-major\",\n",
    "    use_woodbury=True,\n",
    "    D_lim=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_all_gauss = tanh_fit['Gaussian tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_gauss = tanh_fit['Gaussian tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_RHS = tanh_fit['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_RHS = tanh_fit['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_DST = tanh_fit['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_DST = tanh_fit['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_compare(W_all, v_all, w_bar_stack, sort_key=\"abs_v\"):\n",
    "    \"\"\"\n",
    "    Align signs & permutations across draws before comparing linearized mean with posterior mean.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    W_all        : array-like, shape (D, H, p) or (D, p, H) or with stray singleton dims.\n",
    "    v_all        : array-like, shape (D, H) or (D, H, 1) or similar (length H per draw).\n",
    "    w_bar_stack  : array-like, shape (D, H*p) OR (D, H, p) OR (D, 1, H*p), etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W_fix        : (D, H, p)   sign/permutation aligned\n",
    "    v_fix        : (D, H)\n",
    "    wbar_fix     : (D, H, p)\n",
    "    summary      : dict with RMSE, Corr, CosSim, SignAgree (means vs means in aligned basis)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    W_all = np.asarray(W_all)\n",
    "    v_all = np.asarray(v_all)\n",
    "    w_bar_stack = np.asarray(w_bar_stack)\n",
    "\n",
    "    D = W_all.shape[0]\n",
    "\n",
    "    # --- infer H from v (source of truth) ---\n",
    "    v0 = np.squeeze(v_all[0]).ravel()\n",
    "    H = v0.size\n",
    "    if H == 0:\n",
    "        raise ValueError(\"v_all[0] seems empty; cannot infer H.\")\n",
    "    # infer p from w_bar_stack length\n",
    "    wb0 = np.squeeze(w_bar_stack[0]).ravel()\n",
    "    if wb0.size % H != 0:\n",
    "        # fallback: try infer p from W_all[0] after squeezing\n",
    "        W0 = np.squeeze(W_all[0])\n",
    "        if W0.ndim != 2:\n",
    "            # try to drop any singleton dims\n",
    "            W0 = W0.reshape([s for s in W0.shape if s != 1])\n",
    "        if W0.ndim != 2:\n",
    "            raise ValueError(f\"Cannot infer (H,p). v length={H}, but w_bar_stack[0] has {wb0.size} elems \"\n",
    "                             f\"and W_all[0] has shape {np.squeeze(W_all[0]).shape}.\")\n",
    "        h, p_candidate = W0.shape\n",
    "        if h != H and p_candidate == H:\n",
    "            p = h\n",
    "        else:\n",
    "            p = p_candidate\n",
    "    else:\n",
    "        p = wb0.size // H\n",
    "\n",
    "    N = H * p\n",
    "\n",
    "    # alloc outputs\n",
    "    W_fix = np.empty((D, H, p), dtype=float)\n",
    "    v_fix = np.empty((D, H), dtype=float)\n",
    "    wbar_fix = np.empty((D, H, p), dtype=float)\n",
    "\n",
    "    def coerce_W(Wd, H, p):\n",
    "        \"\"\"Return Wd as (H,p). Accepts (H,p), (p,H), or with singleton dims.\"\"\"\n",
    "        A = np.asarray(Wd, dtype=float)\n",
    "        A = np.squeeze(A)\n",
    "        if A.ndim == 2:\n",
    "            h, q = A.shape\n",
    "            if h == H and q == p:\n",
    "                return A\n",
    "            if h == p and q == H:\n",
    "                return A.T\n",
    "            # If one matches H, try reshape to (H, -1)\n",
    "            if h == H and h*q == H*p:\n",
    "                return A.reshape(H, p)\n",
    "            if q == H and h*q == H*p:\n",
    "                return A.T.reshape(H, p)\n",
    "            raise ValueError(f\"Cannot coerce W of shape {A.shape} to (H,p)=({H},{p}).\")\n",
    "        elif A.ndim == 3 and 1 in A.shape:\n",
    "            # squeeze singleton and recurse\n",
    "            return coerce_W(np.squeeze(A), H, p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected W ndim={A.ndim}, shape={A.shape}\")\n",
    "\n",
    "    def coerce_v(vd, H):\n",
    "        \"\"\"Return vd as (H,)\"\"\"\n",
    "        v = np.asarray(vd, dtype=float).squeeze().ravel()\n",
    "        if v.size != H:\n",
    "            raise ValueError(f\"v has size {v.size}, expected H={H}.\")\n",
    "        return v\n",
    "\n",
    "    def coerce_wbar_row(wbd, H, p):\n",
    "        \"\"\"Return wbar row as (H,p) from (N,) or already (H,p).\"\"\"\n",
    "        w = np.asarray(wbd, dtype=float).squeeze().ravel()\n",
    "        if w.size == H * p:\n",
    "            return w.reshape(H, p)\n",
    "        # already 2D?\n",
    "        W2 = np.asarray(wbd, dtype=float).squeeze()\n",
    "        if W2.ndim == 2 and W2.shape == (H, p):\n",
    "            return W2\n",
    "        raise ValueError(f\"w_bar row has {w.size} elems but H*p={H*p} and not (H,p).\")\n",
    "\n",
    "    for d in range(D):\n",
    "        # coerce shapes\n",
    "        Wd = coerce_W(W_all[d], H, p)          # (H,p)\n",
    "        vd = coerce_v(v_all[d], H)             # (H,)\n",
    "        wbd = coerce_wbar_row(w_bar_stack[d], H, p)\n",
    "\n",
    "        # 1) sign fix so v >= 0\n",
    "        s = np.sign(vd)\n",
    "        s[s == 0.0] = 1.0\n",
    "        Wd = Wd * s[:, None]\n",
    "        wbd = wbd * s[:, None]\n",
    "        vd = np.abs(vd)\n",
    "\n",
    "        # 2) permute units by a stable key\n",
    "        if sort_key == \"abs_v\":\n",
    "            idx = np.argsort(-vd)  # descending |v|\n",
    "        elif sort_key == \"abs_v_times_rownorm\":\n",
    "            idx = np.argsort(-(vd * np.linalg.norm(Wd, axis=1)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sort_key: {sort_key}\")\n",
    "\n",
    "        W_fix[d] = Wd[idx]\n",
    "        wbar_fix[d] = wbd[idx]\n",
    "        v_fix[d] = vd[idx]\n",
    "\n",
    "    # Compare means in aligned basis\n",
    "    w_post_mean = W_fix.reshape(D, -1).mean(axis=0)   # (N,)\n",
    "    \n",
    "    \n",
    "    # before computing means\n",
    "    nan_w = np.isnan(wbar_fix).sum()\n",
    "    inf_w = np.isinf(wbar_fix).sum()\n",
    "    if nan_w or inf_w:\n",
    "        print(f\"[warn] wbar_fix contains {nan_w} NaNs and {inf_w} Infs; using nanmean.\")\n",
    "        # You can also decide to drop offending draws instead of nanmean.\n",
    "\n",
    "    w_lin_mean = np.nanmean(wbar_fix.reshape(D, -1), axis=0)   # ignore any remaining NaNs\n",
    "    w_post_mean = np.nanmean(W_fix.reshape(D, -1), axis=0)\n",
    "    \n",
    "    rmse = float(np.sqrt(np.mean((w_lin_mean - w_post_mean)**2)))\n",
    "    corr = float(np.corrcoef(w_lin_mean, w_post_mean)[0, 1])\n",
    "    cos  = float(np.dot(w_lin_mean, w_post_mean) /\n",
    "                 (np.linalg.norm(w_lin_mean) * np.linalg.norm(w_post_mean)))\n",
    "    sign_agree = float(np.mean(np.sign(w_lin_mean) == np.sign(w_post_mean)))\n",
    "\n",
    "    summary = dict(RMSE=rmse, Corr=corr, CosSim=cos, SignAgree=sign_agree,\n",
    "                   H=H, p=p, N=N)\n",
    "    return W_fix, v_fix, wbar_fix, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: pick a \"MAP-like\" representative draw and plot MAP vs. \\bar{w} ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def select_map_like_index(W_fix: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Returns the index of the draw whose aligned W is closest (in Frobenius norm)\n",
    "    to the aligned posterior mean -- a robust MAP/medoid proxy.\n",
    "    W_fix: (D, H, p) aligned weights (output of align_and_compare)\n",
    "    \"\"\"\n",
    "    D = W_fix.shape[0]\n",
    "    mu = W_fix.reshape(D, -1).mean(axis=0)  # posterior mean in aligned basis\n",
    "    diffs = W_fix.reshape(D, -1) - mu[None, :]\n",
    "    d2 = np.einsum('di,di->d', diffs, diffs)  # squared distances\n",
    "    return int(np.argmin(d2))\n",
    "\n",
    "def plot_map_vs_barw(W_fix: np.ndarray, wbar_fix: np.ndarray, title: str = \"\", alpha=0.7):\n",
    "    \"\"\"\n",
    "    Overlay scatter: MAP-like draw's W (dots) vs the same draw's \\bar{w} (crosses).\n",
    "    Both arrays must be aligned: (D, H, p). We auto-pick a representative draw.\n",
    "    \"\"\"\n",
    "    D, H, p = W_fix.shape\n",
    "    idx = select_map_like_index(W_fix)  # representative draw\n",
    "    w_map = W_fix[idx].reshape(-1)\n",
    "    w_bar = wbar_fix[idx].reshape(-1)\n",
    "    \n",
    "    eps = 1e-1                          # Small threshold to see non-zero weights\n",
    "\n",
    "    x = np.arange(1, H*p + 1)\n",
    "    plt.figure(figsize=(10, 3.5), dpi=150)\n",
    "    plt.scatter(x, w_map, s=12, marker='o', label=\"MAP-like $w$\", alpha=alpha)\n",
    "    plt.scatter(x, w_bar, s=18, marker='x', label=r\"Linearized $\\bar{w}$\", alpha=alpha)\n",
    "\n",
    "    # light vertical guides between hidden units\n",
    "    for h in range(1, H):\n",
    "        plt.axvline(h*p + 0.5, color='0.85', lw=1, zorder=0)\n",
    "    \n",
    "    plt.axhline(eps, color='0.85', lw=1, zorder=0)\n",
    "    plt.axhline(-eps, color='0.85', lw=1, zorder=0)\n",
    "\n",
    "    plt.xlabel(\"parameter index (after alignment)\")\n",
    "    plt.ylabel(\"value\")\n",
    "    plt.title(title if title else \"MAP-like $w$ vs linearized $\\~w$\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gaussian: align and plot ---\n",
    "W_fix_g, v_fix_g, wbar_fix_g, summary_g = align_and_compare(W_all_gauss, v_all_gauss, w_bar_gauss, sort_key=\"abs_v\")\n",
    "print(\"Gaussian summary:\", summary_g)\n",
    "plot_map_vs_barw(W_fix_g, wbar_fix_g, title=\"Gaussian prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Regularized Horseshoe: align and plot ---\n",
    "W_fix_r, v_fix_r, wbar_fix_r, summary_r = align_and_compare(W_all_RHS, v_all_RHS, w_bar_RHS, sort_key=\"abs_v\")\n",
    "print(\"RHS summary:\", summary_r)\n",
    "plot_map_vs_barw(W_fix_r, wbar_fix_r, title=\"RHS prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dirichlet Horseshoe & Dirichlet Student-t: align and plot ---\n",
    "W_fix_dhs, v_fix_dhs, wbar_fix_dhs, summary_dhs = align_and_compare(W_all_DHS, v_all_DHS, w_bar_DHS, sort_key=\"abs_v\")\n",
    "print(\"DHS summary:\", summary_dhs)\n",
    "plot_map_vs_barw(W_fix_dhs, wbar_fix_dhs, title=\"DHS prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_fix_dst, v_fix_dst, wbar_fix_dst, summary_dst = align_and_compare(W_all_DST, v_all_DST, w_bar_DST, sort_key=\"abs_v\")\n",
    "print(\"DST summary:\", summary_dst)\n",
    "plot_map_vs_barw(W_fix_dst, wbar_fix_dst, title=\"DST prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stored arrays for your model:\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_Horseshoe_PS.npz\")\n",
    "P_DHS, S_DHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "# (Optional) shrink_PS_DHS = dat[\"shrink_PS\"].astype(np.float64)\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(\n",
    "    tanh_fit, model='Dirichlet Horseshoe tanh'\n",
    ")\n",
    "\n",
    "for d in (0, 17, 123, 777, 1023):\n",
    "    print(f\"\\n=== Draw {d} ===\")\n",
    "    _ = debug_linearization_once(\n",
    "        X, y,\n",
    "        W1[d], b1[d], W2[d], b2[d],\n",
    "        sigma[d], tau_v[d],\n",
    "        P_DHS[d], S_DHS[d],\n",
    "        include_b1_in_Sigma=True, include_b2_in_Sigma=True,\n",
    "        activation=\"tanh\",\n",
    "        vec_order=\"unit-major\",     # flip to 'feature-major' if Step1 errors are large\n",
    "        dense_S_check=False\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
