{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_corr = f\"datasets/friedman_correlated/many\"\n",
    "results_dir_tanh_corr = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "#model_names_tanh_nodewise_corr = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\"]#, \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]\n",
    "model_names_tanh_nodewise_corr = [\"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\", \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]\n",
    "\n",
    "tanh_fits_nodewise_corr = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir_corr) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "\n",
    "    tanh_fit_nodewise_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh_corr,\n",
    "        models=model_names_tanh_nodewise_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    tanh_fits_nodewise_corr[base_config_name] = tanh_fit_nodewise_corr  # use clean key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_corr = f\"datasets/friedman_correlated\"\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir_corr) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit_nodewise_corr = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh_corr,\n",
    "        models=model_names_tanh_nodewise_corr,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    tanh_fits_nodewise_corr[base_config_name] = tanh_fit_nodewise_corr  # use clean key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman/many\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "#model_names_tanh_nodewise = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\"]#, \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]\n",
    "model_names_tanh_nodewise = [\"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\", \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]\n",
    "\n",
    "tanh_fits_nodewise = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "\n",
    "    tanh_fit_nodewise = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_nodewise,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    tanh_fits_nodewise[base_config_name] = tanh_fit_nodewise  # use clean key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman\"\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    \n",
    "    tanh_fit_nodewise = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh_nodewise,\n",
    "        include_prior=False,\n",
    "    )\n",
    "\n",
    "    tanh_fits_nodewise[base_config_name] = tanh_fit_nodewise  # use clean key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE and CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from utils.generate_data import sample_gaussian_copula_uniform\n",
    "\n",
    "def generate_Friedman_data_v2(N=100, D=10, sigma=1.0, test_size=0.2, seed=42, standardize_y=True, return_scale=True):\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.uniform(0, 1, size=(N, D))\n",
    "    x0, x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5.0 * x4\n",
    "    )\n",
    "    y = y_clean + np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    if not standardize_y:\n",
    "        return (X_train, X_test, y_train, y_test) if not return_scale else (X_train, X_test, y_train, y_test, 0.0, 1.0)\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() if y_train.std() > 0 else 1.0\n",
    "\n",
    "    y_train_s = (y_train - y_mean) / y_std\n",
    "    y_test_s = (y_test - y_mean) / y_std\n",
    "\n",
    "    if return_scale:\n",
    "        return X_train, X_test, y_train_s, y_test_s, y_mean, y_std\n",
    "    return X_train, X_test, y_train_s, y_test_s\n",
    "\n",
    "def generate_correlated_Friedman_data_v2(N=100, D=10, sigma=1.0, test_size=0.2, seed=42, standardize_y=True, return_scale=True):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data for Bayesian neural network experiments.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples.\n",
    "        D (int): Number of features.\n",
    "        sigma (float): Noise level.\n",
    "        test_size (float): Proportion for test split.\n",
    "        seed (int): Random seed.\n",
    "        standardize_y (bool): Whether to standardize the response variable.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test, y_mean, y_std) if standardize_y,\n",
    "               else (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    d = 10\n",
    "    S_custom = np.eye(d)\n",
    "    # Block 1 (vars 0..4): high Spearman, 0.7\n",
    "    for i in range(0, 3):\n",
    "        for j in range(i+1, 3):\n",
    "            S_custom[i, j] = S_custom[j, i] = 0.8\n",
    "    # Block 2 (vars 5..9): moderate Spearman, 0.4\n",
    "    for i in range(5, 10):\n",
    "        for j in range(i+1, 10):\n",
    "            S_custom[i, j] = S_custom[j, i] = -0.5\n",
    "    # Cross-block weaker, 0.15\n",
    "    for i in range(0, 5):\n",
    "        for j in range(5, 10):\n",
    "            S_custom[i, j] = S_custom[j, i] = 0.15\n",
    "    # A couple of bespoke pairs:\n",
    "    S_custom[0, 9] = S_custom[9, 0] = 0.4\n",
    "    S_custom[2, 7] = S_custom[7, 2] = 0.9  # very strong (will be projected if infeasible)\n",
    "    S_custom[3, 4] = S_custom[4, 3] = -0.9  # very strong (will be projected if infeasible)\n",
    "    S_custom[1, 6] = S_custom[6, 1] = -0.9  # very strong (will be projected if infeasible)\n",
    "\n",
    "    U, _ = sample_gaussian_copula_uniform(n=10000, S=S_custom, random_state=123)\n",
    "    #X = np.random.uniform(0, 1, size=(N, D))\n",
    "    if N != U.shape[0]:\n",
    "        idx = np.random.choice(U.shape[0], size=N, replace=False)\n",
    "        X = U[idx, :]\n",
    "    else:\n",
    "        X = U\n",
    "\n",
    "    x0, x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5.0 * x4\n",
    "    )\n",
    "\n",
    "    y = y_clean + np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    if not standardize_y:\n",
    "        return (X_train, X_test, y_train, y_test) if not return_scale else (X_train, X_test, y_train, y_test, 0.0, 1.0)\n",
    "\n",
    "    y_mean = y_train.mean()\n",
    "    y_std = y_train.std() if y_train.std() > 0 else 1.0\n",
    "\n",
    "    y_train_s = (y_train - y_mean) / y_std\n",
    "    y_test_s = (y_test - y_mean) / y_std\n",
    "\n",
    "    if return_scale:\n",
    "        return X_train, X_test, y_train_s, y_test_s, y_mean, y_std\n",
    "    return X_train, X_test, y_train_s, y_test_s\n",
    "\n",
    "def make_large_eval_set(\n",
    "    generator_fn,\n",
    "    N_train,\n",
    "    D,\n",
    "    sigma,\n",
    "    seed,\n",
    "    n_eval=5000,\n",
    "    standardize_y=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns X_eval, y_eval (standardized if standardize_y=True), plus y_mean,y_std\n",
    "    defined from the training split.\n",
    "    \"\"\"\n",
    "    N_total = N_train + n_eval\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te, y_mean, y_std = generator_fn(\n",
    "        N=N_total, D=D, sigma=sigma, test_size=n_eval / N_total, seed=seed,\n",
    "        standardize_y=standardize_y, return_scale=True\n",
    "    )\n",
    "    # Now X_te has approx n_eval points (exact given test_size construction).\n",
    "    return X_te, np.asarray(y_te).squeeze(), y_mean, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(\n",
    "    seeds, all_seeds, models, all_fits, get_N_sigma, forward_pass,\n",
    "    folder,\n",
    "    n_eval=5000,\n",
    "    D=10,\n",
    "    standardize_y=True,\n",
    "    # pass the correct generator functions\n",
    "    gen_uncorr=None,\n",
    "    gen_corr=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate on a large generated test set instead of the stored tiny X_test/y_test.\n",
    "    Assumes model was trained on standardized y if standardize_y=True.\n",
    "    \"\"\"\n",
    "    assert gen_corr is not None #and gen_uncorr is not None, \"Pass both generator functions.\"\n",
    "\n",
    "    posterior_means = []\n",
    "\n",
    "    # choose generator based on folder name\n",
    "    def choose_gen(folder):\n",
    "        return gen_corr if \"friedman_correlated\" in folder else gen_uncorr\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed, all_seeds)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "\n",
    "        # Build large eval set consistent with training split standardization\n",
    "        gen_fn = choose_gen(folder)\n",
    "        X_test, y_test, y_mean, y_std = make_large_eval_set(\n",
    "            generator_fn=gen_fn,\n",
    "            N_train=N,\n",
    "            D=D,\n",
    "            sigma=sigma,\n",
    "            seed=seed,\n",
    "            n_eval=n_eval,\n",
    "            standardize_y=standardize_y\n",
    "        )\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                W2 = W2_samples[i]\n",
    "                y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i]).squeeze()\n",
    "                y_hats[i] = y_hat\n",
    "                \n",
    "            posterior_mean = y_hats.mean(axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test)**2))\n",
    "            \n",
    "            out_pm = {\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'n_eval': y_test.shape[0],\n",
    "                'posterior_mean_rmse': posterior_mean_rmse,\n",
    "                'posterior_mean_rmse_orig': posterior_mean_rmse * y_std,  # back to original y scale\n",
    "            }\n",
    "            \n",
    "            posterior_means.append(out_pm)\n",
    "            \n",
    "    return pd.DataFrame(posterior_means)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_tanh\n",
    "N100_seeds_corr = [1, 2, 3, 4, 5]\n",
    "N200_seeds_corr = [6, 7, 8, 9, 10]\n",
    "N500_seeds_corr = [11, 12, 13, 14, 15]\n",
    "all_seeds_corr = [N100_seeds_corr, N200_seeds_corr, N500_seeds_corr]\n",
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "def get_N_sigma(seed, all_seeds):\n",
    "    if seed in all_seeds[0]:\n",
    "        N=100\n",
    "    elif seed in all_seeds[1]:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df_corr = compute_rmse(\n",
    "        seeds=seeds,\n",
    "        all_seeds=all_seeds_corr,\n",
    "        models=model_names_tanh_nodewise_corr,\n",
    "        all_fits=tanh_fits_nodewise_corr,\n",
    "        get_N_sigma=get_N_sigma,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        folder=\"friedman_correlated\",\n",
    "        n_eval=1000,\n",
    "        D=10,\n",
    "        standardize_y=True,\n",
    "        gen_uncorr=generate_Friedman_data_v2,\n",
    "        gen_corr=generate_correlated_Friedman_data_v2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_tanh\n",
    "N100_seeds = [1, 3, 4, 5, 6]\n",
    "N200_seeds = [2, 7, 8, 9, 10]\n",
    "N500_seeds = [11, 12, 13, 14, 15]\n",
    "all_seeds = [N100_seeds, N200_seeds, N500_seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df = compute_rmse(\n",
    "        seeds=seeds,\n",
    "        all_seeds=all_seeds,\n",
    "        models=model_names_tanh_nodewise,\n",
    "        all_fits=tanh_fits_nodewise,\n",
    "        get_N_sigma=get_N_sigma,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        folder=\"friedman\",\n",
    "        n_eval=1000,\n",
    "        D=10,\n",
    "        standardize_y=True,\n",
    "        gen_uncorr=generate_Friedman_data_v2,\n",
    "        gen_corr=generate_correlated_Friedman_data_v2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_over_seeds(df: pd.DataFrame, metric_col: str, agg_fun: str = \"mean\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate metric_col across seeds within each (N, sparsity, model, sigma).\n",
    "    Returns a dataframe with columns:\n",
    "      N, sparsity, model, sigma, n_seeds, center, lo, hi, sd, se\n",
    "    where lo/hi depend on SHOW_BAND + BAND_KIND.\n",
    "    \"\"\"\n",
    "    group_cols = [\"N\", \"model\"]\n",
    "    if \"sigma\" in df.columns:\n",
    "        group_cols = [\"sigma\"] + group_cols\n",
    "\n",
    "    g = df.groupby(group_cols)[metric_col]\n",
    "\n",
    "    out = g.agg(\n",
    "        n_seeds=\"count\",\n",
    "        mean=\"mean\",\n",
    "        median=\"median\",\n",
    "        sd=\"std\",\n",
    "    ).reset_index()\n",
    "\n",
    "    out[\"sd\"] = out[\"sd\"].fillna(0.0)\n",
    "    out[\"se\"] = np.where(out[\"n_seeds\"] > 0, out[\"sd\"] / np.sqrt(out[\"n_seeds\"]), np.nan)\n",
    "\n",
    "    if agg_fun == \"median\":\n",
    "        out[\"center\"] = out[\"median\"]\n",
    "    else:\n",
    "        out[\"center\"] = out[\"mean\"]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df_corr_agg = aggregate_over_seeds(rmse_df_corr, 'posterior_mean_rmse_orig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df_agg = aggregate_over_seeds(rmse_df, 'posterior_mean_rmse_orig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df_corr_agg.sort_values(by=[\"model\", \"N\", \"mean\"]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df_agg.sort_values(by=[\"model\", \"N\", \"mean\"]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from properscoring import crps_ensemble\n",
    "\n",
    "def random_subset_idx(n, n_sub=None, seed=1):\n",
    "    n_sub = n if n_sub is None else n_sub\n",
    "    rng = np.random.default_rng(seed)\n",
    "    return rng.choice(n, size=n_sub, replace=False)\n",
    "\n",
    "def compute_crps_friedman(\n",
    "    seeds, all_seeds, models, all_fits, get_N_sigma, forward_pass,\n",
    "    folder,\n",
    "    n_eval=5000,\n",
    "    D=10,\n",
    "    standardize_y=True,\n",
    "    gen_uncorr=None,\n",
    "    gen_corr=None,\n",
    "    n_sub=1000,                 # subset of eval points for speed; set None for all\n",
    "    subset_seed=123,\n",
    "    sigma_name=\"sigma\",\n",
    "    use_orig_scale=True,        # report CRPS on original y scale\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes CRPS for Friedman on a large generated eval set (n_eval).\n",
    "    Uses posterior predictive draws y = mu + sigma * eps for each posterior draw.\n",
    "    Returns one row per (seed, N, sigma, model) with seed-level summaries.\n",
    "    \"\"\"\n",
    "    assert gen_corr is not None\n",
    "\n",
    "    def choose_gen(folder):\n",
    "        return gen_corr if \"friedman_correlated\" in folder else gen_uncorr\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed, all_seeds)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.2f}_seed{seed}'\n",
    "\n",
    "        gen_fn = choose_gen(folder)\n",
    "        X_test, y_test, y_mean, y_std = make_large_eval_set(\n",
    "            generator_fn=gen_fn,\n",
    "            N_train=N,\n",
    "            D=D,\n",
    "            sigma=sigma,\n",
    "            seed=seed,\n",
    "            n_eval=n_eval,\n",
    "            standardize_y=standardize_y\n",
    "        )\n",
    "\n",
    "        y_test = np.asarray(y_test).reshape(-1)\n",
    "        idx = random_subset_idx(len(y_test), n_sub=n_sub, seed=subset_seed + seed)\n",
    "\n",
    "        y_sub = y_test[idx]\n",
    "        # scale factor to original units (only matters if y is standardized)\n",
    "        scale = (y_std if (standardize_y and use_orig_scale) else 1.0)\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")\n",
    "                W2_samples = fit.stan_variable(\"W_L\")\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")\n",
    "                sig_s = np.asarray(fit.stan_variable(sigma_name)).reshape(-1)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            sig_s = sig_s[:S]  # align just in case\n",
    "\n",
    "            # Compute mu draws on the LARGE eval set, then subset columns\n",
    "            mu_sub = np.zeros((S, len(idx)))  # (S, n_sub)\n",
    "            for i in range(S):\n",
    "                mu = forward_pass(\n",
    "                    X_test,\n",
    "                    W1_samples[i],\n",
    "                    b1_samples[i][0],\n",
    "                    W2_samples[i],\n",
    "                    b2_samples[i]\n",
    "                ).squeeze()\n",
    "                mu_sub[i] = mu[idx]\n",
    "\n",
    "            # Posterior predictive draws for y on the subset\n",
    "            rng = np.random.default_rng(10_000 + seed)  # reproducible per seed\n",
    "            eps = rng.standard_normal(size=mu_sub.shape)\n",
    "            y_samps_sub = mu_sub + sig_s[:, None] * eps   # (S, n_sub)\n",
    "\n",
    "            # CRPS is scale-equivariant: if you multiply y and samples by c, CRPS multiplies by c.\n",
    "            crps_pointwise = crps_ensemble(y_sub * scale, (y_samps_sub * scale).T)  # (n_sub,)\n",
    "\n",
    "            rows.append({\n",
    "                \"seed\": seed,\n",
    "                \"N\": N,\n",
    "                \"sigma\": sigma,\n",
    "                \"model\": model,\n",
    "                \"n_eval\": int(n_eval),\n",
    "                \"n_sub\": int(len(idx)),\n",
    "                \"crps_mean\": float(np.mean(crps_pointwise)),\n",
    "                \"crps_median\": float(np.median(crps_pointwise)),\n",
    "                \"crps_q25\": float(np.quantile(crps_pointwise, 0.25)),\n",
    "                \"crps_q75\": float(np.quantile(crps_pointwise, 0.75)),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian tanh\": \"Gauss\",\n",
    "    \"Regularized Horseshoe tanh\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"DHS\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"DST\",\n",
    "    \"Beta Horseshoe tanh nodewise\": \"BHS\",\n",
    "    \"Beta Student T tanh nodewise\": \"BST\"\n",
    "}\n",
    "\n",
    "def plot_friedman_crps_grouped_box(\n",
    "    df_crps,\n",
    "    model_order,\n",
    "    N_order=(100, 200, 500),\n",
    "    metric_col=\"crps_median\",   # or \"crps_mean\"\n",
    "    abbr=None,\n",
    "    sizes_labels=None,\n",
    "    figsize=(7, 2.8),\n",
    "    title=None,\n",
    "):\n",
    "    if abbr is None:\n",
    "        abbr = {m: m for m in model_order}\n",
    "    labels = [abbr.get(m, m) for m in model_order]\n",
    "\n",
    "    if sizes_labels is None:\n",
    "        sizes_labels = [f\"N={N}\" for N in N_order]\n",
    "\n",
    "    x = np.arange(len(model_order)) + 1\n",
    "    offsets = np.array([-0.22, 0.00, 0.22])[:len(N_order)]\n",
    "    width = 0.16\n",
    "    colors = [\"C0\", \"C1\", \"C2\", \"C3\"][:len(N_order)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    for j, (N, col) in enumerate(zip(N_order, colors)):\n",
    "        data_j = []\n",
    "        for m in model_order:\n",
    "            vals = df_crps.loc[(df_crps[\"model\"] == m) & (df_crps[\"N\"] == N), metric_col].dropna().values\n",
    "            data_j.append(vals)\n",
    "\n",
    "        pos_j = x + offsets[j]\n",
    "        bp = ax.boxplot(\n",
    "            data_j,\n",
    "            positions=pos_j,\n",
    "            widths=width,\n",
    "            patch_artist=True,\n",
    "            showfliers=False\n",
    "        )\n",
    "\n",
    "        for box in bp[\"boxes\"]:\n",
    "            box.set_facecolor(col)\n",
    "            box.set_alpha(0.55)\n",
    "            box.set_edgecolor(\"black\")\n",
    "        for k in [\"whiskers\", \"caps\", \"medians\"]:\n",
    "            for line in bp[k]:\n",
    "                line.set_color(\"black\")\n",
    "                line.set_linewidth(1.0)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel(metric_col.replace(\"_\", \" \").upper())\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    handles = [\n",
    "        Patch(facecolor=colors[i], edgecolor=\"black\", alpha=0.55, label=sizes_labels[i])\n",
    "        for i in range(len(N_order))\n",
    "    ]\n",
    "    ax.legend(handles=handles, title=\"Train size\", frameon=False)\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"figures_for_use_in_paper/friedman_crps_corr_beta.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crps_corr = compute_crps_friedman(\n",
    "    seeds=seeds,\n",
    "    all_seeds=all_seeds_corr,\n",
    "    models=model_names_tanh_nodewise,\n",
    "    all_fits=tanh_fits_nodewise_corr,              # same structure as in RMSE\n",
    "    get_N_sigma=get_N_sigma,\n",
    "    forward_pass=forward_pass_tanh,\n",
    "    folder=\"friedman_correlated\",                  # e.g. \"friedman_correlated\"\n",
    "    n_eval=1000,\n",
    "    D=10,\n",
    "    standardize_y=True,\n",
    "    gen_uncorr=generate_Friedman_data_v2,\n",
    "    gen_corr=generate_correlated_Friedman_data_v2,\n",
    "    n_sub=1000,                      # CRPS subset size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crps = compute_crps_friedman(\n",
    "    seeds=seeds,\n",
    "    all_seeds=all_seeds,\n",
    "    models=model_names_tanh_nodewise,\n",
    "    all_fits=tanh_fits_nodewise,              # same structure as in RMSE\n",
    "    get_N_sigma=get_N_sigma,\n",
    "    forward_pass=forward_pass_tanh,\n",
    "    folder=\"friedman\",                  # e.g. \"friedman_correlated\"\n",
    "    n_eval=1000,\n",
    "    D=10,\n",
    "    standardize_y=True,\n",
    "    gen_uncorr=generate_Friedman_data_v2,\n",
    "    gen_corr=generate_correlated_Friedman_data_v2,\n",
    "    n_sub=1000,                      # CRPS subset size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_friedman_crps_grouped_box(\n",
    "    df_crps=df_crps_corr,\n",
    "    model_order=model_names_tanh_nodewise,\n",
    "    N_order=(100, 200, 500),\n",
    "    metric_col=\"crps_median\",\n",
    "    abbr=abbr,\n",
    "    figsize=(4, 3),\n",
    "    title=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_friedman_crps_grouped_box(\n",
    "    df_crps=df_crps,\n",
    "    model_order=model_names_tanh_nodewise,\n",
    "    N_order=(100, 200, 500),\n",
    "    metric_col=\"crps_median\",\n",
    "    abbr=abbr,\n",
    "    figsize=(4, 3),\n",
    "    title=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
