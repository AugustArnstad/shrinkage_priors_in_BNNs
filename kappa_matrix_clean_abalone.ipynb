{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/abalone\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/abalone\"\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "model_names_nodewise = [\"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\", \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"]\n",
    "\n",
    "\n",
    "full_config_path = \"abalone_N3341_p8\"\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "tanh_fit_nodewise = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_nodewise,\n",
    "    include_prior=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X, X_test, y, y_test = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "# Coerce everything to plain float64 NumPy arrays\n",
    "X      = np.asarray(X, dtype=float)\n",
    "X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "# y often comes as a (n,1) DataFrame/array — flatten to (n,)\n",
    "y      = np.asarray(y, dtype=float).reshape(-1)\n",
    "y_test = np.asarray(y_test, dtype=float).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import extract_model_draws, compute_shrinkage_for_W_block, shrinkage_eigs_and_df\n",
    "from utils.sparsity import local_prune_weights\n",
    "\n",
    "def compute_shrinkage_with_pruning(\n",
    "    X,\n",
    "    W_all, b_all, v_all,          # (D,H,p), (D,H), (D,H)\n",
    "    sigma_all, tau_w_all, tau_v_all,  # (D,), (D,), (D,)\n",
    "    lambda_all,                   # (D,H,p)\n",
    "    activation=\"tanh\",\n",
    "    return_mats=True,             # set False if you only want summaries\n",
    "    include_b1_in_Sigma: bool = True,\n",
    "    include_b2_in_Sigma: bool = True,\n",
    "    sparsity = 0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop over draws and compute R=(P+S)^{-1}P per draw using your single-draw function.\n",
    "    Returns:\n",
    "      R_stack : (D, N, N) with N=H*p  (if return_mats=True, else None)\n",
    "      r_eigs  : (D, N)  sorted eigenvalues in [0,1]\n",
    "      df_eff  : (D,)    effective dof = tr(I-R) = N - tr(R)\n",
    "    \"\"\"\n",
    "    D, H, p = W_all.shape\n",
    "    N = H * p\n",
    "\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    S_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    P_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    G_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    shrink_stack= np.empty((D, N, N)) if return_mats else None\n",
    "    r_eigs  = np.empty((D, N))\n",
    "    df_eff  = np.empty(D)\n",
    "\n",
    "    for d in range(D):\n",
    "        mask = local_prune_weights(W_all[d], sparsity_level=sparsity)\n",
    "        W_pruned = mask[0]*W_all[d]\n",
    "        R, P, S, Sigma_y, _, _ = compute_shrinkage_for_W_block(\n",
    "            X=X,\n",
    "            W0=W_pruned,\n",
    "            b0=b_all[d],\n",
    "            v0=v_all[d],\n",
    "            noise=float(sigma_all[d]),\n",
    "            tau_w=float(tau_w_all[d]),\n",
    "            tau_v=float(tau_v_all[d]),\n",
    "            lambda_tilde=lambda_all[d],\n",
    "            activation=activation,\n",
    "            include_b1_in_Sigma=include_b1_in_Sigma,\n",
    "            include_b2_in_Sigma=include_b2_in_Sigma,\n",
    "        )\n",
    "        p = np.diag(P)                       \n",
    "        P_inv_sqrt = np.diag(1.0/np.sqrt(p))         \n",
    "        G = P_inv_sqrt @ S @ P_inv_sqrt \n",
    "        I = np.identity(N)\n",
    "        shrink_mat = np.linalg.inv(I + G)@G\n",
    "\n",
    "        if return_mats:\n",
    "            R_stack[d] = R\n",
    "            S_stack[d] = S\n",
    "            P_stack[d] = P\n",
    "            G_stack[d] = G\n",
    "            shrink_stack[d] = shrink_mat\n",
    "        \n",
    "        r, df = shrinkage_eigs_and_df(P, S)\n",
    "        r_eigs[d] = np.sort(r)\n",
    "        df_eff[d] = df\n",
    "\n",
    "    return R_stack, S_stack, P_stack, G_stack, shrink_stack, r_eigs, df_eff\n",
    "\n",
    "\n",
    "# W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "#     tanh_fit_nodewise, model='Dirichlet Horseshoe tanh nodewise'\n",
    "# )\n",
    "# R_DHS, S_DHS, P_DHS, G_DHS, shrink_DHS, eigs_DHS, df_eff_DHS = compute_shrinkage_with_pruning(\n",
    "#     X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "#     activation=\"tanh\",\n",
    "#     include_b1_in_Sigma=True,\n",
    "#     include_b2_in_Sigma=True,\n",
    "# )\n",
    "# print(\"done with DHS\")\n",
    "\n",
    "# W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "#     tanh_fit_nodewise, model='Dirichlet Student T tanh nodewise'\n",
    "# )\n",
    "# R_DST, S_DST, P_DST, G_DST, shrink_DST, eigs_DST, df_eff_DST = compute_shrinkage_with_pruning(\n",
    "#     X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "#     activation=\"tanh\",\n",
    "#     include_b1_in_Sigma=True,\n",
    "#     include_b2_in_Sigma=True,\n",
    "# )\n",
    "# print(\"done with DST\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit_nodewise, model='Beta Horseshoe tanh nodewise'\n",
    ")\n",
    "R_BHS, S_BHS, P_BHS, G_BHS, shrink_BHS, eigs_BHS, df_eff_BHS = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with BHS\")\n",
    "\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit_nodewise, model='Dirichlet Student T tanh nodewise'\n",
    ")\n",
    "R_BST, S_BST, P_BST, G_BST, shrink_BST, eigs_BST, df_eff_BST = compute_shrinkage_with_pruning(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with BST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs(\"Abalone_matrices\", exist_ok=True)\n",
    "\n",
    "def save_PS(model_name, P, S):\n",
    "    fn = os.path.join(\"Abalone_matrices\", f\"{model_name.replace(' ', '_')}_PS.npz\")\n",
    "    np.savez_compressed(fn, P=np.asarray(P, dtype=np.float32), S=np.asarray(S, dtype=np.float32))\n",
    "    print(f\"Saved {fn}  with P,S shapes={P.shape},{S.shape}  dtype=float32\")\n",
    "\n",
    "# Call once per model (arrays are shape (4000, 160, 160))\n",
    "# save_PS(\"Dirichlet_Horseshoe_nodewise_sparsity_90\", P_DHS,   S_DHS)\n",
    "# save_PS(\"Dirichlet_StudentT_nodewise_sparsity_90\",  P_DST,   S_DST)\n",
    "save_PS(\"Beta_Horseshoe_nodewise0\", P_BHS,   S_BHS)\n",
    "save_PS(\"Beta_StudentT_nodewise\",  P_BST,   S_BST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import cholesky, solve\n",
    "from utils.kappa_matrix import shrinkage_matrix_stable\n",
    "\n",
    "def build_operators_from_PS(P, S):\n",
    "    \"\"\"\n",
    "    P, S: arrays of shape (S, d, d), SPD per sample.\n",
    "    Returns:\n",
    "      G        : P^{-1/2} S P^{-1/2}\n",
    "      shrink_PS: (P+S)^{-1} S\n",
    "      shrink_G : (I+G)^{-1} G\n",
    "    \"\"\"\n",
    "    S_, d, _ = P.shape\n",
    "    G         = np.empty_like(P, dtype=np.float64)\n",
    "    shrink_PS = np.empty_like(P, dtype=np.float64)\n",
    "    shrink_G  = np.empty_like(P, dtype=np.float64)\n",
    "\n",
    "    I = np.eye(d)\n",
    "\n",
    "    for s in range(S_):\n",
    "        Ps = P[s]; Ss = S[s]\n",
    "\n",
    "        C = cholesky(Ps)            \n",
    "        temp = solve(C.T, Ss)\n",
    "        Gs   = solve(C, temp.T)\n",
    "        G[s] = Gs\n",
    "\n",
    "        Rs = shrinkage_matrix_stable(Ps, Ss)\n",
    "\n",
    "        shrink_PS[s] = np.eye(Ps.shape[0]) - Rs\n",
    "        \n",
    "        B = I + Gs\n",
    "        LB = cholesky(B)\n",
    "        YB = solve(LB, Gs)\n",
    "        XB = solve(LB.T, YB)\n",
    "        shrink_G[s] = XB\n",
    "\n",
    "    return G, shrink_PS, shrink_G\n",
    "\n",
    "\n",
    "# Example usage after reloading a saved NPZ:\n",
    "dat = np.load(\"Abalone_matrices/Gaussian_PS.npz\")\n",
    "P_gauss, S_gauss = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_gauss, shrink_PS_gauss, shrink_G_gauss = build_operators_from_PS(P_gauss, S_gauss)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Regularized_Horseshoe_PS.npz\")\n",
    "P_RHS, S_RHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_RHS, shrink_PS_RHS, shrink_G_RHS = build_operators_from_PS(P_RHS, S_RHS)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_Horseshoe_PS.npz\")\n",
    "P_DHS, S_DHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DHS, shrink_PS_DHS, shrink_G_DHS = build_operators_from_PS(P_DHS, S_DHS)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_StudentT_PS.npz\")\n",
    "P_DST, S_DST = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DST, shrink_PS_DST, shrink_G_DST = build_operators_from_PS(P_DST, S_DST)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Beta_Horseshoe_PS.npz\")\n",
    "P_BHS, S_BHS = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_BHS, shrink_PS_BHS, shrink_G_BHS = build_operators_from_PS(P_BHS, S_BHS)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Beta_StudentT_PS.npz\")\n",
    "P_BST, S_BST = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_BST, shrink_PS_BST, shrink_G_BST = build_operators_from_PS(P_BST, S_BST)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_Horseshoe_nodewise_PS.npz\")\n",
    "P_DHS_nodewise, S_DHS_nodewise = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DHS_nodewise, shrink_PS_DHS_nodewise, shrink_G_DHS_nodewise = build_operators_from_PS(P_DHS_nodewise, S_DHS_nodewise)\n",
    "\n",
    "dat = np.load(\"Abalone_matrices/Dirichlet_StudentT_nodewise_PS.npz\")\n",
    "P_DST_nodewise, S_DST_nodewise = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "G_DST_nodewise, shrink_PS_DST_nodewise, shrink_G_DST_nodewise = build_operators_from_PS(P_DST_nodewise, S_DST_nodewise)\n",
    "\n",
    "# dat = np.load(\"Abalone_matrices/Beta_Horseshoe_nodewise_PS.npz\")\n",
    "# P_BHS_nodewise, S_BHS_nodewise = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "# G_BHS_nodewise, shrink_PS_BHS_nodewise, shrink_G_BHS_nodewise = build_operators_from_PS(P_BHS_nodewise, S_BHS_nodewise)\n",
    "\n",
    "# dat = np.load(\"Abalone_matrices/Beta_StudentT_nodewise_PS.npz\")\n",
    "# P_BST_nodewise, S_BST_nodewise = dat[\"P\"].astype(np.float64), dat[\"S\"].astype(np.float64)\n",
    "# G_BST_nodewise, shrink_PS_BST_nodewise, shrink_G_BST_nodewise = build_operators_from_PS(P_BST_nodewise, S_BST_nodewise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Traces as distributions (df_eff = tr(R) vs total shrinkage = tr(I-R)) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If you also want “total shrinkage”, use your SP_inv_S_* stacks (I - R):\n",
    "tr_SPinvS_gauss = np.trace(shrink_PS_gauss, axis1=1, axis2=2)\n",
    "tr_SPinvS_RHS   = np.trace(shrink_PS_RHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DHS   = np.trace(shrink_PS_DHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DST   = np.trace(shrink_PS_DST,   axis1=1, axis2=2)\n",
    "tr_SPinvS_BHS   = np.trace(shrink_PS_BHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_BST   = np.trace(shrink_PS_BST,   axis1=1, axis2=2)\n",
    "\n",
    "tr_SPinvS_DHS_nodewise   = np.trace(shrink_PS_DHS_nodewise,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DST_nodewise   = np.trace(shrink_PS_DST_nodewise,   axis1=1, axis2=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "# plt.hist(tr_SPinvS_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "# plt.hist(tr_SPinvS_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_SPinvS_DHS,   bins=bins, alpha=0.5, label=\"DHS\", color=\"C2\")\n",
    "plt.hist(tr_SPinvS_DHS_nodewise,   bins=bins, alpha=0.5, label=\"DHS-node\", color=\"C2\")\n",
    "plt.hist(tr_SPinvS_DST,   bins=bins, alpha=0.5, label=\"DST\", color=\"C3\")\n",
    "plt.hist(tr_SPinvS_DST_nodewise,   bins=bins, alpha=0.5, label=\"DST-node\", color=\"C3\")\n",
    "# plt.hist(tr_SPinvS_BHS,   bins=bins, alpha=0.5, label=\"BHS\")\n",
    "# plt.hist(tr_SPinvS_BST,   bins=bins, alpha=0.5, label=\"BST\")\n",
    "plt.xlabel(r\"$tr((P+S)^{-1}S)$\", fontsize=15)\n",
    "plt.ylabel(\"Frequency\", fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"figures_for_use_in_paper/Abalone_m_eff.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "# plt.hist(tr_SPinvS_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "# plt.hist(tr_SPinvS_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_SPinvS_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_SPinvS_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.hist(tr_SPinvS_BHS,   bins=bins, alpha=0.5, label=\"BHS\")\n",
    "plt.hist(tr_SPinvS_BST,   bins=bins, alpha=0.5, label=\"BST\")\n",
    "plt.xlabel(r\"$tr((P+S)^{-1}S)$\", fontsize=15)\n",
    "plt.ylabel(\"Frequency\", fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"figures_for_use_in_paper/Abalone_m_eff.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
