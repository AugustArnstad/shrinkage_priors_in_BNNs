{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/friedman_correlated\"\n",
    "#results_dir_relu = \"results/regression/single_layer/relu/friedman\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/friedman_correlated\"\n",
    "\n",
    "#model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\"]#, \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "#relu_fits = {}\n",
    "tanh_fits = {}\n",
    "\n",
    "files = sorted(f for f in os.listdir(data_dir) if f.endswith(\".npz\"))\n",
    "for fname in files:\n",
    "    base_config_name = fname.replace(\".npz\", \"\")  # e.g., \"GAM_N100_p8_sigma1.00_seed1\"\n",
    "    full_config_path = f\"{base_config_name}\"  # → \"type_1/GAM_N100_p8_sigma1.00_seed1\"\n",
    "    # relu_fit = get_model_fits(\n",
    "    #     config=full_config_path,\n",
    "    #     results_dir=results_dir_relu,\n",
    "    #     models=model_names_relu,\n",
    "    #     include_prior=False,\n",
    "    # )\n",
    "    \n",
    "    tanh_fit = get_model_fits(\n",
    "        config=full_config_path,\n",
    "        results_dir=results_dir_tanh,\n",
    "        models=model_names_tanh,\n",
    "        include_prior=False,\n",
    "    )\n",
    "    \n",
    "\n",
    "    #relu_fits[base_config_name] = relu_fit  # use clean key\n",
    "    tanh_fits[base_config_name] = tanh_fit  # use clean key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from properscoring import crps_ensemble\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path = f\"datasets/friedman_correlated/Friedman_N200_p10_sigma1.0_seed6.npz\"\n",
    "data = np.load(path)\n",
    "X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "rows = []\n",
    "for model_name, model_entry in tanh_fits['Friedman_N200_p10_sigma1.0_seed6'].items():\n",
    "    post = model_entry[\"posterior\"]\n",
    "\n",
    "    # (S, n_test)\n",
    "    y_samps = post.stan_variable(\"output_test\").squeeze(-1)\n",
    "\n",
    "    # Optional: limit to first S draws if desired\n",
    "    # S = min(4000, y_samps.shape[0])\n",
    "    # y_samps = y_samps[:S]\n",
    "\n",
    "    # Posterior-mean predictions and RMSE\n",
    "    y_mean = y_samps.mean(axis=0)                                   # (n_test,)\n",
    "    rmse_post_mean = float(np.sqrt(mean_squared_error(y_test, y_mean)))\n",
    "\n",
    "    # Per-draw RMSEs and their mean\n",
    "    per_draw_rmse = np.sqrt(((y_samps - y_test[None, :])**2).mean(axis=1))  # (S,)\n",
    "    rmse_draw_mean = float(per_draw_rmse.mean())\n",
    "\n",
    "    # CRPS across the ensemble (expects shape (n_test, S))\n",
    "    crps = float(np.mean(crps_ensemble(y_test, y_samps.T)))\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE_posterior_mean\": rmse_post_mean,\n",
    "        \"RMSE_mean_over_draws\": rmse_draw_mean,\n",
    "        \"CRPS\": crps,\n",
    "        \"n_draws\": y_samps.shape[0]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values(\"RMSE_posterior_mean\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparse_rmse_results(seeds, models, all_fits, get_N_sigma, forward_pass,\n",
    "                         sparsity=0.0, prune_fn=None):\n",
    "    results = []\n",
    "    posterior_means = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        N, sigma = get_N_sigma(seed)\n",
    "        dataset_key = f'Friedman_N{N}_p10_sigma{sigma:.1f}_seed{seed}'\n",
    "        path = f\"datasets/friedman_correlated/{dataset_key}.npz\"\n",
    "        try:\n",
    "            data = np.load(path)\n",
    "            X_test, y_test = data[\"X_test\"], data[\"y_test\"]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"[SKIP] File not found: {path}\")\n",
    "            continue\n",
    "\n",
    "        for model in models:\n",
    "            try:\n",
    "                fit = all_fits[dataset_key][model]['posterior']\n",
    "                W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "                W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "                b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "                b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "            except KeyError:\n",
    "                print(f\"[SKIP] Model or posterior not found: {dataset_key} -> {model}\")\n",
    "                continue\n",
    "\n",
    "            S = W1_samples.shape[0]\n",
    "            rmses = np.zeros(S)\n",
    "            #print(y_test.shape)\n",
    "            y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "            for i in range(S):\n",
    "                W1 = W1_samples[i]\n",
    "                W2 = W2_samples[i]\n",
    "\n",
    "                # Apply pruning mask if requested\n",
    "                if prune_fn is not None and sparsity > 0.0:\n",
    "                    masks = prune_fn([W1, W2], sparsity)\n",
    "                    W1 = W1 * masks[0]\n",
    "                    #W2 = W2 * masks[1]\n",
    "\n",
    "                y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i])\n",
    "                y_hats[i] = y_hat.squeeze()  # Store the prediction for each sample\n",
    "                rmses[i] = np.sqrt(np.mean((y_hat.squeeze() - y_test)**2))\n",
    "                \n",
    "            posterior_mean = np.mean(y_hats, axis=0)\n",
    "            posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test.squeeze())**2))\n",
    "\n",
    "            posterior_means.append({\n",
    "                'seed': seed,\n",
    "                'N': N,\n",
    "                'sigma': sigma,\n",
    "                'model': model,\n",
    "                'sparsity': sparsity,\n",
    "                'posterior_mean_rmse': posterior_mean_rmse\n",
    "            })\n",
    "\n",
    "            for i in range(S):\n",
    "                results.append({\n",
    "                    'seed': seed,\n",
    "                    'N': N,\n",
    "                    'sigma': sigma,\n",
    "                    'model': model,\n",
    "                    'sparsity': sparsity,\n",
    "                    'rmse': rmses[i]\n",
    "                })\n",
    "\n",
    "    df_rmse = pd.DataFrame(results)\n",
    "    df_posterior_rmse = pd.DataFrame(posterior_means)\n",
    "\n",
    "    return df_rmse, df_posterior_rmse\n",
    "\n",
    "\n",
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "seeds = [1, 6]#, 11]\n",
    "\n",
    "def get_N_sigma(seed):\n",
    "    if seed == 1:\n",
    "        N=100\n",
    "    elif seed == 6:\n",
    "        N=200\n",
    "    else:\n",
    "        N=500\n",
    "    sigma=1.00\n",
    "    return N, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rmse_relu, df_posterior_rmse_relu = {}, {}\n",
    "df_rmse_tanh, df_posterior_rmse_tanh = {}, {}\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    # df_rmse_relu[sparsity], df_posterior_rmse_relu[sparsity] = compute_sparse_rmse_results(\n",
    "    #     seeds, model_names_relu, relu_fits, get_N_sigma, forward_pass_relu,\n",
    "    #     sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    # )\n",
    "    \n",
    "    df_rmse_tanh[sparsity], df_posterior_rmse_tanh[sparsity] = compute_sparse_rmse_results(\n",
    "        seeds, model_names_tanh, tanh_fits, get_N_sigma, forward_pass_tanh,\n",
    "        sparsity=sparsity, prune_fn=local_prune_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df_rmse_full_relu = pd.concat(\n",
    "#     [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_relu.items()],\n",
    "#     ignore_index=True\n",
    "# )\n",
    "\n",
    "df_rmse_full_tanh = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_tanh.items()],\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_sparsity_rmse(df_dict, metric=\"rmse\", show_boxplot=False, title_prefix='RMSE vs sparsity', sample_points=2000):\n",
    "    \"\"\"\n",
    "    df_dict: dict mapping sparsity -> dataframe containing columns ['model', 'rmse']\n",
    "    show_boxplot: if True, include boxplot+points figure (figure #2)\n",
    "    title_prefix: title text for figure 1\n",
    "    sample_points: number of stripplot points to sample (to avoid huge overplotting)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stack dataframes\n",
    "    frames = []\n",
    "    for sp, df in df_dict.items():\n",
    "        df = df.copy()\n",
    "        df['sparsity'] = float(sp)\n",
    "        frames.append(df)\n",
    "\n",
    "    df_all = pd.concat(frames, axis=0, ignore_index=True)\n",
    "\n",
    "    # Clean\n",
    "    df_all['sparsity'] = df_all['sparsity'].astype(float)\n",
    "    df_all['rmse'] = pd.to_numeric(df_all[metric], errors='coerce')\n",
    "    df_all = df_all.dropna(subset=['rmse', 'sparsity', 'model'])\n",
    "\n",
    "    # Sort models consistently\n",
    "    models_order = sorted(df_all['model'].unique())\n",
    "    df_all['model'] = pd.Categorical(df_all['model'], categories=models_order, ordered=True)\n",
    "\n",
    "    # Summary statistics\n",
    "    summary = (\n",
    "        df_all.groupby(['model', 'sparsity'])\n",
    "        .agg(n=('rmse', 'size'),\n",
    "             mean_rmse=('rmse', 'mean'),\n",
    "             std_rmse=('rmse', 'std'))\n",
    "        .reset_index()\n",
    "    )\n",
    "    summary['sem'] = summary['std_rmse'] / np.sqrt(summary['n'])\n",
    "    summary['ci95'] = 1.96 * summary['sem']\n",
    "    summary['ymin'] = summary['mean_rmse'] - summary['ci95']\n",
    "    summary['ymax'] = summary['mean_rmse'] + summary['ci95']\n",
    "\n",
    "    # Plot styling\n",
    "    sns.set_context('talk')\n",
    "    sns.set_style('whitegrid')\n",
    "\n",
    "    # ---- Figure 1: mean + CI ----\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(\n",
    "        data=summary.sort_values(['model', 'sparsity']),\n",
    "        x='sparsity', y='mean_rmse', hue='model',\n",
    "        linewidth=2.5, marker='o', markersize=7\n",
    "    )\n",
    "    # error bars\n",
    "    for _, row in summary.iterrows():\n",
    "        plt.plot([row['sparsity'], row['sparsity']], [row['ymin'], row['ymax']],\n",
    "                 color=sns.color_palette()[models_order.index(row['model'])], lw=2)\n",
    "\n",
    "    plt.title(f'{title_prefix} (mean ± 95% CI)')\n",
    "    plt.xlabel('Sparsity')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend(title='Model', loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- Optionally: Figure 2 ----\n",
    "    if show_boxplot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(\n",
    "            data=df_all,\n",
    "            x='sparsity', y='rmse', hue='model',\n",
    "            showfliers=False,\n",
    "            linewidth=1.2\n",
    "        )\n",
    "\n",
    "        # Only sample points to avoid heavy overplot\n",
    "        df_sample = df_all.sample(min(len(df_all), sample_points), random_state=42)\n",
    "        sns.stripplot(\n",
    "            data=df_sample,\n",
    "            x='sparsity', y='rmse', hue='model',\n",
    "            dodge=True, size=2, alpha=0.25, palette='dark'\n",
    "        )\n",
    "\n",
    "        # Remove duplicate legend caused by stripplot\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        plt.legend(handles[:len(models_order)], labels[:len(models_order)], title='Model', loc='best')\n",
    "\n",
    "        plt.title('RMSE distribution per sparsity and model')\n",
    "        plt.xlabel('Sparsity')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return df_all, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all, summary = plot_sparsity_rmse(df_rmse_tanh, show_boxplot=False)\n",
    "df_all, summary = plot_sparsity_rmse(df_posterior_rmse_tanh, metric=\"posterior_mean_rmse\", show_boxplot=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian\": \"Gaussian\",\n",
    "    \"Regularized Horseshoe\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DS-T\"\n",
    "}\n",
    "\n",
    "# Clean model names\n",
    "df_rmse_full_relu = df_rmse_full_relu.copy()\n",
    "df_rmse_full_relu[\"model\"] = df_rmse_full_relu[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "df_rmse_full_tanh = df_rmse_full_tanh.copy()\n",
    "df_rmse_full_tanh[\"model\"] = df_rmse_full_tanh[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# Define consistent color palette\n",
    "custom_palette = {\n",
    "    \"Gaussian\": \"C0\",\n",
    "    \"Regularized Horseshoe\": \"C1\",\n",
    "    \"Dirichlet Horseshoe\": \"C2\",\n",
    "    \"Dirichlet Student T\": \"C3\",\n",
    "}\n",
    "\n",
    "# Set up plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 7), sharex=True, sharey=True)\n",
    "\n",
    "activation_data = [(\"ReLU\", df_rmse_full_relu), (\"tanh\", df_rmse_full_tanh)]\n",
    "all_handles_labels = []\n",
    "\n",
    "# Plot\n",
    "for idx, (name, df) in enumerate(activation_data):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x='sparsity', y='rmse',\n",
    "        hue='model', style='N', marker='o', errorbar=None, ax=ax,\n",
    "        palette=custom_palette\n",
    "    )\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    all_handles_labels.extend(zip(handles, labels))\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "    ax.set_title(f\"{name}\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_xlabel(\"Sparsity Level\")\n",
    "    #ax.set_ylim((0.15, 3))\n",
    "    ax.grid(True)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "\n",
    "# Filter only the N-related entries (exclude model names)\n",
    "n_handles = []\n",
    "n_labels = []\n",
    "for h, l in zip(handles, labels):\n",
    "    if l.startswith(\"N=\") or l.isdigit() or l.strip().lower().startswith(\"n\"):  # adjust if your N labels differ\n",
    "        n_handles.append(h)\n",
    "        n_labels.append(l)\n",
    "\n",
    "# Add legend for N (line styles) to the LEFT subplot\n",
    "if n_handles:\n",
    "    axes[0].legend(\n",
    "        n_handles,\n",
    "        n_labels,\n",
    "        title=None,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.02, 0.98),\n",
    "        frameon=True,\n",
    "        fontsize='medium'\n",
    "    )\n",
    "\n",
    "# Clean legend\n",
    "legend_dict = OrderedDict()\n",
    "for handle, label in all_handles_labels:\n",
    "    if label not in {\"model\", \"N\"} and label not in legend_dict:\n",
    "        legend_dict[label] = handle\n",
    "\n",
    "desired_order = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "filtered = [(legend_dict[label], abbr[label]) for label in desired_order if label in legend_dict]\n",
    "\n",
    "if filtered:\n",
    "    filtered_handles, filtered_labels = zip(*filtered)\n",
    "    fig.legend(\n",
    "        filtered_handles,\n",
    "        filtered_labels,\n",
    "        title=None,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.75, 0.9),\n",
    "        ncol=1,\n",
    "        fontsize='large'\n",
    "    )\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_rmse_full_relu = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_posterior_rmse_relu.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_rmse_full_tanh = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_posterior_rmse_tanh.items()],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "\n",
    "abbr = {\n",
    "    \"Gaussian\": \"Gaussian\",\n",
    "    \"Regularized Horseshoe\": \"RHS\",\n",
    "    \"Dirichlet Horseshoe\": \"DHS\",\n",
    "    \"Dirichlet Student T\": \"DS-T\"\n",
    "}\n",
    "\n",
    "# Clean model names\n",
    "df_rmse_full_relu = df_rmse_full_relu.copy()\n",
    "df_rmse_full_relu[\"model\"] = df_rmse_full_relu[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "df_rmse_full_tanh = df_rmse_full_tanh.copy()\n",
    "df_rmse_full_tanh[\"model\"] = df_rmse_full_tanh[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# Define consistent color palette\n",
    "custom_palette = {\n",
    "    \"Gaussian\": \"C0\",\n",
    "    \"Regularized Horseshoe\": \"C1\",\n",
    "    \"Dirichlet Horseshoe\": \"C2\",\n",
    "    \"Dirichlet Student T\": \"C3\",\n",
    "}\n",
    "\n",
    "# Set up plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 7), sharex=True, sharey=True)\n",
    "\n",
    "activation_data = [(\"ReLU\", df_rmse_full_relu), (\"tanh\", df_rmse_full_tanh)]\n",
    "all_handles_labels = []\n",
    "\n",
    "# Plot\n",
    "for idx, (name, df) in enumerate(activation_data):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x='sparsity', y='posterior_mean_rmse',\n",
    "        hue='model', style='N', marker='o', errorbar=None, ax=ax,\n",
    "        palette=custom_palette\n",
    "    )\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    all_handles_labels.extend(zip(handles, labels))\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "    ax.set_title(f\"{name}\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_xlabel(\"Sparsity Level\")\n",
    "    #ax.set_ylim((0.15, 3))\n",
    "    ax.grid(True)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "\n",
    "# Filter only the N-related entries (exclude model names)\n",
    "n_handles = []\n",
    "n_labels = []\n",
    "for h, l in zip(handles, labels):\n",
    "    if l.startswith(\"N=\") or l.isdigit() or l.strip().lower().startswith(\"n\"):  # adjust if your N labels differ\n",
    "        n_handles.append(h)\n",
    "        n_labels.append(l)\n",
    "\n",
    "# Add legend for N (line styles) to the LEFT subplot\n",
    "if n_handles:\n",
    "    axes[0].legend(\n",
    "        n_handles,\n",
    "        n_labels,\n",
    "        title=None,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.02, 0.98),\n",
    "        frameon=True,\n",
    "        fontsize='medium'\n",
    "    )\n",
    "\n",
    "# Clean legend\n",
    "legend_dict = OrderedDict()\n",
    "for handle, label in all_handles_labels:\n",
    "    if label not in {\"model\", \"N\"} and label not in legend_dict:\n",
    "        legend_dict[label] = handle\n",
    "\n",
    "desired_order = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "filtered = [(legend_dict[label], abbr[label]) for label in desired_order if label in legend_dict]\n",
    "\n",
    "if filtered:\n",
    "    filtered_handles, filtered_labels = zip(*filtered)\n",
    "    fig.legend(\n",
    "        filtered_handles,\n",
    "        filtered_labels,\n",
    "        title=None,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.75, 0.9),\n",
    "        ncol=1,\n",
    "        fontsize='large'\n",
    "    )\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize_networks import compute_activation_frequency, extract_all_pruned_means, plot_all_networks_subplots_activations\n",
    "path = \"datasets/friedman/many/Friedman_N100_p10_sigma1.00_seed6.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_activation_colors = {\n",
    "    model_name: compute_activation_frequency(fit, x_train)\n",
    "    for model_name, fit in relu_fits['Friedman_N100_p10_sigma1.00_seed6'].items()\n",
    "}\n",
    "\n",
    "# Flatten and find the global maximum\n",
    "all_freqs = np.concatenate(list(node_activation_colors.values()))\n",
    "global_max = all_freqs.max()\n",
    "print(global_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 10\n",
    "H = 16\n",
    "L = 1\n",
    "out_nodes = 1\n",
    "layer_sizes = [P] + [H]*L + [out_nodes]\n",
    "\n",
    "layer_structure = {\n",
    "    'input_to_hidden': {'name': 'W_1', 'shape': (P, H)},\n",
    "    'hidden_to_output': {'name': 'W_L', 'shape': (H, out_nodes)}\n",
    "}\n",
    "\n",
    "sparsity_level = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model_means = extract_all_pruned_means(relu_fits['Friedman_N100_p10_sigma1.00_seed6'], layer_structure, sparsity_level)\n",
    "\n",
    "p1, widths_1 = plot_all_networks_subplots_activations(pruned_model_means, layer_sizes, node_activation_colors, activation_color_max=global_max, signed_colors=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move Networks.ipynb into this file to show the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import compute_sparse_rmse_results, prune_nodes_by_output_weights\n",
    "\n",
    "df_rmse_node_relu, df_posterior_rmse_node_relu = {}, {}\n",
    "df_rmse_node_tanh, df_posterior_rmse_node_tanh = {}, {}\n",
    "\n",
    "def nodes_to_sparsity(nodes_to_prune_list, total_nodes):\n",
    "    \"\"\"\n",
    "    Convert a list of node counts to prune into sparsity levels.\n",
    "\n",
    "    Args:\n",
    "        nodes_to_prune_list: list of integers (number of nodes to prune).\n",
    "        total_nodes: total number of nodes in the layer.\n",
    "\n",
    "    Returns:\n",
    "        List of sparsity levels between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "    sparsity_levels = [round(n_prune / total_nodes, 4) for n_prune in nodes_to_prune_list]\n",
    "    return sparsity_levels\n",
    "\n",
    "# Suppose you have 16 nodes in the hidden layer\n",
    "total_nodes = 16\n",
    "nodes_to_prune = [0, 1, 2, 4, 6, 8, 10, 12, 14]\n",
    "\n",
    "node_sparsity = nodes_to_sparsity(nodes_to_prune, total_nodes)\n",
    "print(node_sparsity)  \n",
    "# Output: [0.0, 0.0625, 0.125, 0.25, 0.5, 0.75, 0.875]\n",
    "\n",
    "for sparsity in node_sparsity:\n",
    "    df_rmse_node_relu[sparsity], df_posterior_rmse_node_relu[sparsity] = compute_sparse_rmse_results(\n",
    "    seeds, model_names_relu, relu_fits, get_N_sigma, forward_pass_relu,\n",
    "    sparsity=sparsity, prune_fn=prune_nodes_by_output_weights\n",
    ")\n",
    "    \n",
    "    df_rmse_node_tanh[sparsity], df_posterior_rmse_node_tanh[sparsity] = compute_sparse_rmse_results(\n",
    "    seeds, model_names_tanh, tanh_fits, get_N_sigma, forward_pass_tanh,\n",
    "    sparsity=sparsity, prune_fn=prune_nodes_by_output_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_rmse_full_node_relu = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_node_relu.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_rmse_full_node_tanh = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_rmse_node_tanh.items()],\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nodes = 16  # adjust this if needed\n",
    "\n",
    "df_rmse_full_node_relu['nodes_pruned'] = (df_rmse_full_node_relu['sparsity'] * total_nodes).astype(int)\n",
    "df_rmse_full_node_tanh['nodes_pruned'] = (df_rmse_full_node_tanh['sparsity'] * total_nodes).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Clean model names\n",
    "df_rmse_full_node_relu = df_rmse_full_node_relu.copy()\n",
    "df_rmse_full_node_relu[\"model\"] = df_rmse_full_node_relu[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "df_rmse_full_node_tanh = df_rmse_full_node_tanh.copy()\n",
    "df_rmse_full_node_tanh[\"model\"] = df_rmse_full_node_tanh[\"model\"].str.replace(\" tanh\", \"\", regex=False)\n",
    "\n",
    "# Define consistent color palette\n",
    "custom_palette = {\n",
    "    \"Gaussian\": \"C0\",\n",
    "    \"Regularized Horseshoe\": \"C1\",\n",
    "    \"Dirichlet Horseshoe\": \"C2\",\n",
    "    \"Dirichlet Student T\": \"C3\",\n",
    "}\n",
    "\n",
    "# Set up plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5), sharex=True, sharey=True)\n",
    "\n",
    "activation_data = [(\"ReLU\", df_rmse_full_node_relu), (\"tanh\", df_rmse_full_node_tanh)]\n",
    "all_handles_labels = []\n",
    "\n",
    "# Plot\n",
    "for idx, (name, df) in enumerate(activation_data):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x='nodes_pruned', y='rmse',\n",
    "        hue='model', style='N', marker='o', errorbar=None, ax=ax,\n",
    "        palette=custom_palette\n",
    "    )\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    all_handles_labels.extend(zip(handles, labels))\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "    ax.set_title(f\"{name} activation\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "    ax.set_xlabel(\"Nodes pruned\")\n",
    "    ax.grid(True)\n",
    "\n",
    "# Clean legend\n",
    "legend_dict = OrderedDict()\n",
    "for handle, label in all_handles_labels:\n",
    "    if label not in {\"model\", \"N\"} and label not in legend_dict:\n",
    "        legend_dict[label] = handle\n",
    "\n",
    "desired_order = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "filtered = [(legend_dict[label], label) for label in desired_order if label in legend_dict]\n",
    "\n",
    "if filtered:\n",
    "    filtered_handles, filtered_labels = zip(*filtered)\n",
    "    fig.legend(\n",
    "        filtered_handles,\n",
    "        filtered_labels,\n",
    "        title=\"Model\",\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.05),\n",
    "        ncol=2,\n",
    "    )\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize_networks import compute_activation_frequency, extract_all_pruned_means, plot_all_networks_subplots_activations\n",
    "path = \"datasets/friedman/many/Friedman_N100_p10_sigma1.00_seed6.npz\"\n",
    "data = np.load(path)\n",
    "x_train = data[\"X_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_activation_colors = {\n",
    "    model_name: compute_activation_frequency(fit, x_train)\n",
    "    for model_name, fit in relu_fits['Friedman_N100_p10_sigma1.00_seed6'].items()\n",
    "}\n",
    "\n",
    "# Flatten and find the global maximum\n",
    "all_freqs = np.concatenate(list(node_activation_colors.values()))\n",
    "global_max = all_freqs.max()\n",
    "print(global_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NODE PRUNE VISUALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualize_networks import extract_all_pruned_node_means, plot_all_networks_subplots_activations\n",
    "num_nodes_to_prune = 14  # for example\n",
    "pruned_model_means_nodes = extract_all_pruned_node_means(relu_fits['Friedman_N100_p10_sigma1.00_seed6'], layer_structure, num_nodes_to_prune)\n",
    "\n",
    "p_nodes, widths_nodes = plot_all_networks_subplots_activations(\n",
    "    pruned_model_means_nodes, layer_sizes, node_activation_colors,\n",
    "    activation_color_max=global_max, signed_colors=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
