{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "from scipy.stats import norm\n",
    "\n",
    "def nearest_correlation_matrix(A, eps=1e-8, max_iter=5):\n",
    "    \"\"\"\n",
    "    Higham-style projection:\n",
    "    1) symmetrize\n",
    "    2) eigen-decompose and clip negative eigenvalues\n",
    "    3) renormalize to unit diagonal (correlation)\n",
    "    Repeat a few times for stability.\n",
    "    \"\"\"\n",
    "    X = (A + A.T) / 2\n",
    "    for _ in range(max_iter):\n",
    "        # Eigen clip\n",
    "        w, V = eigh(X)\n",
    "        w_clipped = np.maximum(w, eps)\n",
    "        X = (V * w_clipped) @ V.T\n",
    "        # Force exact symmetry\n",
    "        X = (X + X.T) / 2\n",
    "        # Scale to correlation\n",
    "        d = np.sqrt(np.clip(np.diag(X), eps, None))\n",
    "        Dinv = np.diag(1.0 / d)\n",
    "        X = Dinv @ X @ Dinv\n",
    "        X = (X + X.T) / 2\n",
    "        np.fill_diagonal(X, 1.0)\n",
    "    return X\n",
    "\n",
    "def spearman_to_gaussian_corr(S):\n",
    "    \"\"\"\n",
    "    Map a Spearman correlation matrix S to the Gaussian copula\n",
    "    correlation matrix R via R_ij = 2 sin(pi * S_ij / 6).\n",
    "    \"\"\"\n",
    "    S = np.asarray(S)\n",
    "    if S.shape[0] != S.shape[1]:\n",
    "        raise ValueError(\"S must be square.\")\n",
    "    R = 2.0 * np.sin(np.pi * S / 6.0)\n",
    "    np.fill_diagonal(R, 1.0)\n",
    "    return R\n",
    "\n",
    "def sample_gaussian_copula_uniform(n, S, random_state=None):\n",
    "    \"\"\"\n",
    "    Sample n rows of U ~ Gaussian copula with target Spearman matrix S.\n",
    "    Returns an (n, d) array with uniform(0,1) marginals.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    d = S.shape[0]\n",
    "    # Map Spearman -> Gaussian copula correlation\n",
    "    R0 = spearman_to_gaussian_corr(S)\n",
    "    # Project to nearest valid correlation matrix\n",
    "    R = nearest_correlation_matrix(R0)\n",
    "    # Cholesky (add tiny jitter if needed)\n",
    "    jitter = 0\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            L = np.linalg.cholesky(R + jitter * np.eye(d))\n",
    "            break\n",
    "        except np.linalg.LinAlgError:\n",
    "            jitter = 1e-10 if jitter == 0 else jitter * 10\n",
    "    # Sample MVN(0, R)\n",
    "    Z = rng.standard_normal(size=(n, d)) @ L.T\n",
    "    # Push through Phi to get uniforms\n",
    "    U = norm.cdf(Z)\n",
    "    return U, R  # returning R lets you inspect the actual copula correlation used\n",
    "\n",
    "# --- Example 1: 10 variables with a simple AR(1)-style target in Spearman space ---\n",
    "d = 10\n",
    "rho_S = 0.6  # Spearman correlation at lag 1\n",
    "S_AR = np.fromfunction(lambda i, j: rho_S**np.abs(i-j), (d, d))\n",
    "np.fill_diagonal(S_AR, 1.0)\n",
    "\n",
    "#U, R_used = sample_gaussian_copula_uniform(n=5000, S=S_AR, random_state=42)\n",
    "\n",
    "# U is (5000, 10) with uniform marginals and the Gaussian copula specified by R_used\n",
    "# If you want different marginals, say Exp(1) for col 0, Beta(2,5) for col 1, etc.:\n",
    "# from scipy.stats import expon, beta\n",
    "# X0 = expon.ppf(U[:, 0], scale=1.0)\n",
    "# X1 = beta.ppf(U[:, 1], a=2, b=5)\n",
    "# ...and so on for each column\n",
    "\n",
    "# --- Example 2: “varying” pairwise Spearman structure (blocks + custom entries) ---\n",
    "S_custom = np.eye(d)\n",
    "# Block 1 (vars 0..4): high Spearman, 0.7\n",
    "for i in range(0, 3):\n",
    "    for j in range(i+1, 3):\n",
    "        S_custom[i, j] = S_custom[j, i] = 0.8\n",
    "# Block 2 (vars 5..9): moderate Spearman, 0.4\n",
    "for i in range(5, 10):\n",
    "    for j in range(i+1, 10):\n",
    "        S_custom[i, j] = S_custom[j, i] = -0.5\n",
    "# Cross-block weaker, 0.15\n",
    "for i in range(0, 5):\n",
    "    for j in range(5, 10):\n",
    "        S_custom[i, j] = S_custom[j, i] = 0.15\n",
    "# A couple of bespoke pairs:\n",
    "S_custom[0, 9] = S_custom[9, 0] = 0.4\n",
    "S_custom[2, 7] = S_custom[7, 2] = 0.9  # very strong (will be projected if infeasible)\n",
    "S_custom[3, 4] = S_custom[4, 3] = -0.9  # very strong (will be projected if infeasible)\n",
    "S_custom[1, 6] = S_custom[6, 1] = -0.9  # very strong (will be projected if infeasible)\n",
    "\n",
    "U2, R_used2 = sample_gaussian_copula_uniform(n=10000, S=S_custom, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(np.corrcoef(U2, rowvar=False), vmin=-1, vmax=1, cmap='coolwarm')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_correlated_Friedman_data(U, N=100, D=10, sigma=1.0, test_size=0.2, seed=42, standardize_y=True):\n",
    "    \"\"\"\n",
    "    Generate synthetic regression data for Bayesian neural network experiments.\n",
    "\n",
    "    Parameters:\n",
    "        N (int): Number of samples.\n",
    "        D (int): Number of features.\n",
    "        sigma (float): Noise level.\n",
    "        test_size (float): Proportion for test split.\n",
    "        seed (int): Random seed.\n",
    "        standardize_y (bool): Whether to standardize the response variable.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, X_test, y_train, y_test, y_mean, y_std) if standardize_y,\n",
    "               else (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    #X = np.random.uniform(0, 1, size=(N, D))\n",
    "    if N != U.shape[0]:\n",
    "        idx = np.random.choice(U.shape[0], size=N, replace=False)\n",
    "        X = U[idx, :]\n",
    "    else:\n",
    "        X = U\n",
    "\n",
    "    x0, x1, x2, x3, x4 = X[:, 0], X[:, 1], X[:, 2], X[:, 3], X[:, 4]\n",
    "\n",
    "    y_clean = (\n",
    "        10 * np.sin(np.pi * x0 * x1) +\n",
    "        20 * (x2 - 0.5) ** 2 +\n",
    "        10 * x3 +\n",
    "        5.0 * x4\n",
    "    )\n",
    "\n",
    "    y = y_clean + np.random.normal(0, sigma, size=N)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "    if standardize_y:\n",
    "        y_mean = y_train.mean()\n",
    "        y_std = y_train.std() if y_train.std() > 0 else 1.0  # avoid division by zero\n",
    "\n",
    "        y_train = (y_train - y_mean) / y_std\n",
    "        y_test = (y_test - y_mean) / y_std\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = generate_correlated_Friedman_data(U2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sizes = [100, 200, 500]\n",
    "num_sets = range(1, 6)\n",
    "p=10\n",
    "sigma=1.0\n",
    "\n",
    "base_dir = \"datasets/friedman_correlated\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "i=0\n",
    "for N in sizes:\n",
    "    for set in num_sets:\n",
    "        i += 1\n",
    "        X_train, X_test, y_train, y_test = generate_correlated_Friedman_data(\n",
    "            U2, N=N, D=p, sigma=sigma, test_size=0.2, seed=i, standardize_y=True\n",
    "        )\n",
    "        save_dir = f\"datasets/friedman_correlated/many/Friedman_N{N}_p{p}_sigma{sigma}_seed{i}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        np.savez(\n",
    "            f\"{save_dir}.npz\",\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            y_train=y_train,\n",
    "            y_test=y_test,\n",
    "            #y_mean=y_mean,\n",
    "            #y_std=y_std,\n",
    "            N=N,\n",
    "            seed=i\n",
    "        )\n",
    "\n",
    "        print(f\"Saved → {save_dir}.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(f\"datasets/friedman_correlated/U2.npz\", U=U2)\n",
    "#print(\"Saved U2_big → datasets/friedman_correlated/U2_big.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
