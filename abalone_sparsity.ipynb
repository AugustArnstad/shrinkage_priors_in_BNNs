{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GJENSTÅR\n",
    "\n",
    "python3 utils/run_abalone.py --model beta_horseshoe_tanh_nodewise --output_dir results/regression/single_layer/tanh/abalone \n",
    "\n",
    "python3 utils/run_abalone.py --model beta_student_t_tanh_nodewise --output_dir results/regression/single_layer/tanh/abalone \n",
    "\n",
    "python3 utils/run_abalone.py --model dirichlet_student_t_tanh_nodewise --output_dir results/regression/single_layer/tanh/abalone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/abalone\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/abalone\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/abalone\"\n",
    "\n",
    "# model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "# model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "#model_names_relu = [\"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "\n",
    "\n",
    "full_config_path = \"abalone_N334_p8\"\n",
    "# relu_fit = get_model_fits(\n",
    "#     config=full_config_path,\n",
    "#     results_dir=results_dir_relu,\n",
    "#     models=model_names_relu,\n",
    "#     include_prior=False,\n",
    "# )\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/abalone\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/abalone\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/abalone\"\n",
    "\n",
    "# model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "# model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Beta Horseshoe tanh\", \"Beta Student T tanh\"]\n",
    "# model_names_relu_nodewise = [\"Dirichlet Horseshoe\", \"Dirichlet Student T\", \"Beta Horseshoe\", \"Beta Student T\"]\n",
    "model_names_tanh_nodewise = [\"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Student T tanh nodewise\", \"Beta Horseshoe tanh nodewise\", \"Beta Student T tanh nodewise\"] #\"Dirichlet Student T tanh nodewise\", \n",
    "\n",
    "\n",
    "full_config_path = \"abalone_N334_p8\"\n",
    "# relu_fit = get_model_fits(\n",
    "#     config=full_config_path,\n",
    "#     results_dir=results_dir_relu,\n",
    "#     models=model_names_relu,\n",
    "#     include_prior=False,\n",
    "# )\n",
    "\n",
    "tanh_fit_nodewise = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh_nodewise,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST NEW PRUNING SCHEME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_global_mask_from_posterior(\n",
    "    W_samples,\n",
    "    sparsity,\n",
    "    method=\"Eabs\",          # \"Eabs\" or \"Eabs_stability\"\n",
    "    stability_quantile=0.1, # used if method=\"Eabs_stability\"\n",
    "    prune_smallest=True\n",
    "):\n",
    "    \"\"\"\n",
    "    W_samples: array (S, ..., ...) posterior draws of a weight matrix.\n",
    "    sparsity: fraction to prune (q). Keeps (1-q).\n",
    "    Returns mask with same trailing shape as one draw, dtype float {0,1}.\n",
    "    \"\"\"\n",
    "    assert 0.0 <= sparsity < 1.0\n",
    "    S = W_samples.shape[0]\n",
    "    W_abs = np.abs(W_samples)  # (S, ...)\n",
    "\n",
    "    # Importance score a = E|w|\n",
    "    a = W_abs.mean(axis=0)     # (..., ...)\n",
    "\n",
    "    if method == \"Eabs\":\n",
    "        score = a\n",
    "    elif method == \"Eabs_stability\":\n",
    "        # Stability proxy pi = P(|w| > t), where t is a small global quantile of |w|\n",
    "        t = np.quantile(W_abs.reshape(S, -1), stability_quantile)\n",
    "        pi = (W_abs > t).mean(axis=0)\n",
    "        # Combine: emphasize both \"large on average\" and \"consistently non-tiny\"\n",
    "        score = a * pi\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'Eabs' or 'Eabs_stability'\")\n",
    "\n",
    "    # Decide how many to prune\n",
    "    num_params = score.size\n",
    "    k_prune = int(np.floor(sparsity * num_params))\n",
    "    if k_prune == 0:\n",
    "        return np.ones_like(score, dtype=float)\n",
    "\n",
    "    flat = score.reshape(-1)\n",
    "\n",
    "    if prune_smallest:\n",
    "        # prune lowest scores\n",
    "        thresh = np.partition(flat, k_prune - 1)[k_prune - 1]\n",
    "        mask = (score > thresh).astype(float)\n",
    "        # if ties create too many kept/pruned, fix deterministically\n",
    "        # (rare but possible with many equal scores)\n",
    "        if mask.sum() > num_params - k_prune:\n",
    "            # drop some tied-at-threshold entries\n",
    "            idx_tied = np.where(score.reshape(-1) == thresh)[0]\n",
    "            need_drop = int(mask.sum() - (num_params - k_prune))\n",
    "            if need_drop > 0:\n",
    "                mask_flat = mask.reshape(-1)\n",
    "                mask_flat[idx_tied[:need_drop]] = 0.0\n",
    "                mask = mask_flat.reshape(score.shape)\n",
    "        elif mask.sum() < num_params - k_prune:\n",
    "            # add some tied entries if we kept too few\n",
    "            idx_tied = np.where(score.reshape(-1) == thresh)[0]\n",
    "            need_add = int((num_params - k_prune) - mask.sum())\n",
    "            if need_add > 0:\n",
    "                mask_flat = mask.reshape(-1)\n",
    "                # add back from tied\n",
    "                add_candidates = idx_tied[mask_flat[idx_tied] == 0.0]\n",
    "                mask_flat[add_candidates[:need_add]] = 1.0\n",
    "                mask = mask_flat.reshape(score.shape)\n",
    "    else:\n",
    "        # prune largest (not typical)\n",
    "        thresh = np.partition(flat, num_params - k_prune)[num_params - k_prune]\n",
    "        mask = (score < thresh).astype(float)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def precompute_global_masks(\n",
    "    all_fits,\n",
    "    model,\n",
    "    sparsity_levels,\n",
    "    prune_W2=False,\n",
    "    method=\"Eabs_stability\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns dict: sparsity -> (mask_W1, mask_W2 or None)\n",
    "    \"\"\"\n",
    "    fit = all_fits[model][\"posterior\"]\n",
    "\n",
    "    W1_samples = fit.stan_variable(\"W_1\")  # (S, P, H)\n",
    "    W2_samples = fit.stan_variable(\"W_L\")  # (S, H, O) or (S, H) depending on O\n",
    "\n",
    "    masks = {}\n",
    "    for q in sparsity_levels:\n",
    "        mask_W1 = build_global_mask_from_posterior(W1_samples, q, method=method)\n",
    "        mask_W2 = None\n",
    "        if prune_W2:\n",
    "            mask_W2 = build_global_mask_from_posterior(W2_samples, q, method=method)\n",
    "        masks[q] = (mask_W1, mask_W2)\n",
    "    return masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "def _logsumexp(a, axis=None):\n",
    "    amax = np.max(a, axis=axis, keepdims=True)\n",
    "    out = amax + np.log(np.sum(np.exp(a - amax), axis=axis, keepdims=True))\n",
    "    return np.squeeze(out, axis=axis)\n",
    "\n",
    "def gaussian_nll_pointwise(y, mu, sigma):\n",
    "    return 0.5*np.log(2*np.pi*(sigma**2)) + 0.5*((y-mu)**2)/(sigma**2)\n",
    "\n",
    "def compute_sparse_metrics_results_globalmask_large_eval(\n",
    "    models, all_fits, forward_pass,\n",
    "    sparsity=0.0,\n",
    "    masks_cache=None,\n",
    "    prune_W2=False,\n",
    "    compute_nll=True,\n",
    "    noise_var_name=\"sigma\",\n",
    "    frac = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate on a large generated test set instead of the stored tiny X_test/y_test.\n",
    "    Assumes model was trained on standardized y if standardize_y=True.\n",
    "    \"\"\"\n",
    "    posterior_means = []\n",
    "    # Build large eval set consistent with training split standardization\n",
    "    X_train, X_test, y_train, y_test = load_abalone_regression_data(standardized=False, frac=frac)\n",
    "    #y_std = y_train.std()\n",
    "    #y_mean = y_train.mean()\n",
    "\n",
    "    for model in models:\n",
    "        try:\n",
    "            fit = all_fits[model]['posterior']\n",
    "            W1_samples = fit.stan_variable(\"W_1\")           # (S, P, H)\n",
    "            W2_samples = fit.stan_variable(\"W_L\")           # (S, H, O)\n",
    "            b1_samples = fit.stan_variable(\"hidden_bias\")   # (S, O, H)\n",
    "            b2_samples = fit.stan_variable(\"output_bias\")   # (S, O)\n",
    "\n",
    "            noise_samples = None\n",
    "            if compute_nll:\n",
    "                try:\n",
    "                    noise_samples = fit.stan_variable(noise_var_name).squeeze()\n",
    "                except Exception:\n",
    "                    noise_samples = None\n",
    "        except KeyError:\n",
    "            print(f\"[SKIP] Model or posterior not found: -> {model}\")\n",
    "            continue\n",
    "\n",
    "        S = W1_samples.shape[0]\n",
    "        y_hats = np.zeros((S, y_test.shape[0]))\n",
    "\n",
    "        mask_W1 = mask_W2 = None\n",
    "        if masks_cache is not None and sparsity > 0.0:\n",
    "            mask_W1, mask_W2 = masks_cache[(model)][sparsity]\n",
    "\n",
    "        for i in range(S):\n",
    "            W1 = W1_samples[i]\n",
    "            W2 = W2_samples[i]\n",
    "\n",
    "            if mask_W1 is not None:\n",
    "                W1 = W1 * mask_W1\n",
    "            if prune_W2 and (mask_W2 is not None):\n",
    "                W2 = W2 * mask_W2\n",
    "\n",
    "            y_hat = forward_pass(X_test, W1, b1_samples[i][0], W2, b2_samples[i]).squeeze()\n",
    "            y_hats[i] = y_hat\n",
    "\n",
    "        # posterior mean RMSE (standardized scale)\n",
    "        posterior_mean = y_hats.mean(axis=0)\n",
    "        posterior_mean_rmse = np.sqrt(np.mean((posterior_mean - y_test)**2))\n",
    "\n",
    "        out_pm = {\n",
    "            'N': X_train.shape[0],\n",
    "            'model': model,\n",
    "            'sparsity': sparsity,\n",
    "            'n_eval': y_test.shape[0],\n",
    "            'posterior_mean_rmse': posterior_mean_rmse,\n",
    "        }\n",
    "\n",
    "        if compute_nll:\n",
    "            if noise_samples is None:\n",
    "                sig_s = np.ones(S)\n",
    "            else:\n",
    "                sig_s = np.asarray(noise_samples).reshape(-1)[:S]\n",
    "\n",
    "            # Expected NLL\n",
    "            nll_draws = np.array([\n",
    "                gaussian_nll_pointwise(y_test, y_hats[i], sig_s[i]).mean()\n",
    "                for i in range(S)\n",
    "            ])\n",
    "            expected_nll = nll_draws.mean()\n",
    "\n",
    "            # Predictive (mixture) NLL\n",
    "            loglik = -np.stack([\n",
    "                gaussian_nll_pointwise(y_test, y_hats[i], sig_s[i])\n",
    "                for i in range(S)\n",
    "            ], axis=0)  # (S, n_eval)\n",
    "            lppd = (_logsumexp(loglik, axis=0) - np.log(S)).mean()\n",
    "            predictive_nll = -lppd\n",
    "\n",
    "            out_pm[\"expected_nll\"] = expected_nll\n",
    "            out_pm[\"predictive_nll\"] = predictive_nll\n",
    "\n",
    "\n",
    "        posterior_means.append(out_pm)\n",
    "\n",
    "    return pd.DataFrame(posterior_means)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sparsity import forward_pass_relu, forward_pass_tanh, local_prune_weights\n",
    "\n",
    "def build_masks_cache_for_all(\n",
    "    all_fits,\n",
    "    models,\n",
    "    sparsity_levels,\n",
    "    prune_W2=False,\n",
    "    method=\"Eabs_stability\"\n",
    "):\n",
    "    masks_cache = {}\n",
    "    for model in models:\n",
    "        try:\n",
    "            masks_cache[(model)] = precompute_global_masks(\n",
    "                all_fits=all_fits,\n",
    "                model=model,\n",
    "                sparsity_levels=sparsity_levels,\n",
    "                prune_W2=prune_W2,\n",
    "                method=method\n",
    "            )\n",
    "        except KeyError:\n",
    "            print(f\"[SKIP MASKS] Missing fit for -> {model}\")\n",
    "    return masks_cache\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Precompute masks once\n",
    "# masks_relu = build_masks_cache_for_all(relu_fit, model_names_relu, sparsity_levels, prune_W2=False)\n",
    "\n",
    "masks_tanh = build_masks_cache_for_all(tanh_fit, model_names_tanh, sparsity_levels, prune_W2=False)\n",
    "masks_tanh_nodewise = build_masks_cache_for_all(tanh_fit_nodewise, model_names_tanh_nodewise, sparsity_levels, prune_W2=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_post_relu = {}\n",
    "\n",
    "# for q in sparsity_levels:\n",
    "#     df_post_relu[q] = compute_sparse_metrics_results_globalmask_large_eval(\n",
    "#         models=model_names_relu,\n",
    "#         all_fits=relu_fit,\n",
    "#         forward_pass=forward_pass_relu,\n",
    "#         sparsity=q,\n",
    "#         masks_cache=masks_relu,\n",
    "#         prune_W2=False,\n",
    "#         compute_nll=True,\n",
    "#         noise_var_name=\"sigma\",\n",
    "#         frac=0.1\n",
    "#     )\n",
    "\n",
    "df_post_tanh = {}\n",
    "\n",
    "for q in sparsity_levels:\n",
    "    df_post_tanh[q] = compute_sparse_metrics_results_globalmask_large_eval(\n",
    "        models=model_names_tanh,\n",
    "        all_fits=tanh_fit,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        sparsity=q,\n",
    "        masks_cache=masks_tanh,\n",
    "        prune_W2=False,\n",
    "        compute_nll=True,\n",
    "        noise_var_name=\"sigma\",\n",
    "        frac=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_tanh_nodewise = {}\n",
    "\n",
    "for q in sparsity_levels:\n",
    "    df_post_tanh_nodewise[q] = compute_sparse_metrics_results_globalmask_large_eval(\n",
    "        models=model_names_tanh_nodewise,\n",
    "        all_fits=tanh_fit_nodewise,\n",
    "        forward_pass=forward_pass_tanh,\n",
    "        sparsity=q,\n",
    "        masks_cache=masks_tanh_nodewise,\n",
    "        prune_W2=False,\n",
    "        compute_nll=True,\n",
    "        noise_var_name=\"sigma\",\n",
    "        frac=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_post_relu_full = pd.concat(\n",
    "#     [df.assign(sparsity=sparsity) for sparsity, df in df_post_relu.items()],\n",
    "#     ignore_index=True\n",
    "# )\n",
    "\n",
    "df_post_tanh_full = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_post_tanh.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_post_tanh_full_nodewise = pd.concat(\n",
    "    [df.assign(sparsity=sparsity) for sparsity, df in df_post_tanh_nodewise.items()],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_post_tanh_full,df_post_tanh_full_nodewise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_tanh_full[df_post_tanh_full['sparsity']==0.0].sort_values(by=\"posterior_mean_rmse\").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = {\n",
    "    \"Gaussian tanh\": \"C0\",\n",
    "    \"Regularized Horseshoe tanh\": \"C1\",\n",
    "    \"Dirichlet Horseshoe tanh\": \"C2\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"C2\",\n",
    "    \"Dirichlet Student T tanh\": \"C3\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"C3\",\n",
    "    \"Beta Horseshoe tanh\": \"C4\",\n",
    "    \"Beta Horseshoe tanh nodewise\": \"C4\",\n",
    "    \"Beta Student T tanh\": \"C5\",  \n",
    "    \"Beta Student T tanh nodewise\": \"C5\",                 \n",
    "}\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "rmse_col = \"posterior_mean_rmse\"\n",
    "#pll_col  = \"predictive_nll\"   # use *_orig if you prefer\n",
    "\n",
    "df = df_all.copy()\n",
    "#df[\"pll\"] = -df[pll_col]  # convert NLL → PLL\n",
    "\n",
    "df = df[df[\"sparsity\"].isin(sparsity_levels)]\n",
    "df[\"sparsity\"] = pd.Categorical(df[\"sparsity\"],\n",
    "                                categories=sparsity_levels,\n",
    "                                ordered=True)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True)\n",
    "\n",
    "for model, g in df.groupby(\"model\", sort=False):\n",
    "    if model in [\"Gaussian tanh\", \"Dirichlet Student T tanh nodewise\", \"Dirichlet Student T tanh\", \"Beta Student T tanh\", \"Beta Student T tanh nodewise\"]:\n",
    "       continue\n",
    "    g = g.sort_values(\"sparsity\")\n",
    "    if \"nodewise\" in model:\n",
    "        axes[0].plot(g[\"sparsity\"], g[rmse_col], marker=\"v\", label=model, linestyle='dashed', color = colors[model])\n",
    "        axes[1].plot(g[\"sparsity\"], g[\"predictive_nll\"],    marker=\"v\", label=model, linestyle='dashed', color = colors[model])\n",
    "    else:\n",
    "        axes[0].plot(g[\"sparsity\"], g[rmse_col], marker=\"o\", label=model, color = colors[model])\n",
    "        axes[1].plot(g[\"sparsity\"], g[\"predictive_nll\"],    marker=\"o\", label=model, color = colors[model])\n",
    "    \n",
    "axes[0].set_title(f\"RMSE vs sparsity\")\n",
    "axes[1].set_title(f\"PNLL vs sparsity\")\n",
    "\n",
    "axes[0].set_ylabel(\"RMSE\")\n",
    "axes[1].set_ylabel(\"PNLL\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"sparsity\")\n",
    "    ax.set_xticks(sparsity_levels)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "axes[0].legend(loc=\"upper left\",\n",
    "                #bbox_to_anchor=(1.02, 0.5),\n",
    "                frameon=False)\n",
    "axes[1].legend(loc=\"upper left\",\n",
    "                #bbox_to_anchor=(1.02, 0.5),\n",
    "                frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = {\n",
    "    \"Gaussian tanh\": \"C0\",\n",
    "    \"Regularized Horseshoe tanh\": \"C1\",\n",
    "    \"Dirichlet Horseshoe tanh\": \"C2\",\n",
    "    \"Dirichlet Horseshoe tanh nodewise\": \"C2\",\n",
    "    \"Dirichlet Student T tanh\": \"C3\",\n",
    "    \"Dirichlet Student T tanh nodewise\": \"C3\",\n",
    "    \"Beta Horseshoe tanh\": \"C4\",\n",
    "    \"Beta Horseshoe tanh nodewise\": \"C4\",\n",
    "    \"Beta Student T tanh\": \"C5\",  \n",
    "    \"Beta Student T tanh nodewise\": \"C5\",                 \n",
    "}\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "rmse_col = \"posterior_mean_rmse\"\n",
    "#pll_col  = \"predictive_nll\"   # use *_orig if you prefer\n",
    "\n",
    "df = df_all.copy()\n",
    "#df[\"pll\"] = -df[pll_col]  # convert NLL → PLL\n",
    "\n",
    "df = df[df[\"sparsity\"].isin(sparsity_levels)]\n",
    "df[\"sparsity\"] = pd.Categorical(df[\"sparsity\"],\n",
    "                                categories=sparsity_levels,\n",
    "                                ordered=True)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4), sharex=True)\n",
    "\n",
    "for model, g in df.groupby(\"model\", sort=False):\n",
    "    if model in [\"Gaussian tanh\", \"Dirichlet Horseshoe tanh nodewise\", \"Dirichlet Horseshoe tanh\", \"Beta Horseshoe tanh\", \"Beta Horseshoe tanh nodewise\"]:\n",
    "       continue\n",
    "    g = g.sort_values(\"sparsity\")\n",
    "    if \"nodewise\" in model:\n",
    "        axes[0].plot(g[\"sparsity\"], g[rmse_col], marker=\"v\", label=model, linestyle='dashed', color = colors[model])\n",
    "        axes[1].plot(g[\"sparsity\"], g[\"predictive_nll\"],    marker=\"v\", label=model, linestyle='dashed', color = colors[model])\n",
    "    else:\n",
    "        axes[0].plot(g[\"sparsity\"], g[rmse_col], marker=\"o\", label=model, color = colors[model])\n",
    "        axes[1].plot(g[\"sparsity\"], g[\"predictive_nll\"],    marker=\"o\", label=model, color = colors[model])\n",
    "    \n",
    "axes[0].set_title(f\"RMSE vs sparsity\")\n",
    "axes[1].set_title(f\"PNLL vs sparsity\")\n",
    "\n",
    "axes[0].set_ylabel(\"RMSE\")\n",
    "axes[1].set_ylabel(\"PNLL\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"sparsity\")\n",
    "    ax.set_xticks(sparsity_levels)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "axes[0].legend(loc=\"upper left\",\n",
    "                #bbox_to_anchor=(1.02, 0.5),\n",
    "                frameon=False)\n",
    "axes[1].legend(loc=\"upper left\",\n",
    "                #bbox_to_anchor=(1.02, 0.5),\n",
    "                frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZE THE PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"Sex\", \"Length\", \"Diameter\", \"Height\",\n",
    "        \"Whole weight\", \"Shucked weight\", \"Viscera weight\",\n",
    "        \"Shell weight\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _node_sizes_from_degree(deg, base=80, scale=30):\n",
    "    # deg: array of degrees (>=0)\n",
    "    return base + scale * deg\n",
    "\n",
    "def plot_pruned_network_panels(\n",
    "    masks_for_model,                 # dict: q -> (mask_W1, mask_W2)\n",
    "    q_levels=(0.0, 0.5, 0.8, 0.9),    # 4 values recommended\n",
    "    feature_names=None,              # list length P\n",
    "    hidden_names=None,               # list length H (optional)\n",
    "    hidden_bias_meanabs=None,         # array length H (optional): E|b1_j| (or similar)\n",
    "    title_prefix=\"\",\n",
    "):\n",
    "    q_levels = list(q_levels)\n",
    "\n",
    "    # infer dimensions\n",
    "    mask_W1_0, mask_W2_0 = masks_for_model[q_levels[0]]\n",
    "    P, H = mask_W1_0.shape\n",
    "    O = 1  # scalar output\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"x{i+1}\" for i in range(P)]\n",
    "    if hidden_names is None:\n",
    "        hidden_names = [f\"h{j+1}\" for j in range(H)]\n",
    "\n",
    "    # fixed node positions (same in every panel)\n",
    "    # y arranged top-to-bottom\n",
    "    y_inputs = np.linspace(1.0, 0.0, P)\n",
    "    y_hidden = np.linspace(1.0, 0.0, H)\n",
    "    x_in, x_hid, x_out = 0.0, 1.0, 2.0\n",
    "\n",
    "    pos_in = np.column_stack([np.full(P, x_in), y_inputs])\n",
    "    pos_h  = np.column_stack([np.full(H, x_hid), y_hidden])\n",
    "    pos_o  = np.array([[x_out, 0.5]])\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for ax, q in zip(axes, q_levels):\n",
    "        mask_W1, mask_W2 = masks_for_model[q]\n",
    "        if mask_W2 is None:\n",
    "            # if you didn't prune W2, treat as all ones (H x 1)\n",
    "            mask_W2 = np.ones((H, 1), dtype=float)\n",
    "\n",
    "        # degrees\n",
    "        deg_in = mask_W1.sum(axis=1)             # (P,)\n",
    "        deg_h_in = mask_W1.sum(axis=0)           # (H,)\n",
    "        deg_h_out = mask_W2[:, 0].astype(float)  # (H,) since scalar output, 0/1 if pruned\n",
    "\n",
    "        # node sizes\n",
    "        s_in = _node_sizes_from_degree(deg_in, base=80, scale=25)\n",
    "        # Hidden: combine structure + (optional) bias magnitude\n",
    "        if hidden_bias_meanabs is not None:\n",
    "            # normalize bias for sizing (robust-ish)\n",
    "            b = np.asarray(hidden_bias_meanabs).reshape(-1)\n",
    "            b = b / (np.max(b) + 1e-12)\n",
    "            s_h = 120 + 180 * b + 20 * deg_h_in\n",
    "        else:\n",
    "            s_h = _node_sizes_from_degree(deg_h_in, base=120, scale=20)\n",
    "\n",
    "        s_o = 180\n",
    "\n",
    "        # -------- edges: build line segments --------\n",
    "        segs = []\n",
    "        # input -> hidden\n",
    "        ii, jj = np.where(mask_W1 > 0.5)\n",
    "        for i, j in zip(ii, jj):\n",
    "            segs.append([pos_in[i], pos_h[j]])\n",
    "\n",
    "        # hidden -> output (only if mask_W2[j]=1)\n",
    "        alive_h = np.where(deg_h_out > 0.5)[0]\n",
    "        for j in alive_h:\n",
    "            segs.append([pos_h[j], pos_o[0]])\n",
    "\n",
    "        if segs:\n",
    "            lc = LineCollection(segs, linewidths=1.0)\n",
    "            ax.add_collection(lc)\n",
    "\n",
    "        # -------- nodes --------\n",
    "        ax.scatter(pos_in[:, 0], pos_in[:, 1], s=s_in)\n",
    "        ax.scatter(pos_h[:, 0], pos_h[:, 1], s=s_h)\n",
    "        ax.scatter(pos_o[:, 0], pos_o[:, 1], s=s_o)\n",
    "\n",
    "        # labels (keep minimal; can comment out if too busy)\n",
    "        for i, name in enumerate(feature_names):\n",
    "            ax.text(pos_in[i, 0] - 0.03, pos_in[i, 1], name, ha=\"right\", va=\"center\", fontsize=9)\n",
    "        # hidden labels only for “alive-ish” units to reduce clutter:\n",
    "        # (either has incoming or outgoing)\n",
    "        show_h = np.where((deg_h_in > 0) | (deg_h_out > 0.5))[0]\n",
    "        for j in show_h:\n",
    "            ax.text(pos_h[j, 0] + 0.03, pos_h[j, 1], hidden_names[j], ha=\"left\", va=\"center\", fontsize=8)\n",
    "\n",
    "        ax.text(pos_o[0, 0] + 0.03, pos_o[0, 1], \"y\", ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "        # cosmetics\n",
    "        ax.set_title(f\"{title_prefix}q={q}\")\n",
    "        ax.set_xlim(-0.4, 2.4)\n",
    "        ax.set_ylim(-0.05, 1.05)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # annotate with a quick summary\n",
    "        kept_W1 = int(mask_W1.sum())\n",
    "        total_W1 = mask_W1.size\n",
    "        kept_W2 = int(mask_W2.sum())\n",
    "        total_W2 = mask_W2.size\n",
    "        ax.text(\n",
    "            0.0, -0.12,\n",
    "            f\"W1 kept: {kept_W1}/{total_W1}  |  W2 kept: {kept_W2}/{total_W2}  |  alive hidden (out): {kept_W2}\",\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"left\", va=\"top\", fontsize=9\n",
    "        )\n",
    "\n",
    "    # if fewer than 4 q-levels, hide extra axes\n",
    "    for k in range(len(q_levels), 4):\n",
    "        axes[k].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "model = \"Regularized Horseshoe tanh\"\n",
    "masks_for_model = masks_tanh[model]  # dict q -> (mask_W1, mask_W2)\n",
    "fit = tanh_fit[model][\"posterior\"]\n",
    "b1_samples = fit.stan_variable(\"hidden_bias\")   # shape (S, H) or (S, 1, H) depending on your Stan\n",
    "hidden_bias_meanabs = np.mean(np.abs(b1_samples.reshape(b1_samples.shape[0], -1)), axis=0)\n",
    "\n",
    "\n",
    "plot_pruned_network_panels(\n",
    "    masks_for_model=masks_for_model,\n",
    "    q_levels=[0.8, 0.9],\n",
    "    feature_names=feature_names,\n",
    "    hidden_bias_meanabs=hidden_bias_meanabs,  # or None\n",
    "    title_prefix=f\"{model} \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Dirichlet Horseshoe tanh\"\n",
    "masks_for_model = masks_tanh[model]  # dict q -> (mask_W1, mask_W2)\n",
    "fit = tanh_fit[model][\"posterior\"]\n",
    "b1_samples = fit.stan_variable(\"hidden_bias\")   # shape (S, H) or (S, 1, H) depending on your Stan\n",
    "hidden_bias_meanabs = np.mean(np.abs(b1_samples.reshape(b1_samples.shape[0], -1)), axis=0)\n",
    "\n",
    "plot_pruned_network_panels(\n",
    "    masks_for_model=masks_for_model,\n",
    "    q_levels=[0.8, 0.9],\n",
    "    feature_names=feature_names,\n",
    "    hidden_bias_meanabs=hidden_bias_meanabs,  # or None\n",
    "    title_prefix=f\"{model} \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "def plot_input_survival_curves(\n",
    "    masks_for_model,                 # dict: q -> (mask_W1, mask_W2)\n",
    "    feature_names=None,              # list length P\n",
    "    sparsity_levels=None,            # list of q's (sorted)\n",
    "    title=None,\n",
    "):\n",
    "    if sparsity_levels is None:\n",
    "        sparsity_levels = sorted(masks_for_model.keys())\n",
    "    else:\n",
    "        sparsity_levels = list(sparsity_levels)\n",
    "\n",
    "    # infer P, H from first mask\n",
    "    m0 = masks_for_model[sparsity_levels[0]][0]\n",
    "    P, H = m0.shape\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"x{i+1}\" for i in range(P)]\n",
    "\n",
    "    # degrees d_i(q) = sum_j mask[i,j]\n",
    "    D = np.zeros((P, len(sparsity_levels)), dtype=float)\n",
    "    for t, q in enumerate(sparsity_levels):\n",
    "        mask_W1, _ = masks_for_model[q]\n",
    "        D[:, t] = mask_W1.sum(axis=1)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    for i in range(P):\n",
    "        ax.plot(sparsity_levels, D[i, :], marker=\"o\", linewidth=1, label=feature_names[i])\n",
    "\n",
    "    # optional mean curve\n",
    "    ax.plot(sparsity_levels, D.mean(axis=0), marker=\"o\", linewidth=2, label=\"mean (inputs)\")\n",
    "\n",
    "    ax.set_xlabel(\"sparsity q\")\n",
    "    ax.set_ylabel(\"surviving connections per input (degree)\")\n",
    "    ax.set_xticks(sparsity_levels)\n",
    "    ax.set_ylim(-0.5, H + 0.5)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.5)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "sparsity_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# masks_tanh = build_masks_cache_for_all(tanh_fit, model_names_tanh, sparsity_levels, prune_W2=True)\n",
    "# masks_tanh_nodewise = build_masks_cache_for_all(tanh_fit_nodewise, model_names_tanh_nodewise, sparsity_levels, prune_W2=True)\n",
    "model = \"Regularized Horseshoe tanh\"\n",
    "masks_for_model = masks_tanh[model]  # dict q -> (mask_W1, mask_W2)\n",
    "\n",
    "# If you want bias sizes: compute once from the fit (example names; adjust to your Stan variables)\n",
    "fit = tanh_fit[model][\"posterior\"]\n",
    "b1_samples = fit.stan_variable(\"hidden_bias\")   # shape (S, H) or (S, 1, H) depending on your Stan\n",
    "hidden_bias_meanabs = np.mean(np.abs(b1_samples.reshape(b1_samples.shape[0], -1)), axis=0)\n",
    "\n",
    "\n",
    "#sparsity_levels = [0.0, 0.3, 0.5, 0.7]\n",
    "\n",
    "plot_input_survival_curves(\n",
    "    masks_for_model=masks_for_model,\n",
    "    feature_names=feature_names,\n",
    "    sparsity_levels=sparsity_levels,\n",
    "    title=f\"{model}: input survival curves\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Dirichlet Horseshoe tanh\"\n",
    "masks_for_model = masks_tanh[model]  # dict q -> (mask_W1, mask_W2)\n",
    "\n",
    "# If you want bias sizes: compute once from the fit (example names; adjust to your Stan variables)\n",
    "fit = tanh_fit[model][\"posterior\"]\n",
    "b1_samples = fit.stan_variable(\"hidden_bias\")   # shape (S, H) or (S, 1, H) depending on your Stan\n",
    "hidden_bias_meanabs = np.mean(np.abs(b1_samples.reshape(b1_samples.shape[0], -1)), axis=0)\n",
    "\n",
    "plot_input_survival_curves(\n",
    "    masks_for_model=masks_for_model,\n",
    "    feature_names=feature_names,\n",
    "    sparsity_levels=sparsity_levels,\n",
    "    title=f\"{model}: input survival curves\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
