{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#from sklearn.metrics import mean_squared_errosr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_priors = \"results/priors/single_layer/tanh/friedman\"\n",
    "results_dir_posteriors = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "prior_names = [\"Dirichlet Horseshoe\", \"Regularized Horseshoe\", \"Dirichlet Student T\", \"Gaussian\"]\n",
    "posterior_names = [\"Dirichlet Horseshoe tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Gaussian tanh\"]\n",
    "\n",
    "\n",
    "prior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "prior_N200_fits = get_model_fits(\n",
    "    config=\"Friedman_N200_p10_sigma1.00_seed2\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "prior_N500_fits = get_model_fits(\n",
    "    config=\"Friedman_N500_p10_sigma1.00_seed11\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "    \n",
    "posterior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N200_fits = get_model_fits(\n",
    "    config=\"Friedman_N200_p10_sigma1.00_seed2\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N500_fits = get_model_fits(\n",
    "    config=\"Friedman_N500_p10_sigma1.00_seed11\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"datasets/abalone\"\n",
    "results_dir_relu = \"results/regression/single_layer/relu/abalone\"\n",
    "results_dir_tanh = \"results/regression/single_layer/tanh/abalone\"\n",
    "#model_names_relu = [\"Dirichlet Student T\"]\n",
    "model_names_relu = [\"Gaussian\", \"Regularized Horseshoe\", \"Dirichlet Horseshoe\", \"Dirichlet Student T\"]\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "\n",
    "full_config_path = \"abalone_N3341_p8\"\n",
    "relu_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_relu,\n",
    "    models=model_names_relu,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=full_config_path,\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/friedman/Friedman_N500_p10_sigma1.00_seed11.npz\"\n",
    "data = np.load(path)\n",
    "X = data['X_train']\n",
    "y = data['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_data import load_abalone_regression_data\n",
    "X, X_test, y, y_test = load_abalone_regression_data(standardized=False, frac=1.0)\n",
    "# Coerce everything to plain float64 NumPy arrays\n",
    "X      = np.asarray(X, dtype=float)\n",
    "X_test = np.asarray(X_test, dtype=float)\n",
    "\n",
    "# y often comes as a (n,1) DataFrame/array — flatten to (n,)\n",
    "y      = np.asarray(y, dtype=float).reshape(-1)\n",
    "y_test = np.asarray(y_test, dtype=float).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Callable\n",
    "\n",
    "# ---------- Aktivasjon og deriverte ----------\n",
    "\n",
    "def get_activation(activation: str = \"tanh\") -> Tuple[Callable, Callable]:\n",
    "    if activation == \"tanh\":\n",
    "        phi = np.tanh\n",
    "        def dphi(a): return 1.0 - np.tanh(a)**2\n",
    "    elif activation == \"relu\":\n",
    "        def phi(a): return np.maximum(0.0, a)\n",
    "        def dphi(a): return (a > 0.0).astype(a.dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    return phi, dphi\n",
    "\n",
    "# ---------- H(w0) og J_W(w0, v0) ----------\n",
    "\n",
    "def build_hidden_and_jacobian_W(\n",
    "    X: np.ndarray,               # (n, p)\n",
    "    W0: np.ndarray,              # (H, p)  -- vekter i referansepunktet w0\n",
    "    b0: np.ndarray,              # (H,)    -- bias i referansepunktet w0\n",
    "    v0: np.ndarray,              # (H,)    -- utgangsvekter i referansepunktet v0\n",
    "    activation: str = \"tanh\",\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returnerer:\n",
    "      H  : (n, H)         = H(w0)\n",
    "      JW : (n, H*p)       = d(H(w)v)/d vec(W) |_(w0, v0), kolonner ordnet som (h=0..H-1, j=0..p-1)\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    H, pW = W0.shape\n",
    "    assert pW == p\n",
    "    phi, dphi = get_activation(activation)\n",
    "\n",
    "    # Pre- og post-aktivert\n",
    "    A = X @ W0.T + b0[None, :]        # (n, H), a_{i,h}\n",
    "    Phi_mat = phi(A)                     # (n, H), h_{i,h}\n",
    "    dphiA = dphi(A)                   # (n, H)\n",
    "    \n",
    "    # J_W: df/dW_{h,j} = v_h * dphi(a_{i,h}) * x_{i,j}\n",
    "    # For hver node h bygger vi et (n, p)-bidrag og flater ut langs j, og stabler så langs h.\n",
    "    JW_blocks = []\n",
    "    for h in range(H):\n",
    "        # (n,1) * (1,p) -> (n,p)\n",
    "        block_h = (v0[h] * dphiA[:, [h]]) * X \n",
    "        JW_blocks.append(block_h.reshape(n, p))\n",
    "    # Stack kolonnevis i rekkefølge (h, j) -> (n, H*p)\n",
    "    Jb = dphiA * v0[None, :]     # (n, H)\n",
    "    JW = np.hstack([B for B in JW_blocks])\n",
    "    Joutb = np.ones(n)        # (n,)\n",
    "    return Phi_mat, JW, Jb, Joutb\n",
    "\n",
    "# ---------- Sigma_y og P ----------\n",
    "\n",
    "def build_Sigma_y(\n",
    "    Phi_mat: np.ndarray,   # (n, H) = H(w0)\n",
    "    tau_v: float,          # prior std for v\n",
    "    noise: float,          # likelihood std\n",
    "    J_b1: np.ndarray = None,     # (n, H), optional\n",
    "    J_b2: np.ndarray = None,     # (n,),   optional\n",
    "    include_b1: bool = True,\n",
    "    include_b2: bool = True,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Σ_y = τ_v^2 ΦΦ^T + [J_b1 J_b1^T if include_b1] + [J_b2 J_b2^T if include_b2] + σ^2 I_n\n",
    "    \"\"\"\n",
    "    n = Phi_mat.shape[0]\n",
    "    Sigma_y = (tau_v**2) * (Phi_mat @ Phi_mat.T) + (noise**2) * np.eye(n)\n",
    "\n",
    "    if include_b1 and (J_b1 is not None):\n",
    "        Sigma_y = Sigma_y + (J_b1 @ J_b1.T)\n",
    "\n",
    "    if include_b2 and (J_b2 is not None):\n",
    "        Sigma_y = Sigma_y + np.outer(J_b2, J_b2)\n",
    "\n",
    "    return Sigma_y\n",
    "\n",
    "\n",
    "def build_P_from_lambda_tau(\n",
    "    lambda_tilde: np.ndarray,  # (H, p) lokale skalaer for W\n",
    "    tau_w: float               # global skala for w\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    P = τ_w^{-2} Λ^{-1} der Λ = diag(λ^2) for konsistens med uttrykket 1/(1 + τ^2 λ^2 s).\n",
    "    Dvs. diag(P) = 1 / (τ_w^2 * λ^2).\n",
    "    Returnerer P som (H*p, H*p) diagonalmatrise.\n",
    "    \"\"\"\n",
    "    lam_vec = lambda_tilde.reshape(-1)          # (H*p,)\n",
    "    diagP = 1.0 / ( (tau_w**2) * (lam_vec) ) # (H*p,)\n",
    "    return np.diag(diagP)\n",
    "\n",
    "# ---------- S, shrinkage-matrise R = (P+S)^{-1} P ----------\n",
    "\n",
    "def build_S(JW: np.ndarray, Sigma_y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    S = J_W^T Σ_y^{-1} J_W  (Hp x Hp).\n",
    "    Løser via lineær solve for stabilitet: X = Σ_y^{-1} J_W = solve(Σ_y, J_W).\n",
    "    \"\"\"\n",
    "    X = np.linalg.solve(Sigma_y, JW)       # (n, Hp)\n",
    "    return JW.T @ X                        # (Hp, Hp)\n",
    "\n",
    "def shrinkage_matrix(P: np.ndarray, S: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    R = (P+S)^{-1} P. Bruk Cholesky når mulig.\n",
    "    Løser (P+S) * R = P for R.\n",
    "    \"\"\"\n",
    "    A = P + S\n",
    "    # Robust fallback hvis Cholesky feiler\n",
    "    try:\n",
    "        L = np.linalg.cholesky(A)\n",
    "        # L Y = P  -> Y\n",
    "        Y = np.linalg.solve(L, P)\n",
    "        # L^T R = Y -> R\n",
    "        R = np.linalg.solve(L.T, Y)\n",
    "    except np.linalg.LinAlgError:\n",
    "        R = np.linalg.solve(A, P)\n",
    "    return R\n",
    "\n",
    "def shrinkage_matrix_stable(P, S, jitter=0.0):\n",
    "    \"\"\"\n",
    "    Stabil beregning av R = (P+S)^{-1} P via\n",
    "    R = P^{1/2} (I + P^{-1/2} S P^{-1/2})^{-1} P^{1/2}.\n",
    "    Krever at P er diagonal (positiv).\n",
    "    \"\"\"\n",
    "    d = np.diag(P).astype(float)\n",
    "    # Guardrails: ingen nuller/NaN/negativ\n",
    "    eps = 1e-12\n",
    "    d = np.clip(d, eps, np.finfo(float).max)\n",
    "    Phalf    = np.diag(np.sqrt(d))\n",
    "    Pinvhalf = np.diag(1.0 / np.sqrt(d))\n",
    "\n",
    "    M = Pinvhalf @ S @ Pinvhalf\n",
    "    # Jitter for SPD-sikkerhet (skader ikke i praksis)\n",
    "    if jitter > 0:\n",
    "        M = M + jitter * np.eye(M.shape[0])\n",
    "\n",
    "    # (I + M) er SPD -> Cholesky\n",
    "    I = np.eye(M.shape[0])\n",
    "    L = np.linalg.cholesky(I + M)\n",
    "    # (I+M)^{-1} P^{1/2} = (L^T)^{-1} (L)^{-1} P^{1/2}\n",
    "    Z = np.linalg.solve(L, Phalf)\n",
    "    W = np.linalg.solve(L.T, Z)\n",
    "    # R = P^{1/2} * W\n",
    "    #R = Phalf @ W\n",
    "    R = Pinvhalf @ W\n",
    "    # Symmetrer (numerisk)\n",
    "    R = 0.5 * (R + R.T)\n",
    "    return R\n",
    "\n",
    "def shrinkage_eigs_and_df(P, S):\n",
    "    \"\"\"Returner r-eigenverdier og df_eff i P-whitnede koordinater.\"\"\"\n",
    "    d = np.diag(P).astype(float)\n",
    "    eps = 1e-12\n",
    "    Pinvhalf = np.diag(1.0 / np.sqrt(np.maximum(d, eps)))\n",
    "\n",
    "    M = Pinvhalf @ S @ Pinvhalf          # SPD\n",
    "    mu = np.linalg.eigvalsh(M)           # >= 0\n",
    "    r = 1.0 / (1.0 + mu)                 # i (0,1]\n",
    "    df_eff = np.sum(1.0 - r)             # = sum mu/(1+mu) >= 0\n",
    "    return r, df_eff\n",
    "\n",
    "def extract_model_draws(\n",
    "    fit_dict,\n",
    "    model: str,\n",
    "    *,\n",
    "    lambda_effective_candidates = (\"lambda_tilde\", \"lambda_tilde_data\"),\n",
    "    lambda_raw_candidates       = (\"lambda\", \"lambda_data\"),\n",
    "    include_phi_for_dirichlet: bool = True,\n",
    "    phi_name: str = \"phi_data\",\n",
    "    lambda_kind: str = \"effective\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns draws with a flexible way to pick lambda:\n",
    "\n",
    "      W_all      : (D, H, p)\n",
    "      b_all      : (D, H)\n",
    "      v_all      : (D, H)\n",
    "      c_all      : (D,)\n",
    "      sigma_all  : (D,)\n",
    "      tau_w_all  : (D,)\n",
    "      tau_v_all  : (D,)   (ones if not present)\n",
    "      lambda_all : (D, H, p)       <-- chosen lambda (effective/raw) per `lambda_kind`\n",
    "      [lambda_raw_all]             <-- ONLY if lambda_kind == 'both'\n",
    "\n",
    "    Conventions:\n",
    "    - 'effective' lambda = the *regularized* local factor actually used in the weight std/var\n",
    "      (e.g., `lambda_tilde` or `lambda_tilde_data`), optionally multiplied by `phi_data`\n",
    "      for Dirichlet-type models if `include_phi_for_dirichlet=True`.\n",
    "    - 'raw' lambda = the *unregularized* half-Cauchy parameter (e.g., `lambda` or `lambda_data`).\n",
    "\n",
    "    Notes:\n",
    "    - If the model looks Gaussian (by name or because 'tau' is absent), lambdas default to ones.\n",
    "    - Shapes are coerced to (D,H,p) when possible; transposes are handled automatically.\n",
    "    \"\"\"\n",
    "\n",
    "    post = fit_dict[model]['posterior']\n",
    "\n",
    "    def _stan_var_or_none(name):\n",
    "        try:\n",
    "            return np.asarray(post.stan_variable(name))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _coerce_DHp(arr, D, H, p):\n",
    "        \"\"\"Coerce Stan draws to shape (D,H,p). Accepts (D,H,p) or (D,p,H) or (D,p,H,1)/(D,1,H,p).\"\"\"\n",
    "        if arr is None:\n",
    "            return None\n",
    "        shp = arr.shape\n",
    "        if shp == (D, H, p):\n",
    "            return arr\n",
    "        if shp == (D, p, H):\n",
    "            return np.transpose(arr, (0, 2, 1))\n",
    "        # Common fallbacks (rare):\n",
    "        if len(shp) == 4 and shp[0] == D:\n",
    "            # drop singleton dims and retry\n",
    "            squeezed = np.squeeze(arr)\n",
    "            return _coerce_DHp(squeezed, D, H, p)\n",
    "        raise ValueError(f\"Cannot coerce lambda/phi array of shape {shp} to (D,{H},{p}).\")\n",
    "\n",
    "    # === Core weights/bias/sigma ===\n",
    "    # W_1: (D, p, H) -> (D, H, p)\n",
    "    W_1 = _stan_var_or_none(\"W_1\")\n",
    "    if W_1 is None:\n",
    "        raise ValueError(\"Missing 'W_1' in posterior.\")\n",
    "    W_all = np.transpose(W_1, (0, 2, 1))\n",
    "    D, H, p = W_all.shape\n",
    "\n",
    "    # W_L: (D, H, out_nodes=1) -> (D, H)\n",
    "    W_L = _stan_var_or_none(\"W_L\")\n",
    "    if W_L is None:\n",
    "        raise ValueError(\"Missing 'W_L' in posterior.\")\n",
    "    v_all = W_L.reshape(D, -1)\n",
    "\n",
    "    # hidden_bias: (D, 1, H) -> (D, H)\n",
    "    b_1 = _stan_var_or_none(\"hidden_bias\")\n",
    "    if b_1 is None:\n",
    "        raise ValueError(\"Missing 'hidden_bias' in posterior.\")\n",
    "    b_all = b_1.reshape(D, -1)\n",
    "\n",
    "    # output_bias: (D, 1) -> (D,)\n",
    "    b_2 = _stan_var_or_none(\"output_bias\")\n",
    "    if b_2 is None:\n",
    "        raise ValueError(\"Missing 'output_bias' in posterior.\")\n",
    "    c_all = b_2.reshape(D)\n",
    "\n",
    "    # sigma\n",
    "    sigma_all = _stan_var_or_none(\"sigma\")\n",
    "    if sigma_all is None:\n",
    "        raise ValueError(\"Missing 'sigma' in posterior.\")\n",
    "    sigma_all = sigma_all.reshape(D)\n",
    "\n",
    "    # Detect model types (best-effort)\n",
    "    is_gauss     = (\"Gaussian\" in model) or (_stan_var_or_none(\"tau\") is None)\n",
    "    is_dirichlet = (\"Dirichlet\" in model) or (\"DST\" in model)\n",
    "    is_rhs       = (\"Regularized Horseshoe\" in model)\n",
    "\n",
    "    # tau_w / tau_v\n",
    "    if is_gauss:\n",
    "        tau_w_all = np.ones(D)\n",
    "        tau_v_all = np.ones(D)\n",
    "    else:\n",
    "        tau_w = _stan_var_or_none(\"tau\")\n",
    "        if tau_w is None:\n",
    "            # Fallback if naming differs\n",
    "            tau_w = _stan_var_or_none(\"tau_w\")\n",
    "        tau_w_all = tau_w.reshape(D)\n",
    "\n",
    "        tau_v = _stan_var_or_none(\"tau_v\")\n",
    "        tau_v_all = np.ones(D) if tau_v is None else tau_v.reshape(D)\n",
    "\n",
    "    # === Lambda extraction ===\n",
    "    # (1) Effective lambda (regularized)\n",
    "    lam_eff = None\n",
    "    if not is_gauss:\n",
    "        for nm in lambda_effective_candidates:\n",
    "            arr = _stan_var_or_none(nm)\n",
    "            if arr is not None:\n",
    "                lam_eff = _coerce_DHp(arr, D, H, p)\n",
    "                break\n",
    "\n",
    "    # (2) Raw lambda (half-Cauchy)\n",
    "    lam_raw = None\n",
    "    if not is_gauss:\n",
    "        for nm in lambda_raw_candidates:\n",
    "            arr = _stan_var_or_none(nm)\n",
    "            if arr is not None:\n",
    "                lam_raw = _coerce_DHp(arr, D, H, p)\n",
    "                break\n",
    "\n",
    "    # (3) Optional Dirichlet multiplier phi_data\n",
    "    phi_hp = None\n",
    "    if include_phi_for_dirichlet and is_dirichlet:\n",
    "        phi_arr = _stan_var_or_none(phi_name)\n",
    "        if phi_arr is not None:\n",
    "            phi_hp = _coerce_DHp(phi_arr, D, H, p)\n",
    "\n",
    "    # Build the \"lambda_all\" to return per lambda_kind\n",
    "    ones_DHp = np.ones((D, H, p))\n",
    "\n",
    "    def _with_phi(lam):\n",
    "        if lam is None:\n",
    "            return None\n",
    "        return lam * (phi_hp if phi_hp is not None else 1.0)\n",
    "\n",
    "    # Defaults for Gaussian or missing variables\n",
    "    lambda_eff_all = None\n",
    "    lambda_raw_all = None\n",
    "\n",
    "    if is_gauss:\n",
    "        lambda_eff_all = ones_DHp\n",
    "        lambda_raw_all = ones_DHp\n",
    "    else:\n",
    "        # effective\n",
    "        if lam_eff is not None:\n",
    "            lambda_eff_all = _with_phi(lam_eff) if is_dirichlet else lam_eff\n",
    "        else:\n",
    "            # if effective is missing, fall back gracefully\n",
    "            lambda_eff_all = _with_phi(lam_raw) if (is_dirichlet and lam_raw is not None) else (lam_raw if lam_raw is not None else ones_DHp)\n",
    "\n",
    "        # raw\n",
    "        if lam_raw is not None:\n",
    "            lambda_raw_all = lam_raw\n",
    "        else:\n",
    "            # if raw not present, fall back to effective or ones\n",
    "            lambda_raw_all = lam_eff if lam_eff is not None else ones_DHp\n",
    "\n",
    "    # === Return ===\n",
    "    if lambda_kind == \"effective\":\n",
    "        return W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_eff_all\n",
    "    elif lambda_kind == \"raw\":\n",
    "        return W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_raw_all\n",
    "    elif lambda_kind == \"both\":\n",
    "        # returns 9 items (adds lambda_raw_all at the end)\n",
    "        return W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_eff_all, lambda_raw_all\n",
    "    else:\n",
    "        raise ValueError(\"lambda_kind must be one of {'effective','raw','both'}.\")\n",
    "\n",
    "# ------- Knyt alt sammen -------\n",
    "\n",
    "def compute_shrinkage_for_W_block(\n",
    "    X: np.ndarray,\n",
    "    W0: np.ndarray, b0: np.ndarray, v0: np.ndarray,\n",
    "    noise: float, tau_w: float, tau_v: float,\n",
    "    lambda_tilde: np.ndarray,\n",
    "    activation: str = \"tanh\",\n",
    "    include_b1_in_Sigma: bool = True,\n",
    "    include_b2_in_Sigma: bool = True,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returnerer (R, P, S, Sigma_y) der R = (P+S)^{-1} P for W-blokken.\n",
    "    \"\"\"\n",
    "    Phi_mat, JW, Jb1, Jb2 = build_hidden_and_jacobian_W(X, W0, b0, v0, activation=activation)  # (n,H), (n,Hp)\n",
    "    #Sigma_y = build_Sigma_y(Phi_mat, tau_v=tau_v, J_b1=Jb1, J_b2=Jb2, noise=noise)                       # (n,n)\n",
    "    Sigma_y = build_Sigma_y(\n",
    "        Phi_mat,\n",
    "        tau_v=tau_v,\n",
    "        noise=noise,\n",
    "        J_b1=Jb1,\n",
    "        J_b2=Jb2,\n",
    "        include_b1=include_b1_in_Sigma,\n",
    "        include_b2=include_b2_in_Sigma,\n",
    "    )\n",
    "    P = build_P_from_lambda_tau(lambda_tilde, tau_w=tau_w)                        # (Hp,Hp)\n",
    "    S = build_S(JW, Sigma_y)                                                      # (Hp,Hp)\n",
    "    R = shrinkage_matrix_stable(P, S)                                                    # (Hp,Hp)\n",
    "    return R, P, S, Sigma_y, JW, Phi_mat\n",
    "\n",
    "def compute_shrinkage(\n",
    "    X,\n",
    "    W_all, b_all, v_all,          # (D,H,p), (D,H), (D,H)\n",
    "    sigma_all, tau_w_all, tau_v_all,  # (D,), (D,), (D,)\n",
    "    lambda_all,                   # (D,H,p)\n",
    "    activation=\"tanh\",\n",
    "    return_mats=True,             # set False if you only want summaries\n",
    "    include_b1_in_Sigma: bool = True,\n",
    "    include_b2_in_Sigma: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop over draws and compute R=(P+S)^{-1}P per draw using your single-draw function.\n",
    "    Returns:\n",
    "      R_stack : (D, N, N) with N=H*p  (if return_mats=True, else None)\n",
    "      r_eigs  : (D, N)  sorted eigenvalues in [0,1]\n",
    "      df_eff  : (D,)    effective dof = tr(I-R) = N - tr(R)\n",
    "    \"\"\"\n",
    "    D, H, p = W_all.shape\n",
    "    N = H * p\n",
    "\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    S_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    P_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    G_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    shrink_stack= np.empty((D, N, N)) if return_mats else None\n",
    "    r_eigs  = np.empty((D, N))\n",
    "    df_eff  = np.empty(D)\n",
    "\n",
    "    for d in range(D):\n",
    "        R, P, S, Sigma_y, _, _ = compute_shrinkage_for_W_block(\n",
    "            X=X,\n",
    "            W0=W_all[d],\n",
    "            b0=b_all[d],\n",
    "            v0=v_all[d],\n",
    "            noise=float(sigma_all[d]),\n",
    "            tau_w=float(tau_w_all[d]),\n",
    "            tau_v=float(tau_v_all[d]),\n",
    "            lambda_tilde=lambda_all[d],\n",
    "            activation=activation,\n",
    "            include_b1_in_Sigma=include_b1_in_Sigma,\n",
    "            include_b2_in_Sigma=include_b2_in_Sigma,\n",
    "        )\n",
    "        p = np.diag(P)                       \n",
    "        P_inv_sqrt = np.diag(1.0/np.sqrt(p))         \n",
    "        G = P_inv_sqrt @ S @ P_inv_sqrt \n",
    "        I = np.identity(N)\n",
    "        shrink_mat = np.linalg.inv(I + G)@G\n",
    "\n",
    "        if return_mats:\n",
    "            R_stack[d] = R\n",
    "            S_stack[d] = S\n",
    "            P_stack[d] = P\n",
    "            G_stack[d] = G\n",
    "            shrink_stack[d] = shrink_mat\n",
    "        \n",
    "\n",
    "\n",
    "        r, df = shrinkage_eigs_and_df(P, S)\n",
    "        r_eigs[d] = np.sort(r)\n",
    "        df_eff[d] = df\n",
    "\n",
    "    return R_stack, S_stack, P_stack, G_stack, shrink_stack, r_eigs, df_eff\n",
    "\n",
    "\n",
    "# ---------- Minimal kjøreeksempel ----------\n",
    "#draw = 0\n",
    "#W, b1, v, b2, noise, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Gaussian tanh')\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Gaussian tanh'\n",
    ")\n",
    "R_gauss, S_gauss, P_gauss, G_gauss, shrink_gauss, eigs_gauss, df_gauss = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with Gauss\")\n",
    "\n",
    "#W, b1, v, b2, noise, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Regularized Horseshoe tanh')\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Regularized Horseshoe tanh'\n",
    ")\n",
    "# YOU CAN ALSO LOOK AT THE RAW LAMBDA VALUES BY RUNNING:\n",
    "# W, b1, v, b2, noise, tau_w, tau_v, lambda_raw = extract_model_draws(\n",
    "#     posterior_N100_fits, model='Regularized Horseshoe tanh', lambda_kind='raw'\n",
    "# )\n",
    "R_RHS, S_RHS, P_RHS, G_RHS, shrink_RHS, eigs_RHS, df_eff_RHS = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with RHS\")\n",
    "#W, b1, v, b2, noise, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Horseshoe tanh')\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Dirichlet Horseshoe tanh'\n",
    ")\n",
    "R_DHS, S_DHS, P_DHS, G_DHS, shrink_DHS, eigs_DHS, df_eff_DHS = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with DHS\")\n",
    "#W, b1, v, b2, noise, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Student T tanh')\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    tanh_fit, model='Dirichlet Student T tanh'\n",
    ")\n",
    "R_DST, S_DST, P_DST, G_DST, shrink_DST, eigs_DST, df_eff_DST = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with DST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = np.zeros(4000)\n",
    "for i in range(4000):\n",
    "    check[i] = np.all(np.linalg.eigvals(S_DHS[i]) >= 0)\n",
    "\n",
    "print(np.sum(check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on SPD for Abalone:\n",
    "\n",
    "all Gaussian P matrices are spd, 3999/4000 S matrices are spd\n",
    "\n",
    "all RHS P matrices are spd, 3921/4000 RHS S matrices are spd\n",
    "\n",
    "all DHS P matrices are spd, 3980/4000 DHS S matrices are spd\n",
    "\n",
    "all DST P matrices are spd, 3983/4000 DST S matrices are spd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi, jw, jb_1, jb_2 = build_hidden_and_jacobian_W(\n",
    "    X = X,\n",
    "    W0 = W[0],\n",
    "    b0 = b1[0],\n",
    "    v0 = v[0],\n",
    "    activation=\"tanh\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QQ_T = (phi@phi.T) + (jb_1@jb_1.T) + np.outer(jb_2, jb_2)\n",
    "np.linalg.norm(QQ_T, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eigh\n",
    "u = np.zeros()\n",
    "eigvals, eigvecs = eigh(S_gauss[0], P_gauss[0], eigvals_only=False)#, subset_by_index=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import PowerNorm, Normalize, LinearSegmentedColormap\n",
    "import numpy as np\n",
    "\n",
    "def clamp_small(x, tol=1e-9):\n",
    "    return 0.0 if abs(x) < tol else x\n",
    "\n",
    "def add_block_grid(ax, H, p, color=\"w\", lw=0.5):\n",
    "    Hp = H*p\n",
    "    for h in range(1, H):\n",
    "        k = h*p\n",
    "        ax.axhline(k-0.5, color=color, lw=lw)\n",
    "        ax.axvline(k-0.5, color=color, lw=lw)\n",
    "\n",
    "def asymmetric_diverging_cmap(vmin, vmax, neg='#2b6cb0', pos='#d53e4f', eps=1e-12):\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "        vmin, vmax = -1.0, 1.0\n",
    "    p = float(np.clip((0.0 - vmin) / (vmax - vmin), 0.0, 1.0))\n",
    "    # Ensure black at the end if 0 coincides with vmin or vmax\n",
    "    if p <= eps:\n",
    "        colors = [(0.0, 'black'), (1.0, pos)]\n",
    "    elif p >= 1.0 - eps:\n",
    "        colors = [(0.0, neg), (1.0, 'black')]\n",
    "    else:\n",
    "        colors = [(0.0, neg), (p, 'black'), (1.0, pos)]\n",
    "    return LinearSegmentedColormap.from_list('asym_neg_black_pos', colors, N=256)\n",
    "\n",
    "def visualize_models(\n",
    "    matrices, names, H=16, p=10, use_abs=False, q_low=0.05, q_high=0.99,\n",
    "    neg_color='#2b6cb0', pos_color='#d53e4f'\n",
    "):\n",
    "    \"\"\"\n",
    "    Per-model color scale:\n",
    "      - vmin = q_low quantile of values in that matrix\n",
    "      - vmax = q_high quantile\n",
    "      - 0 maps to black, and sits at the correct fractional position between vmin and vmax.\n",
    "    \"\"\"\n",
    "    mats = [np.abs(M) if use_abs else M for M in matrices]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10), dpi=150, constrained_layout=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for ax, M, title in zip(axes, mats, names):\n",
    "        vals = M[np.isfinite(M)]\n",
    "        if vals.size == 0:\n",
    "            vmin, vmax = -1.0, 1.0\n",
    "        else:\n",
    "            vmin = float(np.quantile(vals, q_low))\n",
    "            vmax = float(np.quantile(vals, q_high))\n",
    "            # ensure 0 is inside range so black appears; nudge if necessary\n",
    "            eps = 1e-12\n",
    "            if vmin >= 0: vmin = -eps\n",
    "            if vmax <= 0: vmax =  eps\n",
    "\n",
    "        # Etter at du har beregnet vmin, vmax:\n",
    "        vmin = clamp_small(vmin, tol=1e-9)\n",
    "        vmax = clamp_small(vmax, tol=1e-9)\n",
    "\n",
    "        # Sikre at området ikke kollapser:\n",
    "        if abs(vmax - vmin) < 1e-12:\n",
    "            vmin, vmax = -1e-9, 1e-9\n",
    "        # Build an asymmetric diverging cmap for THIS panel, then use linear Normalize\n",
    "        cmap = asymmetric_diverging_cmap(vmin, vmax, neg=neg_color, pos=pos_color)\n",
    "        norm = PowerNorm(gamma=0.5, vmin=vmin, vmax=vmax)   # gamma<1 boosts mid values; >1 compresses\n",
    "        #norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "        im = ax.imshow(M, aspect='equal', interpolation='nearest', cmap=cmap, norm=norm)\n",
    "\n",
    "        # Optional: draw your block grid if you have this helper defined elsewhere\n",
    "        try:\n",
    "            add_block_grid(ax, H, p)\n",
    "        except NameError:\n",
    "            pass\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Columns\"); ax.set_ylabel(\"Rows\")\n",
    "\n",
    "        cb = fig.colorbar(im, ax=ax, shrink=1.0)#, pad=0.02)\n",
    "        cb.set_label(\"Value\")\n",
    "\n",
    "        # Put a tick at zero explicitly\n",
    "        ticks = np.linspace(vmin, vmax, 5)\n",
    "        ticks[2] = 0.0\n",
    "        ticks.sort()\n",
    "        cb.set_ticks(ticks)\n",
    "\n",
    "        print(f\"{title}: vmin (q={q_low}) = {vmin:.2g}, vmax (q={q_high}) = {vmax:.2g}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def _symmetrize(M):\n",
    "    return 0.5*(M + M.swapaxes(-1, -2))\n",
    "\n",
    "def _spd_log(M, eps=1e-8):\n",
    "    M = _symmetrize(M)\n",
    "    w, U = np.linalg.eigh(M + eps*np.eye(M.shape[-1]))\n",
    "    return U @ np.diag(np.log(np.clip(w, eps, None))) @ U.T\n",
    "\n",
    "def _spd_exp(M):\n",
    "    M = _symmetrize(M)\n",
    "    w, U = np.linalg.eigh(M)\n",
    "    return U @ np.diag(np.exp(w)) @ U.T\n",
    "\n",
    "def log_euclidean_median(stack, eps=1e-8):\n",
    "    \"\"\"\n",
    "    stack: (D, N, N), SPD/PSD. Uses elementwise median in log-domain.\n",
    "    Returns: (N, N) SPD.\n",
    "    \"\"\"\n",
    "    D, N, _ = stack.shape\n",
    "    logs = np.empty_like(stack)\n",
    "    for d in range(D):\n",
    "        logs[d] = _spd_log(stack[d], eps=eps)\n",
    "    med_log = np.median(logs, axis=0)\n",
    "    return _spd_exp(med_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_S = [\n",
    "    log_euclidean_median(S_gauss),\n",
    "    log_euclidean_median(S_RHS),\n",
    "    log_euclidean_median(S_DHS),\n",
    "    log_euclidean_median(S_DST),\n",
    "]\n",
    "names_S = [\"S (Gauss)\", \"S (RHS)\", \"S (DHS)\", \"S (DST)\"]\n",
    "\n",
    "matrices_G = [\n",
    "    log_euclidean_median(G_gauss),\n",
    "    log_euclidean_median(G_RHS),\n",
    "    log_euclidean_median(G_DHS),\n",
    "    log_euclidean_median(G_DST),\n",
    "]\n",
    "\n",
    "names_G = [\"G (Gauss)\", \"G (RHS)\", \"G (DHS)\", \"G (DST)\"]\n",
    "\n",
    "matrices_shrink = [\n",
    "    log_euclidean_median(shrink_gauss),\n",
    "    log_euclidean_median(shrink_RHS),\n",
    "    log_euclidean_median(shrink_DHS),\n",
    "    log_euclidean_median(shrink_DST),\n",
    "]\n",
    "\n",
    "names_shrink = [\"(I+G)^{-1}G (Gauss)\", \"(I+G)^{-1}G (RHS)\", \"(I+G)^{-1}G (DHS)\", \"(I+G)^{-1}G (DST)\"]\n",
    "\n",
    "SP_inv_S_gauss = np.eye(16*8)[:, :] - R_gauss\n",
    "SP_inv_S_RHS = np.eye(16*8)[:, :] - R_RHS\n",
    "SP_inv_S_DHS = np.eye(16*8)[:, :] - R_DHS\n",
    "SP_inv_S_DST = np.eye(16*8)[:, :] - R_DST\n",
    "\n",
    "matrices_operator = [\n",
    "    log_euclidean_median(SP_inv_S_gauss),\n",
    "    log_euclidean_median(SP_inv_S_RHS),\n",
    "    log_euclidean_median(SP_inv_S_DHS),\n",
    "    log_euclidean_median(SP_inv_S_DST),\n",
    "]\n",
    "\n",
    "names_operator = [\"(P+S)^{-1}S (Gauss)\", \"(P+S)^{-1}S (RHS)\", \"(P+S)^{-1}S (DHS)\", \"(P+S)^{-1}S (DST)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_S, names_S, H=16, p=8, use_abs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_G, names_G, H=16, p=8, use_abs=False)#, cmap=\"magma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_shrink, names_shrink, H=16, p=8, use_abs=False)#, cmap=\"magma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_operator, names_operator, H=16, p=8, use_abs=False)#, cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Traces as distributions (df_eff = tr(R) vs total shrinkage = tr(I-R)) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Effective dof: trace of (I+G)^{-1}G per draw\n",
    "tr_R_gauss = np.trace(shrink_gauss, axis1=1, axis2=2)\n",
    "tr_R_RHS   = np.trace(shrink_RHS,   axis1=1, axis2=2)\n",
    "tr_R_DHS   = np.trace(shrink_DHS,   axis1=1, axis2=2)\n",
    "tr_R_DST   = np.trace(shrink_DST,   axis1=1, axis2=2)\n",
    "\n",
    "# If you also want “total shrinkage”, use your SP_inv_S_* stacks (I - R):\n",
    "tr_SPinvS_gauss = np.trace(SP_inv_S_gauss, axis1=1, axis2=2)\n",
    "tr_SPinvS_RHS   = np.trace(SP_inv_S_RHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DHS   = np.trace(SP_inv_S_DHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DST   = np.trace(SP_inv_S_DST,   axis1=1, axis2=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot df_eff distributions\n",
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_R_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_R_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_R_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_R_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(\"trace((I+G)^{-1}G)  [effective dof]\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_SPinvS_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_SPinvS_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_SPinvS_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_SPinvS_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(r\"$tr((P+S)^{-1}S)$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Median eigenvalue curve (with bands) for shrink stacks ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def median_eigcurve(stack, q_lo=0.1, q_hi=0.9):\n",
    "    \"\"\"\n",
    "    stack: (D, N, N) of symmetric PSD matrices with eigenvalues in [0,1].\n",
    "    Returns: dict with 'median', 'lo', 'hi' over the sorted eigenvalues (descending).\n",
    "    \"\"\"\n",
    "    D, N, _ = stack.shape\n",
    "    evals = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        w = np.linalg.eigvalsh(stack[d])\n",
    "        evals[d] = np.sort(w)[::-1]  # descending\n",
    "    med = np.median(evals, axis=0)\n",
    "    lo  = np.quantile(evals, q_lo, axis=0)\n",
    "    hi  = np.quantile(evals, q_hi, axis=0)\n",
    "    return {\"median\": med, \"lo\": lo, \"hi\": hi}\n",
    "\n",
    "curves = {\n",
    "    \"Gauss\": median_eigcurve(shrink_gauss),\n",
    "    \"RHS\":   median_eigcurve(shrink_RHS),\n",
    "    \"DHS\":   median_eigcurve(shrink_DHS),\n",
    "    \"DST\":   median_eigcurve(shrink_DST),\n",
    "}\n",
    "\n",
    "# Plot 2x2 small multiples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8,6), dpi=150, constrained_layout=True)\n",
    "axes = axes.ravel()\n",
    "for ax, (name, c) in zip(axes, curves.items()):\n",
    "    x = np.arange(1, len(c[\"median\"])+1)\n",
    "    ax.plot(x, c[\"median\"], lw=1.8, label=f\"{name} median\")\n",
    "    ax.fill_between(x, c[\"lo\"], c[\"hi\"], alpha=0.25, label=f\"{name} {10}-{90}%\", step=None)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"eigenvalue rank\")\n",
    "    ax.set_ylabel(\"eigenvalue of (I+G)^{-1}G\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"upper right\", fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: blockwise normalized inner-product ratio for a stack of G ---\n",
    "import numpy as np\n",
    "\n",
    "def block_ratio_median(G_stack: np.ndarray, H: int, p: int, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    For each draw/d, computes R_d = |G_d| / sqrt(diag(G_d) diag(G_d)^T) within each (p x p) block h.\n",
    "    Returns the elementwise median over draws (ignoring cross-block entries).\n",
    "    \n",
    "    G_stack : (D, N, N) with N = H*p\n",
    "    Output  : (N, N) median ratio; outside-block entries set to 0 for convenience.\n",
    "    \"\"\"\n",
    "    D, N, _ = G_stack.shape\n",
    "    # block mask: True if (i,j) are in the same hidden-unit block\n",
    "    idx = np.arange(N)\n",
    "    h_idx = idx // p\n",
    "    same_block = (h_idx[:, None] == h_idx[None, :])\n",
    "    \n",
    "    ratios = np.empty_like(G_stack)\n",
    "    for d in range(D):\n",
    "        G = 0.5 * (G_stack[d] + G_stack[d].T)                     # symmetrize\n",
    "        dvec = np.clip(np.diag(G), eps, None)\n",
    "        denom = np.sqrt(np.outer(dvec, dvec))\n",
    "        R = np.divide(np.abs(G), denom, out=np.zeros_like(G), where=(denom > 0))\n",
    "        # keep only within-block entries; mark cross-block as NaN (ignored in median)\n",
    "        R_masked = np.where(same_block, R, np.nan)\n",
    "        ratios[d] = R_masked\n",
    "\n",
    "    # median over draws ignoring NaNs (i.e., cross-block)\n",
    "    R_med = np.nanmedian(ratios, axis=0)\n",
    "    # put zeros outside blocks for cleaner plotting\n",
    "    R_med = np.where(same_block, R_med, 0.0)\n",
    "    # exact ones on the diagonal by definition\n",
    "    np.fill_diagonal(R_med, 1.0)\n",
    "    return R_med\n",
    "\n",
    "# --- Compute median ratio matrices for the four priors ---\n",
    "H, p = 16, 8  # adjust if needed\n",
    "ratio_Gauss = block_ratio_median(G_gauss, H=H, p=p)\n",
    "ratio_RHS   = block_ratio_median(G_RHS,   H=H, p=p)\n",
    "ratio_DHS   = block_ratio_median(G_DHS,   H=H, p=p)\n",
    "ratio_DST   = block_ratio_median(G_DST,   H=H, p=p)\n",
    "\n",
    "\n",
    "# --- (Optional) Scalar summaries: median off-diagonal ratio within blocks ---\n",
    "def offdiag_block_stats(R_med: np.ndarray, H: int, p: int):\n",
    "    N = H * p\n",
    "    idx = np.arange(N)\n",
    "    h_idx = idx // p\n",
    "    same_block = (h_idx[:, None] == h_idx[None, :])\n",
    "    offdiag = same_block & (~np.eye(N, dtype=bool))\n",
    "    vals = R_med[offdiag]\n",
    "    return {\n",
    "        \"median_offdiag\": float(np.median(vals)),\n",
    "        \"q10_offdiag\": float(np.quantile(vals, 0.10)),\n",
    "        \"q90_offdiag\": float(np.quantile(vals, 0.90)),\n",
    "    }\n",
    "\n",
    "print(\"Gauss:\", offdiag_block_stats(ratio_Gauss, H, p))\n",
    "print(\"RHS:  \", offdiag_block_stats(ratio_RHS,   H, p))\n",
    "print(\"DHS:  \", offdiag_block_stats(ratio_DHS,   H, p))\n",
    "print(\"DST:  \", offdiag_block_stats(ratio_DST,   H, p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (1) Bound Σ_y^{-1}: compute c_min, c_max for a single draw ---\n",
    "import numpy as np\n",
    "\n",
    "def sigma_inverse_bounds(Phi_mat, tau_v, noise, J_b1=None, J_b2=None,\n",
    "                         include_b1=True, include_b2=True):\n",
    "    \"\"\"\n",
    "    Constructs Q so that Σ_y = σ^2 I + Q Q^T, then returns:\n",
    "      c_min = 1 / (σ^2 + ||Q||_2^2),   c_max = 1 / σ^2.\n",
    "    Here Q = [ J_b1,  τ_v * Φ,  (J_b2) ] with columns included per flags.\n",
    "    \"\"\"\n",
    "    cols = [tau_v * Phi_mat]  # Φ always contributes with τ_v\n",
    "    if include_b1 and (J_b1 is not None):\n",
    "        cols.insert(0, J_b1)  # [J_b1, τ_v Φ, ...]\n",
    "    if include_b2 and (J_b2 is not None):\n",
    "        cols.append(J_b2[:, None])  # add as a column\n",
    "\n",
    "    Q = np.concatenate(cols, axis=1) if len(cols) > 1 else cols[0]\n",
    "    # spectral norm of Q (largest singular value)\n",
    "    smax = np.linalg.svd(Q, compute_uv=False)[0]\n",
    "    c_min = 1.0 / (noise**2 + smax**2)\n",
    "    c_max = 1.0 / (noise**2)\n",
    "    return c_min, c_max, smax\n",
    "\n",
    "# --- (2) Bound S and G via c_min, c_max; also get α-eigs of A = P^{-1/2} J^T J P^{-1/2} ---\n",
    "def bound_S_and_G(JW, P, c_min, c_max):\n",
    "    \"\"\"\n",
    "    S = J^T Σ_y^{-1} J, with c_min*J^T J ⪯ S ⪯ c_max*J^T J.\n",
    "    Define A = P^{-1/2} J^T J P^{-1/2}; then c_min*A ⪯ G ⪯ c_max*A, where G=P^{-1/2} S P^{-1/2}.\n",
    "    Returns:\n",
    "      JJ      : J^T J\n",
    "      A       : P^{-1/2} (J^T J) P^{-1/2}\n",
    "      alpha   : eigvals(A) sorted ascending\n",
    "      S_lo/up : lower/upper Loewner bounds on S\n",
    "      G_lo/up : lower/upper Loewner bounds on G\n",
    "    \"\"\"\n",
    "    JJ = JW.T @ JW\n",
    "    d = np.diag(P).astype(float)\n",
    "    Pinvhalf = np.diag(1.0 / np.sqrt(np.maximum(d, 1e-12)))\n",
    "    A = Pinvhalf @ JJ @ Pinvhalf\n",
    "    alpha = np.linalg.eigvalsh(A)  # >=0, ascending\n",
    "\n",
    "    S_lower = c_min * JJ\n",
    "    S_upper = c_max * JJ\n",
    "    G_lower = c_min * A\n",
    "    G_upper = c_max * A\n",
    "    return JJ, A, alpha, S_lower, S_upper, G_lower, G_upper\n",
    "\n",
    "# --- (3) Eigenvalue bands for (I+G)^{-1} G and trace/df bounds from α-eigs and c_min/c_max ---\n",
    "def shrinkage_bands_from_alpha(alpha, c_min, c_max):\n",
    "    \"\"\"\n",
    "    For f(t)=t/(1+t), eigenvalues of (I+G)^{-1}G satisfy:\n",
    "       f(c_min*alpha_i) <= λ_i <= f(c_max*alpha_i)\n",
    "    Returns lower/upper arrays (ascending to match alpha).\n",
    "    \"\"\"\n",
    "    f = lambda t: t / (1.0 + t)\n",
    "    lam_lo = f(c_min * alpha)\n",
    "    lam_hi = f(c_max * alpha)\n",
    "    return lam_lo, lam_hi\n",
    "\n",
    "def df_bounds_from_alpha(alpha, c_min, c_max):\n",
    "    \"\"\"Lower/upper bounds on df_eff = tr( (I+G)^{-1}G ).\"\"\"\n",
    "    lam_lo, lam_hi = shrinkage_bands_from_alpha(alpha, c_min, c_max)\n",
    "    return float(np.sum(lam_lo)), float(np.sum(lam_hi))\n",
    "\n",
    "# --- (4) Bands over ALL draws (median band to overlay with your median eigenvalue curve) ---\n",
    "def eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "):\n",
    "    \"\"\"\n",
    "    For each draw:\n",
    "      - build JW, Φ, J_b1, J_b2, get c_min/c_max,\n",
    "      - build P and α-eigenvalues of A,\n",
    "      - compute lower/upper shrinkage eigenvalue bands.\n",
    "    Returns medians (and optional quantiles) across draws:\n",
    "      lam_lo_med, lam_hi_med, df_lo_med, df_hi_med\n",
    "    \"\"\"\n",
    "    D, H, p = W_all.shape\n",
    "    N = H * p\n",
    "    lam_lo_stack = np.empty((D, N))\n",
    "    lam_hi_stack = np.empty((D, N))\n",
    "    df_lo = np.empty(D)\n",
    "    df_hi = np.empty(D)\n",
    "\n",
    "    for d in range(D):\n",
    "        Phi_mat, JW, Jb1, Jb2 = build_hidden_and_jacobian_W(\n",
    "            X, W_all[d], b_all[d], v_all[d], activation=activation\n",
    "        )\n",
    "        c_min, c_max, _ = sigma_inverse_bounds(\n",
    "            Phi_mat, tau_v=float(tau_v_all[d]), noise=float(sigma_all[d]),\n",
    "            J_b1=(Jb1 if include_b1_in_Sigma else None),\n",
    "            J_b2=(Jb2 if include_b2_in_Sigma else None),\n",
    "            include_b1=include_b1_in_Sigma, include_b2=include_b2_in_Sigma\n",
    "        )\n",
    "        P = build_P_from_lambda_tau(lambda_all[d], tau_w=float(tau_w_all[d]))\n",
    "\n",
    "        _, _, alpha, _, _, _, _ = bound_S_and_G(JW, P, c_min, c_max)\n",
    "        lam_lo, lam_hi = shrinkage_bands_from_alpha(alpha, c_min, c_max)\n",
    "        lam_lo_stack[d] = lam_lo\n",
    "        lam_hi_stack[d] = lam_hi\n",
    "        df_lo[d], df_hi[d] = df_bounds_from_alpha(alpha, c_min, c_max)\n",
    "\n",
    "    # Medians across draws (coordinate-wise)\n",
    "    lam_lo_med = np.median(lam_lo_stack, axis=0)\n",
    "    lam_hi_med = np.median(lam_hi_stack, axis=0)\n",
    "    df_lo_med  = float(np.median(df_lo))\n",
    "    df_hi_med  = float(np.median(df_hi))\n",
    "\n",
    "    # (Optional) also return 10/90% ribbons if you want\n",
    "    lam_lo_q10 = np.quantile(lam_lo_stack, 0.10, axis=0)\n",
    "    lam_hi_q90 = np.quantile(lam_hi_stack, 0.90, axis=0)\n",
    "\n",
    "    summary = {\n",
    "        \"df_lo_med\": df_lo_med,\n",
    "        \"df_hi_med\": df_hi_med,\n",
    "        \"df_lo_q10\": float(np.quantile(df_lo, 0.10)),\n",
    "        \"df_hi_q90\": float(np.quantile(df_hi, 0.90)),\n",
    "    }\n",
    "    return lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (5) Quick demo on one model (e.g., RHS): print Σ_y^{-1} bounds and df bounds, then plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: use your RHS arrays (replace with Gauss/DHS/DST as needed)\n",
    "W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_all = \\\n",
    "    extract_model_draws(posterior_N100_fits, model='Gaussian tanh', lambda_kind=\"effective\")\n",
    "\n",
    "# (a) Compute median eigenvalue bands across draws\n",
    "lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, band_summ = eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    ")\n",
    "\n",
    "print(\"Median df_eff bounds (Gaussian):\")\n",
    "print(f\"  lower  (Σ_y^{-1}≈1/(σ^2+||Q||^2)): {band_summ['df_lo_med']:.3f}\")\n",
    "print(f\"  upper  (Σ_y^{-1}≈1/σ^2):           {band_summ['df_hi_med']:.3f}\")\n",
    "\n",
    "# (b) If you ALREADY have the median eigenvalue curve of (I+G)^{-1}G (call it eig_med_shrink),\n",
    "#     great; otherwise compute it from shrink_stack (from compute_shrinkage)\n",
    "try:\n",
    "    nix #median_eigcurve(shrink_gauss) #eig_med_shrink\n",
    "except NameError:\n",
    "    # build shrink_stack and take median eigenvalues (ascending to match bands)\n",
    "    _, _, _, _, shrink_stack, _, _ = compute_shrinkage(\n",
    "        X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "        activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "    )\n",
    "    D, N, _ = shrink_stack.shape\n",
    "    eigs = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        eigs[d] = np.linalg.eigvalsh(shrink_stack[d])  # ascending\n",
    "    eig_med_shrink = np.median(eigs, axis=0)\n",
    "\n",
    "# (c) Overlay: median band vs. median eigenvalue curve\n",
    "x = np.arange(1, lam_lo_med.size + 1)\n",
    "plt.figure(figsize=(7.2, 4.2), dpi=150)\n",
    "plt.fill_between(x, lam_lo_med, lam_hi_med, alpha=0.25, label=\"theory band (median)\")\n",
    "plt.plot(x, eig_med_shrink, lw=1.8, label=\"empirical median eigenvalue\")\n",
    "# (optional) add thinner 10–90% theoretical ribbon\n",
    "plt.fill_between(x, lam_lo_q10, lam_hi_q90, alpha=0.15, label=\"theory band (10–90%)\")\n",
    "\n",
    "plt.xlabel(\"eigenvalue index (ascending)\")\n",
    "plt.ylabel(r\"eigenvalue of $(I+G)^{-1}G$\")\n",
    "plt.title(\"Shrinkage eigenvalue bands vs. empirical curve (Gaussian)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (5) Quick demo on one model (e.g., RHS): print Σ_y^{-1} bounds and df bounds, then plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: use your RHS arrays (replace with Gauss/DHS/DST as needed)\n",
    "W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_all = \\\n",
    "    extract_model_draws(posterior_N100_fits, model='Regularized Horseshoe tanh', lambda_kind=\"effective\")\n",
    "\n",
    "# (a) Compute median eigenvalue bands across draws\n",
    "lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, band_summ = eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    ")\n",
    "\n",
    "print(\"Median df_eff bounds (RHS):\")\n",
    "print(f\"  lower  (Σ_y^{-1}≈1/(σ^2+||Q||^2)): {band_summ['df_lo_med']:.3f}\")\n",
    "print(f\"  upper  (Σ_y^{-1}≈1/σ^2):           {band_summ['df_hi_med']:.3f}\")\n",
    "\n",
    "# (b) If you ALREADY have the median eigenvalue curve of (I+G)^{-1}G (call it eig_med_shrink),\n",
    "#     great; otherwise compute it from shrink_stack (from compute_shrinkage)\n",
    "try:\n",
    "    nix #median_eigcurve(shrink_RHS) #eig_med_shrink\n",
    "except NameError:\n",
    "    # build shrink_stack and take median eigenvalues (ascending to match bands)\n",
    "    _, _, _, _, shrink_stack, _, _ = compute_shrinkage(\n",
    "        X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "        activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "    )\n",
    "    D, N, _ = shrink_stack.shape\n",
    "    eigs = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        eigs[d] = np.linalg.eigvalsh(shrink_stack[d])  # ascending\n",
    "    eig_med_shrink = np.median(eigs, axis=0)\n",
    "\n",
    "# (c) Overlay: median band vs. median eigenvalue curve\n",
    "x = np.arange(1, lam_lo_med.size + 1)\n",
    "plt.figure(figsize=(7.2, 4.2), dpi=150)\n",
    "plt.fill_between(x, lam_lo_med, lam_hi_med, alpha=0.25, label=\"theory band (median)\")\n",
    "plt.plot(x, eig_med_shrink, lw=1.8, label=\"empirical median eigenvalue\")\n",
    "# (optional) add thinner 10–90% theoretical ribbon\n",
    "plt.fill_between(x, lam_lo_q10, lam_hi_q90, alpha=0.15, label=\"theory band (10–90%)\")\n",
    "\n",
    "plt.xlabel(\"eigenvalue index (ascending)\")\n",
    "plt.ylabel(r\"eigenvalue of $(I+G)^{-1}G$\")\n",
    "plt.title(\"Shrinkage eigenvalue bands vs. empirical curve (RHS)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (5) Quick demo on one model (e.g., RHS): print Σ_y^{-1} bounds and df bounds, then plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: use your RHS arrays (replace with Gauss/DHS/DST as needed)\n",
    "W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_all = \\\n",
    "    extract_model_draws(posterior_N100_fits, model='Dirichlet Horseshoe tanh', lambda_kind=\"effective\")\n",
    "\n",
    "# (a) Compute median eigenvalue bands across draws\n",
    "lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, band_summ = eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    ")\n",
    "\n",
    "print(\"Median df_eff bounds (DHS):\")\n",
    "print(f\"  lower  (Σ_y^{-1}≈1/(σ^2+||Q||^2)): {band_summ['df_lo_med']:.3f}\")\n",
    "print(f\"  upper  (Σ_y^{-1}≈1/σ^2):           {band_summ['df_hi_med']:.3f}\")\n",
    "\n",
    "# (b) If you ALREADY have the median eigenvalue curve of (I+G)^{-1}G (call it eig_med_shrink),\n",
    "#     great; otherwise compute it from shrink_stack (from compute_shrinkage)\n",
    "try:\n",
    "    nix #eig_med_shrink #median_eigcurve(shrink_DHS) \n",
    "except NameError:\n",
    "    # build shrink_stack and take median eigenvalues (ascending to match bands)\n",
    "    _, _, _, _, shrink_stack, _, _ = compute_shrinkage(\n",
    "        X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "        activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "    )\n",
    "    D, N, _ = shrink_stack.shape\n",
    "    eigs = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        eigs[d] = np.linalg.eigvalsh(shrink_stack[d])  # ascending\n",
    "    eig_med_shrink = np.median(eigs, axis=0)\n",
    "\n",
    "# (c) Overlay: median band vs. median eigenvalue curve\n",
    "x = np.arange(1, lam_lo_med.size + 1)\n",
    "plt.figure(figsize=(7.2, 4.2), dpi=150)\n",
    "plt.fill_between(x, lam_lo_med, lam_hi_med, alpha=0.25, label=\"theory band (median)\")\n",
    "plt.plot(x, eig_med_shrink, lw=1.8, label=\"empirical median eigenvalue\")\n",
    "# (optional) add thinner 10–90% theoretical ribbon\n",
    "plt.fill_between(x, lam_lo_q10, lam_hi_q90, alpha=0.15, label=\"theory band (10–90%)\")\n",
    "\n",
    "plt.xlabel(\"eigenvalue index (ascending)\")\n",
    "plt.ylabel(r\"eigenvalue of $(I+G)^{-1}G$\")\n",
    "plt.title(\"Shrinkage eigenvalue bands vs. empirical curve (DHS)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (5) Quick demo on one model (e.g., RHS): print Σ_y^{-1} bounds and df bounds, then plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: use your RHS arrays (replace with Gauss/DHS/DST as needed)\n",
    "W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_all = \\\n",
    "    extract_model_draws(posterior_N100_fits, model='Dirichlet Student T tanh', lambda_kind=\"effective\")\n",
    "\n",
    "# (a) Compute median eigenvalue bands across draws\n",
    "lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, band_summ = eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    ")\n",
    "\n",
    "print(\"Median df_eff bounds (DST):\")\n",
    "print(f\"  lower  (Σ_y^{-1}≈1/(σ^2+||Q||^2)): {band_summ['df_lo_med']:.3f}\")\n",
    "print(f\"  upper  (Σ_y^{-1}≈1/σ^2):           {band_summ['df_hi_med']:.3f}\")\n",
    "\n",
    "# (b) If you ALREADY have the median eigenvalue curve of (I+G)^{-1}G (call it eig_med_shrink),\n",
    "#     great; otherwise compute it from shrink_stack (from compute_shrinkage)\n",
    "try:\n",
    "    nix #median_eigcurve(shrink_DST) #eig_med_shrink\n",
    "except NameError:\n",
    "    # build shrink_stack and take median eigenvalues (ascending to match bands)\n",
    "    _, _, _, _, shrink_stack, _, _ = compute_shrinkage(\n",
    "        X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "        activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "    )\n",
    "    D, N, _ = shrink_stack.shape\n",
    "    eigs = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        eigs[d] = np.linalg.eigvalsh(shrink_stack[d])  # ascending\n",
    "    eig_med_shrink = np.median(eigs, axis=0)\n",
    "\n",
    "# (c) Overlay: median band vs. median eigenvalue curve\n",
    "x = np.arange(1, lam_lo_med.size + 1)\n",
    "plt.figure(figsize=(7.2, 4.2), dpi=150)\n",
    "plt.fill_between(x, lam_lo_med, lam_hi_med, alpha=0.25, label=\"theory band (median)\")\n",
    "plt.plot(x, eig_med_shrink, lw=1.8, label=\"empirical median eigenvalue\")\n",
    "# (optional) add thinner 10–90% theoretical ribbon\n",
    "plt.fill_between(x, lam_lo_q10, lam_hi_q90, alpha=0.15, label=\"theory band (10–90%)\")\n",
    "\n",
    "plt.xlabel(\"eigenvalue index (ascending)\")\n",
    "plt.ylabel(r\"eigenvalue of $(I+G)^{-1}G$\")\n",
    "plt.title(\"Shrinkage eigenvalue bands vs. empirical curve (DST)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build linearized $\\bar{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linearized_mean(\n",
    "    X, y,\n",
    "    W_1, b_1, W_2, b_2,          # (D,H,p), (D,H), (D,), (D,H)\n",
    "    noise_all, tau_w_all, tau_v_all,       # (D,), (D,), (D,)\n",
    "    lambda_all,                            # (D,H,p)\n",
    "    activation=\"tanh\",\n",
    "    return_mats=True,\n",
    "    include_b1_in_Sigma: bool = True,      # pass-through to your Σ_y builder (if used downstream)\n",
    "    include_b2_in_Sigma: bool = True,      # pass-through to your Σ_y builder (if used downstream)\n",
    "):\n",
    "    \"\"\"\n",
    "    Per draw d, compute:\n",
    "      - (R_d, P_d, S_d, Sigma_y_d, J_w,d, [J_b,d], Phi_0,d) from your local function\n",
    "      - y*_d = (y - Φ0_d w2,0_d [ - b2,0_d*1 ]) + J_w,d @ vec(W1,0_d) [ + J_b,d @ b1,0_d ]\n",
    "      - g_d  = J_w,d^T (Sigma_y_d^{-1} y*_d)              [via solve]\n",
    "      - bar_w_d = (P_d + S_d)^{-1} g_d                    [via solve]\n",
    "    Returns:\n",
    "      R_stack    : (D, N, N)  (None if return_mats=False)\n",
    "      w_bar_stack: (D, N)\n",
    "    \"\"\"\n",
    "\n",
    "    D, H, p = W_1.shape\n",
    "    N = H * p\n",
    "    n = y.shape[0]\n",
    "\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    w_bar_stack = np.empty((D, N))\n",
    "    w_hat_stack = np.empty((D, N))\n",
    "\n",
    "    y = np.asarray(y, dtype=float).reshape(n)\n",
    "    \n",
    "\n",
    "    for d in range(D):\n",
    "        Phi_mat, JW, Jb1, Jb2 = build_hidden_and_jacobian_W(X, W_1[d], b_1[d], W_2[d], activation=activation)  \n",
    "        Sigma_y = build_Sigma_y(\n",
    "            Phi_mat,\n",
    "            tau_v=tau_v_all[d],\n",
    "            noise=noise_all[d],\n",
    "            J_b1=Jb1,\n",
    "            J_b2=Jb2,\n",
    "            include_b1=include_b1_in_Sigma,\n",
    "            include_b2=include_b2_in_Sigma,\n",
    "        )\n",
    "        P = build_P_from_lambda_tau(lambda_all[d], tau_w=tau_w_all[d])                     \n",
    "        S = build_S(JW, Sigma_y)                                                    \n",
    "        R = shrinkage_matrix_stable(P, S) \n",
    "\n",
    "        w0_vec = W_1[d].reshape(-1)       \n",
    "        y_star = y + (JW @ w0_vec) + (Jb1 @ b1[d])\n",
    "\n",
    "        r = np.linalg.solve(Sigma_y, y_star)   \n",
    "        g = JW.T @ r                           \n",
    "\n",
    "        hat_w = np.linalg.solve(S, g) \n",
    "        bar_w = np.linalg.solve(P + S, g)     \n",
    "\n",
    "        if return_mats:\n",
    "            R_stack[d] = R\n",
    "\n",
    "        w_hat_stack[d] = hat_w\n",
    "        w_bar_stack[d] = bar_w\n",
    "\n",
    "    return R_stack, w_bar_stack, w_hat_stack\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(tanh_fit, model='Gaussian tanh')\n",
    "\n",
    "R_gauss, w_bar_stack_gauss, w_hat_stack_gauss = compute_linearized_mean(X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(tanh_fit, model='Regularized Horseshoe tanh')\n",
    "\n",
    "R_RHS, w_bar_stack_RHS, w_hat_stack_RHS = compute_linearized_mean(X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(tanh_fit, model='Dirichlet Horseshoe tanh')\n",
    "\n",
    "R_DHS, w_bar_stack_DHS, w_hat_stack_DHS = compute_linearized_mean(X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(tanh_fit, model='Dirichlet Student T tanh')\n",
    "\n",
    "R_DST, w_bar_stack_DST, w_hat_stack_DST = compute_linearized_mean(X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def diag_T_from_R(R):\n",
    "    \"\"\"\n",
    "    R: (D, N, N) with R = (P+S)^{-1} P per draw.\n",
    "    Returns diag(T) for each draw, where T = (P+S)^{-1} S = I - R.\n",
    "    Output: (D, N)\n",
    "    \"\"\"\n",
    "    D, N, _ = R.shape\n",
    "    # T = I - R  (broadcast I over draws)\n",
    "    T_diag = np.diagonal(np.eye(N)[None, :, :] - R, axis1=1, axis2=2)\n",
    "    return T_diag  # (D, N)\n",
    "\n",
    "def make_diag_mats(diag_vecs):\n",
    "    \"\"\"\n",
    "    diag_vecs: (D, N)\n",
    "    Returns diag matrices per draw: (D, N, N)\n",
    "    (Only use if you truly need matrices; otherwise stick to vectors.)\n",
    "    \"\"\"\n",
    "    D, N = diag_vecs.shape\n",
    "    M = np.zeros((D, N, N), dtype=diag_vecs.dtype)\n",
    "    idx = np.arange(N)\n",
    "    M[:, idx, idx] = diag_vecs\n",
    "    return M\n",
    "\n",
    "# Example for your four models (R_* are (D,N,N)):\n",
    "Tdiag_gauss = diag_T_from_R(R_gauss)  # (D,N)\n",
    "Tdiag_RHS   = diag_T_from_R(R_RHS)\n",
    "Tdiag_DHS   = diag_T_from_R(R_DHS)\n",
    "Tdiag_DST   = diag_T_from_R(R_DST)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "H, p = 16, 8\n",
    "N = H * p\n",
    "x = np.arange(1, N+1)\n",
    "\n",
    "# ---- full shrinkage: bar_w_full = (I - R) @ hat_w (per draw) ----\n",
    "bar_w_full_gauss = np.einsum('dij,dj->di', SP_inv_S_gauss, w_hat_stack_gauss)\n",
    "bar_w_full_RHS   = np.einsum('dij,dj->di', SP_inv_S_RHS,   w_hat_stack_RHS)\n",
    "bar_w_full_DHS   = np.einsum('dij,dj->di', SP_inv_S_DHS,   w_hat_stack_DHS)\n",
    "bar_w_full_DST   = np.einsum('dij,dj->di', SP_inv_S_DST,   w_hat_stack_DST)\n",
    "\n",
    "# # ---- full shrinkage: bar_w_full = (I - R) @ hat_w (per draw) ----\n",
    "# bar_w_white_gauss = np.einsum('dij,dj->di', shrink_gauss, w_hat_stack_gauss)\n",
    "# bar_w_white_RHS   = np.einsum('dij,dj->di', shrink_RHS,   w_hat_stack_RHS)\n",
    "# bar_w_white_DHS   = np.einsum('dij,dj->di', shrink_DHS,   w_hat_stack_DHS)\n",
    "# bar_w_white_DST   = np.einsum('dij,dj->di', shrink_DST,   w_hat_stack_DST)\n",
    "\n",
    "# ---- diagonal shrinkage: bar_w_diag = diag(T) * hat_w (elementwise) ----\n",
    "bar_w_diag_gauss = Tdiag_gauss * w_hat_stack_gauss\n",
    "bar_w_diag_RHS   = Tdiag_RHS   * w_hat_stack_RHS\n",
    "bar_w_diag_DHS   = Tdiag_DHS   * w_hat_stack_DHS\n",
    "bar_w_diag_DST   = Tdiag_DST   * w_hat_stack_DST\n",
    "\n",
    "# ---- means across draws ----\n",
    "means = {\n",
    "    \"Gaussian\": (bar_w_full_gauss.mean(axis=0), bar_w_diag_gauss.mean(axis=0)),\n",
    "    \"RHS\"     : (bar_w_full_RHS.mean(axis=0),   bar_w_diag_RHS.mean(axis=0)),\n",
    "    \"DHS\"     : (bar_w_full_DHS.mean(axis=0),   bar_w_diag_DHS.mean(axis=0)),\n",
    "    \"DST\"     : (bar_w_full_DST.mean(axis=0),   bar_w_diag_DST.mean(axis=0)),\n",
    "}\n",
    "\n",
    "# ---- plot helper ----\n",
    "def plot_model(ax, title, full_mean, diag_mean):\n",
    "    ax.scatter(x, full_mean, s=12, marker='o', label=r\"$\\bar{w}$ (full)\", alpha=0.9)\n",
    "    ax.scatter(x, diag_mean, s=14, marker='^', label=r\"$\\bar{w}$ (diag)\", alpha=0.9)\n",
    "    for h in range(1, H):\n",
    "        ax.axvline(h*p + 0.5, color='0.85', lw=1, zorder=0)\n",
    "    eps_plot = 1e-1\n",
    "    ax.axhline(eps_plot,  color='0.85', lw=1, zorder=0)\n",
    "    ax.axhline(-eps_plot, color='0.85', lw=1, zorder=0)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"parameter index (after alignment)\")\n",
    "    ax.set_ylabel(\"value\")\n",
    "\n",
    "# ---- 2x2 figure ----\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 7), dpi=150, sharex=True, sharey=False)\n",
    "titles = list(means.keys())\n",
    "for ax, title in zip(axes.ravel(), titles):\n",
    "    full_mean, diag_mean = means[title]\n",
    "    plot_model(ax, title, full_mean, diag_mean)\n",
    "\n",
    "# One shared legend\n",
    "handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper center\", ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_all_gauss = posterior_N500_fits['Gaussian tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_gauss = posterior_N500_fits['Gaussian tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_RHS = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_RHS = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_DHS = posterior_N500_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_DHS = posterior_N500_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_DST = posterior_N500_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_DST = posterior_N500_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_compare(W_all, v_all, w_bar_stack, sort_key=\"abs_v\"):\n",
    "    \"\"\"\n",
    "    Align signs & permutations across draws before comparing linearized mean with posterior mean.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    W_all        : array-like, shape (D, H, p) or (D, p, H) or with stray singleton dims.\n",
    "    v_all        : array-like, shape (D, H) or (D, H, 1) or similar (length H per draw).\n",
    "    w_bar_stack  : array-like, shape (D, H*p) OR (D, H, p) OR (D, 1, H*p), etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W_fix        : (D, H, p)   sign/permutation aligned\n",
    "    v_fix        : (D, H)\n",
    "    wbar_fix     : (D, H, p)\n",
    "    summary      : dict with RMSE, Corr, CosSim, SignAgree (means vs means in aligned basis)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    W_all = np.asarray(W_all)\n",
    "    v_all = np.asarray(v_all)\n",
    "    w_bar_stack = np.asarray(w_bar_stack)\n",
    "\n",
    "    D = W_all.shape[0]\n",
    "\n",
    "    # --- infer H from v (source of truth) ---\n",
    "    v0 = np.squeeze(v_all[0]).ravel()\n",
    "    H = v0.size\n",
    "    if H == 0:\n",
    "        raise ValueError(\"v_all[0] seems empty; cannot infer H.\")\n",
    "    # infer p from w_bar_stack length\n",
    "    wb0 = np.squeeze(w_bar_stack[0]).ravel()\n",
    "    if wb0.size % H != 0:\n",
    "        # fallback: try infer p from W_all[0] after squeezing\n",
    "        W0 = np.squeeze(W_all[0])\n",
    "        if W0.ndim != 2:\n",
    "            # try to drop any singleton dims\n",
    "            W0 = W0.reshape([s for s in W0.shape if s != 1])\n",
    "        if W0.ndim != 2:\n",
    "            raise ValueError(f\"Cannot infer (H,p). v length={H}, but w_bar_stack[0] has {wb0.size} elems \"\n",
    "                             f\"and W_all[0] has shape {np.squeeze(W_all[0]).shape}.\")\n",
    "        h, p_candidate = W0.shape\n",
    "        if h != H and p_candidate == H:\n",
    "            p = h\n",
    "        else:\n",
    "            p = p_candidate\n",
    "    else:\n",
    "        p = wb0.size // H\n",
    "\n",
    "    N = H * p\n",
    "\n",
    "    # alloc outputs\n",
    "    W_fix = np.empty((D, H, p), dtype=float)\n",
    "    v_fix = np.empty((D, H), dtype=float)\n",
    "    wbar_fix = np.empty((D, H, p), dtype=float)\n",
    "\n",
    "    def coerce_W(Wd, H, p):\n",
    "        \"\"\"Return Wd as (H,p). Accepts (H,p), (p,H), or with singleton dims.\"\"\"\n",
    "        A = np.asarray(Wd, dtype=float)\n",
    "        A = np.squeeze(A)\n",
    "        if A.ndim == 2:\n",
    "            h, q = A.shape\n",
    "            if h == H and q == p:\n",
    "                return A\n",
    "            if h == p and q == H:\n",
    "                return A.T\n",
    "            # If one matches H, try reshape to (H, -1)\n",
    "            if h == H and h*q == H*p:\n",
    "                return A.reshape(H, p)\n",
    "            if q == H and h*q == H*p:\n",
    "                return A.T.reshape(H, p)\n",
    "            raise ValueError(f\"Cannot coerce W of shape {A.shape} to (H,p)=({H},{p}).\")\n",
    "        elif A.ndim == 3 and 1 in A.shape:\n",
    "            # squeeze singleton and recurse\n",
    "            return coerce_W(np.squeeze(A), H, p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected W ndim={A.ndim}, shape={A.shape}\")\n",
    "\n",
    "    def coerce_v(vd, H):\n",
    "        \"\"\"Return vd as (H,)\"\"\"\n",
    "        v = np.asarray(vd, dtype=float).squeeze().ravel()\n",
    "        if v.size != H:\n",
    "            raise ValueError(f\"v has size {v.size}, expected H={H}.\")\n",
    "        return v\n",
    "\n",
    "    def coerce_wbar_row(wbd, H, p):\n",
    "        \"\"\"Return wbar row as (H,p) from (N,) or already (H,p).\"\"\"\n",
    "        w = np.asarray(wbd, dtype=float).squeeze().ravel()\n",
    "        if w.size == H * p:\n",
    "            return w.reshape(H, p)\n",
    "        # already 2D?\n",
    "        W2 = np.asarray(wbd, dtype=float).squeeze()\n",
    "        if W2.ndim == 2 and W2.shape == (H, p):\n",
    "            return W2\n",
    "        raise ValueError(f\"w_bar row has {w.size} elems but H*p={H*p} and not (H,p).\")\n",
    "\n",
    "    for d in range(D):\n",
    "        # coerce shapes\n",
    "        Wd = coerce_W(W_all[d], H, p)          # (H,p)\n",
    "        vd = coerce_v(v_all[d], H)             # (H,)\n",
    "        wbd = coerce_wbar_row(w_bar_stack[d], H, p)\n",
    "\n",
    "        # 1) sign fix so v >= 0\n",
    "        s = np.sign(vd)\n",
    "        s[s == 0.0] = 1.0\n",
    "        Wd = Wd * s[:, None]\n",
    "        wbd = wbd * s[:, None]\n",
    "        vd = np.abs(vd)\n",
    "\n",
    "        # 2) permute units by a stable key\n",
    "        if sort_key == \"abs_v\":\n",
    "            idx = np.argsort(-vd)  # descending |v|\n",
    "        elif sort_key == \"abs_v_times_rownorm\":\n",
    "            idx = np.argsort(-(vd * np.linalg.norm(Wd, axis=1)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sort_key: {sort_key}\")\n",
    "\n",
    "        W_fix[d] = Wd[idx]\n",
    "        wbar_fix[d] = wbd[idx]\n",
    "        v_fix[d] = vd[idx]\n",
    "\n",
    "    # Compare means in aligned basis\n",
    "    w_post_mean = W_fix.reshape(D, -1).mean(axis=0)   # (N,)\n",
    "    w_lin_mean  = wbar_fix.reshape(D, -1).mean(axis=0)\n",
    "\n",
    "    rmse = float(np.sqrt(np.mean((w_lin_mean - w_post_mean)**2)))\n",
    "    corr = float(np.corrcoef(w_lin_mean, w_post_mean)[0, 1])\n",
    "    cos  = float(np.dot(w_lin_mean, w_post_mean) /\n",
    "                 (np.linalg.norm(w_lin_mean) * np.linalg.norm(w_post_mean)))\n",
    "    sign_agree = float(np.mean(np.sign(w_lin_mean) == np.sign(w_post_mean)))\n",
    "\n",
    "    summary = dict(RMSE=rmse, Corr=corr, CosSim=cos, SignAgree=sign_agree,\n",
    "                   H=H, p=p, N=N)\n",
    "    return W_fix, v_fix, wbar_fix, summary\n",
    "\n",
    "active_cols   = np.arange(0, 5)     # adjust if your actives are different\n",
    "inactive_cols = np.arange(5, 10)\n",
    "def block_stats(A, B, cols):\n",
    "    a = A[:, cols].ravel(); b = B[:, cols].ravel()\n",
    "    return np.corrcoef(a, b)[0,1], np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_gauss, v_all_gauss, w_bar_stack_gauss, sort_key=\"abs_v\")\n",
    "print(\"Gaussian: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "\n",
    "# H, p = summary[\"H\"], summary[\"p\"]\n",
    "# lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "# post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "# print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "# print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n",
    "\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_RHS, v_all_RHS, w_bar_stack_RHS, sort_key=\"abs_v\")\n",
    "print(\"RHS: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "\n",
    "# H, p = summary[\"H\"], summary[\"p\"]\n",
    "# lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "# post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "# print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "# print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_DHS, v_all_DHS, w_bar_stack_DHS, sort_key=\"abs_v\")\n",
    "print(\"DHS: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "\n",
    "# H, p = summary[\"H\"], summary[\"p\"]\n",
    "# lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "# post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "# print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "# print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n",
    "\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_DST, v_all_DST, w_bar_stack_DST, sort_key=\"abs_v\")\n",
    "print(\"DST: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "\n",
    "# H, p = summary[\"H\"], summary[\"p\"]\n",
    "# lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "# post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "# print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "# print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: pick a \"MAP-like\" representative draw and plot MAP vs. \\bar{w} ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def select_map_like_index(W_fix: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Returns the index of the draw whose aligned W is closest (in Frobenius norm)\n",
    "    to the aligned posterior mean -- a robust MAP/medoid proxy.\n",
    "    W_fix: (D, H, p) aligned weights (output of align_and_compare)\n",
    "    \"\"\"\n",
    "    D = W_fix.shape[0]\n",
    "    mu = W_fix.reshape(D, -1).mean(axis=0)  # posterior mean in aligned basis\n",
    "    diffs = W_fix.reshape(D, -1) - mu[None, :]\n",
    "    d2 = np.einsum('di,di->d', diffs, diffs)  # squared distances\n",
    "    return int(np.argmin(d2))\n",
    "\n",
    "def plot_map_vs_barw(W_fix: np.ndarray, wbar_fix: np.ndarray, title: str = \"\", alpha=0.7):\n",
    "    \"\"\"\n",
    "    Overlay scatter: MAP-like draw's W (dots) vs the same draw's \\bar{w} (crosses).\n",
    "    Both arrays must be aligned: (D, H, p). We auto-pick a representative draw.\n",
    "    \"\"\"\n",
    "    D, H, p = W_fix.shape\n",
    "    idx = select_map_like_index(W_fix)  # representative draw\n",
    "    w_map = W_fix[idx].reshape(-1)\n",
    "    w_bar = wbar_fix[idx].reshape(-1)\n",
    "    \n",
    "    eps = 1e-1                          # Small threshold to see non-zero weights\n",
    "\n",
    "    x = np.arange(1, H*p + 1)\n",
    "    plt.figure(figsize=(10, 3.5), dpi=150)\n",
    "    plt.scatter(x, w_map, s=12, marker='o', label=\"MAP-like $w$\", alpha=alpha)\n",
    "    plt.scatter(x, w_bar, s=18, marker='x', label=r\"Linearized $\\bar{w}$\", alpha=alpha)\n",
    "\n",
    "    # light vertical guides between hidden units\n",
    "    for h in range(1, H):\n",
    "        plt.axvline(h*p + 0.5, color='0.85', lw=1, zorder=0)\n",
    "    \n",
    "    plt.axhline(eps, color='0.85', lw=1, zorder=0)\n",
    "    plt.axhline(-eps, color='0.85', lw=1, zorder=0)\n",
    "\n",
    "    plt.xlabel(\"parameter index (after alignment)\")\n",
    "    plt.ylabel(\"value\")\n",
    "    plt.title(title if title else \"MAP-like $w$ vs linearized $\\~w$\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gaussian: align and plot ---\n",
    "W_fix_g, v_fix_g, wbar_fix_g, summary_g = align_and_compare(W_all_gauss, v_all_gauss, w_bar_stack_gauss, sort_key=\"abs_v\")\n",
    "print(\"Gaussian summary:\", summary_g)\n",
    "plot_map_vs_barw(W_fix_g, wbar_fix_g, title=\"Gaussian prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Regularized Horseshoe: align and plot ---\n",
    "W_fix_r, v_fix_r, wbar_fix_r, summary_r = align_and_compare(W_all_RHS, v_all_RHS, w_bar_stack_RHS, sort_key=\"abs_v\")\n",
    "print(\"RHS summary:\", summary_r)\n",
    "plot_map_vs_barw(W_fix_r, wbar_fix_r, title=\"RHS prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dirichlet Horseshoe & Dirichlet Student-t: align and plot ---\n",
    "W_fix_dhs, v_fix_dhs, wbar_fix_dhs, summary_dhs = align_and_compare(W_all_DHS, v_all_DHS, w_bar_stack_DHS, sort_key=\"abs_v\")\n",
    "print(\"DHS summary:\", summary_dhs)\n",
    "plot_map_vs_barw(W_fix_dhs, wbar_fix_dhs, title=\"DHS prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_fix_dst, v_fix_dst, wbar_fix_dst, summary_dst = align_and_compare(W_all_DST, v_all_DST, w_bar_stack_DST, sort_key=\"abs_v\")\n",
    "print(\"DST summary:\", summary_dst)\n",
    "plot_map_vs_barw(W_fix_dst, wbar_fix_dst, title=\"DST prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forskjell mellom lambda_eff og lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_samples = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"lambda\")[1].flatten()\n",
    "reg_lambda_samples = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"lambda_tilde\")[1].flatten()\n",
    "tau_samples = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"tau\")[1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "P = P_RHS[1]\n",
    "S = S_RHS[1]\n",
    "\n",
    "# Gitt: P (diagonal positiv), S (symmetrisk)\n",
    "p = np.diag(P)                       # diagonalene i P\n",
    "P_inv_sqrt = np.diag(1.0/np.sqrt(p))          # P^{-1/2}\n",
    "W = P_inv_sqrt @ S @ P_inv_sqrt                        # whitened\n",
    "\n",
    "# Symmetrisk EVD\n",
    "r, U = np.linalg.eigh(W)             # r = egenverdier (stigende), U kolonner = egenvektorer\n",
    "\n",
    "inv_lambda2 = 1.0 / (lambda_samples**2)\n",
    "# λ_eff_i^2 = 1 / sum_j (U_{ji}^2 / λ_j^2)\n",
    "lambda_eff_sq = 1.0 / (U**2 @ inv_lambda2)\n",
    "lambda_eff = np.sqrt(lambda_eff_sq)  # shape (160,)\n",
    "\n",
    "print(np.mean(lambda_eff), np.mean(reg_lambda_samples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import halfcauchy\n",
    "\n",
    "# --- QQ plot function ---\n",
    "def qq_plot(data, label, color):\n",
    "    n = len(data)\n",
    "    p = (np.arange(1, n+1) - 0.5) / n\n",
    "    q_theory = halfcauchy.ppf(p, scale=1)  # Half-Cauchy(0,1)\n",
    "    q_emp = np.sort(data)\n",
    "    plt.scatter(q_theory, q_emp, label=label, alpha=0.7, color=color)\n",
    "\n",
    "# --- Tail plot function ---\n",
    "def tail_plot(data, label, color):\n",
    "    sorted_data = np.sort(data)\n",
    "    n = len(data)\n",
    "    surv_emp = np.arange(n, 0, -1) / n  # empirical survival\n",
    "    plt.plot(sorted_data, sorted_data * surv_emp, label=label, color=color)\n",
    "\n",
    "# -----------------------------\n",
    "# QQ plot\n",
    "plt.figure(figsize=(6,6))\n",
    "qq_plot(lambda_eff, \"lambda_eff\", \"C0\")\n",
    "qq_plot(reg_lambda_samples, \"lambda_draws\", \"C1\")\n",
    "lims = [0, max(np.max(lambda_eff), np.max(reg_lambda_samples), 10)]\n",
    "plt.plot(lims, lims, 'k--', lw=1, label=\"y=x\")\n",
    "plt.xlabel(\"Theoretical Half-Cauchy(0,1) quantiles\")\n",
    "plt.ylabel(\"Empirical quantiles\")\n",
    "plt.title(\"QQ-plot vs Half-Cauchy(0,1)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Tail plot\n",
    "x = np.linspace(0.1, 10, 200)\n",
    "surv_theory = 1 - halfcauchy.cdf(x, scale=1)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "tail_plot(lambda_eff, \"lambda_eff\", \"C0\")\n",
    "tail_plot(reg_lambda_samples, \"lambda_draws\", \"C1\")\n",
    "plt.plot(x, x * surv_theory, 'k--', lw=1, label=\"Half-Cauchy(0,1)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"x * Survival(x)\")\n",
    "plt.title(\"Tail diagnostic: x * P(X>x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
