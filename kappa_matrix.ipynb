{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#from sklearn.metrics import mean_squared_errosr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_priors = \"results/priors/single_layer/tanh/friedman\"\n",
    "results_dir_posteriors = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "prior_names = [\"Dirichlet Horseshoe\", \"Regularized Horseshoe\", \"Dirichlet Student T\", \"Gaussian\"]\n",
    "posterior_names = [\"Dirichlet Horseshoe tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Gaussian tanh\"]\n",
    "\n",
    "\n",
    "prior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "prior_N200_fits = get_model_fits(\n",
    "    config=\"Friedman_N200_p10_sigma1.00_seed2\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "prior_N500_fits = get_model_fits(\n",
    "    config=\"Friedman_N500_p10_sigma1.00_seed11\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "    \n",
    "posterior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N200_fits = get_model_fits(\n",
    "    config=\"Friedman_N200_p10_sigma1.00_seed2\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N500_fits = get_model_fits(\n",
    "    config=\"Friedman_N500_p10_sigma1.00_seed11\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/friedman/Friedman_N500_p10_sigma1.00_seed11.npz\"\n",
    "data = np.load(path)\n",
    "X = data['X_train']\n",
    "y = data['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Callable\n",
    "\n",
    "# ---------- Aktivasjon og deriverte ----------\n",
    "\n",
    "def get_activation(activation: str = \"tanh\") -> Tuple[Callable, Callable]:\n",
    "    if activation == \"tanh\":\n",
    "        phi = np.tanh\n",
    "        def dphi(a): return 1.0 - np.tanh(a)**2\n",
    "    elif activation == \"relu\":\n",
    "        def phi(a): return np.maximum(0.0, a)\n",
    "        def dphi(a): return (a > 0.0).astype(a.dtype)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    return phi, dphi\n",
    "\n",
    "# ---------- H(w0) og J_W(w0, v0) ----------\n",
    "\n",
    "def build_hidden_and_jacobian_W(\n",
    "    X: np.ndarray,               # (n, p)\n",
    "    W0: np.ndarray,              # (H, p)  -- vekter i referansepunktet w0\n",
    "    b0: np.ndarray,              # (H,)    -- bias i referansepunktet w0\n",
    "    v0: np.ndarray,              # (H,)    -- utgangsvekter i referansepunktet v0\n",
    "    activation: str = \"tanh\",\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returnerer:\n",
    "      H  : (n, H)         = H(w0)\n",
    "      JW : (n, H*p)       = d(H(w)v)/d vec(W) |_(w0, v0), kolonner ordnet som (h=0..H-1, j=0..p-1)\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "    H, pW = W0.shape\n",
    "    assert pW == p\n",
    "    phi, dphi = get_activation(activation)\n",
    "\n",
    "    # Pre- og post-aktivert\n",
    "    A = X @ W0.T + b0[None, :]        # (n, H), a_{i,h}\n",
    "    Hmat = phi(A)                     # (n, H), h_{i,h}\n",
    "    dphiA = dphi(A)                   # (n, H)\n",
    "\n",
    "    # J_W: df/dW_{h,j} = v_h * dphi(a_{i,h}) * x_{i,j}\n",
    "    # For hver node h bygger vi et (n, p)-bidrag og flater ut langs j, og stabler så langs h.\n",
    "    JW_blocks = []\n",
    "    for h in range(H):\n",
    "        # (n,1) * (1,p) -> (n,p)\n",
    "        block_h = (v0[h] * dphiA[:, [h]]) * X \n",
    "        JW_blocks.append(block_h.reshape(n, p))\n",
    "    # Stack kolonnevis i rekkefølge (h, j) -> (n, H*p)\n",
    "    Jb = dphiA * v0[None, :]     # (n, H)\n",
    "    JW = np.hstack([B for B in JW_blocks])\n",
    "    return Hmat, JW, Jb\n",
    "\n",
    "# ---------- Sigma_y og P ----------\n",
    "\n",
    "def build_Sigma_y(\n",
    "    Hmat: np.ndarray,     # (n, H) = H(w0)\n",
    "    tau_v: float,         # prior std for v\n",
    "    J_b: np.ndarray,      # Jacobian for bias\n",
    "    sigma: float          # støy std i likelihood\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Σ_y = J_bJ_b^T + τ_v^2 H H^T + σ^2 I_n\n",
    "    \"\"\"\n",
    "    n = Hmat.shape[0]\n",
    "    return (tau_v**2) * (Hmat @ Hmat.T) + (J_b@J_b.T) + (sigma**2) * np.eye(n)\n",
    "\n",
    "def build_P_from_lambda_tau(\n",
    "    lambda_tilde: np.ndarray,  # (H, p) lokale skalaer for W\n",
    "    tau_w: float               # global skala for w\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    P = τ_w^{-2} Λ^{-1} der Λ = diag(λ^2) for konsistens med uttrykket 1/(1 + τ^2 λ^2 s).\n",
    "    Dvs. diag(P) = 1 / (τ_w^2 * λ^2).\n",
    "    Returnerer P som (H*p, H*p) diagonalmatrise.\n",
    "    \"\"\"\n",
    "    lam_vec = lambda_tilde.reshape(-1)          # (H*p,)\n",
    "    diagP = 1.0 / ( (tau_w**2) * (lam_vec) ) # (H*p,)\n",
    "    return np.diag(diagP)\n",
    "\n",
    "# ---------- S, shrinkage-matrise R = (P+S)^{-1} P ----------\n",
    "\n",
    "def build_S(JW: np.ndarray, Sigma_y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    S = J_W^T Σ_y^{-1} J_W  (Hp x Hp).\n",
    "    Løser via lineær solve for stabilitet: X = Σ_y^{-1} J_W = solve(Σ_y, J_W).\n",
    "    \"\"\"\n",
    "    X = np.linalg.solve(Sigma_y, JW)       # (n, Hp)\n",
    "    return JW.T @ X                        # (Hp, Hp)\n",
    "\n",
    "def shrinkage_matrix(P: np.ndarray, S: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    R = (P+S)^{-1} P. Bruk Cholesky når mulig.\n",
    "    Løser (P+S) * R = P for R.\n",
    "    \"\"\"\n",
    "    A = P + S\n",
    "    # Robust fallback hvis Cholesky feiler\n",
    "    try:\n",
    "        L = np.linalg.cholesky(A)\n",
    "        # L Y = P  -> Y\n",
    "        Y = np.linalg.solve(L, P)\n",
    "        # L^T R = Y -> R\n",
    "        R = np.linalg.solve(L.T, Y)\n",
    "    except np.linalg.LinAlgError:\n",
    "        R = np.linalg.solve(A, P)\n",
    "    return R\n",
    "\n",
    "def shrinkage_matrix_stable(P, S, jitter=0.0):\n",
    "    \"\"\"\n",
    "    Stabil beregning av R = (P+S)^{-1} P via\n",
    "    R = P^{1/2} (I + P^{-1/2} S P^{-1/2})^{-1} P^{1/2}.\n",
    "    Krever at P er diagonal (positiv).\n",
    "    \"\"\"\n",
    "    d = np.diag(P).astype(float)\n",
    "    # Guardrails: ingen nuller/NaN/negativ\n",
    "    eps = 1e-12\n",
    "    d = np.clip(d, eps, np.finfo(float).max)\n",
    "    Phalf    = np.diag(np.sqrt(d))\n",
    "    Pinvhalf = np.diag(1.0 / np.sqrt(d))\n",
    "\n",
    "    M = Pinvhalf @ S @ Pinvhalf\n",
    "    # Jitter for SPD-sikkerhet (skader ikke i praksis)\n",
    "    if jitter > 0:\n",
    "        M = M + jitter * np.eye(M.shape[0])\n",
    "\n",
    "    # (I + M) er SPD -> Cholesky\n",
    "    I = np.eye(M.shape[0])\n",
    "    L = np.linalg.cholesky(I + M)\n",
    "    # (I+M)^{-1} P^{1/2} = (L^T)^{-1} (L)^{-1} P^{1/2}\n",
    "    Z = np.linalg.solve(L, Phalf)\n",
    "    W = np.linalg.solve(L.T, Z)\n",
    "    # R = P^{1/2} * W\n",
    "    R = Phalf @ W\n",
    "    # Symmetrer (numerisk)\n",
    "    R = 0.5 * (R + R.T)\n",
    "    return R\n",
    "\n",
    "def shrinkage_eigs_and_df(P, S):\n",
    "    \"\"\"Returner r-eigenverdier og df_eff i P-whitnede koordinater.\"\"\"\n",
    "    d = np.diag(P).astype(float)\n",
    "    eps = 1e-12\n",
    "    Pinvhalf = np.diag(1.0 / np.sqrt(np.maximum(d, eps)))\n",
    "\n",
    "    M = Pinvhalf @ S @ Pinvhalf          # SPD\n",
    "    mu = np.linalg.eigvalsh(M)           # >= 0\n",
    "    r = 1.0 / (1.0 + mu)                 # i (0,1]\n",
    "    df_eff = np.sum(1.0 - r)             # = sum mu/(1+mu) >= 0\n",
    "    return r, df_eff\n",
    "\n",
    "def extract_model_draws(fit_dict, model: str):\n",
    "    \"\"\"\n",
    "    Returnerer ALL draws, med 'lambda_all' definert som EFFEKTIV VARIANSFAKTOR per vekt:\n",
    "      Gaussian:                 lambda_all = 1\n",
    "      Regularized Horseshoe:    lambda_all = lambda_tilde\n",
    "      Dirichlet (DHS/DST):      lambda_all = lambda_tilde_data * phi_data\n",
    "\n",
    "    Shapes:\n",
    "      W_all      : (D, H, p)\n",
    "      b_all      : (D, H)\n",
    "      v_all      : (D, H)\n",
    "      c_all      : (D,)\n",
    "      sigma_all  : (D,)\n",
    "      tau_w_all  : (D,)\n",
    "      tau_v_all  : (D,)   (ones if not in fit)\n",
    "      lambda_all : (D, H, p)  <-- effektiv variansfaktor\n",
    "    \"\"\"\n",
    "    post = fit_dict[model]['posterior']\n",
    "\n",
    "    # W_1: (D, p, H) -> (D, H, p)\n",
    "    W_1 = np.asarray(post.stan_variable(\"W_1\"))\n",
    "    W_all = np.transpose(W_1, (0, 2, 1))\n",
    "    D, H, p = W_all.shape\n",
    "\n",
    "    # W_L: (D, H, out_nodes=1) -> (D, H)\n",
    "    W_L = np.asarray(post.stan_variable(\"W_L\"))\n",
    "    v_all = W_L.reshape(D, -1)\n",
    "\n",
    "    # hidden_bias: (D, 1, H) -> (D, H)\n",
    "    b_1 = np.asarray(post.stan_variable(\"hidden_bias\"))\n",
    "    b_all = b_1.reshape(D, -1)\n",
    "\n",
    "    # output_bias: (D, 1) -> (D,)\n",
    "    b_2 = np.asarray(post.stan_variable(\"output_bias\"))\n",
    "    c_all = b_2.reshape(D)\n",
    "\n",
    "    # sigma: (D,)\n",
    "    sigma_all = np.asarray(post.stan_variable(\"sigma\")).reshape(D)\n",
    "\n",
    "    # Modell-flagg\n",
    "    is_gauss      = (\"Gaussian\" in model)\n",
    "    is_rhs        = (\"Regularized Horseshoe\" in model)\n",
    "    is_dirichlet  = (\"Dirichlet\" in model) or (\"DST\" in model)\n",
    "\n",
    "    # tau_w / tau_v\n",
    "    if is_gauss:\n",
    "        tau_w_all = np.ones(D)\n",
    "        tau_v_all = np.ones(D)\n",
    "    else:\n",
    "        tau_w_all = np.asarray(post.stan_variable(\"tau\")).reshape(D)\n",
    "        try:\n",
    "            tau_v_all = np.asarray(post.stan_variable(\"tau_v\")).reshape(D)\n",
    "        except Exception:\n",
    "            tau_v_all = np.ones(D)\n",
    "\n",
    "    # Effektiv lokal variansfaktor lambda_all\n",
    "    if is_gauss:\n",
    "        lambda_all = np.ones((D, H, p))\n",
    "    else:\n",
    "        lam_name = \"lambda_tilde\" if is_rhs else \"lambda_tilde_data\"\n",
    "        lam = np.asarray(post.stan_variable(lam_name))\n",
    "        # Bring til (D, H, p)\n",
    "        if lam.shape[1:] == (H, p):\n",
    "            lam_var = lam\n",
    "        else:\n",
    "            lam_var = np.transpose(lam, (0, 2, 1))\n",
    "\n",
    "        if is_dirichlet:\n",
    "            # phi_data: sannsynlige shapes (D, H, p) eller (D, p, H)\n",
    "            phi = np.asarray(post.stan_variable(\"phi_data\"))\n",
    "            if phi.shape[1:] == (H, p):\n",
    "                phi_hp = phi\n",
    "            else:\n",
    "                phi_hp = np.transpose(phi, (0, 2, 1))\n",
    "            # Stan: stddev = tau * sqrt(lambda_tilde) * sqrt(phi)\n",
    "            #  => var = tau^2 * lambda_tilde * phi\n",
    "            lambda_all = lam_var * phi_hp\n",
    "        else:\n",
    "            # RHS: var = tau^2 * lambda_tilde\n",
    "            lambda_all = lam_var\n",
    "\n",
    "    return W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_all\n",
    "\n",
    "# ------- Knyt alt sammen -------\n",
    "\n",
    "def compute_shrinkage_for_W_block(\n",
    "    X: np.ndarray,\n",
    "    W0: np.ndarray, b0: np.ndarray, v0: np.ndarray,\n",
    "    sigma: float, tau_w: float, tau_v: float,\n",
    "    lambda_tilde: np.ndarray,\n",
    "    activation: str = \"tanh\"\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returnerer (R, P, S, Sigma_y) der R = (P+S)^{-1} P for W-blokken.\n",
    "    \"\"\"\n",
    "    Hmat, JW, Jb = build_hidden_and_jacobian_W(X, W0, b0, v0, activation=activation)  # (n,H), (n,Hp)\n",
    "    Sigma_y = build_Sigma_y(Hmat, tau_v=tau_v, J_b=Jb, sigma=sigma)                       # (n,n)\n",
    "    P = build_P_from_lambda_tau(lambda_tilde, tau_w=tau_w)                        # (Hp,Hp)\n",
    "    S = build_S(JW, Sigma_y)                                                      # (Hp,Hp)\n",
    "    R = shrinkage_matrix_stable(P, S)                                                    # (Hp,Hp)\n",
    "    return R, P, S, Sigma_y, JW, Hmat\n",
    "\n",
    "\n",
    "def compute_shrinkage(\n",
    "    X,\n",
    "    W_all, b_all, v_all,          # (D,H,p), (D,H), (D,H)\n",
    "    sigma_all, tau_w_all, tau_v_all,  # (D,), (D,), (D,)\n",
    "    lambda_all,                   # (D,H,p)\n",
    "    activation=\"tanh\",\n",
    "    return_mats=True,             # set False if you only want summaries\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop over draws and compute R=(P+S)^{-1}P per draw using your single-draw function.\n",
    "    Returns:\n",
    "      R_stack : (D, N, N) with N=H*p  (if return_mats=True, else None)\n",
    "      r_eigs  : (D, N)  sorted eigenvalues in [0,1]\n",
    "      df_eff  : (D,)    effective dof = tr(I-R) = N - tr(R)\n",
    "    \"\"\"\n",
    "    D, H, p = W_all.shape\n",
    "    N = H * p\n",
    "\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    S_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    P_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    W_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    shrink_stack= np.empty((D, N, N)) if return_mats else None\n",
    "    r_eigs  = np.empty((D, N))\n",
    "    df_eff  = np.empty(D)\n",
    "\n",
    "    for d in range(D):\n",
    "        R, P, S, Sigma_y, _, _ = compute_shrinkage_for_W_block(\n",
    "            X=X,\n",
    "            W0=W_all[d],\n",
    "            b0=b_all[d],\n",
    "            v0=v_all[d],\n",
    "            sigma=float(sigma_all[d]),\n",
    "            tau_w=float(tau_w_all[d]),\n",
    "            tau_v=float(tau_v_all[d]),\n",
    "            lambda_tilde=lambda_all[d],\n",
    "            activation=activation,\n",
    "        )\n",
    "        p = np.diag(P)                       \n",
    "        P_inv_sqrt = np.diag(1.0/np.sqrt(p))         \n",
    "        W = P_inv_sqrt @ S @ P_inv_sqrt \n",
    "        I = np.identity(N)\n",
    "        shrink_mat = np.linalg.inv(I + W)@W\n",
    "\n",
    "        if return_mats:\n",
    "            R_stack[d] = R\n",
    "            S_stack[d] = S\n",
    "            P_stack[d] = P\n",
    "            W_stack[d] = W\n",
    "            shrink_stack[d] = shrink_mat\n",
    "        \n",
    "\n",
    "\n",
    "        r, df = shrinkage_eigs_and_df(P, S)\n",
    "        r_eigs[d] = np.sort(r)\n",
    "        df_eff[d] = df\n",
    "\n",
    "    return R_stack, S_stack, P_stack, W_stack, shrink_stack, r_eigs, df_eff\n",
    "\n",
    "\n",
    "# ---------- Minimal kjøreeksempel ----------\n",
    "#draw = 0\n",
    "W, b1, v, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Gaussian tanh')\n",
    "\n",
    "R_gauss, S_gauss, P_gauss, W_gauss, shrink_gauss, eigs_gauss, df_eff_gauss = compute_shrinkage(X, W, b1, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "#R_gauss, P_gauss, S_gauss, Sigma_y_gauss = compute_shrinkage_for_W_block(X, W, b1, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W, b1, v, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Regularized Horseshoe tanh')\n",
    "R_RHS, S_RHS, P_RHS, W_RHS, shrink_RHS, eigs_RHS, df_eff_RHS = compute_shrinkage(X, W, b1, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "# R_RHS, P_RHS, S_RHS, Sigma_y_RHS = compute_shrinkage_for_W_block(X, W, b1, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W, b1, v, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Horseshoe tanh')\n",
    "R_DHS, S_DHS, P_DHS, W_DHS, shrink_DHS, eigs_DHS, df_eff_DHS = compute_shrinkage(X, W, b1, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "# R_DHS, P_DHS, S_DHS, Sigma_y_DHS = compute_shrinkage_for_W_block(X, W, b1, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W, b1, v, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Student T tanh')\n",
    "R_DST, S_DST, P_DST, W_DST, shrink_DST, eigs_DST, df_eff_DST = compute_shrinkage(X, W, b1, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "# R_DST, P_DST, S_DST, Sigma_y_DST = compute_shrinkage_for_W_block(X, W, b1, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import SymLogNorm, Normalize, TwoSlopeNorm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_block_grid(ax, H, p, color=\"w\", lw=0.5):\n",
    "    Hp = H*p\n",
    "    for h in range(1, H):\n",
    "        k = h*p\n",
    "        ax.axhline(k-0.5, color=color, lw=lw)\n",
    "        ax.axvline(k-0.5, color=color, lw=lw)\n",
    "\n",
    "def visualize_models(\n",
    "    matrices, names, H=16, p=10, use_abs=False, cmap=\"magma\",\n",
    "    q_low=0.05, q_high=0.95\n",
    "):\n",
    "    \"\"\"\n",
    "    Viser heatmaps av matriser med felles, robust fargeskala:\n",
    "    vmin = q_low-kvantilen over ALLE matrisene\n",
    "    vmax = q_high-kvantilen over ALLE matrisene\n",
    "    Verdier utenfor [vmin, vmax] klippes til endene.\n",
    "    \"\"\"\n",
    "    # valgfritt absoluttbeløp\n",
    "    mats = [np.abs(M) if use_abs else M for M in matrices]\n",
    "\n",
    "    # Samle alle endelige verdier\n",
    "    all_vals = np.concatenate([M[np.isfinite(M)].ravel() for M in mats]) if mats else np.array([])\n",
    "\n",
    "    if all_vals.size == 0:\n",
    "        vmin, vmax = -1.0, 1.0\n",
    "    else:\n",
    "        vmin = float(np.quantile(all_vals, q_low))\n",
    "        vmax = float(np.quantile(all_vals, q_high))\n",
    "        if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "            # fallback hvis alt er likt/NaN\n",
    "            m = float(np.nanmean(all_vals)) if np.isfinite(np.nanmean(all_vals)) else 0.0\n",
    "            vmin, vmax = m - 1.0, m + 1.0\n",
    "\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    #norm = TwoSlopeNorm(vmin=vmin, vcenter=0.0, vmax=vmax)\n",
    "\n",
    "    # Figur\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 10), dpi=150, constrained_layout=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    im = None\n",
    "    for ax, M, title in zip(axes, mats, names):\n",
    "        Mplot = np.clip(M, vmin, vmax)  # klipp utfor intervallet\n",
    "        im = ax.imshow(Mplot, aspect='equal', interpolation='nearest', cmap=cmap, norm=norm)\n",
    "        add_block_grid(ax, H, p)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Columns\")\n",
    "        ax.set_ylabel(\"Rows\")\n",
    "\n",
    "    # Felles colorbar\n",
    "    if im is not None:\n",
    "        fig.colorbar(im, ax=axes.tolist(), label=(\"|Value|\" if use_abs else \"Value\"))\n",
    "\n",
    "    # (valgfritt) print hva som faktisk ble brukt\n",
    "    print(f\"Felles kvantiler: vmin (q={q_low}) = {vmin:.6g}, vmax (q={q_high}) = {vmax:.6g}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example call:\n",
    "# matrices_R = [\n",
    "#     R_gauss.mean(axis=0), #[0],\n",
    "#     R_RHS.mean(axis=0), #[0],\n",
    "#     R_DHS.mean(axis=0), #[0],\n",
    "#     R_DST.mean(axis=0), #[0]\n",
    "# ]\n",
    "\n",
    "# matrices_S = [\n",
    "#     S_gauss.mean(axis=0),\n",
    "#     S_RHS.mean(axis=0),\n",
    "#     S_DHS.mean(axis=0),\n",
    "#     S_DST.mean(axis=0)\n",
    "# ]\n",
    "\n",
    "# matrices_P_inv = [\n",
    "#     np.linalg.inv(P_gauss.mean(axis=0)), #[0],\n",
    "#     np.linalg.inv(P_RHS.mean(axis=0)), #[0],\n",
    "#     np.linalg.inv(P_DHS.mean(axis=0)), #[0],\n",
    "#     np.linalg.inv(P_DST.mean(axis=0)), #[0]\n",
    "# ]\n",
    "\n",
    "matrices_W = [\n",
    "    W_gauss.mean(axis=0), #[0],\n",
    "    W_RHS.mean(axis=0), #[0],\n",
    "    W_DHS.mean(axis=0), #[0],\n",
    "    W_DST.mean(axis=0), #[0]\n",
    "]\n",
    "\n",
    "# matrices_med = [\n",
    "#     np.median(W_gauss, axis=0), #[0],\n",
    "#     np.median(W_RHS, axis=0), #[0],\n",
    "#     np.median(W_DHS, axis=0), #[0],\n",
    "#     np.median(W_DST, axis=0), #[0]\n",
    "# ]\n",
    "\n",
    "# matrices_shrink = [\n",
    "#     np.mean(shrink_gauss, axis=0), #[0],\n",
    "#     np.mean(shrink_RHS, axis=0), #[0],\n",
    "#     np.mean(shrink_DHS, axis=0), #[0],\n",
    "#     np.mean(shrink_DST, axis=0), #[0]\n",
    "# ]\n",
    "\n",
    "\n",
    "names = [\n",
    "    \"R (Gauss)\", #\"Data shrinkage (Gauss)\", \"Prior shrinkage (Gauss)\",\n",
    "    \"R (RHS)\",   #\"Data shrinkage (RHS)\",   \"Prior shrinkage (RHS)\",\n",
    "    \"R (DHS)\",   #\"Data shrinkage (DHS)\",   \"Prior shrinkage (DHS)\",\n",
    "    \"R (DST)\",   #\"Data shrinkage (DST)\",   \"Prior shrinkage (DST)\",\n",
    "]\n",
    "\n",
    "visualize_models(matrices_W, names, H=16, p=10, use_abs=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------- #\n",
    "# 1) Spectra for W and (I-R)\n",
    "# ----------------------------- #\n",
    "def spectra_from_R_W(W_stack):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "      W_stack : (D, N, N), symmetric up to numerical noise\n",
    "\n",
    "    Returns\n",
    "      omega   : (D, N)    eigenvalues of W  (>=0)\n",
    "      r       : (D, N)    shrinkage eigenvalues of (I-R) = (I+W)^{-1} W  in (0,1)\n",
    "      df      : (D,)      effective degrees of freedom = sum(r)\n",
    "    \"\"\"\n",
    "    # symmetrize for numerical safety\n",
    "    W_sym = 0.5 * (W_stack + np.swapaxes(W_stack, 1, 2))\n",
    "    # eigs of W (PSD), clip tiny negatives from roundoff\n",
    "    omega = np.linalg.eigvalsh(W_sym)\n",
    "    omega = np.clip(omega, 0.0, None)\n",
    "    # shrinkage eigs of (I-R): r = omega/(1+omega)  (same eigenvectors as W)\n",
    "    r = omega / (1.0 + omega)\n",
    "    df = r.sum(axis=1)\n",
    "    return omega, r, df\n",
    "\n",
    "# ----------------------------- #\n",
    "# 2) Plots for the spectra\n",
    "# ----------------------------- #\n",
    "def plot_spectra(omega, r, model_name=\"\", bins=60):\n",
    "    \"\"\"\n",
    "    Pooled views across all draws and all modes.\n",
    "    \"\"\"\n",
    "    om = omega.ravel()\n",
    "    rr = r.ravel()\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4.5))\n",
    "\n",
    "    # left: histogram of log(1+omega) (handles wide range)\n",
    "    ax[0].hist(np.log1p(om), bins=bins, density=True)\n",
    "    ax[0].set_xlabel(\"log(1 + ω)  (ω eigenvalues of W)\")\n",
    "    ax[0].set_ylabel(\"Density\")\n",
    "    ax[0].set_title(f\"{model_name} — spectrum of W\")\n",
    "\n",
    "    # right: ECDF of r in (0,1)\n",
    "    rr_sorted = np.sort(rr)\n",
    "    y = np.linspace(0, 1, rr_sorted.size, endpoint=True)\n",
    "    ax[1].plot(rr_sorted, y)\n",
    "    ax[1].set_xlim(0, 1)\n",
    "    ax[1].set_xlabel(\"r = ω/(1+ω)  (eigs of (I+W)^{-1}W ≡ (I-R))\")\n",
    "    ax[1].set_ylabel(\"ECDF\")\n",
    "    ax[1].set_title(f\"{model_name} — shrinkage eigenvalues\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_df(df, model_name=\"\"):\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.hist(df, bins=50, density=True)\n",
    "    plt.xlabel(\"df = sum of shrinkage eigenvalues\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(f\"{model_name} — effective d.f.\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------------- #\n",
    "# 3) Block strength (10x10 blocks)\n",
    "# ----------------------------- #\n",
    "def block_strength_grid(M, H=16, p=10, stat=\"mean_abs\"):\n",
    "    \"\"\"\n",
    "    One matrix -> (H,H) grid of block strengths.\n",
    "    Blocks are p×p; diagonal blocks correspond to units.\n",
    "    stat: \"mean_abs\" (default) | \"fro\" | \"spec\"\n",
    "    \"\"\"\n",
    "    B = np.zeros((H, H), dtype=float)\n",
    "    for i in range(H):\n",
    "        rs = slice(i*p, (i+1)*p)\n",
    "        for j in range(H):\n",
    "            cs = slice(j*p, (j+1)*p)\n",
    "            blk = M[rs, cs]\n",
    "            if stat == \"mean_abs\":\n",
    "                val = np.mean(np.abs(blk))\n",
    "            elif stat == \"fro\":\n",
    "                val = np.linalg.norm(blk, \"fro\") / (p*np.sqrt(p))  # size-normalized\n",
    "            elif stat == \"spec\":\n",
    "                # spectral norm; guard tiny blocks\n",
    "                s = np.linalg.svd(blk, compute_uv=False)\n",
    "                val = float(s[0]) if s.size else 0.0\n",
    "            else:\n",
    "                raise ValueError(\"stat must be 'mean_abs', 'fro', or 'spec'\")\n",
    "            B[i, j] = val\n",
    "    return B\n",
    "\n",
    "def summarize_blocks_many(M_stack, H=16, p=10, stat=\"mean_abs\"):\n",
    "    \"\"\"\n",
    "    Stack of matrices -> mean grid (H,H), and diag block stats across draws.\n",
    "    Returns:\n",
    "      mean_grid : (H,H)  mean block strengths across draws\n",
    "      diag_mu   : (H,)   mean of diagonal block strengths across draws\n",
    "      diag_lo   : (H,)   5th percentile per diagonal block\n",
    "      diag_hi   : (H,)   95th percentile per diagonal block\n",
    "    \"\"\"\n",
    "    D, N, _ = M_stack.shape\n",
    "    grids = np.empty((D, H, H))\n",
    "    diag_vals = np.empty((D, H))\n",
    "    for d in range(D):\n",
    "        G = block_strength_grid(M_stack[d], H=H, p=p, stat=stat)\n",
    "        grids[d] = G\n",
    "        diag_vals[d] = np.diag(G)\n",
    "    mean_grid = grids.mean(axis=0)\n",
    "    diag_mu = diag_vals.mean(axis=0)\n",
    "    diag_lo = np.quantile(diag_vals, 0.05, axis=0)\n",
    "    diag_hi = np.quantile(diag_vals, 0.95, axis=0)\n",
    "    return mean_grid, diag_mu, diag_lo, diag_hi\n",
    "\n",
    "def plot_block_heatmaps(mean_grid_W, mean_grid_shrink, stat=\"mean_abs\", model_name=\"\"):\n",
    "    vmin = min(mean_grid_W.min(), mean_grid_shrink.min())\n",
    "    vmax = max(mean_grid_W.max(), mean_grid_shrink.max())\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(11, 4.8))\n",
    "    im0 = ax[0].imshow(mean_grid_W, vmin=vmin, vmax=vmax, origin=\"lower\", aspect=\"equal\")\n",
    "    ax[0].set_title(f\"{model_name}: block strength of W  ({stat})\")\n",
    "    ax[0].set_xlabel(\"block j\"); ax[0].set_ylabel(\"block i\")\n",
    "    im1 = ax[1].imshow(mean_grid_shrink, vmin=vmin, vmax=vmax, origin=\"lower\", aspect=\"equal\")\n",
    "    ax[1].set_title(f\"{model_name}: block strength of (I-R)  ({stat})\")\n",
    "    ax[1].set_xlabel(\"block j\"); ax[1].set_ylabel(\"block i\")\n",
    "    cbar = fig.colorbar(im1, ax=ax.ravel().tolist(), shrink=0.9)\n",
    "    cbar.set_label(\"block strength\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_diag_blocks(diag_mu_W, diag_lo_W, diag_hi_W,\n",
    "                     diag_mu_S, diag_lo_S, diag_hi_S,\n",
    "                     model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Diagonal (unit-wise) block strength: mean with 5–95% bands for W and (I-R).\n",
    "    \"\"\"\n",
    "    H = diag_mu_W.size\n",
    "    x = np.arange(H)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    # W\n",
    "    plt.plot(x, diag_mu_W, label=\"W (mean)\", lw=2)\n",
    "    plt.fill_between(x, diag_lo_W, diag_hi_W, alpha=0.15)\n",
    "    # shrinkage\n",
    "    plt.plot(x, diag_mu_S, label=\"(I-R) (mean)\", lw=2)\n",
    "    plt.fill_between(x, diag_lo_S, diag_hi_S, alpha=0.15)\n",
    "    plt.xlabel(\"Hidden unit (block index)\")\n",
    "    plt.ylabel(\"Diagonal block strength\")\n",
    "    plt.title(f\"{model_name}: diagonal block strengths (mean ± 5–95%)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, p = 16, 10\n",
    "\n",
    "# Spectra\n",
    "omega, r, df = spectra_from_R_W(W_gauss)\n",
    "plot_spectra(omega, r, model_name=\"Your model\")\n",
    "plot_df(df, model_name=\"Your model\")\n",
    "\n",
    "# Block strengths\n",
    "# (I-R) is the global shrinkage operator; compute it directly\n",
    "I_minus_R_stack = np.eye(H*p)[None, :, :] - R_gauss\n",
    "\n",
    "# Mean block heatmaps (same color scale)\n",
    "meanW, dmuW, dloW, dhiW = summarize_blocks_many(W_gauss, H=H, p=p, stat=\"mean_abs\")\n",
    "meanS, dmuS, dloS, dhiS = summarize_blocks_many(I_minus_R_stack, H=H, p=p, stat=\"mean_abs\")\n",
    "\n",
    "plot_block_heatmaps(meanW, meanS, stat=\"mean_abs\", model_name=\"Your model\")\n",
    "plot_diag_blocks(dmuW, dloW, dhiW, dmuS, dloS, dhiS, model_name=\"Your model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, U = np.linalg.eigh(W_DST.mean(axis=0))\n",
    "\n",
    "\n",
    "shrink = 1/(1+w)\n",
    "\n",
    "print(shrink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, U = np.linalg.eigh(W_RHS.mean(axis=0))\n",
    "\n",
    "\n",
    "shrink = 1/(1+w)\n",
    "\n",
    "print(shrink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------- summaries -------\n",
    "def summarize_df(df):\n",
    "    df = np.asarray(df).reshape(-1)\n",
    "    return {\n",
    "        \"mean\": float(np.mean(df)),\n",
    "        \"sd\":   float(np.std(df, ddof=1)),\n",
    "        \"q10\":  float(np.quantile(df, 0.10)),\n",
    "        \"q50\":  float(np.quantile(df, 0.50)),\n",
    "        \"q90\":  float(np.quantile(df, 0.90)),\n",
    "    }\n",
    "\n",
    "def summarize_eigs(eigs, small=0.1, large=0.9):\n",
    "    # eigs: (D,N) of r \\in [0,1]\n",
    "    r = np.asarray(eigs).reshape(-1)\n",
    "    return {\n",
    "        \"median_r\": float(np.median(r)),\n",
    "        \"q10_r\":    float(np.quantile(r, 0.10)),\n",
    "        \"q90_r\":    float(np.quantile(r, 0.90)),\n",
    "        \"frac_small(<{:.2f})\".format(small): float(np.mean(r < small)),\n",
    "        \"frac_large(>{:.2f})\".format(large): float(np.mean(r > large)),\n",
    "    }\n",
    "\n",
    "# ------- ECDF plotting helpers -------\n",
    "\n",
    "def plot_hist_eigs(model_eigs_dict, bins=50, title=\"\"):\n",
    "    \"\"\"\n",
    "    model_eigs_dict: {\"Name\": eigs}, where eigs has shape (D, N) or (D*N,)\n",
    "                     and contains r-eigenvalues in [0, 1].\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(9,5))\n",
    "\n",
    "    # common bins across models for fair comparison\n",
    "    if isinstance(bins, int):\n",
    "        bins = np.linspace(0.0, 1.0, bins+1)\n",
    "\n",
    "    for name, eigs in model_eigs_dict.items():\n",
    "        x = np.asarray(eigs).ravel()\n",
    "        x = np.clip(x, 0.0, 1.0)               # safety\n",
    "        plt.hist(x, bins=bins, density=True, alpha=0.5, label=name)\n",
    "\n",
    "    plt.xlabel(\"r eigenvalues (0 = no shrink, 1 = hard shrink)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------- optional: rho transform (prior/data ratio) -------\n",
    "def plot_hist_log1p_rho(model_eigs_dict, bins=50, eps=1e-12, title=\"\"):\n",
    "    \"\"\"\n",
    "    model_eigs_dict: {\"Name\": eigs}, eigs shape (D,N) or (D*N,)\n",
    "                     r-eigenvalues in [0,1].\n",
    "    Plots overlaid histograms of log(1 + rho), rho = r/(1-r).\n",
    "    \"\"\"\n",
    "    # flatten & transform for pooled bin edges\n",
    "    pooled = []\n",
    "    for eigs in model_eigs_dict.values():\n",
    "        r = np.asarray(eigs).ravel()\n",
    "        r = np.clip(r, 0.0, 1.0 - eps)        # avoid division by zero at r=1\n",
    "        rho = r / (1.0 - r)\n",
    "        pooled.append(np.log1p(rho))\n",
    "    pooled = np.concatenate(pooled)\n",
    "\n",
    "    # common bins (use pooled range, cap extreme tail)\n",
    "    if isinstance(bins, int):\n",
    "        hi = float(np.quantile(pooled, 0.995))  # ignore extreme 0.5% tail\n",
    "        bins = np.linspace(0.0, hi, bins + 1)\n",
    "\n",
    "    plt.figure(figsize=(9,5))\n",
    "    for name, eigs in model_eigs_dict.items():\n",
    "        r = np.asarray(eigs).ravel()\n",
    "        r = np.clip(r, 0.0, 1.0 - eps)\n",
    "        rho = r / (1.0 - r)\n",
    "        x = np.log1p(rho)\n",
    "        plt.hist(x, bins=bins, density=True, alpha=0.5, label=name)\n",
    "\n",
    "    plt.xlabel(\"log(1 + rho),  rho = r/(1-r)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------- run summaries -------\n",
    "models_eigs = {\n",
    "    \"Gaussian\": eigs_gauss,\n",
    "    \"RHS\":      eigs_RHS,\n",
    "    \"DHS\":      eigs_DHS,\n",
    "    \"DST\":      eigs_DST,\n",
    "}\n",
    "models_df = {\n",
    "    \"Gaussian\": df_eff_gauss,\n",
    "    \"RHS\":      df_eff_RHS,\n",
    "    \"DHS\":      df_eff_DHS,\n",
    "    \"DST\":      df_eff_DST,\n",
    "}\n",
    "\n",
    "print(\"df_eff summaries:\")\n",
    "for name, df in models_df.items():\n",
    "    print(name, summarize_df(df))\n",
    "\n",
    "print(\"\\nEigenvalue summaries:\")\n",
    "for name, eigs in models_eigs.items():\n",
    "    print(name, summarize_eigs(eigs, small=0.1, large=0.9))\n",
    "\n",
    "# Plots to visualize differences in shrinkage\n",
    "plot_hist_eigs(models_eigs, title=\"Histogram of shrinkage eigenvalues r\")\n",
    "plot_hist_log1p_rho(models_eigs, title=\"Histogram of log(1+rho)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forskjell mellom lambda_eff og lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_samples = prior_N100_fits['Regularized Horseshoe']['posterior'].stan_variable(\"lambda\")[1].flatten()\n",
    "reg_lambda_samples = prior_N100_fits['Regularized Horseshoe']['posterior'].stan_variable(\"lambda_tilde\")[1].flatten()\n",
    "tau_samples = prior_N100_fits['Regularized Horseshoe']['posterior'].stan_variable(\"tau\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "P = P_RHS[1]\n",
    "S = S_RHS[1]\n",
    "\n",
    "# Gitt: P (diagonal positiv), S (symmetrisk)\n",
    "p = np.diag(P)                       # diagonalene i P\n",
    "P_inv_sqrt = np.diag(1.0/np.sqrt(p))          # P^{-1/2}\n",
    "W = P_inv_sqrt @ S @ P_inv_sqrt                        # whitened\n",
    "\n",
    "# Symmetrisk EVD\n",
    "r, U = np.linalg.eigh(W)             # r = egenverdier (stigende), U kolonner = egenvektorer\n",
    "\n",
    "inv_lambda2 = 1.0 / (lambda_samples**2)\n",
    "# λ_eff_i^2 = 1 / sum_j (U_{ji}^2 / λ_j^2)\n",
    "lambda_eff_sq = 1.0 / (U**2 @ inv_lambda2)\n",
    "lambda_eff = np.sqrt(lambda_eff_sq)  # shape (160,)\n",
    "\n",
    "lambda_eff.shape, lambda_eff[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(lambda_eff), np.median(reg_lambda_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lam_eff = lambda_eff  # your 160 values\n",
    "s_hat = np.median(lam_eff)\n",
    "\n",
    "# QQ points\n",
    "p = (np.arange(1, len(lam_eff)+1) - 0.5) / len(lam_eff)\n",
    "q_theory = s_hat * np.tan(0.5*np.pi*p)\n",
    "q_emp = np.sort(lam_eff)\n",
    "\n",
    "# q_emp vs q_theory ~ line y=x if Half-Cauchy(s_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import halfcauchy\n",
    "\n",
    "# --- QQ plot function ---\n",
    "def qq_plot(data, label, color):\n",
    "    n = len(data)\n",
    "    p = (np.arange(1, n+1) - 0.5) / n\n",
    "    q_theory = halfcauchy.ppf(p, scale=1)  # Half-Cauchy(0,1)\n",
    "    q_emp = np.sort(data)\n",
    "    plt.scatter(q_theory, q_emp, label=label, alpha=0.7, color=color)\n",
    "\n",
    "# --- Tail plot function ---\n",
    "def tail_plot(data, label, color):\n",
    "    sorted_data = np.sort(data)\n",
    "    n = len(data)\n",
    "    surv_emp = np.arange(n, 0, -1) / n  # empirical survival\n",
    "    plt.plot(sorted_data, sorted_data * surv_emp, label=label, color=color)\n",
    "\n",
    "# -----------------------------\n",
    "# QQ plot\n",
    "plt.figure(figsize=(6,6))\n",
    "qq_plot(lambda_eff, \"lambda_eff\", \"C0\")\n",
    "qq_plot(reg_lambda_samples, \"lambda_draws\", \"C1\")\n",
    "lims = [0, max(np.max(lambda_eff), np.max(reg_lambda_samples), 10)]\n",
    "plt.plot(lims, lims, 'k--', lw=1, label=\"y=x\")\n",
    "plt.xlabel(\"Theoretical Half-Cauchy(0,1) quantiles\")\n",
    "plt.ylabel(\"Empirical quantiles\")\n",
    "plt.title(\"QQ-plot vs Half-Cauchy(0,1)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Tail plot\n",
    "x = np.linspace(0.1, 10, 200)\n",
    "surv_theory = 1 - halfcauchy.cdf(x, scale=1)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "tail_plot(lambda_eff, \"lambda_eff\", \"C0\")\n",
    "tail_plot(reg_lambda_samples, \"lambda_draws\", \"C1\")\n",
    "plt.plot(x, x * surv_theory, 'k--', lw=1, label=\"Half-Cauchy(0,1)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"x * Survival(x)\")\n",
    "plt.title(\"Tail diagnostic: x * P(X>x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build linearized $\\bar{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linearized_mean(\n",
    "    X, y,\n",
    "    W_all, b1_all, b2_all, v_all,          # (D,H,p), (D,H), (D,), (D,H)\n",
    "    sigma_all, tau_w_all, tau_v_all,       # (D,), (D,), (D,)\n",
    "    lambda_all,                            # (D,H,p)\n",
    "    activation=\"tanh\",\n",
    "    return_mats=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Per draw d, compute:\n",
    "      - R_d, P_d, S_d, Sigma_y_d, J_d from your local function\n",
    "      - y*_d = (y - b2_d*1) + J_d @ vec(W0_d)\n",
    "      - g_d  = J_d^T (Sigma_y_d^{-1} y*_d)  [via solve]\n",
    "      - bar_w_d = (P_d + S_d)^{-1} g_d      [via solve]\n",
    "\n",
    "    Returns:\n",
    "      R_stack    : (D, N, N)  (None if return_mats=False)\n",
    "      w_bar_stack: (D, N)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    D, H, p = W_all.shape\n",
    "    N = H * p\n",
    "    n = y.shape[0]\n",
    "\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    w_bar_stack = np.empty((D, N))\n",
    "\n",
    "    # ensure 1-D copy of y\n",
    "    y = np.asarray(y, dtype=float).reshape(n)\n",
    "\n",
    "    for d in range(D):\n",
    "        # Unchanged local call; MUST return (R, P, S, Sigma_y, J, H)\n",
    "        R, P, S, Sigma_y, J, _ = compute_shrinkage_for_W_block(\n",
    "            X=X,\n",
    "            W0=W_all[d],\n",
    "            b0=b1_all[d],\n",
    "            v0=v_all[d],\n",
    "            sigma=float(sigma_all[d]),\n",
    "            tau_w=float(tau_w_all[d]),\n",
    "            tau_v=float(tau_v_all[d]),\n",
    "            lambda_tilde=lambda_all[d],\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "        # --- Build y* = (y - c*1) + J @ vec(W0) ---\n",
    "        c_d = float(b2_all[d])                 # output bias per draw (scalar)\n",
    "        z = y - c_d                            # centered response\n",
    "        w0_vec = W_all[d].reshape(-1)          # (N,)\n",
    "        y_star = z + J @ w0_vec                # (n,)\n",
    "\n",
    "        # --- Compute g = J^T Sigma_y^{-1} y* using solves (stable) ---\n",
    "        # r = Sigma_y^{-1} y*\n",
    "        r = np.linalg.solve(Sigma_y, y_star)   # (n,)\n",
    "        g = J.T @ r                             # (N,)\n",
    "\n",
    "        # --- bar_w = (P + S)^{-1} g  (stable solve; no explicit inverse) ---\n",
    "        bar_w = np.linalg.solve(P + S, g)      # (N,)\n",
    "\n",
    "        if return_mats:\n",
    "            R_stack[d] = R\n",
    "\n",
    "        w_bar_stack[d] = bar_w\n",
    "\n",
    "    return R_stack, w_bar_stack\n",
    "\n",
    "W, b1, v, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Gaussian tanh')\n",
    "\n",
    "R_stack_gauss, w_bar_stack_gauss = compute_linearized_mean(X, y, W, b1, b2, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W, b1, v, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Regularized Horseshoe tanh')\n",
    "\n",
    "R_stack_RHS, w_bar_stack_RHS = compute_linearized_mean(X, y, W, b1, b2, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W, b1, v, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Horseshoe tanh')\n",
    "\n",
    "R_stack_DHS, w_bar_stack_DHS = compute_linearized_mean(X, y, W, b1, b2, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W, b1, v, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Student T tanh')\n",
    "\n",
    "R_stack_DST, w_bar_stack_DST = compute_linearized_mean(X, y, W, b1, b2, v, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_all_gauss = posterior_N100_fits['Gaussian tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_gauss = posterior_N100_fits['Gaussian tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_RHS = posterior_N100_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_RHS = posterior_N100_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_DHS = posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_DHS = posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_DST = posterior_N100_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_DST = posterior_N100_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_compare(W_all, v_all, w_bar_stack, sort_key=\"abs_v\"):\n",
    "    \"\"\"\n",
    "    Align signs & permutations across draws before comparing linearized mean with posterior mean.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    W_all        : array-like, shape (D, H, p) or (D, p, H) or with stray singleton dims.\n",
    "    v_all        : array-like, shape (D, H) or (D, H, 1) or similar (length H per draw).\n",
    "    w_bar_stack  : array-like, shape (D, H*p) OR (D, H, p) OR (D, 1, H*p), etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W_fix        : (D, H, p)   sign/permutation aligned\n",
    "    v_fix        : (D, H)\n",
    "    wbar_fix     : (D, H, p)\n",
    "    summary      : dict with RMSE, Corr, CosSim, SignAgree (means vs means in aligned basis)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    W_all = np.asarray(W_all)\n",
    "    v_all = np.asarray(v_all)\n",
    "    w_bar_stack = np.asarray(w_bar_stack)\n",
    "\n",
    "    D = W_all.shape[0]\n",
    "\n",
    "    # --- infer H from v (source of truth) ---\n",
    "    v0 = np.squeeze(v_all[0]).ravel()\n",
    "    H = v0.size\n",
    "    if H == 0:\n",
    "        raise ValueError(\"v_all[0] seems empty; cannot infer H.\")\n",
    "    # infer p from w_bar_stack length\n",
    "    wb0 = np.squeeze(w_bar_stack[0]).ravel()\n",
    "    if wb0.size % H != 0:\n",
    "        # fallback: try infer p from W_all[0] after squeezing\n",
    "        W0 = np.squeeze(W_all[0])\n",
    "        if W0.ndim != 2:\n",
    "            # try to drop any singleton dims\n",
    "            W0 = W0.reshape([s for s in W0.shape if s != 1])\n",
    "        if W0.ndim != 2:\n",
    "            raise ValueError(f\"Cannot infer (H,p). v length={H}, but w_bar_stack[0] has {wb0.size} elems \"\n",
    "                             f\"and W_all[0] has shape {np.squeeze(W_all[0]).shape}.\")\n",
    "        h, p_candidate = W0.shape\n",
    "        if h != H and p_candidate == H:\n",
    "            p = h\n",
    "        else:\n",
    "            p = p_candidate\n",
    "    else:\n",
    "        p = wb0.size // H\n",
    "\n",
    "    N = H * p\n",
    "\n",
    "    # alloc outputs\n",
    "    W_fix = np.empty((D, H, p), dtype=float)\n",
    "    v_fix = np.empty((D, H), dtype=float)\n",
    "    wbar_fix = np.empty((D, H, p), dtype=float)\n",
    "\n",
    "    def coerce_W(Wd, H, p):\n",
    "        \"\"\"Return Wd as (H,p). Accepts (H,p), (p,H), or with singleton dims.\"\"\"\n",
    "        A = np.asarray(Wd, dtype=float)\n",
    "        A = np.squeeze(A)\n",
    "        if A.ndim == 2:\n",
    "            h, q = A.shape\n",
    "            if h == H and q == p:\n",
    "                return A\n",
    "            if h == p and q == H:\n",
    "                return A.T\n",
    "            # If one matches H, try reshape to (H, -1)\n",
    "            if h == H and h*q == H*p:\n",
    "                return A.reshape(H, p)\n",
    "            if q == H and h*q == H*p:\n",
    "                return A.T.reshape(H, p)\n",
    "            raise ValueError(f\"Cannot coerce W of shape {A.shape} to (H,p)=({H},{p}).\")\n",
    "        elif A.ndim == 3 and 1 in A.shape:\n",
    "            # squeeze singleton and recurse\n",
    "            return coerce_W(np.squeeze(A), H, p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected W ndim={A.ndim}, shape={A.shape}\")\n",
    "\n",
    "    def coerce_v(vd, H):\n",
    "        \"\"\"Return vd as (H,)\"\"\"\n",
    "        v = np.asarray(vd, dtype=float).squeeze().ravel()\n",
    "        if v.size != H:\n",
    "            raise ValueError(f\"v has size {v.size}, expected H={H}.\")\n",
    "        return v\n",
    "\n",
    "    def coerce_wbar_row(wbd, H, p):\n",
    "        \"\"\"Return wbar row as (H,p) from (N,) or already (H,p).\"\"\"\n",
    "        w = np.asarray(wbd, dtype=float).squeeze().ravel()\n",
    "        if w.size == H * p:\n",
    "            return w.reshape(H, p)\n",
    "        # already 2D?\n",
    "        W2 = np.asarray(wbd, dtype=float).squeeze()\n",
    "        if W2.ndim == 2 and W2.shape == (H, p):\n",
    "            return W2\n",
    "        raise ValueError(f\"w_bar row has {w.size} elems but H*p={H*p} and not (H,p).\")\n",
    "\n",
    "    for d in range(D):\n",
    "        # coerce shapes\n",
    "        Wd = coerce_W(W_all[d], H, p)          # (H,p)\n",
    "        vd = coerce_v(v_all[d], H)             # (H,)\n",
    "        wbd = coerce_wbar_row(w_bar_stack[d], H, p)\n",
    "\n",
    "        # 1) sign fix so v >= 0\n",
    "        s = np.sign(vd)\n",
    "        s[s == 0.0] = 1.0\n",
    "        Wd = Wd * s[:, None]\n",
    "        wbd = wbd * s[:, None]\n",
    "        vd = np.abs(vd)\n",
    "\n",
    "        # 2) permute units by a stable key\n",
    "        if sort_key == \"abs_v\":\n",
    "            idx = np.argsort(-vd)  # descending |v|\n",
    "        elif sort_key == \"abs_v_times_rownorm\":\n",
    "            idx = np.argsort(-(vd * np.linalg.norm(Wd, axis=1)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sort_key: {sort_key}\")\n",
    "\n",
    "        W_fix[d] = Wd[idx]\n",
    "        wbar_fix[d] = wbd[idx]\n",
    "        v_fix[d] = vd[idx]\n",
    "\n",
    "    # Compare means in aligned basis\n",
    "    w_post_mean = W_fix.reshape(D, -1).mean(axis=0)   # (N,)\n",
    "    w_lin_mean  = wbar_fix.reshape(D, -1).mean(axis=0)\n",
    "\n",
    "    rmse = float(np.sqrt(np.mean((w_lin_mean - w_post_mean)**2)))\n",
    "    corr = float(np.corrcoef(w_lin_mean, w_post_mean)[0, 1])\n",
    "    cos  = float(np.dot(w_lin_mean, w_post_mean) /\n",
    "                 (np.linalg.norm(w_lin_mean) * np.linalg.norm(w_post_mean)))\n",
    "    sign_agree = float(np.mean(np.sign(w_lin_mean) == np.sign(w_post_mean)))\n",
    "\n",
    "    summary = dict(RMSE=rmse, Corr=corr, CosSim=cos, SignAgree=sign_agree,\n",
    "                   H=H, p=p, N=N)\n",
    "    return W_fix, v_fix, wbar_fix, summary\n",
    "\n",
    "active_cols   = np.arange(0, 5)     # adjust if your actives are different\n",
    "inactive_cols = np.arange(5, 10)\n",
    "def block_stats(A, B, cols):\n",
    "    a = A[:, cols].ravel(); b = B[:, cols].ravel()\n",
    "    return np.corrcoef(a, b)[0,1], np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_gauss, v_all_gauss, w_bar_stack_gauss, sort_key=\"abs_v\")\n",
    "print(\"Gaussian: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_RHS, v_all_RHS, w_bar_stack_RHS, sort_key=\"abs_v\")\n",
    "\n",
    "H, p = summary[\"H\"], summary[\"p\"]\n",
    "lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n",
    "print(\"RHS: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_DHS, v_all_DHS, w_bar_stack_DHS, sort_key=\"abs_v\")\n",
    "\n",
    "H, p = summary[\"H\"], summary[\"p\"]\n",
    "lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n",
    "print(\"DHS: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_DST, v_all_DST, w_bar_stack_DST, sort_key=\"abs_v\")\n",
    "\n",
    "H, p = summary[\"H\"], summary[\"p\"]\n",
    "lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n",
    "print(\"DST: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "\n",
    "H, p = summary[\"H\"], summary[\"p\"]\n",
    "lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
