{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_priors = \"results/priors/single_layer/tanh/friedman\"\n",
    "results_dir_posteriors = \"results/regression/single_layer/tanh/friedman\"\n",
    "\n",
    "prior_names = [\"Dirichlet Horseshoe\", \"Regularized Horseshoe\", \"Dirichlet Student T\", \"Gaussian\"]\n",
    "posterior_names = [\"Dirichlet Horseshoe tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Student T tanh\", \"Gaussian tanh\"]\n",
    "\n",
    "\n",
    "prior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "prior_N200_fits = get_model_fits(\n",
    "    config=\"Friedman_N200_p10_sigma1.00_seed2\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "prior_N500_fits = get_model_fits(\n",
    "    config=\"Friedman_N500_p10_sigma1.00_seed11\",\n",
    "    results_dir=results_dir_priors,\n",
    "    models=prior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "    \n",
    "posterior_N100_fits = get_model_fits(\n",
    "    config=\"Friedman_N100_p10_sigma1.00_seed1\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N200_fits = get_model_fits(\n",
    "    config=\"Friedman_N200_p10_sigma1.00_seed2\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n",
    "\n",
    "posterior_N500_fits = get_model_fits(\n",
    "    config=\"Friedman_N500_p10_sigma1.00_seed11\",\n",
    "    results_dir=results_dir_posteriors,\n",
    "    models=posterior_names,\n",
    "    include_prior=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/friedman/Friedman_N100_p10_sigma1.00_seed1.npz\"\n",
    "data = np.load(path)\n",
    "X = data['X_train']\n",
    "y = data['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import extract_model_draws, compute_shrinkage\n",
    "\n",
    "# ---------- Minimal kjøreeksempel ----------\n",
    "#draw = 0\n",
    "#W, b1, v, b2, noise, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Gaussian tanh')\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    posterior_N100_fits, model='Gaussian tanh'\n",
    ")\n",
    "R_gauss, S_gauss, P_gauss, G_gauss, shrink_gauss, eigs_gauss, df_gauss = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with Gauss\")\n",
    "\n",
    "#W, b1, v, b2, noise, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Regularized Horseshoe tanh')\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    posterior_N100_fits, model='Regularized Horseshoe tanh'\n",
    ")\n",
    "# YOU CAN ALSO LOOK AT THE RAW LAMBDA VALUES BY RUNNING:\n",
    "# W, b1, v, b2, noise, tau_w, tau_v, lambda_raw = extract_model_draws(\n",
    "#     posterior_N100_fits, model='Regularized Horseshoe tanh', lambda_kind='raw'\n",
    "# )\n",
    "R_RHS, S_RHS, P_RHS, G_RHS, shrink_RHS, eigs_RHS, df_eff_RHS = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with RHS\")\n",
    "#W, b1, v, b2, noise, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Horseshoe tanh')\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    posterior_N100_fits, model='Dirichlet Horseshoe tanh'\n",
    ")\n",
    "R_DHS, S_DHS, P_DHS, G_DHS, shrink_DHS, eigs_DHS, df_eff_DHS = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with DHS\")\n",
    "#W, b1, v, b2, noise, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Student T tanh')\n",
    "W, b1, v, b2, noise, tau_w, tau_v, lambda_eff = extract_model_draws(\n",
    "    posterior_N100_fits, model='Dirichlet Student T tanh'\n",
    ")\n",
    "R_DST, S_DST, P_DST, G_DST, shrink_DST, eigs_DST, df_eff_DST = compute_shrinkage(\n",
    "    X, W, b1, v, noise, tau_w, tau_v, lambda_eff,\n",
    "    activation=\"tanh\",\n",
    "    include_b1_in_Sigma=True,\n",
    "    include_b2_in_Sigma=True,\n",
    ")\n",
    "print(\"done with DST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = np.zeros(4000)\n",
    "for i in range(4000):\n",
    "    check[i] = np.all(np.linalg.eigvals(S_DHS[i]) >= 0)\n",
    "\n",
    "print(np.sum(check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on SPD for Abalone:\n",
    "\n",
    "all Gaussian P matrices are spd, 3999/4000 S matrices are spd\n",
    "\n",
    "all RHS P matrices are spd, 3921/4000 RHS S matrices are spd\n",
    "\n",
    "all DHS P matrices are spd, 3980/4000 DHS S matrices are spd\n",
    "\n",
    "all DST P matrices are spd, 3983/4000 DST S matrices are spd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import visualize_models\n",
    "\n",
    "matrices_S = [\n",
    "    np.mean(S_gauss, axis=0),\n",
    "    np.mean(S_RHS, axis=0),\n",
    "    np.mean(S_DHS, axis=0),\n",
    "    np.mean(S_DST, axis=0),\n",
    "]\n",
    "names_S = [\"S (Gauss)\", \"S (RHS)\", \"S (DHS)\", \"S (DST)\"]\n",
    "\n",
    "matrices_G = [\n",
    "    np.mean((G_gauss), axis=0),\n",
    "    np.mean((G_RHS), axis=0),\n",
    "    np.mean((G_DHS), axis=0),\n",
    "    np.mean((G_DST), axis=0),\n",
    "]\n",
    "\n",
    "names_G = [\"G (Gauss)\", \"G (RHS)\", \"G (DHS)\", \"G (DST)\"]\n",
    "\n",
    "matrices_shrink = [\n",
    "    np.mean((shrink_gauss), axis=0),\n",
    "    np.mean((shrink_RHS), axis=0),\n",
    "    np.mean((shrink_DHS), axis=0),\n",
    "    np.mean((shrink_DST), axis=0),\n",
    "]\n",
    "\n",
    "names_shrink = [\"(I+G)^{-1}G (Gauss)\", \"(I+G)^{-1}G (RHS)\", \"(I+G)^{-1}G (DHS)\", \"(I+G)^{-1}G (DST)\"]\n",
    "\n",
    "SP_inv_S_gauss = np.eye(16*10)[:, :] - R_gauss\n",
    "SP_inv_S_RHS = np.eye(16*10)[:, :] - R_RHS\n",
    "SP_inv_S_DHS = np.eye(16*10)[:, :] - R_DHS\n",
    "SP_inv_S_DST = np.eye(16*10)[:, :] - R_DST\n",
    "\n",
    "matrices_operator = [\n",
    "    np.mean((SP_inv_S_gauss), axis=0),\n",
    "    np.mean((SP_inv_S_RHS), axis=0),\n",
    "    np.mean((SP_inv_S_DHS), axis=0),\n",
    "    np.mean((SP_inv_S_DST), axis=0),\n",
    "]\n",
    "\n",
    "names_operator = [\"(P+S)^{-1}S (Gauss)\", \"(P+S)^{-1}S (RHS)\", \"(P+S)^{-1}S (DHS)\", \"(P+S)^{-1}S (DST)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_S, names_S, H=16, p=10, use_abs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_G, names_G, H=16, p=10, use_abs=False)#, cmap=\"magma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_shrink, names_shrink, H=16, p=10, use_abs=False)#, cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_models(matrices_operator, names_operator, H=16, p=10, use_abs=False)#, cmap=\"magma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_point1_aligned(A, B, nameA=\"A\", nameB=\"B\",\n",
    "                        H=16, p=8, use_abs=False, q_low=0.05, q_high=0.99):\n",
    "    \"\"\"\n",
    "    Point (1): Best-scale–aligned difference.\n",
    "      - Panel 1: A\n",
    "      - Panel 2: c*·B  (c* = <A,B>_F / ||B||_F^2)\n",
    "      - Panel 3: A - c*·B\n",
    "      - Panel 4: (blank filler)\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, float); B = np.asarray(B, float)\n",
    "    num = np.sum(A * B)\n",
    "    den = np.sum(B * B) if np.sum(B * B) != 0 else 1.0\n",
    "    c_star = num / den\n",
    "    cosF = num / (np.linalg.norm(A, \"fro\") * (np.linalg.norm(B, \"fro\") + 1e-12))\n",
    "\n",
    "    mats  = [A, c_star * B, A - c_star * B, np.zeros_like(A)]\n",
    "    names = [\n",
    "        f\"{nameA}\",\n",
    "        f\"{nameB} scaled (c*={c_star:.3g})\",\n",
    "        f\"Aligned diff: {nameA} − {c_star:.3g}·{nameB}\\ncos_F={cosF:.3f}\",\n",
    "        \"(unused)\"\n",
    "    ]\n",
    "    return mats, names #visualize_models(mats, names, H=H, p=p, use_abs=use_abs, q_low=q_low, q_high=q_high)\n",
    "\n",
    "def plot_point2_unit_energy(A, B, nameA=\"A\", nameB=\"B\",\n",
    "                            H=16, p=8, use_abs=False, q_low=0.05, q_high=0.99):\n",
    "    \"\"\"\n",
    "    Point (2): Unit-energy (Frobenius-normalized) side-by-side + difference.\n",
    "      - Panel 1: A / ||A||_F\n",
    "      - Panel 2: B / ||B||_F\n",
    "      - Panel 3: (A/||A||_F) − (B/||B||_F)\n",
    "      - Panel 4: (unused filler)\n",
    "    \"\"\"\n",
    "    A = np.asarray(A, float); B = np.asarray(B, float)\n",
    "    Af = A / (np.linalg.norm(A, \"fro\") + 1e-12)\n",
    "    Bf = B / (np.linalg.norm(B, \"fro\") + 1e-12)\n",
    "\n",
    "    mats  = [Af, Bf, Af - Bf, np.zeros_like(A)]\n",
    "    names = [\n",
    "        f\"{nameA} / ||{nameA}||_F\",\n",
    "        f\"{nameB} / ||{nameB}||_F\",\n",
    "        \"Difference (unit-energy)\",\n",
    "        \"(unused)\"\n",
    "    ]\n",
    "    visualize_models(mats, names, H=H, p=p, use_abs=use_abs, q_low=q_low, q_high=q_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_DHS_v_RHS, names_DHS_v_RHS = plot_point1_aligned(np.mean((SP_inv_S_DHS), axis=0), np.mean((SP_inv_S_RHS), axis=0), \"DHS\", \"RHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_DHS_v_gauss, names_DHS_v_gauss = plot_point1_aligned(np.mean((SP_inv_S_DHS), axis=0), np.mean((SP_inv_S_gauss), axis=0), \"DHS\", \"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_DST_v_RHS, names_DST_v_RHS = plot_point1_aligned(np.mean((SP_inv_S_DST), axis=0), np.mean((SP_inv_S_RHS), axis=0), \"DST\", \"RHS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_DST_v_gauss, names_DST_v_gauss = plot_point1_aligned(np.mean((SP_inv_S_DST), axis=0), np.mean((SP_inv_S_gauss), axis=0), \"DST\", \"Gauss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats_combined = [\n",
    "    mats_DHS_v_RHS[2],\n",
    "    mats_DHS_v_gauss[2],\n",
    "    mats_DST_v_RHS[2],\n",
    "    mats_DST_v_gauss[2],\n",
    "]\n",
    "\n",
    "names_combined = [\n",
    "    names_DHS_v_RHS[2],\n",
    "    names_DHS_v_gauss[2],\n",
    "    names_DST_v_RHS[2],\n",
    "    names_DST_v_gauss[2],\n",
    "]\n",
    "\n",
    "visualize_models(mats_combined, names_combined, H=16, p=10, use_abs=False, q_low=0.05, q_high=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(G_gauss[0])\n",
    "eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Traces as distributions (df_eff = tr(R) vs total shrinkage = tr(I-R)) ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Effective dof: trace of (I+G)^{-1}G per draw\n",
    "tr_R_gauss = np.trace(shrink_gauss, axis1=1, axis2=2)\n",
    "tr_R_RHS   = np.trace(shrink_RHS,   axis1=1, axis2=2)\n",
    "tr_R_DHS   = np.trace(shrink_DHS,   axis1=1, axis2=2)\n",
    "tr_R_DST   = np.trace(shrink_DST,   axis1=1, axis2=2)\n",
    "\n",
    "# If you also want “total shrinkage”, use your SP_inv_S_* stacks (I - R):\n",
    "tr_SPinvS_gauss = np.trace(SP_inv_S_gauss, axis1=1, axis2=2)\n",
    "tr_SPinvS_RHS   = np.trace(SP_inv_S_RHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DHS   = np.trace(SP_inv_S_DHS,   axis1=1, axis2=2)\n",
    "tr_SPinvS_DST   = np.trace(SP_inv_S_DST,   axis1=1, axis2=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot df_eff distributions\n",
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_R_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_R_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_R_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_R_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(\"trace((I+G)^{-1}G)  [effective dof]\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4), dpi=150)\n",
    "bins = 40\n",
    "plt.hist(tr_SPinvS_gauss, bins=bins, alpha=0.5, label=\"Gauss\")\n",
    "plt.hist(tr_SPinvS_RHS,   bins=bins, alpha=0.5, label=\"RHS\")\n",
    "plt.hist(tr_SPinvS_DHS,   bins=bins, alpha=0.5, label=\"DHS\")\n",
    "plt.hist(tr_SPinvS_DST,   bins=bins, alpha=0.5, label=\"DST\")\n",
    "plt.xlabel(r\"$tr((P+S)^{-1}S)$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Median eigenvalue curve (with bands) for shrink stacks ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def median_eigcurve(stack, q_lo=0.1, q_hi=0.9):\n",
    "    \"\"\"\n",
    "    stack: (D, N, N) of symmetric PSD matrices with eigenvalues in [0,1].\n",
    "    Returns: dict with 'median', 'lo', 'hi' over the sorted eigenvalues (descending).\n",
    "    \"\"\"\n",
    "    D, N, _ = stack.shape\n",
    "    evals = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        w = np.linalg.eigvalsh(stack[d])\n",
    "        evals[d] = np.sort(w)[::-1]  # descending\n",
    "    med = np.median(evals, axis=0)\n",
    "    lo  = np.quantile(evals, q_lo, axis=0)\n",
    "    hi  = np.quantile(evals, q_hi, axis=0)\n",
    "    return {\"median\": med, \"lo\": lo, \"hi\": hi}\n",
    "\n",
    "curves = {\n",
    "    \"Gauss\": median_eigcurve(shrink_gauss),\n",
    "    \"RHS\":   median_eigcurve(shrink_RHS),\n",
    "    \"DHS\":   median_eigcurve(shrink_DHS),\n",
    "    \"DST\":   median_eigcurve(shrink_DST),\n",
    "}\n",
    "\n",
    "# Plot 2x2 small multiples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8,6), dpi=150, constrained_layout=True)\n",
    "axes = axes.ravel()\n",
    "for ax, (name, c) in zip(axes, curves.items()):\n",
    "    x = np.arange(1, len(c[\"median\"])+1)\n",
    "    ax.plot(x, c[\"median\"], lw=1.8, label=f\"{name} median\")\n",
    "    ax.fill_between(x, c[\"lo\"], c[\"hi\"], alpha=0.25, label=f\"{name} {10}-{90}%\", step=None)\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"eigenvalue rank\")\n",
    "    ax.set_ylabel(\"eigenvalue of (I+G)^{-1}G\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc=\"upper right\", fontsize=8)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: blockwise normalized inner-product ratio for a stack of G ---\n",
    "import numpy as np\n",
    "\n",
    "def block_ratio_median(G_stack: np.ndarray, H: int, p: int, eps: float = 1e-12):\n",
    "    \"\"\"\n",
    "    For each draw/d, computes R_d = |G_d| / sqrt(diag(G_d) diag(G_d)^T) within each (p x p) block h.\n",
    "    Returns the elementwise median over draws (ignoring cross-block entries).\n",
    "    \n",
    "    G_stack : (D, N, N) with N = H*p\n",
    "    Output  : (N, N) median ratio; outside-block entries set to 0 for convenience.\n",
    "    \"\"\"\n",
    "    D, N, _ = G_stack.shape\n",
    "    # block mask: True if (i,j) are in the same hidden-unit block\n",
    "    idx = np.arange(N)\n",
    "    h_idx = idx // p\n",
    "    same_block = (h_idx[:, None] == h_idx[None, :])\n",
    "    \n",
    "    ratios = np.empty_like(G_stack)\n",
    "    for d in range(D):\n",
    "        G = 0.5 * (G_stack[d] + G_stack[d].T)                     # symmetrize\n",
    "        dvec = np.clip(np.diag(G), eps, None)\n",
    "        denom = np.sqrt(np.outer(dvec, dvec))\n",
    "        R = np.divide(np.abs(G), denom, out=np.zeros_like(G), where=(denom > 0))\n",
    "        # keep only within-block entries; mark cross-block as NaN (ignored in median)\n",
    "        R_masked = np.where(same_block, R, np.nan)\n",
    "        ratios[d] = R_masked\n",
    "\n",
    "    # median over draws ignoring NaNs (i.e., cross-block)\n",
    "    R_med = np.nanmedian(ratios, axis=0)\n",
    "    # put zeros outside blocks for cleaner plotting\n",
    "    R_med = np.where(same_block, R_med, 0.0)\n",
    "    # exact ones on the diagonal by definition\n",
    "    np.fill_diagonal(R_med, 1.0)\n",
    "    return R_med\n",
    "\n",
    "# --- Compute median ratio matrices for the four priors ---\n",
    "H, p = 16, 10  # adjust if needed\n",
    "ratio_Gauss = block_ratio_median(G_gauss, H=H, p=p)\n",
    "ratio_RHS   = block_ratio_median(G_RHS,   H=H, p=p)\n",
    "ratio_DHS   = block_ratio_median(G_DHS,   H=H, p=p)\n",
    "ratio_DST   = block_ratio_median(G_DST,   H=H, p=p)\n",
    "\n",
    "\n",
    "# --- (Optional) Scalar summaries: median off-diagonal ratio within blocks ---\n",
    "def offdiag_block_stats(R_med: np.ndarray, H: int, p: int):\n",
    "    N = H * p\n",
    "    idx = np.arange(N)\n",
    "    h_idx = idx // p\n",
    "    same_block = (h_idx[:, None] == h_idx[None, :])\n",
    "    offdiag = same_block & (~np.eye(N, dtype=bool))\n",
    "    vals = R_med[offdiag]\n",
    "    return {\n",
    "        \"median_offdiag\": float(np.median(vals)),\n",
    "        \"q10_offdiag\": float(np.quantile(vals, 0.10)),\n",
    "        \"q90_offdiag\": float(np.quantile(vals, 0.90)),\n",
    "    }\n",
    "\n",
    "print(\"Gauss:\", offdiag_block_stats(ratio_Gauss, H, p))\n",
    "print(\"RHS:  \", offdiag_block_stats(ratio_RHS,   H, p))\n",
    "print(\"DHS:  \", offdiag_block_stats(ratio_DHS,   H, p))\n",
    "print(\"DST:  \", offdiag_block_stats(ratio_DST,   H, p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (1) Bound Σ_y^{-1}: compute c_min, c_max for a single draw ---\n",
    "import numpy as np\n",
    "\n",
    "def sigma_inverse_bounds(Phi_mat, tau_v, noise, J_b1=None, J_b2=None,\n",
    "                         include_b1=True, include_b2=True):\n",
    "    \"\"\"\n",
    "    Constructs Q so that Σ_y = σ^2 I + Q Q^T, then returns:\n",
    "      c_min = 1 / (σ^2 + ||Q||_2^2),   c_max = 1 / σ^2.\n",
    "    Here Q = [ J_b1,  τ_v * Φ,  (J_b2) ] with columns included per flags.\n",
    "    \"\"\"\n",
    "    cols = [tau_v * Phi_mat]  # Φ always contributes with τ_v\n",
    "    if include_b1 and (J_b1 is not None):\n",
    "        cols.insert(0, J_b1)  # [J_b1, τ_v Φ, ...]\n",
    "    if include_b2 and (J_b2 is not None):\n",
    "        cols.append(J_b2[:, None])  # add as a column\n",
    "\n",
    "    Q = np.concatenate(cols, axis=1) if len(cols) > 1 else cols[0]\n",
    "    # spectral norm of Q (largest singular value)\n",
    "    smax = np.linalg.svd(Q, compute_uv=False)[0]\n",
    "    c_min = 1.0 / (noise**2 + smax**2)\n",
    "    c_max = 1.0 / (noise**2)\n",
    "    return c_min, c_max, smax\n",
    "\n",
    "# --- (2) Bound S and G via c_min, c_max; also get α-eigs of A = P^{-1/2} J^T J P^{-1/2} ---\n",
    "def bound_S_and_G(JW, P, c_min, c_max):\n",
    "    \"\"\"\n",
    "    S = J^T Σ_y^{-1} J, with c_min*J^T J ⪯ S ⪯ c_max*J^T J.\n",
    "    Define A = P^{-1/2} J^T J P^{-1/2}; then c_min*A ⪯ G ⪯ c_max*A, where G=P^{-1/2} S P^{-1/2}.\n",
    "    Returns:\n",
    "      JJ      : J^T J\n",
    "      A       : P^{-1/2} (J^T J) P^{-1/2}\n",
    "      alpha   : eigvals(A) sorted ascending\n",
    "      S_lo/up : lower/upper Loewner bounds on S\n",
    "      G_lo/up : lower/upper Loewner bounds on G\n",
    "    \"\"\"\n",
    "    JJ = JW.T @ JW\n",
    "    d = np.diag(P).astype(float)\n",
    "    Pinvhalf = np.diag(1.0 / np.sqrt(np.maximum(d, 1e-12)))\n",
    "    A = Pinvhalf @ JJ @ Pinvhalf\n",
    "    alpha = np.linalg.eigvalsh(A)  # >=0, ascending\n",
    "\n",
    "    S_lower = c_min * JJ\n",
    "    S_upper = c_max * JJ\n",
    "    G_lower = c_min * A\n",
    "    G_upper = c_max * A\n",
    "    return JJ, A, alpha, S_lower, S_upper, G_lower, G_upper\n",
    "\n",
    "# --- (3) Eigenvalue bands for (I+G)^{-1} G and trace/df bounds from α-eigs and c_min/c_max ---\n",
    "def shrinkage_bands_from_alpha(alpha, c_min, c_max):\n",
    "    \"\"\"\n",
    "    For f(t)=t/(1+t), eigenvalues of (I+G)^{-1}G satisfy:\n",
    "       f(c_min*alpha_i) <= λ_i <= f(c_max*alpha_i)\n",
    "    Returns lower/upper arrays (ascending to match alpha).\n",
    "    \"\"\"\n",
    "    f = lambda t: t / (1.0 + t)\n",
    "    lam_lo = f(c_min * alpha)\n",
    "    lam_hi = f(c_max * alpha)\n",
    "    return lam_lo, lam_hi\n",
    "\n",
    "def df_bounds_from_alpha(alpha, c_min, c_max):\n",
    "    \"\"\"Lower/upper bounds on df_eff = tr( (I+G)^{-1}G ).\"\"\"\n",
    "    lam_lo, lam_hi = shrinkage_bands_from_alpha(alpha, c_min, c_max)\n",
    "    return float(np.sum(lam_lo)), float(np.sum(lam_hi))\n",
    "\n",
    "# --- (4) Bands over ALL draws (median band to overlay with your median eigenvalue curve) ---\n",
    "def eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "):\n",
    "    \"\"\"\n",
    "    For each draw:\n",
    "      - build JW, Φ, J_b1, J_b2, get c_min/c_max,\n",
    "      - build P and α-eigenvalues of A,\n",
    "      - compute lower/upper shrinkage eigenvalue bands.\n",
    "    Returns medians (and optional quantiles) across draws:\n",
    "      lam_lo_med, lam_hi_med, df_lo_med, df_hi_med\n",
    "    \"\"\"\n",
    "    D, H, p = W_all.shape\n",
    "    N = H * p\n",
    "    lam_lo_stack = np.empty((D, N))\n",
    "    lam_hi_stack = np.empty((D, N))\n",
    "    df_lo = np.empty(D)\n",
    "    df_hi = np.empty(D)\n",
    "\n",
    "    for d in range(D):\n",
    "        Phi_mat, JW, Jb1, Jb2 = build_hidden_and_jacobian_W(\n",
    "            X, W_all[d], b_all[d], v_all[d], activation=activation\n",
    "        )\n",
    "        c_min, c_max, _ = sigma_inverse_bounds(\n",
    "            Phi_mat, tau_v=float(tau_v_all[d]), noise=float(sigma_all[d]),\n",
    "            J_b1=(Jb1 if include_b1_in_Sigma else None),\n",
    "            J_b2=(Jb2 if include_b2_in_Sigma else None),\n",
    "            include_b1=include_b1_in_Sigma, include_b2=include_b2_in_Sigma\n",
    "        )\n",
    "        P = build_P_from_lambda_tau(lambda_all[d], tau_w=float(tau_w_all[d]))\n",
    "\n",
    "        _, _, alpha, _, _, _, _ = bound_S_and_G(JW, P, c_min, c_max)\n",
    "        lam_lo, lam_hi = shrinkage_bands_from_alpha(alpha, c_min, c_max)\n",
    "        lam_lo_stack[d] = lam_lo\n",
    "        lam_hi_stack[d] = lam_hi\n",
    "        df_lo[d], df_hi[d] = df_bounds_from_alpha(alpha, c_min, c_max)\n",
    "\n",
    "    # Medians across draws (coordinate-wise)\n",
    "    lam_lo_med = np.median(lam_lo_stack, axis=0)\n",
    "    lam_hi_med = np.median(lam_hi_stack, axis=0)\n",
    "    df_lo_med  = float(np.median(df_lo))\n",
    "    df_hi_med  = float(np.median(df_hi))\n",
    "\n",
    "    # (Optional) also return 10/90% ribbons if you want\n",
    "    lam_lo_q10 = np.quantile(lam_lo_stack, 0.10, axis=0)\n",
    "    lam_hi_q90 = np.quantile(lam_hi_stack, 0.90, axis=0)\n",
    "\n",
    "    summary = {\n",
    "        \"df_lo_med\": df_lo_med,\n",
    "        \"df_hi_med\": df_hi_med,\n",
    "        \"df_lo_q10\": float(np.quantile(df_lo, 0.10)),\n",
    "        \"df_hi_q90\": float(np.quantile(df_hi, 0.90)),\n",
    "    }\n",
    "    return lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (5) Quick demo on one model (e.g., RHS): print Σ_y^{-1} bounds and df bounds, then plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: use your RHS arrays (replace with Gauss/DHS/DST as needed)\n",
    "W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_all = \\\n",
    "    extract_model_draws(posterior_N100_fits, model='Gaussian tanh', lambda_kind=\"effective\")\n",
    "\n",
    "# (a) Compute median eigenvalue bands across draws\n",
    "lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, band_summ = eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    ")\n",
    "\n",
    "print(\"Median df_eff bounds (Gaussian):\")\n",
    "print(f\"  lower  (Σ_y^{-1}≈1/(σ^2+||Q||^2)): {band_summ['df_lo_med']:.3f}\")\n",
    "print(f\"  upper  (Σ_y^{-1}≈1/σ^2):           {band_summ['df_hi_med']:.3f}\")\n",
    "\n",
    "# (b) If you ALREADY have the median eigenvalue curve of (I+G)^{-1}G (call it eig_med_shrink),\n",
    "#     great; otherwise compute it from shrink_stack (from compute_shrinkage)\n",
    "try:\n",
    "    nix #median_eigcurve(shrink_gauss) #eig_med_shrink\n",
    "except NameError:\n",
    "    # build shrink_stack and take median eigenvalues (ascending to match bands)\n",
    "    _, _, _, _, shrink_stack, _, _ = compute_shrinkage(\n",
    "        X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "        activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "    )\n",
    "    D, N, _ = shrink_stack.shape\n",
    "    eigs = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        eigs[d] = np.linalg.eigvalsh(shrink_stack[d])  # ascending\n",
    "    eig_med_shrink = np.median(eigs, axis=0)\n",
    "\n",
    "# (c) Overlay: median band vs. median eigenvalue curve\n",
    "x = np.arange(1, lam_lo_med.size + 1)\n",
    "plt.figure(figsize=(7.2, 4.2), dpi=150)\n",
    "plt.fill_between(x, lam_lo_med, lam_hi_med, alpha=0.25, label=\"theory band (median)\")\n",
    "plt.plot(x, eig_med_shrink, lw=1.8, label=\"empirical median eigenvalue\")\n",
    "# (optional) add thinner 10–90% theoretical ribbon\n",
    "plt.fill_between(x, lam_lo_q10, lam_hi_q90, alpha=0.15, label=\"theory band (10–90%)\")\n",
    "\n",
    "plt.xlabel(\"eigenvalue index (ascending)\")\n",
    "plt.ylabel(r\"eigenvalue of $(I+G)^{-1}G$\")\n",
    "plt.title(\"Shrinkage eigenvalue bands vs. empirical curve (Gaussian)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (5) Quick demo on one model (e.g., RHS): print Σ_y^{-1} bounds and df bounds, then plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: use your RHS arrays (replace with Gauss/DHS/DST as needed)\n",
    "W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_all = \\\n",
    "    extract_model_draws(posterior_N100_fits, model='Regularized Horseshoe tanh', lambda_kind=\"effective\")\n",
    "\n",
    "# (a) Compute median eigenvalue bands across draws\n",
    "lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, band_summ = eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    ")\n",
    "\n",
    "print(\"Median df_eff bounds (RHS):\")\n",
    "print(f\"  lower  (Σ_y^{-1}≈1/(σ^2+||Q||^2)): {band_summ['df_lo_med']:.3f}\")\n",
    "print(f\"  upper  (Σ_y^{-1}≈1/σ^2):           {band_summ['df_hi_med']:.3f}\")\n",
    "\n",
    "# (b) If you ALREADY have the median eigenvalue curve of (I+G)^{-1}G (call it eig_med_shrink),\n",
    "#     great; otherwise compute it from shrink_stack (from compute_shrinkage)\n",
    "try:\n",
    "    nix #median_eigcurve(shrink_RHS) #eig_med_shrink\n",
    "except NameError:\n",
    "    # build shrink_stack and take median eigenvalues (ascending to match bands)\n",
    "    _, _, _, _, shrink_stack, _, _ = compute_shrinkage(\n",
    "        X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "        activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "    )\n",
    "    D, N, _ = shrink_stack.shape\n",
    "    eigs = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        eigs[d] = np.linalg.eigvalsh(shrink_stack[d])  # ascending\n",
    "    eig_med_shrink = np.median(eigs, axis=0)\n",
    "\n",
    "# (c) Overlay: median band vs. median eigenvalue curve\n",
    "x = np.arange(1, lam_lo_med.size + 1)\n",
    "plt.figure(figsize=(7.2, 4.2), dpi=150)\n",
    "plt.fill_between(x, lam_lo_med, lam_hi_med, alpha=0.25, label=\"theory band (median)\")\n",
    "plt.plot(x, eig_med_shrink, lw=1.8, label=\"empirical median eigenvalue\")\n",
    "# (optional) add thinner 10–90% theoretical ribbon\n",
    "plt.fill_between(x, lam_lo_q10, lam_hi_q90, alpha=0.15, label=\"theory band (10–90%)\")\n",
    "\n",
    "plt.xlabel(\"eigenvalue index (ascending)\")\n",
    "plt.ylabel(r\"eigenvalue of $(I+G)^{-1}G$\")\n",
    "plt.title(\"Shrinkage eigenvalue bands vs. empirical curve (RHS)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (5) Quick demo on one model (e.g., RHS): print Σ_y^{-1} bounds and df bounds, then plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: use your RHS arrays (replace with Gauss/DHS/DST as needed)\n",
    "W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_all = \\\n",
    "    extract_model_draws(posterior_N100_fits, model='Dirichlet Horseshoe tanh', lambda_kind=\"effective\")\n",
    "\n",
    "# (a) Compute median eigenvalue bands across draws\n",
    "lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, band_summ = eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    ")\n",
    "\n",
    "print(\"Median df_eff bounds (DHS):\")\n",
    "print(f\"  lower  (Σ_y^{-1}≈1/(σ^2+||Q||^2)): {band_summ['df_lo_med']:.3f}\")\n",
    "print(f\"  upper  (Σ_y^{-1}≈1/σ^2):           {band_summ['df_hi_med']:.3f}\")\n",
    "\n",
    "# (b) If you ALREADY have the median eigenvalue curve of (I+G)^{-1}G (call it eig_med_shrink),\n",
    "#     great; otherwise compute it from shrink_stack (from compute_shrinkage)\n",
    "try:\n",
    "    nix #eig_med_shrink #median_eigcurve(shrink_DHS) \n",
    "except NameError:\n",
    "    # build shrink_stack and take median eigenvalues (ascending to match bands)\n",
    "    _, _, _, _, shrink_stack, _, _ = compute_shrinkage(\n",
    "        X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "        activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "    )\n",
    "    D, N, _ = shrink_stack.shape\n",
    "    eigs = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        eigs[d] = np.linalg.eigvalsh(shrink_stack[d])  # ascending\n",
    "    eig_med_shrink = np.median(eigs, axis=0)\n",
    "\n",
    "# (c) Overlay: median band vs. median eigenvalue curve\n",
    "x = np.arange(1, lam_lo_med.size + 1)\n",
    "plt.figure(figsize=(7.2, 4.2), dpi=150)\n",
    "plt.fill_between(x, lam_lo_med, lam_hi_med, alpha=0.25, label=\"theory band (median)\")\n",
    "plt.plot(x, eig_med_shrink, lw=1.8, label=\"empirical median eigenvalue\")\n",
    "# (optional) add thinner 10–90% theoretical ribbon\n",
    "plt.fill_between(x, lam_lo_q10, lam_hi_q90, alpha=0.15, label=\"theory band (10–90%)\")\n",
    "\n",
    "plt.xlabel(\"eigenvalue index (ascending)\")\n",
    "plt.ylabel(r\"eigenvalue of $(I+G)^{-1}G$\")\n",
    "plt.title(\"Shrinkage eigenvalue bands vs. empirical curve (DHS)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (5) Quick demo on one model (e.g., RHS): print Σ_y^{-1} bounds and df bounds, then plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: use your RHS arrays (replace with Gauss/DHS/DST as needed)\n",
    "W_all, b_all, v_all, c_all, sigma_all, tau_w_all, tau_v_all, lambda_all = \\\n",
    "    extract_model_draws(posterior_N100_fits, model='Dirichlet Student T tanh', lambda_kind=\"effective\")\n",
    "\n",
    "# (a) Compute median eigenvalue bands across draws\n",
    "lam_lo_med, lam_hi_med, lam_lo_q10, lam_hi_q90, band_summ = eigen_bands_over_draws(\n",
    "    X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "    activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    ")\n",
    "\n",
    "print(\"Median df_eff bounds (DST):\")\n",
    "print(f\"  lower  (Σ_y^{-1}≈1/(σ^2+||Q||^2)): {band_summ['df_lo_med']:.3f}\")\n",
    "print(f\"  upper  (Σ_y^{-1}≈1/σ^2):           {band_summ['df_hi_med']:.3f}\")\n",
    "\n",
    "# (b) If you ALREADY have the median eigenvalue curve of (I+G)^{-1}G (call it eig_med_shrink),\n",
    "#     great; otherwise compute it from shrink_stack (from compute_shrinkage)\n",
    "try:\n",
    "    nix #median_eigcurve(shrink_DST) #eig_med_shrink\n",
    "except NameError:\n",
    "    # build shrink_stack and take median eigenvalues (ascending to match bands)\n",
    "    _, _, _, _, shrink_stack, _, _ = compute_shrinkage(\n",
    "        X, W_all, b_all, v_all, sigma_all, tau_w_all, tau_v_all, lambda_all,\n",
    "        activation=\"tanh\", include_b1_in_Sigma=True, include_b2_in_Sigma=True\n",
    "    )\n",
    "    D, N, _ = shrink_stack.shape\n",
    "    eigs = np.empty((D, N))\n",
    "    for d in range(D):\n",
    "        eigs[d] = np.linalg.eigvalsh(shrink_stack[d])  # ascending\n",
    "    eig_med_shrink = np.median(eigs, axis=0)\n",
    "\n",
    "# (c) Overlay: median band vs. median eigenvalue curve\n",
    "x = np.arange(1, lam_lo_med.size + 1)\n",
    "plt.figure(figsize=(7.2, 4.2), dpi=150)\n",
    "plt.fill_between(x, lam_lo_med, lam_hi_med, alpha=0.25, label=\"theory band (median)\")\n",
    "plt.plot(x, eig_med_shrink, lw=1.8, label=\"empirical median eigenvalue\")\n",
    "# (optional) add thinner 10–90% theoretical ribbon\n",
    "plt.fill_between(x, lam_lo_q10, lam_hi_q90, alpha=0.15, label=\"theory band (10–90%)\")\n",
    "\n",
    "plt.xlabel(\"eigenvalue index (ascending)\")\n",
    "plt.ylabel(r\"eigenvalue of $(I+G)^{-1}G$\")\n",
    "plt.title(\"Shrinkage eigenvalue bands vs. empirical curve (DST)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build linearized $\\bar{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.kappa_matrix import build_hidden_and_jacobian_W, build_Sigma_y, build_S, build_P_from_lambda_tau, shrinkage_matrix_stable\n",
    "\n",
    "def compute_linearized_mean(\n",
    "    X, y,\n",
    "    W_1, b_1, W_2, b_2,          # (D,H,p), (D,H), (D,), (D,H)\n",
    "    noise_all, tau_w_all, tau_v_all,       # (D,), (D,), (D,)\n",
    "    lambda_all,                            # (D,H,p)\n",
    "    activation=\"tanh\",\n",
    "    return_mats=True,\n",
    "    include_b1_in_Sigma: bool = True,      # pass-through to your Σ_y builder (if used downstream)\n",
    "    include_b2_in_Sigma: bool = True,      # pass-through to your Σ_y builder (if used downstream)\n",
    "):\n",
    "    \"\"\"\n",
    "    Per draw d, compute:\n",
    "      - (R_d, P_d, S_d, Sigma_y_d, J_w,d, [J_b,d], Phi_0,d) from your local function\n",
    "      - y*_d = (y - Φ0_d w2,0_d [ - b2,0_d*1 ]) + J_w,d @ vec(W1,0_d) [ + J_b,d @ b1,0_d ]\n",
    "      - g_d  = J_w,d^T (Sigma_y_d^{-1} y*_d)              [via solve]\n",
    "      - bar_w_d = (P_d + S_d)^{-1} g_d                    [via solve]\n",
    "    Returns:\n",
    "      R_stack    : (D, N, N)  (None if return_mats=False)\n",
    "      w_bar_stack: (D, N)\n",
    "    \"\"\"\n",
    "\n",
    "    D, H, p = W_1.shape\n",
    "    N = H * p\n",
    "    n = y.shape[0]\n",
    "\n",
    "    R_stack = np.empty((D, N, N)) if return_mats else None\n",
    "    w_bar_stack = np.empty((D, N))\n",
    "    w_hat_stack = np.empty((D, N))\n",
    "\n",
    "    y = np.asarray(y, dtype=float).reshape(n)\n",
    "    \n",
    "\n",
    "    for d in range(D):\n",
    "        Phi_mat, JW, Jb1, Jb2 = build_hidden_and_jacobian_W(X, W_1[d], b_1[d], W_2[d], activation=activation)  \n",
    "        Sigma_y = build_Sigma_y(\n",
    "            Phi_mat,\n",
    "            tau_v=tau_v_all[d],\n",
    "            noise=noise_all[d],\n",
    "            J_b1=Jb1,\n",
    "            J_b2=Jb2,\n",
    "            include_b1=include_b1_in_Sigma,\n",
    "            include_b2=include_b2_in_Sigma,\n",
    "        )\n",
    "        P = build_P_from_lambda_tau(lambda_all[d], tau_w=tau_w_all[d])                     \n",
    "        S = build_S(JW, Sigma_y)                                                    \n",
    "        R = shrinkage_matrix_stable(P, S) \n",
    "\n",
    "        w0_vec = W_1[d].reshape(-1)       \n",
    "        y_star = y + (JW @ w0_vec) + (Jb1 @ b1[d])\n",
    "\n",
    "        r = np.linalg.solve(Sigma_y, y_star)   \n",
    "        g = JW.T @ r                           \n",
    "\n",
    "        hat_w = np.linalg.solve(S, g) \n",
    "        bar_w = np.linalg.solve(P + S, g)     \n",
    "\n",
    "        if return_mats:\n",
    "            R_stack[d] = R\n",
    "\n",
    "        w_hat_stack[d] = hat_w\n",
    "        w_bar_stack[d] = bar_w\n",
    "\n",
    "    return R_stack, w_bar_stack, w_hat_stack\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Gaussian tanh')\n",
    "\n",
    "R_gauss, w_bar_stack_gauss, w_hat_stack_gauss = compute_linearized_mean(X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Regularized Horseshoe tanh')\n",
    "\n",
    "R_RHS, w_bar_stack_RHS, w_hat_stack_RHS = compute_linearized_mean(X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Horseshoe tanh')\n",
    "\n",
    "R_DHS, w_bar_stack_DHS, w_hat_stack_DHS = compute_linearized_mean(X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n",
    "W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde = extract_model_draws(posterior_N100_fits, model='Dirichlet Student T tanh')\n",
    "\n",
    "R_DST, w_bar_stack_DST, w_hat_stack_DST = compute_linearized_mean(X, y, W1, b1, W2, b2, sigma, tau_w, tau_v, lambda_tilde, activation=\"tanh\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---------- alignment primitives (from earlier) ----------\n",
    "def energy_on_top_eigs_S(S, v, k=None, eps=1e-10):\n",
    "    s_eval, U = np.linalg.eigh(S)                  # ascending\n",
    "    idx = np.argsort(s_eval)[::-1]                 # descending\n",
    "    s_eval = s_eval[idx]; U = U[:, idx]\n",
    "    coeff = U.T @ v\n",
    "    energy = coeff**2\n",
    "    total = energy.sum() if energy.sum() > 0 else 1.0\n",
    "    if k is None:\n",
    "        return np.cumsum(energy)/total, s_eval\n",
    "    return float(np.cumsum(energy)[k-1]/total), s_eval\n",
    "\n",
    "def shrinkage_weighted_alignment_S_basis(S, P, v, k=None, eps=1e-10):\n",
    "    s_eval, U = np.linalg.eigh(S)\n",
    "    s_eval_clip = np.clip(s_eval, eps, None)\n",
    "    S_sqrt  = U @ np.diag(np.sqrt(s_eval_clip)) @ U.T\n",
    "    S_isqrt = U @ np.diag(1.0/np.sqrt(s_eval_clip)) @ U.T\n",
    "    A = S + P\n",
    "    # Build K = S^{1/2} (S+P)^{-1} S^{1/2}\n",
    "    K = S_sqrt @ np.linalg.solve(A, S_sqrt)\n",
    "    # R^{1/2} = S^{-1/2} K^{1/2} S^{1/2}; we only need u = R^{1/2} v\n",
    "    ke, V = np.linalg.eigh((K+K.T)*0.5)\n",
    "    ke = np.clip(ke, 0.0, 1.0)\n",
    "    Khalf = V @ np.diag(np.sqrt(ke)) @ V.T\n",
    "    u = S_isqrt @ (Khalf @ (S_sqrt @ v))  # u = R^{1/2} v\n",
    "\n",
    "    # Project onto S-eigenbasis (same U sorted by s_eval ↓)\n",
    "    idx_desc = np.argsort(s_eval)[::-1]\n",
    "    U_desc = U[:, idx_desc]\n",
    "    coeff = U_desc.T @ u\n",
    "    energy = coeff**2\n",
    "    total = energy.sum() if energy.sum() > 0 else 1.0\n",
    "    if k is None:\n",
    "        return np.cumsum(energy)/total\n",
    "    return float(np.cumsum(energy)[k-1]/total)\n",
    "\n",
    "def shrinkage_alignment_K_basis(S, P, v, k=None, eps=1e-10):\n",
    "    s_eval, U = np.linalg.eigh(S)\n",
    "    s_eval_clip = np.clip(s_eval, eps, None)\n",
    "    S_sqrt = U @ np.diag(np.sqrt(s_eval_clip)) @ U.T\n",
    "    A = S + P\n",
    "    K = S_sqrt @ np.linalg.solve(A, S_sqrt)        # PSD, eigenvalues in [0,1]\n",
    "    k_eval, V = np.linalg.eigh((K+K.T)*0.5)\n",
    "    idx = np.argsort(k_eval)[::-1]                 # sort by κ desc\n",
    "    k_eval = k_eval[idx]; V = V[:, idx]\n",
    "    w = S_sqrt @ v\n",
    "    coeff = V.T @ w\n",
    "    energy = coeff**2\n",
    "    total = energy.sum() if energy.sum() > 0 else 1.0\n",
    "    if k is None:\n",
    "        return np.cumsum(energy)/total, k_eval\n",
    "    return float(np.cumsum(energy)[k-1]/total), k_eval\n",
    "\n",
    "# ---------- per-draw metric ----------\n",
    "def alignment_metrics_for_draw(S, P, v, k=10):\n",
    "    A_plain_k, _   = energy_on_top_eigs_S(S, v, k=k)\n",
    "    A_shrinkS_k    = shrinkage_weighted_alignment_S_basis(S, P, v, k=k)\n",
    "    A_Kbasis_k, _  = shrinkage_alignment_K_basis(S, P, v, k=k)\n",
    "    return A_plain_k, A_shrinkS_k, A_Kbasis_k\n",
    "\n",
    "# ---------- summary across D draws for a model ----------\n",
    "def summarize_model_alignment(S_list, P_list, v_list, k=10, qs=(0.05, 0.5, 0.95)):\n",
    "    D = len(S_list)\n",
    "    A_plain  = np.empty(D)\n",
    "    A_shrS   = np.empty(D)\n",
    "    A_Kbasis = np.empty(D)\n",
    "    for d in range(D):\n",
    "        Ap, As, Ak = alignment_metrics_for_draw(S_list[d], P_list[d], v_list[d], k=k)\n",
    "        A_plain[d]  = Ap\n",
    "        A_shrS[d]   = As\n",
    "        A_Kbasis[d] = Ak\n",
    "    def stats(x):\n",
    "        return {\n",
    "            \"mean\": float(np.mean(x)),\n",
    "            \"sd\":   float(np.std(x, ddof=1)) if len(x)>1 else 0.0,\n",
    "            **{f\"q{int(100*q)}\": float(np.quantile(x, q)) for q in qs}\n",
    "        }\n",
    "    return {\"A_plain\": stats(A_plain),\n",
    "            \"A_shrinkS\": stats(A_shrS),\n",
    "            \"A_Kbasis\": stats(A_Kbasis)}\n",
    "\n",
    "# ---------- run for all models you have ----------\n",
    "# Expect arrays/lists of length D:\n",
    "#   S_gauss, P_gauss, w_bar_stack_gauss\n",
    "#   S_RHS,   P_RHS,   w_bar_stack_RHS\n",
    "#   S_DHS,   P_DHS,   w_bar_stack_DHS\n",
    "#   S_DST,   P_DST,   w_bar_stack_DST\n",
    "# If a model is missing, just omit it from the dict.\n",
    "\n",
    "models = {}\n",
    "models[\"Gaussian\"] = (S_gauss, P_gauss, w_bar_stack_gauss)\n",
    "models[\"RHS\"]      = (S_RHS,   P_RHS,   w_bar_stack_RHS)\n",
    "models[\"DHS\"]      = (S_DHS,   P_DHS,   w_bar_stack_DHS)\n",
    "models[\"DST\"]      = (S_DST,   P_DST,   w_bar_stack_DST)\n",
    "\n",
    "k = 10  # top-k eigenspace\n",
    "summaries = {}\n",
    "for name, (S_list, P_list, v_list) in models.items():\n",
    "    summaries[name] = summarize_model_alignment(S_list, P_list, v_list, k=k)\n",
    "\n",
    "# Pretty print\n",
    "for name, res in summaries.items():\n",
    "    print(f\"\\n=== {name} (A_{k}) ===\")\n",
    "    for key, st in res.items():\n",
    "        print(f\"{key:>10}: mean={st['mean']:.3f}  sd={st['sd']:.3f}  \"\n",
    "              f\"q05={st['q5']:.3f}  q50={st['q50']:.3f}  q95={st['q95']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_curve_S_gauss, s_vals_gauss = energy_on_top_eigs_S(S=S_gauss[0], v = w_bar_stack_gauss[0])                 # plain Σ-alignment\n",
    "A_curve_shrink_S_gauss = shrinkage_weighted_alignment_S_basis(S=S_gauss[0], P=P_gauss[0], v = w_bar_stack_gauss[0])  # Σ-basis but shrinkage-weighted\n",
    "A_curve_K_gauss, kappa_vals_gauss = shrinkage_alignment_K_basis(S=S_gauss[0], P=P_gauss[0], v = w_bar_stack_gauss[0])      # shrinkage-basis alignment\n",
    "\n",
    "# Examples: report A_10 in each notion\n",
    "A10_plain_gauss   = A_curve_S_gauss[9]\n",
    "A10_shrinkS_gauss = A_curve_shrink_S_gauss[9]\n",
    "A10_Kbasis_gauss  = A_curve_K_gauss[9]\n",
    "\n",
    "A_curve_S_DST, s_vals_DST = energy_on_top_eigs_S(S=S_DST[0], v = w_bar_stack_DST[0])                 # plain Σ-alignment\n",
    "A_curve_shrink_S_DST = shrinkage_weighted_alignment_S_basis(S=S_DST[0], P=P_DST[0], v = w_bar_stack_DST[0])  # Σ-basis but shrinkage-weighted\n",
    "A_curve_K_DST, kappa_vals_DST = shrinkage_alignment_K_basis(S=S_DST[0], P=P_DST[0], v = w_bar_stack_DST[0])      # shrinkage-basis alignment\n",
    "\n",
    "# Examples: report A_10 in each notion\n",
    "A10_plain_DST   = A_curve_S_DST[9]\n",
    "A10_shrinkS_DST = A_curve_shrink_S_DST[9]\n",
    "A10_Kbasis_DST  = A_curve_K_DST[9]\n",
    "\n",
    "print(\"Gaussian: \\n A-plain = \", A10_plain_gauss, \"\\n A-shrinks = \", A10_shrinkS_gauss, \"\\n A-basis = \", A10_Kbasis_gauss)\n",
    "print(\"DST: \\n A-plain = \", A10_plain_DST, \"\\n A-shrinks = \", A10_shrinkS_DST, \"\\n A-basis = \", A10_Kbasis_DST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def diag_T_from_R(R):\n",
    "    \"\"\"\n",
    "    R: (D, N, N) with R = (P+S)^{-1} P per draw.\n",
    "    Returns diag(T) for each draw, where T = (P+S)^{-1} S = I - R.\n",
    "    Output: (D, N)\n",
    "    \"\"\"\n",
    "    D, N, _ = R.shape\n",
    "    # T = I - R  (broadcast I over draws)\n",
    "    T_diag = np.diagonal(np.eye(N)[None, :, :] - R, axis1=1, axis2=2)\n",
    "    return T_diag  # (D, N)\n",
    "\n",
    "def make_diag_mats(diag_vecs):\n",
    "    \"\"\"\n",
    "    diag_vecs: (D, N)\n",
    "    Returns diag matrices per draw: (D, N, N)\n",
    "    (Only use if you truly need matrices; otherwise stick to vectors.)\n",
    "    \"\"\"\n",
    "    D, N = diag_vecs.shape\n",
    "    M = np.zeros((D, N, N), dtype=diag_vecs.dtype)\n",
    "    idx = np.arange(N)\n",
    "    M[:, idx, idx] = diag_vecs\n",
    "    return M\n",
    "\n",
    "# Example for your four models (R_* are (D,N,N)):\n",
    "Tdiag_gauss = diag_T_from_R(R_gauss)  # (D,N)\n",
    "Tdiag_RHS   = diag_T_from_R(R_RHS)\n",
    "Tdiag_DHS   = diag_T_from_R(R_DHS)\n",
    "Tdiag_DST   = diag_T_from_R(R_DST)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(w_hat_stack_DST.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "H, p = 16, 10\n",
    "N = H * p\n",
    "x = np.arange(1, N+1)\n",
    "\n",
    "# # ---- full shrinkage: bar_w_full = (I - R) @ hat_w (per draw) ----\n",
    "# bar_w_full_gauss = np.einsum('dij,dj->di', SP_inv_S_gauss, w_hat_stack_gauss)\n",
    "# bar_w_full_RHS   = np.einsum('dij,dj->di', SP_inv_S_RHS,   w_hat_stack_RHS)\n",
    "# bar_w_full_DHS   = np.einsum('dij,dj->di', SP_inv_S_DHS,   w_hat_stack_DHS)\n",
    "# bar_w_full_DST   = np.einsum('dij,dj->di', SP_inv_S_DST,   w_hat_stack_DST)\n",
    "\n",
    "# # # ---- full shrinkage: bar_w_full = (I - R) @ hat_w (per draw) ----\n",
    "# # bar_w_white_gauss = np.einsum('dij,dj->di', shrink_gauss, w_hat_stack_gauss)\n",
    "# # bar_w_white_RHS   = np.einsum('dij,dj->di', shrink_RHS,   w_hat_stack_RHS)\n",
    "# # bar_w_white_DHS   = np.einsum('dij,dj->di', shrink_DHS,   w_hat_stack_DHS)\n",
    "# # bar_w_white_DST   = np.einsum('dij,dj->di', shrink_DST,   w_hat_stack_DST)\n",
    "\n",
    "# # ---- diagonal shrinkage: bar_w_diag = diag(T) * hat_w (elementwise) ----\n",
    "# bar_w_diag_gauss = Tdiag_gauss * w_hat_stack_gauss\n",
    "# bar_w_diag_RHS   = Tdiag_RHS   * w_hat_stack_RHS\n",
    "# bar_w_diag_DHS   = Tdiag_DHS   * w_hat_stack_DHS\n",
    "# bar_w_diag_DST   = Tdiag_DST   * w_hat_stack_DST\n",
    "\n",
    "# ---- means across draws ----\n",
    "means = {\n",
    "    \"Gaussian\": (w_bar_stack_gauss.mean(axis=0), w_hat_stack_gauss.mean(axis=0)),\n",
    "    \"RHS\"     : (w_bar_stack_RHS.mean(axis=0),   w_hat_stack_RHS.mean(axis=0)),\n",
    "    \"DHS\"     : (w_bar_stack_DHS.mean(axis=0),   w_hat_stack_DHS.mean(axis=0)),\n",
    "    \"DST\"     : (w_bar_stack_DST.mean(axis=0),   w_hat_stack_DST.mean(axis=0)),\n",
    "}\n",
    "\n",
    "# ---- plot helper ----\n",
    "def plot_model(ax, title, full_mean, diag_mean):\n",
    "    ax.scatter(x, full_mean, s=12, marker='o', label=r\"$\\bar{w}$\", alpha=0.9)\n",
    "    ax.scatter(x, diag_mean, s=14, marker='^', label=r\"$\\hat{w}$\", alpha=0.9)\n",
    "    for h in range(1, H):\n",
    "        ax.axvline(h*p + 0.5, color='0.85', lw=1, zorder=0)\n",
    "    eps_plot = 1e-1\n",
    "    ax.axhline(eps_plot,  color='0.85', lw=1, zorder=0)\n",
    "    ax.axhline(-eps_plot, color='0.85', lw=1, zorder=0)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"parameter index (after alignment)\")\n",
    "    ax.set_ylabel(\"value\")\n",
    "\n",
    "# ---- 2x2 figure ----\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 7), dpi=150, sharex=True, sharey=False)\n",
    "titles = list(means.keys())\n",
    "for ax, title in zip(axes.ravel(), titles):\n",
    "    full_mean, diag_mean = means[title]\n",
    "    plot_model(ax, title, full_mean, diag_mean)\n",
    "\n",
    "# One shared legend\n",
    "handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper center\", ncol=2, frameon=False, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_all_gauss = posterior_N500_fits['Gaussian tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_gauss = posterior_N500_fits['Gaussian tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_RHS = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_RHS = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_DHS = posterior_N500_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_DHS = posterior_N500_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_L\")\n",
    "\n",
    "W_all_DST = posterior_N500_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_1\")\n",
    "v_all_DST = posterior_N500_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_and_compare(W_all, v_all, w_bar_stack, sort_key=\"abs_v\"):\n",
    "    \"\"\"\n",
    "    Align signs & permutations across draws before comparing linearized mean with posterior mean.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    W_all        : array-like, shape (D, H, p) or (D, p, H) or with stray singleton dims.\n",
    "    v_all        : array-like, shape (D, H) or (D, H, 1) or similar (length H per draw).\n",
    "    w_bar_stack  : array-like, shape (D, H*p) OR (D, H, p) OR (D, 1, H*p), etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    W_fix        : (D, H, p)   sign/permutation aligned\n",
    "    v_fix        : (D, H)\n",
    "    wbar_fix     : (D, H, p)\n",
    "    summary      : dict with RMSE, Corr, CosSim, SignAgree (means vs means in aligned basis)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    W_all = np.asarray(W_all)\n",
    "    v_all = np.asarray(v_all)\n",
    "    w_bar_stack = np.asarray(w_bar_stack)\n",
    "\n",
    "    D = W_all.shape[0]\n",
    "\n",
    "    # --- infer H from v (source of truth) ---\n",
    "    v0 = np.squeeze(v_all[0]).ravel()\n",
    "    H = v0.size\n",
    "    if H == 0:\n",
    "        raise ValueError(\"v_all[0] seems empty; cannot infer H.\")\n",
    "    # infer p from w_bar_stack length\n",
    "    wb0 = np.squeeze(w_bar_stack[0]).ravel()\n",
    "    if wb0.size % H != 0:\n",
    "        # fallback: try infer p from W_all[0] after squeezing\n",
    "        W0 = np.squeeze(W_all[0])\n",
    "        if W0.ndim != 2:\n",
    "            # try to drop any singleton dims\n",
    "            W0 = W0.reshape([s for s in W0.shape if s != 1])\n",
    "        if W0.ndim != 2:\n",
    "            raise ValueError(f\"Cannot infer (H,p). v length={H}, but w_bar_stack[0] has {wb0.size} elems \"\n",
    "                             f\"and W_all[0] has shape {np.squeeze(W_all[0]).shape}.\")\n",
    "        h, p_candidate = W0.shape\n",
    "        if h != H and p_candidate == H:\n",
    "            p = h\n",
    "        else:\n",
    "            p = p_candidate\n",
    "    else:\n",
    "        p = wb0.size // H\n",
    "\n",
    "    N = H * p\n",
    "\n",
    "    # alloc outputs\n",
    "    W_fix = np.empty((D, H, p), dtype=float)\n",
    "    v_fix = np.empty((D, H), dtype=float)\n",
    "    wbar_fix = np.empty((D, H, p), dtype=float)\n",
    "\n",
    "    def coerce_W(Wd, H, p):\n",
    "        \"\"\"Return Wd as (H,p). Accepts (H,p), (p,H), or with singleton dims.\"\"\"\n",
    "        A = np.asarray(Wd, dtype=float)\n",
    "        A = np.squeeze(A)\n",
    "        if A.ndim == 2:\n",
    "            h, q = A.shape\n",
    "            if h == H and q == p:\n",
    "                return A\n",
    "            if h == p and q == H:\n",
    "                return A.T\n",
    "            # If one matches H, try reshape to (H, -1)\n",
    "            if h == H and h*q == H*p:\n",
    "                return A.reshape(H, p)\n",
    "            if q == H and h*q == H*p:\n",
    "                return A.T.reshape(H, p)\n",
    "            raise ValueError(f\"Cannot coerce W of shape {A.shape} to (H,p)=({H},{p}).\")\n",
    "        elif A.ndim == 3 and 1 in A.shape:\n",
    "            # squeeze singleton and recurse\n",
    "            return coerce_W(np.squeeze(A), H, p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected W ndim={A.ndim}, shape={A.shape}\")\n",
    "\n",
    "    def coerce_v(vd, H):\n",
    "        \"\"\"Return vd as (H,)\"\"\"\n",
    "        v = np.asarray(vd, dtype=float).squeeze().ravel()\n",
    "        if v.size != H:\n",
    "            raise ValueError(f\"v has size {v.size}, expected H={H}.\")\n",
    "        return v\n",
    "\n",
    "    def coerce_wbar_row(wbd, H, p):\n",
    "        \"\"\"Return wbar row as (H,p) from (N,) or already (H,p).\"\"\"\n",
    "        w = np.asarray(wbd, dtype=float).squeeze().ravel()\n",
    "        if w.size == H * p:\n",
    "            return w.reshape(H, p)\n",
    "        # already 2D?\n",
    "        W2 = np.asarray(wbd, dtype=float).squeeze()\n",
    "        if W2.ndim == 2 and W2.shape == (H, p):\n",
    "            return W2\n",
    "        raise ValueError(f\"w_bar row has {w.size} elems but H*p={H*p} and not (H,p).\")\n",
    "\n",
    "    for d in range(D):\n",
    "        # coerce shapes\n",
    "        Wd = coerce_W(W_all[d], H, p)          # (H,p)\n",
    "        vd = coerce_v(v_all[d], H)             # (H,)\n",
    "        wbd = coerce_wbar_row(w_bar_stack[d], H, p)\n",
    "\n",
    "        # 1) sign fix so v >= 0\n",
    "        s = np.sign(vd)\n",
    "        s[s == 0.0] = 1.0\n",
    "        Wd = Wd * s[:, None]\n",
    "        wbd = wbd * s[:, None]\n",
    "        vd = np.abs(vd)\n",
    "\n",
    "        # 2) permute units by a stable key\n",
    "        if sort_key == \"abs_v\":\n",
    "            idx = np.argsort(-vd)  # descending |v|\n",
    "        elif sort_key == \"abs_v_times_rownorm\":\n",
    "            idx = np.argsort(-(vd * np.linalg.norm(Wd, axis=1)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sort_key: {sort_key}\")\n",
    "\n",
    "        W_fix[d] = Wd[idx]\n",
    "        wbar_fix[d] = wbd[idx]\n",
    "        v_fix[d] = vd[idx]\n",
    "\n",
    "    # Compare means in aligned basis\n",
    "    w_post_mean = W_fix.reshape(D, -1).mean(axis=0)   # (N,)\n",
    "    w_lin_mean  = wbar_fix.reshape(D, -1).mean(axis=0)\n",
    "\n",
    "    rmse = float(np.sqrt(np.mean((w_lin_mean - w_post_mean)**2)))\n",
    "    corr = float(np.corrcoef(w_lin_mean, w_post_mean)[0, 1])\n",
    "    cos  = float(np.dot(w_lin_mean, w_post_mean) /\n",
    "                 (np.linalg.norm(w_lin_mean) * np.linalg.norm(w_post_mean)))\n",
    "    sign_agree = float(np.mean(np.sign(w_lin_mean) == np.sign(w_post_mean)))\n",
    "\n",
    "    summary = dict(RMSE=rmse, Corr=corr, CosSim=cos, SignAgree=sign_agree,\n",
    "                   H=H, p=p, N=N)\n",
    "    return W_fix, v_fix, wbar_fix, summary\n",
    "\n",
    "active_cols   = np.arange(0, 5)     # adjust if your actives are different\n",
    "inactive_cols = np.arange(5, 10)\n",
    "def block_stats(A, B, cols):\n",
    "    a = A[:, cols].ravel(); b = B[:, cols].ravel()\n",
    "    return np.corrcoef(a, b)[0,1], np.sqrt(np.mean((a-b)**2))\n",
    "\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_gauss, v_all_gauss, w_bar_stack_gauss, sort_key=\"abs_v\")\n",
    "print(\"Gaussian: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "\n",
    "# H, p = summary[\"H\"], summary[\"p\"]\n",
    "# lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "# post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "# print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "# print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n",
    "\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_RHS, v_all_RHS, w_bar_stack_RHS, sort_key=\"abs_v\")\n",
    "print(\"RHS: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "\n",
    "# H, p = summary[\"H\"], summary[\"p\"]\n",
    "# lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "# post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "# print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "# print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_DHS, v_all_DHS, w_bar_stack_DHS, sort_key=\"abs_v\")\n",
    "print(\"DHS: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "\n",
    "# H, p = summary[\"H\"], summary[\"p\"]\n",
    "# lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "# post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "# print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "# print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n",
    "\n",
    "W_fix, v_fix, wbar_fix, summary = align_and_compare(W_all_DST, v_all_DST, w_bar_stack_DST, sort_key=\"abs_v\")\n",
    "print(\"DST: \\n\", summary)\n",
    "w_post_mean = W_fix.reshape(W_fix.shape[0], -1).mean(axis=0)\n",
    "w_lin_mean  = wbar_fix.reshape(wbar_fix.shape[0], -1).mean(axis=0)\n",
    "nrmse = np.linalg.norm(w_lin_mean - w_post_mean) / np.linalg.norm(w_post_mean)\n",
    "r2 = 1 - np.sum((w_lin_mean - w_post_mean)**2) / np.sum((w_post_mean - w_post_mean.mean())**2)\n",
    "print(f\"nRMSE: {nrmse:.3f}, R^2: {r2:.3f} \\n\")\n",
    "\n",
    "# H, p = summary[\"H\"], summary[\"p\"]\n",
    "# lin = wbar_fix.mean(axis=0).reshape(H, p)\n",
    "# post = W_fix.mean(axis=0).reshape(H, p)\n",
    "\n",
    "# print(\"Active  -> Corr, RMSE:\", block_stats(lin, post, active_cols))\n",
    "# print(\"Inactive-> Corr, RMSE:\", block_stats(lin, post, inactive_cols), \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: pick a \"MAP-like\" representative draw and plot MAP vs. \\bar{w} ---\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def select_map_like_index(W_fix: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Returns the index of the draw whose aligned W is closest (in Frobenius norm)\n",
    "    to the aligned posterior mean -- a robust MAP/medoid proxy.\n",
    "    W_fix: (D, H, p) aligned weights (output of align_and_compare)\n",
    "    \"\"\"\n",
    "    D = W_fix.shape[0]\n",
    "    mu = W_fix.reshape(D, -1).mean(axis=0)  # posterior mean in aligned basis\n",
    "    diffs = W_fix.reshape(D, -1) - mu[None, :]\n",
    "    d2 = np.einsum('di,di->d', diffs, diffs)  # squared distances\n",
    "    return int(np.argmin(d2))\n",
    "\n",
    "def plot_map_vs_barw(W_fix: np.ndarray, wbar_fix: np.ndarray, title: str = \"\", alpha=0.7):\n",
    "    \"\"\"\n",
    "    Overlay scatter: MAP-like draw's W (dots) vs the same draw's \\bar{w} (crosses).\n",
    "    Both arrays must be aligned: (D, H, p). We auto-pick a representative draw.\n",
    "    \"\"\"\n",
    "    D, H, p = W_fix.shape\n",
    "    idx = select_map_like_index(W_fix)  # representative draw\n",
    "    w_map = W_fix[idx].reshape(-1)\n",
    "    w_bar = wbar_fix[idx].reshape(-1)\n",
    "    \n",
    "    eps = 1e-1                          # Small threshold to see non-zero weights\n",
    "\n",
    "    x = np.arange(1, H*p + 1)\n",
    "    plt.figure(figsize=(10, 3.5), dpi=150)\n",
    "    plt.scatter(x, w_map, s=12, marker='o', label=\"MAP-like $w$\", alpha=alpha)\n",
    "    plt.scatter(x, w_bar, s=18, marker='x', label=r\"Linearized $\\bar{w}$\", alpha=alpha)\n",
    "\n",
    "    # light vertical guides between hidden units\n",
    "    for h in range(1, H):\n",
    "        plt.axvline(h*p + 0.5, color='0.85', lw=1, zorder=0)\n",
    "    \n",
    "    plt.axhline(eps, color='0.85', lw=1, zorder=0)\n",
    "    plt.axhline(-eps, color='0.85', lw=1, zorder=0)\n",
    "\n",
    "    plt.xlabel(\"parameter index (after alignment)\")\n",
    "    plt.ylabel(\"value\")\n",
    "    plt.title(title if title else \"MAP-like $w$ vs linearized $\\~w$\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gaussian: align and plot ---\n",
    "W_fix_g, v_fix_g, wbar_fix_g, summary_g = align_and_compare(W_all_gauss, v_all_gauss, w_bar_stack_gauss, sort_key=\"abs_v\")\n",
    "print(\"Gaussian summary:\", summary_g)\n",
    "plot_map_vs_barw(W_fix_g, wbar_fix_g, title=\"Gaussian prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Regularized Horseshoe: align and plot ---\n",
    "W_fix_r, v_fix_r, wbar_fix_r, summary_r = align_and_compare(W_all_RHS, v_all_RHS, w_bar_stack_RHS, sort_key=\"abs_v\")\n",
    "print(\"RHS summary:\", summary_r)\n",
    "plot_map_vs_barw(W_fix_r, wbar_fix_r, title=\"RHS prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dirichlet Horseshoe & Dirichlet Student-t: align and plot ---\n",
    "W_fix_dhs, v_fix_dhs, wbar_fix_dhs, summary_dhs = align_and_compare(W_all_DHS, v_all_DHS, w_bar_stack_DHS, sort_key=\"abs_v\")\n",
    "print(\"DHS summary:\", summary_dhs)\n",
    "plot_map_vs_barw(W_fix_dhs, wbar_fix_dhs, title=\"DHS prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_fix_dst, v_fix_dst, wbar_fix_dst, summary_dst = align_and_compare(W_all_DST, v_all_DST, w_bar_stack_DST, sort_key=\"abs_v\")\n",
    "print(\"DST summary:\", summary_dst)\n",
    "plot_map_vs_barw(W_fix_dst, wbar_fix_dst, title=\"DST prior: MAP-like $w$ vs linearized $\\\\bar{w}$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utforske U og $\\Lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- konstanter og blokkindekser (tilpass hvis din vec-rekkefølge er annerledes)\n",
    "H, p = 16, 10\n",
    "BLOCKS = [slice(h*p, (h+1)*p) for h in range(H)]\n",
    "\n",
    "def block_energy(U, blocks=BLOCKS):\n",
    "    BE = np.empty((U.shape[1], len(blocks)))  # (modes, H)\n",
    "    for b, sl in enumerate(blocks):\n",
    "        BE[:, b] = (U[sl, :]**2).sum(axis=0)\n",
    "    BE /= BE.sum(axis=1, keepdims=True)\n",
    "    return BE  # (modes, H)\n",
    "\n",
    "def evd_metrics(G):\n",
    "    w, U = np.linalg.eigh(G)                 # G sym/PSD\n",
    "    # sorter synkende på w\n",
    "    order = np.argsort(w)[::-1]\n",
    "    w, U = w[order], U[:, order]\n",
    "    rho = w / (1.0 + w)\n",
    "    m_eff = rho.sum()\n",
    "    ipr = (U**4).sum(axis=0)                  # inverse participation ratio\n",
    "    eff_support = 1.0 / ipr                   # effektiv støtte\n",
    "    return dict(w=w, U=U, rho=rho, m_eff=m_eff, ipr=ipr, eff_supp=eff_support)\n",
    "\n",
    "def m_eff_blocks_from_G(G):\n",
    "    M  = evd_metrics(G)\n",
    "    BE = block_energy(M['U'], BLOCKS)        # (modes, H)\n",
    "    m_eff_b = (M['rho'][:, None] * BE).sum(axis=0)  # (H,)\n",
    "    return m_eff_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Forutsetter at du har disse =====\n",
    "W2_gauss_samps = prior_N500_fits['Gaussian']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "W2_RHS_samps = prior_N500_fits['Regularized Horseshoe']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "W2_DHS_samps = prior_N500_fits['Dirichlet Horseshoe']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "W2_DST_samps = prior_N500_fits['Dirichlet Student T']['posterior'].stan_variable(\"W_L\")#[:100]\n",
    "\n",
    "S = 4000\n",
    "\n",
    "# --- beregn m_eff per blokk for ALLE samples\n",
    "m_eff_blocks_GAUSS = np.zeros((S, H))\n",
    "m_eff_blocks_RHS   = np.zeros((S, H))\n",
    "m_eff_blocks_DHS   = np.zeros((S, H))\n",
    "m_eff_blocks_DST = np.zeros((S, H))\n",
    "for s in range(S):\n",
    "    m_eff_blocks_GAUSS[s] = m_eff_blocks_from_G(G_gauss[s])\n",
    "    m_eff_blocks_RHS[s] = m_eff_blocks_from_G(G_RHS[s])\n",
    "    m_eff_blocks_DHS[s]   = m_eff_blocks_from_G(G_DHS[s])\n",
    "    m_eff_blocks_DST[s]   = m_eff_blocks_from_G(G_DST[s])\n",
    "    \n",
    "\n",
    "# --- klargjør |W2| i samme form\n",
    "W2_GAUSS_flat = np.abs(np.atleast_2d(W2_gauss_samps).reshape(S, H))\n",
    "W2_RHS_flat   = np.abs(np.atleast_2d(W2_RHS_samps).reshape(S, H))\n",
    "W2_DHS_flat   = np.abs(np.atleast_2d(W2_DHS_samps).reshape(S, H))\n",
    "W2_DST_flat   = np.abs(np.atleast_2d(W2_DST_samps).reshape(S, H))\n",
    "\n",
    "# --- flate til 1D for scatter\n",
    "x_gau = m_eff_blocks_GAUSS.ravel()\n",
    "y_gau = W2_GAUSS_flat.ravel()\n",
    "x_rhs = m_eff_blocks_RHS.ravel()\n",
    "y_rhs = W2_RHS_flat.ravel()\n",
    "x_dhs = m_eff_blocks_DHS.ravel()\n",
    "y_dhs = W2_DHS_flat.ravel()\n",
    "x_dst = m_eff_blocks_DST.ravel()\n",
    "y_dst = W2_DST_flat.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_gau, y_gau, label=\"Gaussian\", s=8, alpha=0.35)\n",
    "plt.scatter(x_rhs, y_rhs, label=\"RHS\", s=8, alpha=0.35)\n",
    "plt.scatter(x_dhs, y_dhs, label=\"DHS\", s=8, alpha=0.35)\n",
    "plt.scatter(x_dst, y_dst, label=\"DST\", s=8, alpha=0.35)\n",
    "plt.xlabel(r\"$m_{\\mathrm{eff}}^{(b)}$\")\n",
    "plt.ylabel(r\"$|W_2|$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def idempotence_likeness(A, eps=1e-12):\n",
    "    likeness = np.linalg.norm(A @ A - A, 'fro') / (np.linalg.norm(A, 'fro') + eps)\n",
    "    return likeness\n",
    "\n",
    "def svd_effective_rank(A, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      s_sorted : singular values sorted descending\n",
    "      rho      : normalized singular weights (s / ||s||_1)\n",
    "      erank    : exp(Shannon entropy of rho)\n",
    "    \"\"\"\n",
    "    # singular values only\n",
    "    s = np.linalg.svd(A, compute_uv=False)\n",
    "    s_sorted = np.sort(s)[::-1]\n",
    "    s_sum = s_sorted.sum()\n",
    "    if s_sum <= eps:\n",
    "        # all-zero block: define uniform weights\n",
    "        rho = np.ones_like(s_sorted) / len(s_sorted)\n",
    "    else:\n",
    "        rho = s_sorted / s_sum\n",
    "    H = -np.sum(rho * np.log(rho + eps))\n",
    "    erank = float(np.exp(H))\n",
    "    return s_sorted, rho, erank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 4000\n",
    "blocks = 16\n",
    "p=10\n",
    "\n",
    "erank_gauss = np.zeros((S, blocks))\n",
    "erank_RHS = np.zeros((S, blocks))\n",
    "erank_DHS = np.zeros((S, blocks))\n",
    "erank_DST = np.zeros((S, blocks))\n",
    "\n",
    "idempotent_gauss = np.zeros((S, blocks))\n",
    "idempotent_RHS = np.zeros((S, blocks))\n",
    "idempotent_DHS = np.zeros((S, blocks))\n",
    "idempotent_DST = np.zeros((S, blocks))\n",
    "\n",
    "for b in range(blocks):\n",
    "    for i in range(S):\n",
    "        _, _, erank_gauss[i, b] = svd_effective_rank(SP_inv_S_gauss[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        _, _, erank_RHS[i, b]   = svd_effective_rank(SP_inv_S_RHS[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        _, _, erank_DHS[i, b]   = svd_effective_rank(SP_inv_S_DHS[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        _, _, erank_DST[i, b]   = svd_effective_rank(SP_inv_S_DST[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        \n",
    "        idempotent_gauss[i, b] = idempotence_likeness(SP_inv_S_gauss[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        idempotent_RHS[i, b]   = idempotence_likeness(SP_inv_S_RHS[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        idempotent_DHS[i, b]   = idempotence_likeness(SP_inv_S_DHS[i, (b*p):(p+b*p), (b*p):(p+b*p)])\n",
    "        idempotent_DST[i, b]   = idempotence_likeness(SP_inv_S_DST[i, (b*p):(p+b*p), (b*p):(p+b*p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0, 16), erank_gauss.mean(axis=0), label=\"Gauss\", marker='o')\n",
    "plt.plot(np.arange(0, 16), erank_RHS.mean(axis=0), label=\"RHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), erank_DHS.mean(axis=0), label=\"DHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), erank_DST.mean(axis=0), label=\"DST\", marker='o')\n",
    "plt.title(\"Effective rank of blocks\")\n",
    "plt.xlabel(\"Block\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0, 16), idempotent_gauss.mean(axis=0), label=\"Gauss\", marker='o')\n",
    "plt.plot(np.arange(0, 16), idempotent_RHS.mean(axis=0), label=\"RHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), idempotent_DHS.mean(axis=0), label=\"DHS\", marker='o')\n",
    "plt.plot(np.arange(0, 16), idempotent_DST.mean(axis=0), label=\"DST\", marker='o')\n",
    "plt.title(\"Idempotence error of blocks\")\n",
    "plt.xlabel(\"Block\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESSY BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipr_GAUSS = np.zeros((S, H*p))\n",
    "ipr_RHS   = np.zeros((S, H*p))\n",
    "ipr_DHS   = np.zeros((S, H*p))\n",
    "ipr_DST = np.zeros((S, H*p))\n",
    "\n",
    "eff_supp_GAUSS = np.zeros((S, H*p))\n",
    "eff_supp_RHS   = np.zeros((S, H*p))\n",
    "eff_supp_DHS   = np.zeros((S, H*p))\n",
    "eff_supp_DST = np.zeros((S, H*p))\n",
    "\n",
    "for s in range(S):\n",
    "    ipr_GAUSS[s] = evd_metrics(G_gauss[s])['ipr']\n",
    "    ipr_RHS[s] = evd_metrics(G_RHS[s])['ipr']\n",
    "    ipr_DHS[s]   = evd_metrics(G_DHS[s])['ipr']\n",
    "    ipr_DST[s]   = evd_metrics(G_DST[s])['ipr']\n",
    "    eff_supp_GAUSS[s] = evd_metrics(G_gauss[s])['eff_supp']\n",
    "    eff_supp_RHS[s] = evd_metrics(G_RHS[s])['eff_supp']\n",
    "    eff_supp_DHS[s]   = evd_metrics(G_DHS[s])['eff_supp']\n",
    "    eff_supp_DST[s]   = evd_metrics(G_DST[s])['eff_supp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(ipr_GAUSS.mean(axis=0), label=\"Gaussian\", alpha=0.5, bins=60)\n",
    "plt.hist(ipr_RHS.mean(axis=0), label=\"RHS\", alpha=0.5, bins=60)\n",
    "plt.hist(ipr_DHS.mean(axis=0), label=\"DHS\", alpha=0.5, bins=60)\n",
    "plt.hist(ipr_DST.mean(axis=0), label=\"DST\", alpha=0.5, bins=60)\n",
    "plt.vlines(1/160, ymin=0, ymax=80, color=\"black\")\n",
    "#plt.vlines(1/16, ymin=0, ymax=80, color=\"orange\")\n",
    "plt.xlabel(r\"$m_{\\mathrm{eff}}^{(b)}$\")\n",
    "plt.ylabel(r\"$|W_2|$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(eff_supp_GAUSS.mean(axis=0), label=\"Gaussian\", alpha=0.5, bins=60)\n",
    "plt.hist(eff_supp_RHS.mean(axis=0), label=\"RHS\", alpha=0.5, bins=60)\n",
    "plt.hist(eff_supp_DHS.mean(axis=0), label=\"DHS\", alpha=0.5, bins=60)\n",
    "plt.hist(eff_supp_DST.mean(axis=0), label=\"DST\", alpha=0.5, bins=60)\n",
    "#plt.vlines(1/160, ymin=0, ymax=80, color=\"black\")\n",
    "#plt.vlines(1/16, ymin=0, ymax=80, color=\"orange\")\n",
    "plt.xlabel(r\"$m_{\\mathrm{eff}}^{(b)}$\")\n",
    "plt.ylabel(r\"$|W_2|$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_eff(G, lambda_samples, tau_samples):\n",
    "    lambda_vec = lambda_samples.reshape(-1)\n",
    "    lambda_inv_diag = 1.0 / (tau_samples*lambda_vec)\n",
    "    _, U = np.linalg.eigh(G)  \n",
    "    # u_j^T Λ^{-1} u_j = sum_i Λ^{-1}_{ii} * U_{i,j}^2\n",
    "    denom = (lambda_inv_diag[:, None] * (U**2)).sum(axis=0)   # (160,)\n",
    "    return 1.0 / denom  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2_gauss_samps = posterior_N100_fits['Gaussian tanh']['posterior'].stan_variable(\"W_L\")#[:1000]\n",
    "W2_RHS_samps = posterior_N100_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"W_L\")#[:1000]\n",
    "W2_DHS_samps = posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"W_L\")#[:1000]\n",
    "W2_DST_samps = posterior_N100_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"W_L\")#[:1000]\n",
    "\n",
    "lambda_gauss = np.ones((4000, 16, 10))\n",
    "lambda_raw_RHS = posterior_N100_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"lambda\")#[:1000]\n",
    "lambda_raw_DHS = posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"lambda_data\")#[:1000]\n",
    "lambda_raw_DST = posterior_N100_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"lambda\")#[:1000]\n",
    "\n",
    "lambda_RHS = posterior_N100_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"lambda_tilde\")#[:1000]\n",
    "lambda_DHS = posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"lambda_tilde_data\")#[:1000]\n",
    "lambda_DST = posterior_N100_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"lambda_tilde_data\")#[:1000]\n",
    "\n",
    "# tau_RHS = posterior_N100_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"tau\")\n",
    "# tau_DHS = posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"tau\")\n",
    "# tau_DST = posterior_N100_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"tau\")\n",
    "\n",
    "xi_DHS = posterior_N100_fits['Dirichlet Horseshoe tanh']['posterior'].stan_variable(\"phi_data\")#[:1000]\n",
    "xi_DST = posterior_N100_fits['Dirichlet Student T tanh']['posterior'].stan_variable(\"phi_data\")#[:1000]\n",
    "S=4000\n",
    "scale_DHS = np.zeros((S, H*p))\n",
    "scale_DST = np.zeros((S, H*p))\n",
    "\n",
    "lambda_eff_GAUSS = np.zeros((S, H*p))\n",
    "lambda_eff_RHS   = np.zeros((S, H*p))\n",
    "lambda_eff_DHS   = np.zeros((S, H*p))\n",
    "lambda_eff_DST = np.zeros((S, H*p))\n",
    "\n",
    "\n",
    "for s in range(S):\n",
    "    lambda_eff_GAUSS[s] = lambda_eff(G_gauss[s], lambda_gauss[s], 1)\n",
    "    lambda_eff_RHS[s]   = lambda_eff(G_RHS[s], lambda_RHS[s], 1)\n",
    "    lambda_eff_DHS[s]   = lambda_eff(G_DHS[s], lambda_DHS[s]*xi_DHS[s], 1)\n",
    "    lambda_eff_DST[s]   = lambda_eff(G_DST[s], lambda_DST[s]*xi_DST[s], 1)\n",
    "    scale_DHS[s]           = (lambda_DHS[s]*xi_DHS[s]).flatten()\n",
    "    scale_DST[s]           = (lambda_DST[s]*xi_DST[s]).flatten()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def masking(x):\n",
    "    x = np.asarray(x)\n",
    "    return x.ravel()[x.ravel() < 5]\n",
    "\n",
    "# mask all\n",
    "lambda_eff_RHS = masking(lambda_eff_RHS)\n",
    "lambda_RHS     = masking(lambda_RHS)\n",
    "\n",
    "lambda_eff_DHS = masking(lambda_eff_DHS)\n",
    "scale_DHS      = masking(scale_DHS)\n",
    "\n",
    "lambda_eff_DST = masking(lambda_eff_DST)\n",
    "scale_DST      = masking(scale_DST)\n",
    "\n",
    "# common bins for all three panels\n",
    "bins = np.linspace(0, 5, 51)\n",
    "bins2 = np.linspace(0, 0.5, 51)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharex=False, sharey=True)\n",
    "\n",
    "# (A) RHS: shrunk vs original\n",
    "ax = axes[0]\n",
    "ax.hist(lambda_eff_RHS, bins=bins, alpha=0.5, label=\"RHS shrunk\")\n",
    "ax.hist(lambda_RHS,     bins=bins, alpha=0.5, label=\"RHS original\")\n",
    "ax.set_title(\"(A) RHS\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.legend()\n",
    "\n",
    "# (B) DHS: shrunk vs original\n",
    "ax = axes[1]\n",
    "ax.hist(lambda_eff_DHS, bins=bins2, alpha=0.5, label=\"DHS shrunk\")\n",
    "ax.hist(scale_DHS,      bins=bins2, alpha=0.5, label=\"DHS original\")\n",
    "ax.set_title(\"(B) DHS\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.legend()\n",
    "\n",
    "# (C) DST: shrunk vs original\n",
    "ax = axes[2]\n",
    "ax.hist(lambda_eff_DST, bins=bins2, alpha=0.5, label=\"DST shrunk\")\n",
    "ax.hist(scale_DST,      bins=bins2, alpha=0.5, label=\"DST original\")\n",
    "ax.set_title(\"(C) DST\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def masking(x):\n",
    "    x = np.asarray(x)\n",
    "    return x.ravel()[x.ravel() < 5]\n",
    "\n",
    "def ccdf_tail(x, q=0.9):\n",
    "    \"\"\"Return tail x-values and CCDF (1-ECDF) starting at the q-quantile.\"\"\"\n",
    "    x = np.sort(np.asarray(x))\n",
    "    if x.size == 0:\n",
    "        return x, x\n",
    "    t = np.quantile(x, q)\n",
    "    xt = x[x >= t]\n",
    "    n = x.size\n",
    "    # ECDF at each xt (inclusive): rank/n; CCDF = 1 - ECDF + 1/n for right-continuity\n",
    "    ranks = np.arange(n - xt.size + 1, n + 1)  # positions of xt in the sorted array (1-based)\n",
    "    ccdf = 1.0 - ranks / n\n",
    "    return xt, ccdf\n",
    "\n",
    "# --- mask as you had\n",
    "lambda_eff_RHS = masking(lambda_eff_RHS)\n",
    "lambda_RHS     = masking(lambda_RHS)\n",
    "\n",
    "lambda_eff_DHS = masking(lambda_eff_DHS)\n",
    "scale_DHS      = masking(scale_DHS)\n",
    "\n",
    "lambda_eff_DST = masking(lambda_eff_DST)\n",
    "scale_DST      = masking(scale_DST)\n",
    "\n",
    "# Tail quantiles (adjust if you want e.g. top 5%: q_tail=0.95)\n",
    "q_tail_RHS = 0.9\n",
    "q_tail_DHS = 0.9\n",
    "q_tail_DST = 0.9\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True, sharex=True)\n",
    "\n",
    "# (A) RHS tails\n",
    "ax = axes[0]\n",
    "x1, s1 = ccdf_tail(lambda_eff_RHS, q=q_tail_RHS)\n",
    "x2, s2 = ccdf_tail(lambda_RHS,     q=q_tail_RHS)\n",
    "ax.semilogy(x1, s1, label=\"RHS shrunk\")\n",
    "ax.semilogy(x2, s2, label=\"RHS original\")\n",
    "ax.set_title(\"(A) RHS — Tail CCDF\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"1 - ECDF\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (B) DHS tails\n",
    "ax = axes[1]\n",
    "x1, s1 = ccdf_tail(lambda_eff_DHS, q=q_tail_DHS)\n",
    "x2, s2 = ccdf_tail(scale_DHS,      q=q_tail_DHS)\n",
    "ax.semilogy(x1, s1, label=\"DHS shrunk\")\n",
    "ax.semilogy(x2, s2, label=\"DHS original\")\n",
    "ax.set_title(\"(B) DHS — Tail CCDF\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# (C) DST tails\n",
    "ax = axes[2]\n",
    "x1, s1 = ccdf_tail(lambda_eff_DST, q=q_tail_DST)\n",
    "x2, s2 = ccdf_tail(scale_DST,      q=q_tail_DST)\n",
    "ax.semilogy(x1, s1, label=\"DST shrunk\")\n",
    "ax.semilogy(x2, s2, label=\"DST original\")\n",
    "ax.set_title(\"(C) DST — Tail CCDF\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, p = 16, 10  # 16 blokker, hver på størrelse 10 => 160 totalt\n",
    "\n",
    "def make_block_slices(H=H, p=p):\n",
    "    # Antar vec(W1) = radvis vektorisering (hver rad i W1 er én blokk)\n",
    "    # Blokk h bruker indekser [h*p : (h+1)*p)\n",
    "    return [slice(h*p, (h+1)*p) for h in range(H)]\n",
    "\n",
    "BLOCKS = make_block_slices(H, p)\n",
    "\n",
    "def eigh_sorted_by(value, U, key, descending=True):\n",
    "    \"\"\"Sorter egenpar (value, U) etter nøkkel 'key(value)'.\"\"\"\n",
    "    order = np.argsort(key(value))\n",
    "    if descending:\n",
    "        order = order[::-1]\n",
    "    return value[order], U[:, order], order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evd_metrics(G):\n",
    "    # Egenverdier/-vektorer; G er symmetrisk PSD\n",
    "    w, U = np.linalg.eigh(G)                  # w stigende\n",
    "    w, U, order = eigh_sorted_by(w, U, key=lambda x: x)  # sorter synkende\n",
    "    rho = w / (1.0 + w)                       # shrinkage-skalaren per modus\n",
    "    ipr = (U**4).sum(axis=0)                  # inverse participation ratio\n",
    "    eff_support = 1.0 / ipr                   # effektiv støtte\n",
    "    m_eff = rho.sum()\n",
    "    return dict(w=w, U=U, rho=rho, ipr=ipr, eff_support=eff_support, m_eff=m_eff, order=order)\n",
    "\n",
    "def block_energy(U, blocks=BLOCKS):\n",
    "    \"\"\"\n",
    "    U: (d, m) egenvektorer i kolonner. Returnerer BE (m, H) med sum_b BE[j,b] = 1.\n",
    "    \"\"\"\n",
    "    m = U.shape[1]\n",
    "    H = len(blocks)\n",
    "    BE = np.empty((m, H))\n",
    "    for b, sl in enumerate(blocks):\n",
    "        BE[:, b] = (U[sl, :]**2).sum(axis=0)\n",
    "    # normaliser for robusthet (burde allerede sum=1)\n",
    "    BE /= BE.sum(axis=1, keepdims=True)\n",
    "    return BE\n",
    "\n",
    "def dominant_block_per_mode(BE):\n",
    "    return BE.argmax(axis=1)  # (m,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meff_per_block(rho, BE):\n",
    "    \"\"\"\n",
    "    rho: (m,), BE: (m, H)\n",
    "    \"\"\"\n",
    "    return (rho[:, None] * BE).sum(axis=0)  # (H,)\n",
    "\n",
    "def analyze_G(G, name=\"model\"):\n",
    "    M = evd_metrics(G)\n",
    "    BE = block_energy(M['U'], BLOCKS)\n",
    "    dom = dominant_block_per_mode(BE)\n",
    "    m_eff_b = meff_per_block(M['rho'], BE)\n",
    "    out = dict(name=name, **M, BE=BE, dominant_block=dom, m_eff_blocks=m_eff_b)\n",
    "    return out\n",
    "\n",
    "res_gauss = analyze_G(G_gauss_test, name=\"Gaussian\")\n",
    "res_dhs   = analyze_G(G_DHS_test,   name=\"DHS\")\n",
    "\n",
    "print(\"m_eff (Gaussian):\", res_gauss['m_eff'])\n",
    "print(\"m_eff (DHS):\",     res_dhs['m_eff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_eff_sq(U, Lambda_inv_diag):\n",
    "    # Lambda_inv_diag: (d,) diagonalen i Λ^{-1}\n",
    "    return 1.0 / (U**2 * Lambda_inv_diag[None, :]).sum(axis=0)\n",
    "\n",
    "def lambda_eff_per_mode(res, lambda_samps):  # lambda_samps: (H, p)\n",
    "    # Λ^{-1} diag = 1 / λ^2 i vektor-rekkefølgen (blokkvis, p pr blokk)\n",
    "    lambda_vec = lambda_samps.reshape(-1)             # (H*p,)\n",
    "    Lambda_inv_diag = 1.0 / (lambda_vec)          # (H*p,)\n",
    "    # din eksisterende funksjon:\n",
    "    lam_eff_sq = lambda_eff_sq(res['U'], Lambda_inv_diag)   # (m,)\n",
    "    return lam_eff_sq\n",
    "\n",
    "def summarize_lambda_eff_by_block(res, lam_eff_sq, top_k_modes=10):\n",
    "    dom = res['dominant_block']   # (m,)\n",
    "    # per blokk: median/kvartiler av λ_eff (sqrt for å rapportere λ_eff, ikke kvadratet)\n",
    "    H = res['BE'].shape[1]\n",
    "    rows = []\n",
    "    lam_eff = np.sqrt(lam_eff_sq)\n",
    "    for b in range(H):\n",
    "        idx = np.where(dom == b)[0]\n",
    "        if len(idx) == 0:\n",
    "            rows.append(dict(block=b, n_modes=0, lam_eff_med=np.nan,\n",
    "                             lam_eff_q25=np.nan, lam_eff_q75=np.nan))\n",
    "            continue\n",
    "        vals = lam_eff[idx]\n",
    "        rows.append(dict(block=b, n_modes=len(idx),\n",
    "                         lam_eff_med=np.median(vals),\n",
    "                         lam_eff_q25=np.percentile(vals, 25),\n",
    "                         lam_eff_q75=np.percentile(vals, 75)))\n",
    "    # topp-moduser etter blokk-energi for sanity/illustrasjon\n",
    "    top_idx = np.argsort(res['rho'])[::-1][:top_k_modes]\n",
    "    return rows, top_idx\n",
    "\n",
    "\n",
    "# Eksempel (Gaussian):\n",
    "lam_eff_sq_gauss = lambda_eff_per_mode(res_gauss, lambda_samps_gauss)\n",
    "rows_gauss, top_idx_gauss = summarize_lambda_eff_by_block(res_gauss, lam_eff_sq_gauss)\n",
    "\n",
    "# (DHS) samme:\n",
    "lam_eff_sq_dhs = lambda_eff_per_mode(res_dhs, lambda_samps_dhs)\n",
    "rows_dhs, top_idx_dhs = summarize_lambda_eff_by_block(res_dhs, lam_eff_sq_dhs)\n",
    "\n",
    "print(\"λ_eff per blokk (DHS):\")\n",
    "for r in rows_dhs:\n",
    "    print(f\" Blokk {r['block']:2d}: n={r['n_modes']}, median λ_eff={r['lam_eff_med']:.4g} \"\n",
    "          f\"[{r['lam_eff_q25']:.4g}, {r['lam_eff_q75']:.4g}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_output_coupling(res, W2, lam_eff_sq, top=10, power=1):\n",
    "    \"\"\"\n",
    "    W2: (H,) eller (H,1). power=1 -> |w2|, power=2 -> w2^2.\n",
    "    Returnerer liten tabell for topp 'top' moduser etter kobling.\n",
    "    \"\"\"\n",
    "    w2 = W2.reshape(-1)\n",
    "    w2p = np.abs(w2)**power\n",
    "    # c_j = sum_b BE[j,b] * |w2_b|^power\n",
    "    c = (res['BE'] * w2p[None, :]).sum(axis=1)\n",
    "    idx = np.argsort(c)[::-1][:top]\n",
    "    lam_eff = np.sqrt(lam_eff_sq)\n",
    "    rows = []\n",
    "    for j in idx:\n",
    "        be = res['BE'][j]\n",
    "        b_dom = int(np.argmax(be))\n",
    "        rows.append(dict(\n",
    "            mode=int(j),\n",
    "            coupling=float(c[j]),\n",
    "            dom_block=b_dom,\n",
    "            BE_dom=float(be[b_dom]),\n",
    "            lambda_eff=float(lam_eff[j]),\n",
    "            rho=float(res['rho'][j])\n",
    "        ))\n",
    "    return rows\n",
    "\n",
    "# Eksempel:\n",
    "rows_coup_gauss = mode_output_coupling(res_gauss, W2_gauss, lam_eff_sq_gauss, top=12, power=1)\n",
    "rows_coup_dhs   = mode_output_coupling(res_dhs,   W2_DHS,   lam_eff_sq_dhs,   top=12, power=1)\n",
    "\n",
    "print(\"\\nTopp output-koblede moduser (DHS):\")\n",
    "for r in rows_coup_dhs[:8]:\n",
    "    print(f\" j={r['mode']:3d}  c={r['coupling']:.4g}  blk={r['dom_block']:2d} \"\n",
    "          f\"BE_blk={r['BE_dom']:.2f}  λ_eff={r['lambda_eff']:.4g}  ρ={r['rho']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_level_triplet(res, W2, lambda_samps, lam_eff_sq, be_weighted=True):\n",
    "    H, p = lambda_samps.shape\n",
    "    w2 = W2.reshape(-1)\n",
    "    # prior-press (harmonisk skala) per blokk:\n",
    "    lam_harm_b = []\n",
    "    for b in range(H):\n",
    "        lam_b = lambda_samps[b]        # (p,)\n",
    "        lam_harm = 1.0 / np.sqrt(np.mean(1.0 / (lam_b**2)))\n",
    "        lam_harm_b.append(lam_harm)\n",
    "    lam_harm_b = np.array(lam_harm_b)\n",
    "\n",
    "    # modus-baserte λ_eff i blokken\n",
    "    lam_eff = np.sqrt(lam_eff_sq)\n",
    "    dom = res['dominant_block']\n",
    "    lam_eff_block = np.zeros(H)\n",
    "    for b in range(H):\n",
    "        idx = np.where(dom == b)[0]\n",
    "        if len(idx) == 0:\n",
    "            lam_eff_block[b] = np.nan\n",
    "            continue\n",
    "        if be_weighted:\n",
    "            # BE-vektet gjennomsnitt blant de modusene som domineres av blokken\n",
    "            w = res['BE'][idx, b] + 1e-12\n",
    "            w /= w.sum()\n",
    "            lam_eff_block[b] = (w * lam_eff[idx]).sum()\n",
    "        else:\n",
    "            lam_eff_block[b] = lam_eff[idx].mean()\n",
    "\n",
    "    # samle trippel\n",
    "    df_rows = []\n",
    "    for b in range(H):\n",
    "        df_rows.append(dict(\n",
    "            block=b,\n",
    "            m_eff_b=float(res['m_eff_blocks'][b]),\n",
    "            abs_w2=float(np.abs(w2[b])),\n",
    "            lambda_harm=float(lam_harm_b[b]),\n",
    "            lambda_eff_modes=float(lam_eff_block[b])\n",
    "        ))\n",
    "    return df_rows\n",
    "\n",
    "# Eksempel (Gaussian):\n",
    "trip_gauss = block_level_triplet(res_gauss, W2_gauss, lambda_samps_gauss, lam_eff_sq_gauss)\n",
    "print(\"\\nBlokk-nivå (Gaussian):\")\n",
    "for r in trip_gauss:\n",
    "    print(f\" blk {r['block']:2d} | m_eff={r['m_eff_b']:.3f} | |w2|={r['abs_w2']:.3g} \"\n",
    "          f\"| λ_harm={r['lambda_harm']:.4g} | λ_eff(mod)= {r['lambda_eff_modes']:.4g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_DHS = block_level_triplet(res_dhs, W2_DHS, lambda_samps_dhs, lam_eff_sq_dhs)\n",
    "print(\"\\nBlokk-nivå (Gaussian):\")\n",
    "for r in trip_DHS:\n",
    "    print(f\" blk {r['block']:2d} | m_eff={r['m_eff_b']:.3f} | |w2|={r['abs_w2']:.3g} \"\n",
    "          f\"| λ_harm={r['lambda_harm']:.4g} | λ_eff(mod)= {r['lambda_eff_modes']:.4g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(res_dhs['m_eff_blocks'], np.abs(W2_DHS), label = \"DHS\")\n",
    "plt.scatter(res_gauss['m_eff_blocks'], np.abs(W2_gauss), label = \"Gaussian\")\n",
    "plt.xlabel(r\"$m_{eff}$\")\n",
    "plt.ylabel(r\"$|W2|$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#plt.hist(lambda_samps_dhs.reshape(-1), alpha=0.5, label = r\"$\\lambda^2$\")\n",
    "plt.hist(lam_eff_sq_dhs, alpha=0.5, label = r\"$\\lambda^2_{eff}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(np.arange(len(res_gauss['m_eff_blocks'])), res_gauss['m_eff_blocks'])\n",
    "ax.set_title(\"Gaussian: m_eff per blokk\")\n",
    "ax.set_xlabel(\"Blokk (hidden unit)\")\n",
    "ax.set_ylabel(\"m_eff^(b)\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(np.arange(len(res_dhs['m_eff_blocks'])), res_dhs['m_eff_blocks'])\n",
    "ax.set_title(\"DHS: m_eff per blokk\")\n",
    "ax.set_xlabel(\"Blokk (hidden unit)\")\n",
    "ax.set_ylabel(\"m_eff^(b)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_block_energy_for_top_modes(res, top=8, title_prefix=\"\"):\n",
    "    idx = np.argsort(res['rho'])[::-1][:top]\n",
    "    BE  = res['BE'][idx, :]  # (top, H)\n",
    "    fig, ax = plt.subplots()\n",
    "    # “striped” barplot: én stolpe per modus med H 'stabletter'\n",
    "    # (for enkelhet: bruk linjeplot av kumulative summer for å vise profil)\n",
    "    for r in range(BE.shape[0]):\n",
    "        ax.plot(BE[r], marker='o')\n",
    "    ax.set_title(f\"{title_prefix}: blokk-energi for topp {top} moduser (hver kurve=ett u_j)\")\n",
    "    ax.set_xlabel(\"Blokk\")\n",
    "    ax.set_ylabel(\"Blokk-energi (sum=1)\")\n",
    "    plt.show()\n",
    "\n",
    "plot_block_energy_for_top_modes(res_gauss, top=2, title_prefix=\"Gaussian\")\n",
    "plot_block_energy_for_top_modes(res_dhs,   top=2, title_prefix=\"DHS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forskjell mellom lambda_eff og lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_samples = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"lambda\")[1].flatten()\n",
    "reg_lambda_samples = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"lambda_tilde\")[1].flatten()\n",
    "tau_samples = posterior_N500_fits['Regularized Horseshoe tanh']['posterior'].stan_variable(\"tau\")[1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "P = P_RHS[1]\n",
    "S = S_RHS[1]\n",
    "\n",
    "# Gitt: P (diagonal positiv), S (symmetrisk)\n",
    "p = np.diag(P)                       # diagonalene i P\n",
    "P_inv_sqrt = np.diag(1.0/np.sqrt(p))          # P^{-1/2}\n",
    "W = P_inv_sqrt @ S @ P_inv_sqrt                        # whitened\n",
    "\n",
    "# Symmetrisk EVD\n",
    "r, U = np.linalg.eigh(W)             # r = egenverdier (stigende), U kolonner = egenvektorer\n",
    "\n",
    "inv_lambda2 = 1.0 / (lambda_samples**2)\n",
    "# λ_eff_i^2 = 1 / sum_j (U_{ji}^2 / λ_j^2)\n",
    "lambda_eff_sq = 1.0 / (U**2 @ inv_lambda2)\n",
    "lambda_eff = np.sqrt(lambda_eff_sq)  # shape (160,)\n",
    "\n",
    "print(np.mean(lambda_eff), np.mean(reg_lambda_samples))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import halfcauchy\n",
    "\n",
    "# --- QQ plot function ---\n",
    "def qq_plot(data, label, color):\n",
    "    n = len(data)\n",
    "    p = (np.arange(1, n+1) - 0.5) / n\n",
    "    q_theory = halfcauchy.ppf(p, scale=1)  # Half-Cauchy(0,1)\n",
    "    q_emp = np.sort(data)\n",
    "    plt.scatter(q_theory, q_emp, label=label, alpha=0.7, color=color)\n",
    "\n",
    "# --- Tail plot function ---\n",
    "def tail_plot(data, label, color):\n",
    "    sorted_data = np.sort(data)\n",
    "    n = len(data)\n",
    "    surv_emp = np.arange(n, 0, -1) / n  # empirical survival\n",
    "    plt.plot(sorted_data, sorted_data * surv_emp, label=label, color=color)\n",
    "\n",
    "# -----------------------------\n",
    "# QQ plot\n",
    "plt.figure(figsize=(6,6))\n",
    "qq_plot(lambda_eff, \"lambda_eff\", \"C0\")\n",
    "qq_plot(reg_lambda_samples, \"lambda_draws\", \"C1\")\n",
    "lims = [0, max(np.max(lambda_eff), np.max(reg_lambda_samples), 10)]\n",
    "plt.plot(lims, lims, 'k--', lw=1, label=\"y=x\")\n",
    "plt.xlabel(\"Theoretical Half-Cauchy(0,1) quantiles\")\n",
    "plt.ylabel(\"Empirical quantiles\")\n",
    "plt.title(\"QQ-plot vs Half-Cauchy(0,1)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Tail plot\n",
    "x = np.linspace(0.1, 10, 200)\n",
    "surv_theory = 1 - halfcauchy.cdf(x, scale=1)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "tail_plot(lambda_eff, \"lambda_eff\", \"C0\")\n",
    "tail_plot(reg_lambda_samples, \"lambda_draws\", \"C1\")\n",
    "plt.plot(x, x * surv_theory, 'k--', lw=1, label=\"Half-Cauchy(0,1)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"x * Survival(x)\")\n",
    "plt.title(\"Tail diagnostic: x * P(X>x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
