{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# params\n",
    "n, pin, d = 200, 100, 8\n",
    "H_star, r = 20, 5\n",
    "active_scale, inactive_scale = 1.5, 0.1\n",
    "x_noise, y_noise = 0.05, 0.3\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# orthonormal A (pin x d), A^T A = I_d\n",
    "A_rand = rng.standard_normal((pin, d))\n",
    "A, _ = np.linalg.qr(A_rand)\n",
    "\n",
    "# latent Z and observed inputs X\n",
    "Z = rng.standard_normal((n, d))\n",
    "X = Z @ A.T + x_noise * rng.standard_normal((n, pin))\n",
    "\n",
    "# post-activation feature map H = tanh(Z @ B^T) with anisotropy\n",
    "B = rng.standard_normal((H_star, d))\n",
    "\n",
    "# make first r rows orthonormal, scale active/inactive\n",
    "Qb, _ = np.linalg.qr(B[:r, :].T)\n",
    "B[:r, :] = Qb[:, :r].T\n",
    "B[:r, :] *= active_scale\n",
    "B[r:, :] *= inactive_scale\n",
    "\n",
    "H = np.tanh(Z @ B.T)\n",
    "\n",
    "# output weights supported on first r coords\n",
    "w_star = np.zeros(H_star)\n",
    "w_star[:r] = rng.standard_normal(r)\n",
    "\n",
    "# targets\n",
    "y = H @ w_star + y_noise * rng.standard_normal(n)\n",
    "\n",
    "# simple train/val split indices\n",
    "perm = rng.permutation(n)\n",
    "n_tr = int(0.8 * n)\n",
    "tr_idx, va_idx = perm[:n_tr], perm[n_tr:]\n",
    "\n",
    "X_train, y_train = X[tr_idx], y[tr_idx]\n",
    "X_test,   y_test   = X[va_idx], y[va_idx]\n",
    "\n",
    "# quick shapes check\n",
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape, H.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir_tanh = \"results/ridgeless/alignment/priors\"\n",
    "model_names_tanh = [\"Gaussian tanh\", \"Regularized Horseshoe tanh\", \"Dirichlet Horseshoe tanh\", \"Dirichlet Student T tanh\"]\n",
    "\n",
    "tanh_fit = get_model_fits(\n",
    "    config=\"\",\n",
    "    results_dir=results_dir_tanh,\n",
    "    models=model_names_tanh,\n",
    "    include_prior=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from properscoring import crps_ensemble\n",
    "\n",
    "results = []\n",
    "rmse_per_model = {}   # stores per-draw RMSE arrays if you want to plot later\n",
    "S = 4000              # cap draws if you want\n",
    "\n",
    "for model_name, model_entry in tanh_fit.items():\n",
    "    posterior = model_entry[\"posterior\"]\n",
    "    \n",
    "    # Posterior predictive draws on test set: (S_all, n_test)\n",
    "    y_draws = posterior.stan_variable(\"output_test\").squeeze(-1)\n",
    "    if y_draws.shape[0] > S:\n",
    "        y_draws = y_draws[:S]\n",
    "\n",
    "    # --- Per-draw RMSE (vectorized) ---\n",
    "    # MSE per draw: mean over test points\n",
    "    print(y_draws.shape, y_test.shape)\n",
    "    mse_draws = ((y_draws - y_test[None, :])**2).mean(axis=1)\n",
    "    rmse_draws = np.sqrt(mse_draws)  # shape (S,)\n",
    "    rmse_per_model[model_name] = rmse_draws\n",
    "\n",
    "    # --- Ensemble CRPS (proper scoring; lower is better) ---\n",
    "    # properscoring expects forecasts shape (n_obs, n_members)\n",
    "    crps_vec = crps_ensemble(y_test, y_draws.T)      # shape (n_test,)\n",
    "    crps_mean = float(crps_vec.mean())\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"RMSE_per_draw_mean\": float(rmse_draws.mean()),\n",
    "        \"RMSE_per_draw_median\": float(np.median(rmse_draws)),\n",
    "        \"RMSE_per_draw_p25\": float(np.percentile(rmse_draws, 25)),\n",
    "        \"RMSE_per_draw_p75\": float(np.percentile(rmse_draws, 75)),\n",
    "        \"CRPS_ensemble_mean\": crps_mean,\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tanh_act(A): \n",
    "    return np.tanh(A)\n",
    "\n",
    "def post_acts(X, W1, b1):\n",
    "    # X: (n,p), W1: (H,p), b1: (H,)\n",
    "    return tanh_act(X @ W1.T + b1[None,:])\n",
    "\n",
    "def cov_spectrum(Z):\n",
    "    Zc = Z - Z.mean(0, keepdims=True)\n",
    "    Sig = (Zc.T @ Zc) / Zc.shape[0]\n",
    "    lam, U = np.linalg.eigh(Sig)\n",
    "    idx = lam.argsort()[::-1]\n",
    "    return Sig, lam[idx], U[:, idx]\n",
    "\n",
    "def ridge_fit(Z, y, lam=1e-3):\n",
    "    Z1 = np.c_[np.ones((Z.shape[0],1)), Z]\n",
    "    H = Z1.T @ Z1 + lam * Z1.shape[0] * np.eye(Z1.shape[1])\n",
    "    theta = np.linalg.solve(H, Z1.T @ y)\n",
    "    return theta  # [b, w]\n",
    "\n",
    "def ridge_pred(Z, theta):\n",
    "    Z1 = np.c_[np.ones((Z.shape[0],1)), Z]\n",
    "    return Z1 @ theta\n",
    "\n",
    "def min_norm_fit(Z, y):\n",
    "    # Add bias column\n",
    "    Z1 = np.c_[np.ones((Z.shape[0],1)), Z]   # (n, 1+H)\n",
    "    theta = np.linalg.pinv(Z1) @ y          # min-norm solution\n",
    "    return theta\n",
    "\n",
    "def min_norm_pred(Z, theta):\n",
    "    Z1 = np.c_[np.ones((Z.shape[0],1)), Z]\n",
    "    return Z1 @ theta\n",
    "\n",
    "\n",
    "def r2_score(y, yhat):\n",
    "    ss_res = np.sum((y - yhat)**2)\n",
    "    ss_tot = np.sum((y - y.mean())**2) + 1e-12\n",
    "    return 1.0 - ss_res/ss_tot\n",
    "\n",
    "def align_geom(WL, U, k):\n",
    "    # fraction of ||WL||^2 captured by top-k eigenspace of Sigma_z\n",
    "    WL = np.ravel(WL)  # (H,)\n",
    "    num = np.linalg.norm(U[:, :k].T @ WL)**2\n",
    "    den = np.linalg.norm(WL)**2 + 1e-12\n",
    "    return float(num / den)\n",
    "\n",
    "def align_signal(WL, lam, U, Sig, k=None):\n",
    "    WL = np.ravel(WL)  # (H,)\n",
    "    if k is None:\n",
    "        num = ((WL @ U)**2 * lam).sum()\n",
    "    else:\n",
    "        num = ((WL @ U[:, :k])**2 * lam[:k]).sum()\n",
    "    den = WL @ Sig @ WL + 1e-12\n",
    "    return float(num / den)\n",
    "\n",
    "\n",
    "def rks_padding(X_train, X_test, H_base, target_gamma, rng, act=np.tanh, scale=1.0):\n",
    "    \"\"\"\n",
    "    Add random features so that p_aug / n_tr ≈ target_gamma,\n",
    "    starting from H_base base features (e.g., your learned Z has H_base=16).\n",
    "    \"\"\"\n",
    "    n_tr = X_train.shape[0]\n",
    "    p_in = X_train.shape[1]\n",
    "    p_target = int(np.ceil(target_gamma * n_tr))\n",
    "    m_extra = max(0, p_target - H_base)\n",
    "    if m_extra == 0:\n",
    "        return np.zeros((X_train.shape[0],0)), np.zeros((X_test.shape[0],0))\n",
    "\n",
    "    W = rng.standard_normal((m_extra, p_in)) * (scale / np.sqrt(p_in))\n",
    "    b = rng.standard_normal(m_extra) * scale\n",
    "\n",
    "    Z_extra_tr = act(X_train @ W.T + b[None, :])\n",
    "    Z_extra_te = act(X_test  @ W.T + b[None, :])\n",
    "    return Z_extra_tr, Z_extra_te\n",
    "\n",
    "def orthogonal_noise_padding(Z_tr_base, Z_te_base, m_extra, rng):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create m_extra nuisance features that are (train-)orthogonal to the columns of Z_tr_base.\n",
    "    We generate Gaussian proposals and project out the Z span.\n",
    "    The same projection is applied to test features to avoid leakage.\n",
    "    \"\"\"\n",
    "    n_tr = Z_tr_base.shape[0]\n",
    "    # projector onto span(Z_tr_base) in the sample space (rows = samples)\n",
    "    # P = Z (Z^+), where Z^+ is pseudoinverse minimizing ||Z Z^+ - I||\n",
    "    Z = Z_tr_base\n",
    "    Z_pinv = np.linalg.pinv(Z)               # shape (H_base, n_tr)\n",
    "    P = Z @ Z_pinv                           # (n_tr, n_tr)\n",
    "    I = np.eye(n_tr)\n",
    "\n",
    "    # train extra\n",
    "    G_tr = rng.standard_normal((n_tr, m_extra))\n",
    "    N_tr = (I - P) @ G_tr                    # residuals orthogonal to Z columns\n",
    "    # test extra: project using the same Z-based map\n",
    "    # Need to map test rows through the same column-space removal:\n",
    "    # Build the linear map M such that for any vector g, residual = g - Z (Z^+ g)\n",
    "    # For test: N_te = G_te - Z_te @ (Z_pinv @ G_tr_basis)? Instead, use the same random generator,\n",
    "    # then remove the Z_te component with Z_pinv defined on train:\n",
    "    G_te = rng.standard_normal((Z_te_base.shape[0], m_extra))\n",
    "    N_te = G_te - Z_te_base @ (Z_pinv @ G_tr)  # matches the component directions removed on train\n",
    "\n",
    "    # standardize columns for numerical stability\n",
    "    def std_cols(A):\n",
    "        s = A.std(axis=0, keepdims=True) + 1e-12\n",
    "        return (A - A.mean(axis=0, keepdims=True)) / s\n",
    "\n",
    "    return std_cols(N_tr), std_cols(N_te)\n",
    "\n",
    "\n",
    "def _std_shapes(W1_s, b1_s, WL_s, p_in):\n",
    "    # W1_s: (p,H) or (H,p) -> make (H,p)\n",
    "    W1_s = np.asarray(W1_s)\n",
    "    if W1_s.ndim != 2:\n",
    "        W1_s = W1_s.squeeze()\n",
    "    # if first dim equals p_in, it's (p,H) -> transpose\n",
    "    if W1_s.shape[0] == p_in:\n",
    "        W1_s = W1_s.T\n",
    "    # b1_s: (1,H) or (H,) -> make (H,)\n",
    "    b1_s = np.asarray(b1_s).squeeze().ravel()\n",
    "    # WL_s: (H,1) or (1,H) or (H,) -> make (H,)\n",
    "    WL_s = np.asarray(WL_s).squeeze().ravel()\n",
    "    return W1_s, b1_s, WL_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def analyze_model(posterior, X_train, y_train, X_val, y_val, fit = \"min_norm\",\n",
    "                  ks=(1,2,3,5,10), lam_ridge=1e-3, draws=20, thin=1):\n",
    "    W1_all = posterior.stan_variable('W_1')          # (S, p, H) in your case\n",
    "    b1_all = posterior.stan_variable('hidden_bias')  # (S, 1, H)\n",
    "    WL_all = posterior.stan_variable('W_L')          # (S, H, 1)\n",
    "\n",
    "    # handle possible 3D WL -> (S,H)\n",
    "    if WL_all.ndim == 3:\n",
    "        WL_all = WL_all.squeeze(-1)\n",
    "\n",
    "    S_avail = W1_all.shape[0]\n",
    "    S = min(draws, S_avail)\n",
    "    idxs = np.arange(0, S*thin, thin)  # e.g., thin=5 -> 0,5,10,...\n",
    "\n",
    "    p_in = X_train.shape[1]\n",
    "\n",
    "    results = { 'varfrac@k': {k: [] for k in ks},\n",
    "                'align_geom@k': {k: [] for k in ks},\n",
    "                'align_signal@k': {k: [] for k in ks},\n",
    "                'R2_full': [], 'R2_topk': {k: [] for k in ks} }\n",
    "\n",
    "    for s in idxs:\n",
    "        # ---- standardize shapes per draw ----\n",
    "        W1_s, b1_s, WL_s = _std_shapes(W1_all[s], b1_all[s], WL_all[s], p_in)\n",
    "\n",
    "        # ---- features on train/val ----\n",
    "        Ztr = post_acts(X_train, W1_s, b1_s)   # (n_tr, H)\n",
    "        Zva = post_acts(X_val,   W1_s, b1_s)   # (n_val, H)\n",
    "\n",
    "        # ---- covariance & PCs ----\n",
    "        Sig, lam, U = cov_spectrum(Ztr)\n",
    "        vf = lam / (lam.sum() + 1e-12)\n",
    "\n",
    "        # ---- ridge on all features ----\n",
    "        if fit == \"min_norm\":\n",
    "            theta = min_norm_fit(Ztr, y_train)\n",
    "            yhat  = min_norm_pred(Zva, theta)\n",
    "        else:    \n",
    "            theta = ridge_fit(Ztr, y_train, lam=lam_ridge)\n",
    "            yhat  = ridge_pred(Zva, theta)\n",
    "        results['R2_full'].append(r2_score(y_val, yhat))\n",
    "\n",
    "        # ---- top-k probes & alignment ----\n",
    "        for k in ks:\n",
    "            k = min(k, Ztr.shape[1])\n",
    "            results['varfrac@k'][k].append(vf[:k].sum())\n",
    "            results['align_geom@k'][k].append(align_geom(WL_s, U, k))\n",
    "            results['align_signal@k'][k].append(align_signal(WL_s, lam, U, Sig, k=k))\n",
    "\n",
    "            Ztr_k = Ztr @ U[:, :k]\n",
    "            Zva_k = Zva @ U[:, :k]\n",
    "            theta_k = ridge_fit(Ztr_k, y_train, lam=lam_ridge)\n",
    "            yhat_k  = ridge_pred(Zva_k, theta_k)\n",
    "            results['R2_topk'][k].append(r2_score(y_val, yhat_k))\n",
    "\n",
    "    med = lambda a: float(np.median(np.asarray(a)))\n",
    "    summary = {\n",
    "        'R2_full_med': med(results['R2_full']),\n",
    "        'varfrac@k_med': {k: med(v) for k,v in results['varfrac@k'].items()},\n",
    "        'align_geom@k_med': {k: med(v) for k,v in results['align_geom@k'].items()},\n",
    "        'align_signal@k_med': {k: med(v) for k,v in results['align_signal@k'].items()},\n",
    "        'R2_topk_med': {k: med(v) for k,v in results['R2_topk'].items()},\n",
    "    }\n",
    "    return summary, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "post_gauss  = tanh_fit['Gaussian tanh']['posterior']\n",
    "post_RHS = tanh_fit['Regularized Horseshoe tanh']['posterior']\n",
    "post_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior']\n",
    "post_DST = tanh_fit['Dirichlet Student T tanh']['posterior']\n",
    "\n",
    "\n",
    "ks = (1,2,3,5,10)\n",
    "gauss_sum,  _ = analyze_model(post_gauss,  X_train, y_train, X_test, y_test, ks=ks, lam_ridge=1e-3, draws=20, thin=5)\n",
    "RHS_sum, _ = analyze_model(post_RHS, X_train, y_train, X_test, y_test, ks=ks, lam_ridge=1e-3, draws=20, thin=5)\n",
    "DHS_sum, _ = analyze_model(post_DHS, X_train, y_train, X_test, y_test, ks=ks, lam_ridge=1e-3, draws=20, thin=5)\n",
    "DST_sum, _ = analyze_model(post_DST, X_train, y_train, X_test, y_test, ks=ks, lam_ridge=1e-3, draws=20, thin=5)\n",
    "\n",
    "print('\\n--- GAUSSIAN prior ---')\n",
    "print('R2_full (med):', gauss_sum['R2_full_med'])\n",
    "print('Top-k varfrac (med):', gauss_sum['varfrac@k_med'])\n",
    "print('Align geom (med):', gauss_sum['align_geom@k_med'])\n",
    "print('Align signal (med):', gauss_sum['align_signal@k_med'])\n",
    "print('R2_topk (med):', gauss_sum['R2_topk_med'])\n",
    "\n",
    "print('--- RHS prior ---')\n",
    "print('R2_full (med):', RHS_sum['R2_full_med'])\n",
    "print('Top-k varfrac (med):', RHS_sum['varfrac@k_med'])\n",
    "print('Align geom (med):', RHS_sum['align_geom@k_med'])\n",
    "print('Align signal (med):', RHS_sum['align_signal@k_med'])\n",
    "print('R2_topk (med):', RHS_sum['R2_topk_med'])\n",
    "\n",
    "print('--- DHS prior ---')\n",
    "print('R2_full (med):', DHS_sum['R2_full_med'])\n",
    "print('Top-k varfrac (med):', DHS_sum['varfrac@k_med'])\n",
    "print('Align geom (med):', DHS_sum['align_geom@k_med'])\n",
    "print('Align signal (med):', DHS_sum['align_signal@k_med'])\n",
    "print('R2_topk (med):', DHS_sum['R2_topk_med'])\n",
    "\n",
    "print('--- DST prior ---')\n",
    "print('R2_full (med):', DST_sum['R2_full_med'])\n",
    "print('Top-k varfrac (med):', DST_sum['varfrac@k_med'])\n",
    "print('Align geom (med):', DST_sum['align_geom@k_med'])\n",
    "print('Align signal (med):', DST_sum['align_signal@k_med'])\n",
    "print('R2_topk (med):', DST_sum['R2_topk_med'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def analyze_intensive_augmented_model(\n",
    "    posterior, X_train, y_train, X_val, y_val, \n",
    "    draws=20, thin=1,\n",
    "    gamma_list=(0.9, 1.1, 1.25, 1.5, 2.0, 3.0, 5.0, 8.0),\n",
    "    padding_mode='orth',          # 'orth' (recommended) or 'rks'\n",
    "    rng=None,\n",
    "    # stability & scaling knobs\n",
    "    rcond=1e-10,                  # epsilon for lstsq\n",
    "    standardize_extras=True,\n",
    "    tail_scale=0.2,               # shrink extras so they stay in the covariance tail\n",
    "    rks_act=np.tanh,\n",
    "    rks_scale=0.1                 # scale for RKS weights/bias (effective ~ rks_scale/sqrt(p_in))\n",
    "):\n",
    "    \"\"\"\n",
    "    Ridgeless-only risk vs gamma by augmenting *hidden* features (post-activations).\n",
    "    Padding is constructed to preserve signal in the leading eigenspace (extras live in the tail),\n",
    "    so risk should decrease for large gamma in aligned settings.\n",
    "\n",
    "    Returns:\n",
    "        summary: {\n",
    "            'gamma': [..],\n",
    "            'rmse_val_med': [...],\n",
    "            'rmse_tr_med':  [...],\n",
    "            'p_aug_med':    [...]\n",
    "        },\n",
    "        results: dict with per-gamma lists over draws (for debugging/plots)\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    # --- pull posterior samples\n",
    "    W1_all = posterior.stan_variable('W_1')          # (S, p, H) or (S, H, p)\n",
    "    b1_all = posterior.stan_variable('hidden_bias')  # (S, 1, H) or (S, H)\n",
    "    WL_all = posterior.stan_variable('W_L')          # (S, H, 1) or (S, H); not used here but retained for consistency\n",
    "\n",
    "    if WL_all.ndim == 3:\n",
    "        WL_all = WL_all.squeeze(-1)\n",
    "\n",
    "    S_avail = W1_all.shape[0]\n",
    "    S = min(draws, S_avail)\n",
    "    idxs = np.arange(0, S*thin, thin)\n",
    "\n",
    "    p_in = X_train.shape[1]\n",
    "    n_tr = X_train.shape[0]\n",
    "    gammas = list(gamma_list)\n",
    "\n",
    "    # --------- inner helpers (kept local so we only 'change this function') ---------\n",
    "    def _standardize_pair(A_tr, A_va):\n",
    "        if not standardize_extras:\n",
    "            return A_tr, A_va\n",
    "        mu = A_tr.mean(axis=0, keepdims=True)\n",
    "        sd = A_tr.std(axis=0, keepdims=True) + 1e-12\n",
    "        return (A_tr - mu) / sd, (A_va - mu) / sd\n",
    "\n",
    "    def _ridgeless_solve(Z, y):\n",
    "        \"\"\"\n",
    "        Min-norm ridgeless: solve Z1 theta ≈ y (least-squares) with bias column included.\n",
    "        lstsq returns the min-norm solution in underdetermined cases.\n",
    "        \"\"\"\n",
    "        Z1 = np.c_[np.ones((Z.shape[0], 1)), Z]\n",
    "        theta, *_ = np.linalg.lstsq(Z1, y, rcond=rcond)\n",
    "        return theta\n",
    "\n",
    "    def _predict(Z, theta):\n",
    "        Z1 = np.c_[np.ones((Z.shape[0], 1)), Z]\n",
    "        return Z1 @ theta\n",
    "\n",
    "    def _rks_padding_from_X(H_base, g, Xtr, Xva):\n",
    "        p_target = int(np.ceil(g * n_tr))\n",
    "        m_extra = max(0, p_target - H_base)\n",
    "        if m_extra == 0:\n",
    "            return np.zeros((Xtr.shape[0], 0)), np.zeros((Xva.shape[0], 0)), H_base\n",
    "        W = rng.standard_normal((m_extra, p_in)) * (rks_scale / np.sqrt(p_in))\n",
    "        b = rng.standard_normal(m_extra) * rks_scale\n",
    "        Z_extra_tr = rks_act(Xtr @ W.T + b[None, :])\n",
    "        Z_extra_va = rks_act(Xva @ W.T + b[None, :])\n",
    "        Z_extra_tr, Z_extra_va = _standardize_pair(Z_extra_tr, Z_extra_va)\n",
    "        Z_extra_tr *= tail_scale\n",
    "        Z_extra_va *= tail_scale\n",
    "        return Z_extra_tr, Z_extra_va, H_base + m_extra\n",
    "\n",
    "    def _orth_padding_from_Z(Ztr_base, Zva_base, g):\n",
    "        \"\"\"\n",
    "        Create extras that are orthogonal (on train) to span(Ztr_base) in *sample space*,\n",
    "        then standardize + tail-shrink. This keeps added columns in the covariance tail.\n",
    "        \"\"\"\n",
    "        H_base = Ztr_base.shape[1]\n",
    "        p_target = int(np.ceil(g * n_tr))\n",
    "        m_extra = max(0, p_target - H_base)\n",
    "        if m_extra == 0:\n",
    "            return np.zeros((Ztr_base.shape[0], 0)), np.zeros((Zva_base.shape[0], 0)), H_base\n",
    "\n",
    "        # projector onto span(Z) in sample space\n",
    "        Z = Ztr_base\n",
    "        Z_pinv = np.linalg.pinv(Z)        # (H_base, n_tr)\n",
    "        P = Z @ Z_pinv                    # (n_tr, n_tr)\n",
    "        I = np.eye(n_tr)\n",
    "\n",
    "        # proposals and removal on train\n",
    "        G_tr = rng.standard_normal((n_tr, m_extra))\n",
    "        N_tr = (I - P) @ G_tr\n",
    "\n",
    "        # map the *same directions* to validation\n",
    "        G_va = rng.standard_normal((Zva_base.shape[0], m_extra))\n",
    "        N_va = G_va - Zva_base @ (Z_pinv @ G_tr)\n",
    "\n",
    "        # standardize + tail shrink\n",
    "        N_tr, N_va = _standardize_pair(N_tr, N_va)\n",
    "        N_tr *= tail_scale\n",
    "        N_va *= tail_scale\n",
    "        return N_tr, N_va, H_base + m_extra\n",
    "    # -------------------------------------------------------------------------------\n",
    "\n",
    "    # storage\n",
    "    results = {\n",
    "        'gamma': gammas,\n",
    "        'rmse_val': {g: [] for g in gammas},\n",
    "        'rmse_tr':  {g: [] for g in gammas},\n",
    "        'p_aug':    {g: [] for g in gammas},\n",
    "    }\n",
    "    pred_val = {g: [] for g in gammas}\n",
    "\n",
    "    # --------- main loop over posterior draws ---------\n",
    "    for s in idxs:\n",
    "        # shapes & base hidden features\n",
    "        W1_s, b1_s, _ = _std_shapes(W1_all[s], b1_all[s], WL_all[s], p_in)\n",
    "        Ztr = post_acts(X_train, W1_s, b1_s)   # (n_tr, H_base)\n",
    "        Zva = post_acts(X_val,   W1_s, b1_s)   # (n_val, H_base)\n",
    "        H_base = Ztr.shape[1]\n",
    "\n",
    "        for g in gammas:\n",
    "            # augment hidden features to reach target gamma\n",
    "            if padding_mode == 'orth':\n",
    "                Z_extra_tr, Z_extra_va, p_aug = _orth_padding_from_Z(Ztr, Zva, g)\n",
    "            elif padding_mode == 'rks':\n",
    "                Z_extra_tr, Z_extra_va, p_aug = _rks_padding_from_X(H_base, g, X_train, X_val)\n",
    "            else:\n",
    "                raise ValueError(\"padding_mode must be 'orth' or 'rks'\")\n",
    "\n",
    "            Z_aug_tr = np.c_[Ztr, Z_extra_tr]\n",
    "            Z_aug_va = np.c_[Zva, Z_extra_va]\n",
    "\n",
    "            # ridgeless min-norm on augmented hidden features\n",
    "            theta = _ridgeless_solve(Z_aug_tr, y_train)\n",
    "            yhat_tr = _predict(Z_aug_tr, theta)\n",
    "            yhat_va = _predict(Z_aug_va, theta)\n",
    "            pred_val[g].append(yhat_va)\n",
    "\n",
    "            rmse_tr = float(np.sqrt(np.mean((y_train - yhat_tr)**2)))\n",
    "            rmse_va = float(np.sqrt(np.mean((y_val   - yhat_va)**2)))\n",
    "\n",
    "            results['rmse_tr'][g].append(rmse_tr)\n",
    "            results['rmse_val'][g].append(rmse_va)\n",
    "            results['p_aug'][g].append(p_aug)\n",
    "            \n",
    "    # ---- Compute bias, variance, risk per gamma ----\n",
    "    bias = {}\n",
    "    variance = {}\n",
    "    risk = {}\n",
    "\n",
    "    y_val = y_val.ravel()\n",
    "\n",
    "    for g in gammas:\n",
    "        if len(pred_val[g]) == 0:\n",
    "            bias[g] = variance[g] = risk[g] = float('nan')\n",
    "            continue\n",
    "\n",
    "        Y = np.vstack(pred_val[g])    # (S, n_val)\n",
    "        mean_pred = Y.mean(axis=0)    # (n_val,)\n",
    "\n",
    "        bias[g] = float(((mean_pred - y_val)**2).mean())\n",
    "        variance[g] = float(((Y - mean_pred)**2).mean())\n",
    "        risk[g] = bias[g] + variance[g]\n",
    "\n",
    "    # aggregate (median over draws)\n",
    "    med = lambda a: float(np.median(np.asarray(a))) if len(a) else float('nan')\n",
    "\n",
    "    summary = {\n",
    "        'gamma': gammas,\n",
    "        'rmse_val_med': [med(results['rmse_val'][g]) for g in gammas],\n",
    "        'rmse_tr_med':  [med(results['rmse_tr'][g])  for g in gammas],\n",
    "        'p_aug_med':    [med(results['p_aug'][g])    for g in gammas],\n",
    "        'used_padding': padding_mode,\n",
    "        'tail_scale': tail_scale\n",
    "    }\n",
    "    summary['bias_med'] = [bias[g] for g in gammas]\n",
    "    summary['variance_med'] = [variance[g] for g in gammas]\n",
    "    summary['risk_med'] = [risk[g] for g in gammas]\n",
    "\n",
    "    return summary, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def analyze_intensive_augmented_model_2(\n",
    "    posterior, X_train, y_train, X_val, y_val, \n",
    "    draws=20, thin=1,\n",
    "    gamma_list=(0.9, 1.1, 1.25, 1.5, 2.0, 3.0, 5.0, 8.0),\n",
    "    padding_mode='orth',          # 'orth' (recommended) or 'rks'\n",
    "    rng=None,\n",
    "    # stability & scaling knobs\n",
    "    rcond=1e-10,                  # epsilon for lstsq\n",
    "    standardize_extras=True,\n",
    "    tail_scale=0.2,               # shrink extras so they stay in the covariance tail\n",
    "    rks_act=np.tanh,\n",
    "    rks_scale=0.1                 # scale for RKS weights/bias (effective ~ rks_scale/sqrt(p_in))\n",
    "):\n",
    "    \"\"\"\n",
    "    Ridgeless-only: augment hidden features to target gamma and solve min-norm LS.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "    summary : dict\n",
    "        {\n",
    "          'gamma': [...],\n",
    "          'used_padding': 'orth' | 'rks',\n",
    "          'tail_scale': float,\n",
    "          # YOUR REQUESTED CURVE: one RMSE value per gamma = mean over draws\n",
    "          'rmse_mean_val': [...],      # mean over draws of per-draw validation RMSE\n",
    "          'rmse_mean_tr':  [...],      # mean over draws of per-draw train RMSE\n",
    "          # Ensemble (across draws) decomposition curves:\n",
    "          'bias_curve':     [...],\n",
    "          'variance_curve': [...],\n",
    "          'risk_curve':     [...]\n",
    "        }\n",
    "\n",
    "    results : dict\n",
    "        {\n",
    "          'pred_val': {gamma: [yhat_val_draw1, ..., yhat_val_drawS]},  # each (n_val,)\n",
    "          'per_sample': {\n",
    "              gamma: {\n",
    "                  'rmse_tr': [...], 'rmse_val': [...], 'mse_val': [...], 'p_aug': [...]\n",
    "              }, ...\n",
    "          }\n",
    "        }\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(0)\n",
    "\n",
    "    # --- pull posterior samples\n",
    "    W1_all = posterior.stan_variable('W_1')          # (S, p, H) or (S, H, p)\n",
    "    b1_all = posterior.stan_variable('hidden_bias')  # (S, 1, H) or (S, H)\n",
    "    WL_all = posterior.stan_variable('W_L')          # (S, H, 1) or (S, H); not used directly\n",
    "\n",
    "    if WL_all.ndim == 3:\n",
    "        WL_all = WL_all.squeeze(-1)\n",
    "\n",
    "    S_avail = W1_all.shape[0]\n",
    "    S = min(draws, S_avail)\n",
    "    idxs = np.arange(0, S*thin, thin)\n",
    "\n",
    "    p_in = X_train.shape[1]\n",
    "    n_tr = X_train.shape[0]\n",
    "    gammas = list(gamma_list)\n",
    "\n",
    "    # --------- inner helpers ---------\n",
    "    def _standardize_pair(A_tr, A_va):\n",
    "        if not standardize_extras:\n",
    "            return A_tr, A_va\n",
    "        mu = A_tr.mean(axis=0, keepdims=True)\n",
    "        sd = A_tr.std(axis=0, keepdims=True) + 1e-12\n",
    "        return (A_tr - mu) / sd, (A_va - mu) / sd\n",
    "\n",
    "    def _ridgeless_solve(Z, y):\n",
    "        # Min-norm LS (with bias column)\n",
    "        Z1 = np.c_[np.ones((Z.shape[0], 1)), Z]\n",
    "        theta, *_ = np.linalg.lstsq(Z1, y, rcond=rcond)\n",
    "        return theta\n",
    "\n",
    "    def _predict(Z, theta):\n",
    "        Z1 = np.c_[np.ones((Z.shape[0], 1)), Z]\n",
    "        return Z1 @ theta\n",
    "\n",
    "    def _rks_padding_from_X(H_base, g, Xtr, Xva):\n",
    "        p_target = int(np.ceil(g * n_tr))\n",
    "        m_extra = max(0, p_target - H_base)\n",
    "        if m_extra == 0:\n",
    "            return np.zeros((Xtr.shape[0], 0)), np.zeros((Xva.shape[0], 0)), H_base\n",
    "        W = rng.standard_normal((m_extra, p_in)) * (rks_scale / np.sqrt(p_in))\n",
    "        b = rng.standard_normal(m_extra) * rks_scale\n",
    "        Z_extra_tr = rks_act(Xtr @ W.T + b[None, :])\n",
    "        Z_extra_va = rks_act(Xva @ W.T + b[None, :])\n",
    "        Z_extra_tr, Z_extra_va = _standardize_pair(Z_extra_tr, Z_extra_va)\n",
    "        Z_extra_tr *= tail_scale\n",
    "        Z_extra_va *= tail_scale\n",
    "        return Z_extra_tr, Z_extra_va, H_base + m_extra\n",
    "\n",
    "    def _orth_padding_from_Z(Ztr_base, Zva_base, g):\n",
    "        \"\"\"\n",
    "        Create extras orthogonal (on train) to span(Ztr_base) in *sample space*,\n",
    "        then standardize + tail-shrink.\n",
    "        \"\"\"\n",
    "        H_base = Ztr_base.shape[1]\n",
    "        p_target = int(np.ceil(g * n_tr))\n",
    "        m_extra = max(0, p_target - H_base)\n",
    "        if m_extra == 0:\n",
    "            return np.zeros((Ztr_base.shape[0], 0)), np.zeros((Zva_base.shape[0], 0)), H_base\n",
    "\n",
    "        Z = Ztr_base\n",
    "        Z_pinv = np.linalg.pinv(Z)        # (H_base, n_tr)\n",
    "        P = Z @ Z_pinv                    # (n_tr, n_tr)\n",
    "        I = np.eye(n_tr)\n",
    "\n",
    "        G_tr = rng.standard_normal((n_tr, m_extra))\n",
    "        N_tr = (I - P) @ G_tr\n",
    "\n",
    "        G_va = rng.standard_normal((Zva_base.shape[0], m_extra))\n",
    "        N_va = G_va - Zva_base @ (Z_pinv @ G_tr)\n",
    "\n",
    "        N_tr, N_va = _standardize_pair(N_tr, N_va)\n",
    "        N_tr *= tail_scale\n",
    "        N_va *= tail_scale\n",
    "        return N_tr, N_va, H_base + m_extra\n",
    "    # ---------------------------------\n",
    "\n",
    "    # per-gamma storage, per posterior draw\n",
    "    pred_val = {g: [] for g in gammas}  # list of yhat_va arrays, one per draw\n",
    "    per_sample = {g: {'rmse_tr': [], 'rmse_val': [], 'mse_val': [], 'p_aug': []} for g in gammas}\n",
    "\n",
    "    # --------- main loop over posterior draws ---------\n",
    "    for s in range(1):#idxs:\n",
    "        # shapes & base hidden features\n",
    "        W1_s, b1_s, _ = _std_shapes(W1_all[s], b1_all[s], WL_all[s], p_in)\n",
    "        Ztr = post_acts(X_train, W1_s, b1_s)   # (n_tr, H_base)\n",
    "        Zva = post_acts(X_val,   W1_s, b1_s)   # (n_val, H_base)\n",
    "        H_base = Ztr.shape[1]\n",
    "\n",
    "        for g in gammas:\n",
    "            # augment hidden features to reach target gamma\n",
    "            if padding_mode == 'orth':\n",
    "                Z_extra_tr, Z_extra_va, p_aug = _orth_padding_from_Z(Ztr, Zva, g)\n",
    "            elif padding_mode == 'rks':\n",
    "                Z_extra_tr, Z_extra_va, p_aug = _rks_padding_from_X(H_base, g, X_train, X_val)\n",
    "            else:\n",
    "                raise ValueError(\"padding_mode must be 'orth' or 'rks'\")\n",
    "\n",
    "            Z_aug_tr = np.c_[Ztr, Z_extra_tr]\n",
    "            Z_aug_va = np.c_[Zva, Z_extra_va]\n",
    "\n",
    "            # ridgeless min-norm on augmented hidden features\n",
    "            theta   = _ridgeless_solve(Z_aug_tr, y_train)\n",
    "            yhat_tr = _predict(Z_aug_tr, theta)\n",
    "            yhat_va = _predict(Z_aug_va, theta)\n",
    "\n",
    "            # store per-draw predictions (for ensemble bias/variance computation)\n",
    "            pred_val[g].append(yhat_va)\n",
    "\n",
    "            # per-draw metrics\n",
    "            rmse_tr = float(np.sqrt(np.mean((y_train - yhat_tr)**2)))\n",
    "            rmse_va = float(np.sqrt(np.mean((y_val   - yhat_va)**2)))\n",
    "            mse_va  = float(np.mean((y_val - yhat_va)**2))   # per-draw MSE\n",
    "\n",
    "            per_sample[g]['rmse_tr'].append(rmse_tr)\n",
    "            per_sample[g]['rmse_val'].append(rmse_va)\n",
    "            per_sample[g]['mse_val'].append(mse_va)\n",
    "            per_sample[g]['p_aug'].append(p_aug)\n",
    "\n",
    "    # --------- ensemble bias/variance/risk curves ----------\n",
    "    y_val_vec = y_val.ravel()\n",
    "    bias_curve = []\n",
    "    variance_curve = []\n",
    "    risk_curve = []\n",
    "    rmse_mean_val = []  # << one RMSE value per gamma = mean over draws\n",
    "    rmse_mean_tr  = []\n",
    "\n",
    "    for g in gammas:\n",
    "        # mean RMSE across draws (what you asked for)\n",
    "        vals = per_sample[g]['rmse_val']\n",
    "        trs  = per_sample[g]['rmse_tr']\n",
    "        rmse_mean_val.append(float(np.mean(vals)) if len(vals) else float('nan'))\n",
    "        rmse_mean_tr.append(float(np.mean(trs)) if len(trs) else float('nan'))\n",
    "\n",
    "        # bias/variance/risk from ensemble predictions\n",
    "        preds = pred_val[g]\n",
    "        if len(preds) == 0:\n",
    "            bias_curve.append(float('nan'))\n",
    "            variance_curve.append(float('nan'))\n",
    "            risk_curve.append(float('nan'))\n",
    "            continue\n",
    "\n",
    "        Y = np.vstack(preds)             # (S, n_val)\n",
    "        mean_pred = Y.mean(axis=0)       # posterior-mean predictor on val\n",
    "        bias_g = float(((mean_pred - y_val_vec)**2).mean())\n",
    "        var_g  = float(((Y - mean_pred)**2).mean())\n",
    "        bias_curve.append(bias_g)\n",
    "        variance_curve.append(var_g)\n",
    "        risk_curve.append(bias_g + var_g)\n",
    "\n",
    "    summary = {\n",
    "        'gamma': gammas,\n",
    "        'used_padding': padding_mode,\n",
    "        'tail_scale': tail_scale,\n",
    "        'rmse_mean_val': rmse_mean_val,     # << plot THIS vs gamma (one value per gamma)\n",
    "        'rmse_mean_tr':  rmse_mean_tr,\n",
    "        'bias_curve': bias_curve,\n",
    "        'variance_curve': variance_curve,\n",
    "        'risk_curve': risk_curve\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        'pred_val': pred_val,     # per gamma: list of yhat_val arrays, one per draw\n",
    "        'per_sample': per_sample  # per gamma: dict with per-draw rmse_tr/rmse_val/mse_val/p_aug\n",
    "    }\n",
    "\n",
    "    return summary, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_gauss  = tanh_fit['Gaussian tanh']['posterior']\n",
    "post_RHS = tanh_fit['Regularized Horseshoe tanh']['posterior']\n",
    "post_DHS = tanh_fit['Dirichlet Horseshoe tanh']['posterior']\n",
    "post_DST = tanh_fit['Dirichlet Student T tanh']['posterior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_gauss, results_gauss = analyze_intensive_augmented_model_2(\n",
    "    post_gauss, X_train, y_train, X_test, y_test,\n",
    "    draws=20, thin=1,\n",
    "    gamma_list=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.5, 2.0, 3.0, 5.0, 7.0, 9.0, 11, 13, 15, 17, 19, 25, 50, 100),\n",
    "    padding_mode='rks',          # 'orth' (recommended) or 'rks'\n",
    "    rng=None,\n",
    "    # stability & scaling knobs\n",
    "    rcond=1e-10,                  # epsilon for lstsq\n",
    "    standardize_extras=True,\n",
    "    tail_scale=0.9,               # shrink extras so they stay in the covariance tail\n",
    "    rks_act=np.tanh,\n",
    "    rks_scale=0.5                 # scale for RKS weights/bias (effective ~ rks_scale/sqrt(p_in))\n",
    ")\n",
    "\n",
    "summary_RHS, results_RHS = analyze_intensive_augmented_model_2(\n",
    "    post_RHS, X_train, y_train, X_test, y_test,\n",
    "    draws=20, thin=1,\n",
    "    gamma_list=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.5, 2.0, 3.0, 5.0, 7.0, 9.0, 11, 13, 15, 17, 19, 25, 50, 100),\n",
    "    padding_mode='rks',          # 'orth' (recommended) or 'rks'\n",
    "    rng=None,\n",
    "    # stability & scaling knobs\n",
    "    rcond=1e-10,                  # epsilon for lstsq\n",
    "    standardize_extras=True,\n",
    "    tail_scale=0.9,               # shrink extras so they stay in the covariance tail\n",
    "    rks_act=np.tanh,\n",
    "    rks_scale=0.5                 # scale for RKS weights/bias (effective ~ rks_scale/sqrt(p_in))\n",
    ")\n",
    "\n",
    "summary_DHS, results_DHS = analyze_intensive_augmented_model_2(\n",
    "    post_DHS, X_train, y_train, X_test, y_test,\n",
    "    draws=20, thin=1,\n",
    "    gamma_list=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.5, 2.0, 3.0, 5.0, 7.0, 9.0, 11, 13, 15, 17, 19, 25, 50, 100),\n",
    "    padding_mode='rks',          # 'orth' (recommended) or 'rks'\n",
    "    rng=None,\n",
    "    # stability & scaling knobs\n",
    "    rcond=1e-10,                  # epsilon for lstsq\n",
    "    standardize_extras=True,\n",
    "    tail_scale=0.9,               # shrink extras so they stay in the covariance tail\n",
    "    rks_act=np.tanh,\n",
    "    rks_scale=0.5                 # scale for RKS weights/bias (effective ~ rks_scale/sqrt(p_in))\n",
    ")\n",
    "\n",
    "summary_DST, results_DST = analyze_intensive_augmented_model_2(\n",
    "    post_DST, X_train, y_train, X_test, y_test,\n",
    "    draws=20, thin=1,\n",
    "    gamma_list=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.1, 1.5, 2.0, 3.0, 5.0, 7.0, 9.0, 11, 13, 15, 17, 19, 25, 50, 100),\n",
    "    padding_mode='rks',          # 'orth' (recommended) or 'rks'\n",
    "    rng=None,\n",
    "    # stability & scaling knobs\n",
    "    rcond=1e-10,                  # epsilon for lstsq\n",
    "    standardize_extras=True,\n",
    "    tail_scale=0.9,               # shrink extras so they stay in the covariance tail\n",
    "    rks_act=np.tanh,\n",
    "    rks_scale=0.5                 # scale for RKS weights/bias (effective ~ rks_scale/sqrt(p_in))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(summary_gauss['gamma'], summary_gauss['bias_curve'], label=\"Gauss\")\n",
    "plt.plot(summary_RHS['gamma'], summary_RHS['bias_curve'], label=\"RHS\")\n",
    "plt.plot(summary_DHS['gamma'], summary_DHS['bias_curve'], label=\"DHS\")\n",
    "plt.plot(summary_DST['gamma'], summary_DST['bias_curve'], label=\"DST\")\n",
    "plt.xlabel(f\"$\\gamma$\")\n",
    "plt.ylabel(\"Bias\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(summary_gauss['gamma'], summary_gauss['variance_curve'], label=\"Gauss\")\n",
    "plt.plot(summary_RHS['gamma'], summary_RHS['variance_curve'], label=\"RHS\")\n",
    "plt.plot(summary_DHS['gamma'], summary_DHS['variance_curve'], label=\"DHS\")\n",
    "plt.plot(summary_DST['gamma'], summary_DST['variance_curve'], label=\"DST\")\n",
    "plt.xlabel(f\"$\\gamma$\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(summary_gauss['gamma'], summary_gauss['risk_curve'], label=\"Gauss\")\n",
    "plt.plot(summary_RHS['gamma'], summary_RHS['risk_curve'], label=\"RHS\")\n",
    "plt.plot(summary_DHS['gamma'], summary_DHS['risk_curve'], label=\"DHS\")\n",
    "plt.plot(summary_DST['gamma'], summary_DST['risk_curve'], label=\"DST\")\n",
    "plt.xlabel(f\"$\\gamma$\")\n",
    "plt.ylabel(\"Risk\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(summary_gauss['gamma'], summary_gauss['rmse_mean_val'], label=\"Gauss\")\n",
    "plt.plot(summary_RHS['gamma'], summary_RHS['rmse_mean_val'], label=\"RHS\")\n",
    "plt.plot(summary_DHS['gamma'], summary_DHS['rmse_mean_val'], label=\"DHS\")\n",
    "plt.plot(summary_DST['gamma'], summary_DST['rmse_mean_val'], label=\"DST\")\n",
    "plt.xlabel(f\"$\\gamma$\")\n",
    "plt.ylabel(\"Test RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_DST['rmse_mean_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(summary_gauss['gamma'], summary_gauss['rmse_mean_tr'], label=\"Gauss\")\n",
    "plt.plot(summary_RHS['gamma'], summary_RHS['rmse_mean_tr'], label=\"RHS\")\n",
    "plt.plot(summary_DHS['gamma'], summary_DHS['rmse_mean_tr'], label=\"DHS\")\n",
    "plt.plot(summary_DST['gamma'], summary_DST['rmse_mean_tr'], label=\"DST\")\n",
    "plt.xlabel(f\"$\\gamma$\")\n",
    "plt.ylabel(\"Train RMSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
