{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os; sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__) if '__file__' in globals() else os.getcwd(), '..')))\n",
    "#import os; os.chdir(os.path.dirname(os.getcwd()))\n",
    "from utils.model_loader import get_model_fits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# params\n",
    "n, pin, d = 200, 100, 8\n",
    "H_star, r = 20, 5\n",
    "active_scale, inactive_scale = 1.5, 0.1\n",
    "x_noise, y_noise = 0.05, 0.3\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# orthonormal A (pin x d), A^T A = I_d\n",
    "A_rand = rng.standard_normal((pin, d))\n",
    "A, _ = np.linalg.qr(A_rand)\n",
    "\n",
    "# latent Z and observed inputs X\n",
    "Z = rng.standard_normal((n, d))\n",
    "X = Z @ A.T + x_noise * rng.standard_normal((n, pin))\n",
    "\n",
    "# post-activation feature map H = tanh(Z @ B^T) with anisotropy\n",
    "B = rng.standard_normal((H_star, d))\n",
    "\n",
    "# make first r rows orthonormal, scale active/inactive\n",
    "Qb, _ = np.linalg.qr(B[:r, :].T)\n",
    "B[:r, :] = Qb[:, :r].T\n",
    "B[:r, :] *= active_scale\n",
    "B[r:, :] *= inactive_scale\n",
    "\n",
    "H = np.tanh(Z @ B.T)\n",
    "\n",
    "# output weights supported on first r coords\n",
    "w_star = np.zeros(H_star)\n",
    "w_star[:r] = rng.standard_normal(r)\n",
    "\n",
    "# targets\n",
    "y = H @ w_star + y_noise * rng.standard_normal(n)\n",
    "\n",
    "# simple train/val split indices\n",
    "perm = rng.permutation(n)\n",
    "n_tr = int(0.8 * n)\n",
    "tr_idx, va_idx = perm[:n_tr], perm[n_tr:]\n",
    "\n",
    "X_train, y_train = X[tr_idx], y[tr_idx]\n",
    "X_test,   y_test   = X[va_idx], y[va_idx]\n",
    "\n",
    "# quick shapes check\n",
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape, H.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def effective_rank(eigs):\n",
    "    p = eigs / (eigs.sum() + 1e-12)\n",
    "    h = -(p * np.log(p + 1e-12)).sum()\n",
    "    return float(np.exp(h))\n",
    "\n",
    "# covariance & spectrum of H\n",
    "Hc = H - H.mean(axis=0, keepdims=True)\n",
    "Sigma_H = (Hc.T @ Hc) / Hc.shape[0]\n",
    "eigvals, U = np.linalg.eigh(Sigma_H)\n",
    "order = eigvals.argsort()[::-1]\n",
    "eigvals, U = eigvals[order], U[:, order]\n",
    "\n",
    "def align_geom(w, U, k):\n",
    "    num = np.linalg.norm(U[:, :k].T @ w)**2\n",
    "    den = np.linalg.norm(w)**2 + 1e-12\n",
    "    return float(num / den)\n",
    "\n",
    "print(\"Top-10 eigenvalues:\", np.round(eigvals[:10], 3))\n",
    "print(\"Top-5 variance fraction:\", float(eigvals[:5].sum() / eigvals.sum()))\n",
    "print(\"Effective rank (exp entropy):\", effective_rank(eigvals))\n",
    "for k in (1, 2, 3, 5, 10):\n",
    "    print(f\"Alignment@{k}:\", round(align_geom(w_star, U, k), 3))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(eigvals)+1), eigvals, marker='o')\n",
    "plt.xlabel('Component index')\n",
    "plt.ylabel('Eigenvalue of Sigma_H')\n",
    "plt.title('Post-activation spectrum (H)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RMSE={rmse:.3f}, R²={r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_model_local(model_name, config_name, X_train, X_test, y_train, y_test, args):\n",
    "    from cmdstanpy import CmdStanModel\n",
    "    from utils.stan_data_generator import make_stan_data\n",
    "    from utils.io_helpers import save_metadata\n",
    "    import os, shutil\n",
    "    import numpy as np\n",
    "    \n",
    "    # Set seed for reproducibility if provided\n",
    "    seed = getattr(args, 'seed', None)\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    task = \"prior\"\n",
    "    args.num_classes = 1  # Still needed\n",
    "\n",
    "    stan_data = make_stan_data(model_name, task, X_train, y_train, X_test, args)\n",
    "\n",
    "    model_path = f\"bnn_prior_models/{model_name}.stan\"\n",
    "    model = CmdStanModel(stan_file=model_path, force_compile=True)\n",
    "\n",
    "    fit = model.sample(\n",
    "        data=stan_data,\n",
    "        chains=4,\n",
    "        iter_sampling=args.samples,\n",
    "        iter_warmup=args.burnin_samples,\n",
    "        adapt_delta=0.8,\n",
    "        parallel_chains=4,\n",
    "        show_console=False,\n",
    "        #max_treedepth = 12,\n",
    "    )\n",
    "    \n",
    "    if args.data_config == \"uci\": \n",
    "        if args.standardize:\n",
    "            output_dir = os.path.join(\n",
    "            args.model_output_dir, \"standardized\"\n",
    "        )\n",
    "        else:\n",
    "            output_dir = os.path.join(\n",
    "                args.model_output_dir\n",
    "            )\n",
    "    else:\n",
    "        output_dir = os.path.join(\n",
    "            args.model_output_dir\n",
    "        )\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    save_metadata(output_dir, args, config_name)\n",
    "\n",
    "    for i, path in enumerate(fit.runset.csv_files, start=1):\n",
    "        shutil.copy(path, os.path.join(output_dir, f\"chain_{i}.csv\"))\n",
    "\n",
    "    print(f\"[✓] Saved results to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from cmdstanpy import set_cmdstan_path\n",
    "\n",
    "# Sett CmdStan-stien\n",
    "set_cmdstan_path(\"/Users/augustarnstad/.cmdstan/cmdstan-2.36.0\")\n",
    "run_regression_model_local(\n",
    "    model_name=\"dirichlet_student_t\",\n",
    "    config_name=\"latent_model\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    args=argparse.Namespace(\n",
    "        N=X_train.shape[0],\n",
    "        p=X_train.shape[1],\n",
    "        sigma=None,\n",
    "        data=\"\",\n",
    "        standardize=False,\n",
    "        test_shift=None,\n",
    "        model=\"dirichlet_student_t\",\n",
    "        H=16,\n",
    "        L=1,\n",
    "        config=\"Latent\",\n",
    "        seed=1,\n",
    "        data_config=\"realworld\",\n",
    "        model_output_dir=\"results/ridgeless/alignment/priors/dirichlet_student_t_tanh\",\n",
    "        burnin_samples=1000,\n",
    "        samples=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_regression_model_local(\n",
    "    model_name=\"dirichlet_horseshoe\",\n",
    "    config_name=\"latent_model\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    args=argparse.Namespace(\n",
    "        N=X_train.shape[0],\n",
    "        p=X_train.shape[1],\n",
    "        sigma=None,\n",
    "        data=\"\",\n",
    "        standardize=False,\n",
    "        test_shift=None,\n",
    "        model=\"dirichlet_horseshoe\",\n",
    "        H=16,\n",
    "        L=1,\n",
    "        config=\"Latent\",\n",
    "        seed=1,\n",
    "        data_config=\"realworld\",\n",
    "        model_output_dir=\"results/ridgeless/alignment/priors/dirichlet_horseshoe_tanh\",\n",
    "        burnin_samples=1000,\n",
    "        samples=1000,\n",
    "    )\n",
    ")\n",
    "\n",
    "run_regression_model_local(\n",
    "    model_name=\"regularized_horseshoe\",\n",
    "    config_name=\"latent_model\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    args=argparse.Namespace(\n",
    "        N=X_train.shape[0],\n",
    "        p=X_train.shape[1],\n",
    "        sigma=None,\n",
    "        data=\"\",\n",
    "        standardize=False,\n",
    "        test_shift=None,\n",
    "        model=\"regularized_horseshoe\",\n",
    "        H=16,\n",
    "        L=1,\n",
    "        config=\"Latent\",\n",
    "        seed=1,\n",
    "        data_config=\"realworld\",\n",
    "        model_output_dir=\"results/ridgeless/alignment/priors/regularized_horseshoe_tanh\",\n",
    "        burnin_samples=1000,\n",
    "        samples=1000,\n",
    "    )\n",
    ")\n",
    "\n",
    "run_regression_model_local(\n",
    "    model_name=\"gaussian\",\n",
    "    config_name=\"latent_model\",\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    y_train=y_train,\n",
    "    y_test=y_test,\n",
    "    args=argparse.Namespace(\n",
    "        N=X_train.shape[0],\n",
    "        p=X_train.shape[1],\n",
    "        sigma=None,\n",
    "        data=\"\",\n",
    "        standardize=False,\n",
    "        test_shift=None,\n",
    "        model=\"gaussian\",\n",
    "        H=16,\n",
    "        L=1,\n",
    "        config=\"Latent\",\n",
    "        seed=1,\n",
    "        data_config=\"realworld\",\n",
    "        model_output_dir=\"results/ridgeless/alignment/priors/gaussian_tanh\",\n",
    "        burnin_samples=1000,\n",
    "        samples=1000,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing why allocating larger ξ to larger |β| reduces the negative log-posterior\n",
    "# Focus on p=2 (so ξ2 = 1 − ξ1). Includes:\n",
    "# 1) Objective vs ξ1 for α ∈ {0.5, 1.5, 3.0} and several (β1, β2) pairs.\n",
    "# 2) Marginal gain plot: β1^2/ξ1^2 − β2^2/(1−ξ1)^2 (sign tells which way to move mass).\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ξ-part of the negative log-posterior for p=2 at fixed betas and α\n",
    "def nll_xi1(xi1, beta1, beta2, tau=1.0, alpha1=1.5, alpha2=1.5, eps=1e-12):\n",
    "    # keep ξ in (0,1) to avoid log/division issues\n",
    "    xi1 = np.clip(xi1, eps, 1 - eps)\n",
    "    xi2 = 1.0 - xi1\n",
    "    # Prior from β|ξ ~ N(0, τ^2 diag(ξ)):\n",
    "    quad = 0.5 * (beta1**2 / xi1 + beta2**2 / xi2) / (tau**2)\n",
    "    normal_norm = 0.5 * (np.log(xi1) + np.log(xi2))  # normalization term from Gaussian prior\n",
    "    # Dirichlet prior on ξ ⇒ (3/2 − α_i) log ξ_i in the NLL\n",
    "    dirichlet_term = (1.5 - alpha1) * np.log(xi1) + (1.5 - alpha2) * np.log(xi2)\n",
    "    return quad + normal_norm + dirichlet_term  # up to constants w.r.t. ξ\n",
    "\n",
    "# Config\n",
    "beta_pairs = [(10.0, 1.0)]#, (1.0, 1.0), (1.0, 2.0)]\n",
    "alphas = [0.1]\n",
    "tau = 1.0\n",
    "xi1_grid = np.linspace(1e-4, 1 - 1e-4, 2000)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for b1, b2 in beta_pairs:\n",
    "    xi = xi1_grid\n",
    "    marg_gain = (b1**2)/(xi**2) - (b2**2)/((1 - xi)**2)\n",
    "    marg_gain = np.clip(marg_gain, -1e3, 1e3)  # keep plot readable near boundaries\n",
    "    plt.plot(xi, marg_gain, label=f\"|β|=({abs(b1):.1f},{abs(b2):.1f})\")\n",
    "    x_star = abs(b1) / (abs(b1) + abs(b2))\n",
    "    plt.axhline(0.0, linestyle=\"-\", linewidth=1)\n",
    "    plt.axvline(x_star, linestyle=\"--\", alpha=0.8)\n",
    "    plt.axvline(0.5, linestyle=\":\", alpha=0.8)\n",
    "\n",
    "# plt.title(\"Marginal gain of shifting mass to ξ₁ (no Dirichlet term)\\n>0 ⇒ shift to ξ₁; <0 ⇒ shift to ξ₂\")\n",
    "# plt.xlabel(\"ξ₁   (with ξ₂ = 1 − ξ₁)\")\n",
    "# plt.ylabel(\"β₁²/ξ₁² − β₂²/(1−ξ₁)²\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_1 = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "xi_2 = np.array([0.75, 0.2, 0.03, 0.02])\n",
    "xi_3 = np.array([0.02, 0.03, 0.2, 0.75])\n",
    "\n",
    "beta = np.array([5, 2, 1, 0.25])\n",
    "\n",
    "print(\"First: \", np.sum((beta**2)/(xi_1)) + np.sum(1.4*np.log(xi_1)), \"\\n\")\n",
    "print(\"Second: \", np.sum((beta**2)/(xi_2)) + np.sum(1.4*np.log(xi_2)), \"\\n\")\n",
    "print(\"Third: \", np.sum((beta**2)/(xi_3)) + np.sum(1.4*np.log(xi_3)), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dirichlet–Gaussian objective on the 2-simplex (p=3), up to constants\n",
    "def F(xi1, xi2, beta, tau=1.0, alpha=1.2):  # use alpha != 1.5 to show Dirichlet effect\n",
    "    xi3 = 1.0 - xi1 - xi2\n",
    "    if xi3 <= 0: \n",
    "        return np.nan\n",
    "    quad = 0.5 * np.sum((np.array(beta)**2) / (tau**2 * np.array([xi1, xi2, xi3])))\n",
    "    logterm = (1.5 - alpha) * np.sum(np.log([xi1, xi2, xi3]))\n",
    "    return quad + logterm\n",
    "\n",
    "# grid over simplex\n",
    "n = 500\n",
    "x = np.linspace(1e-4, 1 - 1e-4, n)\n",
    "xi1, xi2 = np.meshgrid(x, x)\n",
    "mask = (xi1 + xi2 < 1.0)\n",
    "\n",
    "beta = [1.0, 1.0, 1.0]      # squared coefficients drive where the minimum is\n",
    "alpha = 0.5                # < 1.5 encourages concentration (stronger coupling)\n",
    "tau   = 1.0\n",
    "\n",
    "Z = np.full_like(xi1, np.nan, dtype=float)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if mask[i, j]:\n",
    "            Z[i, j] = F(xi1[i, j], xi2[i, j], beta, tau=tau, alpha=alpha)\n",
    "\n",
    "# find argmin (on grid)\n",
    "k = np.nanargmin(Z)\n",
    "i_min, j_min = np.unravel_index(k, Z.shape)\n",
    "x1_min, x2_min = float(xi1[i_min, j_min]), float(xi2[i_min, j_min])\n",
    "\n",
    "# color-scale fix: clip extremes so interior structure is visible\n",
    "vmin = np.nanpercentile(Z, 1)\n",
    "vmax = np.nanpercentile(Z, 95)  # ignore huge boundary spikes\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "im = plt.imshow(Z, origin='lower', extent=[0,1,0,1], cmap='viridis',\n",
    "                vmin=vmin, vmax=vmax, aspect='equal')\n",
    "plt.colorbar(im, label=\"Negative log-prior  F(ξ)  (clipped)\")\n",
    "\n",
    "# simplex boundary\n",
    "t = np.linspace(0,1,400)\n",
    "plt.plot(t, 1-t, 'w--', lw=1)\n",
    "\n",
    "# mark argmin\n",
    "plt.scatter([x1_min], [x2_min], c='red', s=40, marker='*', zorder=3, label='argmin')\n",
    "\n",
    "plt.title(f\"Dirichlet–Gaussian over simplex\\nβ={tuple(beta)},  α={alpha}\")\n",
    "plt.xlabel(\"ξ₁\"); plt.ylabel(\"ξ₂\")\n",
    "plt.xlim(0,1); plt.ylim(0,1)\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
